From e0c7933b870447924483a40612be9a3edf2d7931 Mon Sep 17 00:00:00 2001
From: Bingwu Zhang <xtex@aosc.io>
Date: Fri, 7 Feb 2025 09:10:19 +0800
Subject: [PATCH 4/4] skia: backport & cherry-pick fixes for LoongArch64 LSX
 vectorization

This changeset backports/cherry-picks the following changes from Skia:
- Ibe6082c8e6cf1e4f9e1976caa8967644dd1eacdf
- I5751928376acbe298d923b465cc9303bd5bac2bb
- Iacc9feef956830b53559d1f7568b292ba6023690
- I1e6183b987002ed09621b111a6012a0f186233f1

This should address compiler errors when building LibreOffice for
LoongArch64 targets with LSX enabled and LASX disabled.

Tested-by: Bingwu Zhang <xtex@aosc.io>
Link: https://skia-review.googlesource.com/c/skia/+/948016
Link: https://skia-review.googlesource.com/c/skia/+/948616
Link: https://skia-review.googlesource.com/c/skia/+/909436
Link: https://skia-review.googlesource.com/c/skia/+/948976
Change-Id: I87fa242c44296b222a8c70b19169b06d62f566b3
Signed-off-by: Bingwu Zhang <xtex@aosc.io>
---
 ...ly-cast-vector-types-around-LSX-intr.patch | 162 +++++++++
 ...-bugs-in-type-conversion-on-Loongarc.patch | 117 ++++++
 ...tor-types-around-LSX-intrinsics-in-S.patch | 343 ++++++++++++++++++
 ...g64-Honor-existing-LASX-LSX-settings.patch |  35 ++
 external/skia/UnpackedTarball_skia.mk         |   4 +
 5 files changed, 661 insertions(+)
 create mode 100644 external/skia/0001-loong64-Explicitly-cast-vector-types-around-LSX-intr.patch
 create mode 100644 external/skia/0002-loong64-fix-some-bugs-in-type-conversion-on-Loongarc.patch
 create mode 100644 external/skia/0003-loong64-Cast-vector-types-around-LSX-intrinsics-in-S.patch
 create mode 100644 external/skia/0004-loong64-Honor-existing-LASX-LSX-settings.patch

diff --git a/external/skia/0001-loong64-Explicitly-cast-vector-types-around-LSX-intr.patch b/external/skia/0001-loong64-Explicitly-cast-vector-types-around-LSX-intr.patch
new file mode 100644
index 000000000000..a36b902addeb
--- /dev/null
+++ b/external/skia/0001-loong64-Explicitly-cast-vector-types-around-LSX-intr.patch
@@ -0,0 +1,162 @@
+From 0a56f52282ede1404a509e5b6d16365fd2e9b5b3 Mon Sep 17 00:00:00 2001
+From: Bingwu Zhang <xtex@aosc.io>
+Date: Thu, 6 Feb 2025 23:34:34 +0800
+Subject: [PATCH 1/4] [loong64] Explicitly cast vector types around LSX
+ intrinsics
+X-Developer-Signature: v=1; a=openpgp-sha256; l=6961; i=xtex@aosc.io;
+ h=from:subject; bh=LP0lBN41umXLY0sL3K4hnjMP/cIcIGqkNfo2OxLxbSI=;
+ b=owGbwMvMwCW2U4Ij7wZL9ETG02pJDOnLZbKzf64LXvyd7dHZfsVnmbse9J9LXi5xceI2J5k+3
+ tr831ozO0pZGMS4GGTFFFmKDBu8WXXS+UWXlcvCzGFlAhnCwMUpABPZx8XwPy7U8OIp/tdf53Xm
+ KK2W9Ahm2GV1Q0U478e/xW9Xve8zes/IsI67h3vzo1srF7mc3aXJZNLu4l70gcdE9Fal+s+J4qf
+ VOQE=
+X-Developer-Key: i=xtex@aosc.io; a=openpgp;
+ fpr=7231804B052C670F15A6771DB918086ED8045B91
+
+In I5ec3ae47f69d520218619721bed5a48eb87da194
+("[loong64] Optimize gauss blur imagefilter."), LSX vectorization
+support was added to the gauss blur imagefilter.
+Some code added in that change heavily depended on the compiler's
+implicit conversions between vectors with differing element count and
+types.
+
+GCC provides a '-flax-vector-conversions' option to toggle the implicit
+conversion behavior, saying that it "should not be used for new code".
+On some (most?) systems, that option is disabled by default, leading to
+compilation errors, because all LSX intrinsics functions from
+lsxintrin.h expect __m128i arguments and return __m128i values.
+
+This CL wraps vector intrinsics invocations with explicit type
+conversions, to avoid implicit conversion errors.
+Replacing v4u32 variables with __m128i is not preferred because v4u32 do
+better at reflecting the number and type of elements of vectors, while
+__m128i only shows the bit size.
+
+Fixes: I5ec3ae47f69d520218619721bed5a48eb87da194
+Change-Id: Ibe6082c8e6cf1e4f9e1976caa8967644dd1eacdf
+Signed-off-by: Bingwu Zhang <xtex@aosc.io>
+---
+ AUTHORS                   |  1 +
+ src/core/SkBlurEngine.cpp | 53 ++++++++++++++++++++-------------------
+ 2 files changed, 28 insertions(+), 26 deletions(-)
+
+diff --git a/AUTHORS b/AUTHORS
+index f533d9a8e5bf..03d0ac82a412 100755
+--- a/AUTHORS
++++ b/AUTHORS
+@@ -24,6 +24,7 @@ Amazon, Inc <*@amazon.com>
+ Anthony Catel <paraboul@gmail.com>
+ Andrew Kurushin <ajax16384@gmail.com>
+ Bharat Ahuja <ahujabharat93@gmail.com>
++Bingwu Zhang <xtex@aosc.io>
+ Biswapriyo Nath <nathbappai@gmail.com>
+ Brian Salomon <briansalomon@gmail.com>
+ Callum Moffat <smartercallum@gmail.com>
+diff --git a/src/core/SkBlurEngine.cpp b/src/core/SkBlurEngine.cpp
+index 2c18eb0fb2e7..064937ba861a 100644
+--- a/src/core/SkBlurEngine.cpp
++++ b/src/core/SkBlurEngine.cpp
+@@ -330,62 +330,63 @@ private:
+         skvx::Vec<4, uint32_t>* buffer0Cursor = fBuffer0Cursor;
+         skvx::Vec<4, uint32_t>* buffer1Cursor = fBuffer1Cursor;
+         skvx::Vec<4, uint32_t>* buffer2Cursor = fBuffer2Cursor;
+-        v4u32 sum0 = __lsx_vld(fSum0, 0); // same as skvx::Vec<4, uint32_t>::Load(fSum0);
+-        v4u32 sum1 = __lsx_vld(fSum1, 0);
+-        v4u32 sum2 = __lsx_vld(fSum2, 0);
++        v4u32 sum0 = (v4u32)__lsx_vld(fSum0, 0);  // same as skvx::Vec<4, uint32_t>::Load(fSum0);
++        v4u32 sum1 = (v4u32)__lsx_vld(fSum1, 0);
++        v4u32 sum2 = (v4u32)__lsx_vld(fSum2, 0);
+ 
+         auto processValue = [&](v4u32& vLeadingEdge){
+           sum0 += vLeadingEdge;
+           sum1 += sum0;
+           sum2 += sum1;
+ 
+-          v4u32 divisorFactor = __lsx_vreplgr2vr_w(fDivider.divisorFactor());
+-          v4u32 blurred = __lsx_vmuh_w(divisorFactor, sum2);
++          v4u32 divisorFactor = (v4u32)__lsx_vreplgr2vr_w(fDivider.divisorFactor());
++          v4u32 blurred = (v4u32)__lsx_vmuh_w((__m128i)divisorFactor, (__m128i)sum2);
+ 
+-          v4u32 buffer2Value = __lsx_vld(buffer2Cursor, 0); //Not fBuffer0Cursor, out of bounds.
++          v4u32 buffer2Value =
++                  (v4u32)__lsx_vld(buffer2Cursor, 0);  // Not fBuffer0Cursor, out of bounds.
+           sum2 -= buffer2Value;
+-          __lsx_vst(sum1, (void *)buffer2Cursor, 0);
++          __lsx_vst((__m128i)sum1, (void*)buffer2Cursor, 0);
+           buffer2Cursor = (buffer2Cursor + 1) < fBuffersEnd ? buffer2Cursor + 1 : fBuffer2;
+-          v4u32 buffer1Value = __lsx_vld(buffer1Cursor, 0);
++          v4u32 buffer1Value = (v4u32)__lsx_vld(buffer1Cursor, 0);
+           sum1 -= buffer1Value;
+-          __lsx_vst(sum0, (void *)buffer1Cursor, 0);
++          __lsx_vst((__m128i)sum0, (void*)buffer1Cursor, 0);
+           buffer1Cursor = (buffer1Cursor + 1) < fBuffer2 ? buffer1Cursor + 1 : fBuffer1;
+-          v4u32 buffer0Value = __lsx_vld(buffer0Cursor, 0);
++          v4u32 buffer0Value = (v4u32)__lsx_vld(buffer0Cursor, 0);
+           sum0 -= buffer0Value;
+-          __lsx_vst(vLeadingEdge, (void *)buffer0Cursor, 0);
++          __lsx_vst((__m128i)vLeadingEdge, (void*)buffer0Cursor, 0);
+           buffer0Cursor = (buffer0Cursor + 1) < fBuffer1 ? buffer0Cursor + 1 : fBuffer0;
+ 
+           v16u8 shuf = {0x0,0x4,0x8,0xc,0x0};
+-          v16u8 ret = __lsx_vshuf_b(blurred, blurred, shuf);
++          v16u8 ret = (v16u8)__lsx_vshuf_b((__m128i)blurred, (__m128i)blurred, (__m128i)shuf);
+           return ret;
+         };
+ 
+-        v4u32 zero = __lsx_vldi(0x0);
++        v4u32 zero = (v4u32)__lsx_vldi(0x0);
+         if (!src && !dst) {
+             while (n --> 0) {
+                 (void)processValue(zero);
+             }
+         } else if (src && !dst) {
+             while (n --> 0) {
+-                v4u32 edge = __lsx_vinsgr2vr_w(zero, *src, 0);
+-                edge = __lsx_vilvl_b(zero, edge);
+-                edge = __lsx_vilvl_h(zero, edge);
++                v4u32 edge = (v4u32)__lsx_vinsgr2vr_w((__m128i)zero, *src, 0);
++                edge = (v4u32)__lsx_vilvl_b((__m128i)zero, (__m128i)edge);
++                edge = (v4u32)__lsx_vilvl_h((__m128i)zero, (__m128i)edge);
+                 (void)processValue(edge);
+                 src += srcStride;
+             }
+         } else if (!src && dst) {
+             while (n --> 0) {
+-                v4u32 ret = processValue(zero);
+-                __lsx_vstelm_w(ret, dst, 0, 0); // 3rd is offset, 4th is idx.
++                v4u32 ret = (v4u32)processValue(zero);
++                __lsx_vstelm_w((__m128i)ret, dst, 0, 0); // 3rd is offset, 4th is idx.
+                 dst += dstStride;
+             }
+         } else if (src && dst) {
+             while (n --> 0) {
+-                v4u32 edge = __lsx_vinsgr2vr_w(zero, *src, 0);
+-                edge = __lsx_vilvl_b(zero, edge);
+-                edge = __lsx_vilvl_h(zero, edge);
+-                v4u32 ret = processValue(edge);
+-                __lsx_vstelm_w(ret, dst, 0, 0);
++                v4u32 edge = (v4u32)__lsx_vinsgr2vr_w((__m128i)zero, *src, 0);
++                edge = (v4u32)__lsx_vilvl_b((__m128i)zero, (__m128i)edge);
++                edge = (v4u32)__lsx_vilvl_h((__m128i)zero, (__m128i)edge);
++                v4u32 ret = (v4u32)processValue(edge);
++                __lsx_vstelm_w((__m128i)ret, dst, 0, 0);
+                 src += srcStride;
+                 dst += dstStride;
+             }
+@@ -396,9 +397,9 @@ private:
+         fBuffer1Cursor = buffer1Cursor;
+         fBuffer2Cursor = buffer2Cursor;
+ 
+-        __lsx_vst(sum0, fSum0, 0);
+-        __lsx_vst(sum1, fSum1, 0);
+-        __lsx_vst(sum2, fSum2, 0);
++        __lsx_vst((__m128i)sum0, fSum0, 0);
++        __lsx_vst((__m128i)sum1, fSum1, 0);
++        __lsx_vst((__m128i)sum2, fSum2, 0);
+ #else
+         skvx::Vec<4, uint32_t>* buffer0Cursor = fBuffer0Cursor;
+         skvx::Vec<4, uint32_t>* buffer1Cursor = fBuffer1Cursor;
+
+base-commit: b988efa06f8aa3bfeaa18c5b8c716ff244ab43cf
+-- 
+2.48.1
+
diff --git a/external/skia/0002-loong64-fix-some-bugs-in-type-conversion-on-Loongarc.patch b/external/skia/0002-loong64-fix-some-bugs-in-type-conversion-on-Loongarc.patch
new file mode 100644
index 000000000000..9ab86f84295c
--- /dev/null
+++ b/external/skia/0002-loong64-fix-some-bugs-in-type-conversion-on-Loongarc.patch
@@ -0,0 +1,117 @@
+From fd3d9055af45e692c24c875e0c77ba2eeecfc872 Mon Sep 17 00:00:00 2001
+From: Kai Zou <double1kai@gmail.com>
+Date: Fri, 18 Oct 2024 10:01:39 +0800
+Subject: [PATCH 2/4] [loong64] fix some bugs in type conversion on Loongarch
+X-Developer-Signature: v=1; a=openpgp-sha256; l=4144; i=xtex@aosc.io;
+ h=from:subject; bh=2HkcqJoD1Fc4uN+qqdBVMf2X3dsWGrG71D8JO1ZCf+M=;
+ b=owGbwMvMwCW2U4Ij7wZL9ETG02pJDOnLZbJ/TZfoffbi4rKfV9Q+ZEkp/I0V8L3OMT31485jv
+ wrmdVvs7yhlYRDjYpAVU2QpMmzwZtVJ5xddVi4LM4eVCWQIAxenAEwkOIaR4TFPfVfz9Umbeebp
+ 6aev/Dunp+/gAREWsSDT2GMSa5a0iDH8D3qjcrriuvfMo6vsL6W7X42QVZZ/zf71a+CO97fqGq9
+ MZQQA
+X-Developer-Key: i=xtex@aosc.io; a=openpgp;
+ fpr=7231804B052C670F15A6771DB918086ED8045B91
+
+Backported of 7e8c7385e673 ("fix some bugs in type conversion on
+Loongarch").
+
+Change-Id: I5751928376acbe298d923b465cc9303bd5bac2bb
+Reviewed-on: https://skia-review.googlesource.com/c/skia/+/909436
+Reviewed-by: Greg Daniel <egdaniel@google.com>
+Reviewed-by: Ben Wagner <bungeman@google.com>
+Commit-Queue: Ben Wagner <bungeman@google.com>
+Signed-off-by: Bingwu Zhang <xtex@aosc.io>
+---
+ src/opts/SkRasterPipeline_opts.h | 30 +++++++++++++++---------------
+ 1 file changed, 15 insertions(+), 15 deletions(-)
+
+diff --git a/src/opts/SkRasterPipeline_opts.h b/src/opts/SkRasterPipeline_opts.h
+index e1783a589075..d12968d876b5 100644
+--- a/src/opts/SkRasterPipeline_opts.h
++++ b/src/opts/SkRasterPipeline_opts.h
+@@ -943,22 +943,22 @@ namespace SK_OPTS_NS {
+     SI F   abs_  (F v)          { return (F)__lasx_xvand_v((I32)v, (I32)(0-v));     }
+     SI I32 abs_(I32 v)          { return max(v, -v);                     }
+     SI F   rcp_approx(F v)      { return __lasx_xvfrecip_s(v);           }
+-    SI F   rcp_precise (F v)    { F e = rcp_approx(v); return e * nmad(v, e, 2.0f); }
++    SI F   rcp_precise (F v)    { F e = rcp_approx(v); return e * nmad(v, e, F() + 2.0f); }
+     SI F   rsqrt_approx (F v)   { return __lasx_xvfrsqrt_s(v);           }
+     SI F    sqrt_(F v)          { return __lasx_xvfsqrt_s(v);            }
+ 
+     SI U32 iround(F v) {
+-        F t = F(0.5);
++        F t = F() + 0.5f;
+         return __lasx_xvftintrz_w_s(v + t);
+     }
+ 
+     SI U32 round(F v) {
+-        F t = F(0.5);
++        F t = F() + 0.5f;
+         return __lasx_xvftintrz_w_s(v + t);
+     }
+ 
+     SI U32 round(F v, F scale) {
+-        F t = F(0.5);
++        F t = F() + 0.5f;
+         return __lasx_xvftintrz_w_s(mad(v, scale, t));
+     }
+ 
+@@ -993,8 +993,8 @@ namespace SK_OPTS_NS {
+ 
+     template <typename T>
+     SI V<T> gather(const T* p, U32 ix) {
+-        return { p[ix[0]], p[ix[1]], p[ix[2]], p[ix[3]],
+-                 p[ix[4]], p[ix[5]], p[ix[6]], p[ix[7]], };
++        return V<T>{ p[ix[0]], p[ix[1]], p[ix[2]], p[ix[3]],
++                     p[ix[4]], p[ix[5]], p[ix[6]], p[ix[7]], };
+     }
+ 
+     template <typename V, typename S>
+@@ -1147,20 +1147,20 @@ namespace SK_OPTS_NS {
+     SI F   abs_(F v)            { return (F)__lsx_vand_v((I32)v, (I32)(0-v));       }
+     SI I32 abs_(I32 v)          { return max(v, -v);                     }
+     SI F   rcp_approx (F v)     { return __lsx_vfrecip_s(v);             }
+-    SI F   rcp_precise (F v)    { F e = rcp_approx(v); return e * nmad(v, e, 2.0f); }
++    SI F   rcp_precise (F v)    { F e = rcp_approx(v); return e * nmad(v, e, F() + 2.0f); }
+     SI F   rsqrt_approx (F v)   { return __lsx_vfrsqrt_s(v);             }
+     SI F    sqrt_(F v)          { return __lsx_vfsqrt_s (v);             }
+ 
+     SI U32 iround(F v) {
+-        F t = F(0.5);
++        F t = F() + 0.5f;
+         return __lsx_vftintrz_w_s(v + t); }
+ 
+     SI U32 round(F v) {
+-        F t = F(0.5);
++        F t = F() + 0.5f;
+         return __lsx_vftintrz_w_s(v + t); }
+ 
+     SI U32 round(F v, F scale) {
+-        F t = F(0.5);
++        F t = F() + 0.5f;
+         return __lsx_vftintrz_w_s(mad(v, scale, t)); }
+ 
+     SI U16 pack(U32 v) {
+@@ -1196,15 +1196,15 @@ namespace SK_OPTS_NS {
+ 
+     template <typename T>
+     SI V<T> gather(const T* p, U32 ix) {
+-        return {p[ix[0]], p[ix[1]], p[ix[2]], p[ix[3]]};
++        return V<T>{p[ix[0]], p[ix[1]], p[ix[2]], p[ix[3]]};
+     }
+     // Using 'int*' prevents data from passing through floating-point registers.
+     SI F   gather(const int*    p, int ix0, int ix1, int ix2, int ix3) {
+        F ret = {0.0};
+-       ret = __lsx_vinsgr2vr_w(ret, p[ix0], 0);
+-       ret = __lsx_vinsgr2vr_w(ret, p[ix1], 1);
+-       ret = __lsx_vinsgr2vr_w(ret, p[ix2], 2);
+-       ret = __lsx_vinsgr2vr_w(ret, p[ix3], 3);
++       ret = (F)__lsx_vinsgr2vr_w(ret, p[ix0], 0);
++       ret = (F)__lsx_vinsgr2vr_w(ret, p[ix1], 1);
++       ret = (F)__lsx_vinsgr2vr_w(ret, p[ix2], 2);
++       ret = (F)__lsx_vinsgr2vr_w(ret, p[ix3], 3);
+        return ret;
+     }
+ 
+-- 
+2.48.1
+
diff --git a/external/skia/0003-loong64-Cast-vector-types-around-LSX-intrinsics-in-S.patch b/external/skia/0003-loong64-Cast-vector-types-around-LSX-intrinsics-in-S.patch
new file mode 100644
index 000000000000..965138db65a0
--- /dev/null
+++ b/external/skia/0003-loong64-Cast-vector-types-around-LSX-intrinsics-in-S.patch
@@ -0,0 +1,343 @@
+From 1b04927d7070229b26c39f1b0cbab94e453fe2a9 Mon Sep 17 00:00:00 2001
+From: Bingwu Zhang <xtex@aosc.io>
+Date: Fri, 7 Feb 2025 19:12:58 +0800
+Subject: [PATCH 3/4] [loong64] Cast vector types around LSX intrinsics in
+ SkRasterPipeline
+X-Developer-Signature: v=1; a=openpgp-sha256; l=15461; i=xtex@aosc.io;
+ h=from:subject; bh=iBzmKblF+IhukVjoDebnErffOhhSkYL51JXbELdrOqc=;
+ b=owGbwMvMwCW2U4Ij7wZL9ETG02pJDOnLZbIvGex8+5jNcwlbrc/MunzZC2+3R3zwrHs+1dB16
+ 2EztXfbO0pZGMS4GGTFFFmKDBu8WXXS+UWXlcvCzGFlAhnCwMUpABNxjGD4w/v81Tu576tlL16z
+ F+6+63B28VmDQDXfMzr7fBvnr08Mu8rIsHzy0v+KIpcE92kGLXL4uaP6hemire9WXUi62mfdGns
+ tkAEA
+X-Developer-Key: i=xtex@aosc.io; a=openpgp;
+ fpr=7231804B052C670F15A6771DB918086ED8045B91
+
+Like what has been done in Ibe6082c8e6cf1e4f9e1976caa8967644dd1eacdf,
+this CL adds explicit cast to arguments and returns of LSX intrinsics,
+in order to address compilation failures on systems where implicit
+vector type casting is disallowed by default.
+
+Fixes: Icead4afada8258dfcd658a638d47ac5e1a85d2c0
+Change-Id: Iacc9feef956830b53559d1f7568b292ba6023690
+Signed-off-by: Bingwu Zhang <xtex@aosc.io>
+---
+ src/opts/SkRasterPipeline_opts.h | 152 +++++++++++++++----------------
+ 1 file changed, 76 insertions(+), 76 deletions(-)
+
+diff --git a/src/opts/SkRasterPipeline_opts.h b/src/opts/SkRasterPipeline_opts.h
+index d12968d876b5..a0b0a3de8bd9 100644
+--- a/src/opts/SkRasterPipeline_opts.h
++++ b/src/opts/SkRasterPipeline_opts.h
+@@ -933,14 +933,14 @@ namespace SK_OPTS_NS {
+ 
+     SI F   min(F a, F b)        { return __lasx_xvfmin_s(a,b); }
+     SI F   max(F a, F b)        { return __lasx_xvfmax_s(a,b); }
+-    SI I32 min(I32 a, I32 b)    { return __lasx_xvmin_w(a,b);  }
+-    SI U32 min(U32 a, U32 b)    { return __lasx_xvmin_wu(a,b); }
+-    SI I32 max(I32 a, I32 b)    { return __lasx_xvmax_w(a,b);  }
+-    SI U32 max(U32 a, U32 b)    { return __lasx_xvmax_wu(a,b); }
++    SI I32 min(I32 a, I32 b)    { return (I32)__lasx_xvmin_w((__m256i)a, (__m256i)b);  }
++    SI U32 min(U32 a, U32 b)    { return (U32)__lasx_xvmin_wu((__m256i)a, (__m256i)b); }
++    SI I32 max(I32 a, I32 b)    { return (I32)__lasx_xvmax_w((__m256i)a, (__m256i)b);  }
++    SI U32 max(U32 a, U32 b)    { return (U32)__lasx_xvmax_wu((__m256i)a, (__m256i)b); }
+ 
+     SI F   mad(F f, F m, F a)   { return __lasx_xvfmadd_s(f, m, a);      }
+     SI F   nmad(F f, F m, F a)  { return __lasx_xvfmadd_s(-f, m, a);    }
+-    SI F   abs_  (F v)          { return (F)__lasx_xvand_v((I32)v, (I32)(0-v));     }
++    SI F   abs_  (F v)          { return (F)__lasx_xvand_v((__m256i)(I32)v, (__m256i)(I32)(0-v));     }
+     SI I32 abs_(I32 v)          { return max(v, -v);                     }
+     SI F   rcp_approx(F v)      { return __lasx_xvfrecip_s(v);           }
+     SI F   rcp_precise (F v)    { F e = rcp_approx(v); return e * nmad(v, e, F() + 2.0f); }
+@@ -949,22 +949,22 @@ namespace SK_OPTS_NS {
+ 
+     SI U32 iround(F v) {
+         F t = F() + 0.5f;
+-        return __lasx_xvftintrz_w_s(v + t);
++        return (U32)__lasx_xvftintrz_w_s((__m256)(v + t));
+     }
+ 
+     SI U32 round(F v) {
+         F t = F() + 0.5f;
+-        return __lasx_xvftintrz_w_s(v + t);
++        return (U32)__lasx_xvftintrz_w_s((__m256)(v + t));
+     }
+ 
+     SI U32 round(F v, F scale) {
+         F t = F() + 0.5f;
+-        return __lasx_xvftintrz_w_s(mad(v, scale, t));
++        return (U32)__lasx_xvftintrz_w_s(mad(v, scale, t));
+     }
+ 
+     SI U16 pack(U32 v) {
+-        return __lsx_vpickev_h(__lsx_vsat_wu(emulate_lasx_d_xr2vr_h(v), 15),
+-                               __lsx_vsat_wu(emulate_lasx_d_xr2vr_l(v), 15));
++        return (U16)__lsx_vpickev_h(__lsx_vsat_wu(emulate_lasx_d_xr2vr_h((__m256i)v), 15),
++                               __lsx_vsat_wu(emulate_lasx_d_xr2vr_l((__m256i)v), 15));
+     }
+ 
+     SI U8 pack(U16 v) {
+@@ -974,12 +974,12 @@ namespace SK_OPTS_NS {
+     }
+ 
+     SI bool any(I32 c){
+-        v8i32 retv = (v8i32)__lasx_xvmskltz_w(__lasx_xvslt_wu(__lasx_xvldi(0), c));
++        v8i32 retv = (v8i32)__lasx_xvmskltz_w(__lasx_xvslt_wu(__lasx_xvldi(0), (__m256i)c));
+         return (retv[0] | retv[4]) != 0b0000;
+     }
+ 
+     SI bool all(I32 c){
+-        v8i32 retv = (v8i32)__lasx_xvmskltz_w(__lasx_xvslt_wu(__lasx_xvldi(0), c));
++        v8i32 retv = (v8i32)__lasx_xvmskltz_w(__lasx_xvslt_wu(__lasx_xvldi(0), (__m256i)c));
+         return (retv[0] & retv[4]) == 0b1111;
+     }
+ 
+@@ -1012,16 +1012,16 @@ namespace SK_OPTS_NS {
+     }
+ 
+     SI void load2(const uint16_t* ptr, U16* r, U16* g) {
+-        U16 _0123 = __lsx_vld(ptr, 0),
+-            _4567 = __lsx_vld(ptr, 16);
+-        *r = __lsx_vpickev_h(__lsx_vsat_w(__lsx_vsrai_w(__lsx_vslli_w(_4567, 16), 16), 15),
++        U16 _0123 = (U16)__lsx_vld(ptr, 0),
++            _4567 = (U16)__lsx_vld(ptr, 16);
++        *r = (U16)__lsx_vpickev_h(__lsx_vsat_w(__lsx_vsrai_w(__lsx_vslli_w(_4567, 16), 16), 15),
+                              __lsx_vsat_w(__lsx_vsrai_w(__lsx_vslli_w(_0123, 16), 16), 15));
+-        *g = __lsx_vpickev_h(__lsx_vsat_w(__lsx_vsrai_w(_4567, 16), 15),
++        *g = (U16)__lsx_vpickev_h(__lsx_vsat_w(__lsx_vsrai_w(_4567, 16), 15),
+                              __lsx_vsat_w(__lsx_vsrai_w(_0123, 16), 15));
+     }
+     SI void store2(uint16_t* ptr, U16 r, U16 g) {
+-        auto _0123 = __lsx_vilvl_h(g, r),
+-             _4567 = __lsx_vilvh_h(g, r);
++        auto _0123 = __lsx_vilvl_h((__m128i)g, (__m128i)r),
++             _4567 = __lsx_vilvh_h((__m128i)g, (__m128i)r);
+         __lsx_vst(_0123, ptr, 0);
+         __lsx_vst(_4567, ptr, 16);
+     }
+@@ -1042,17 +1042,17 @@ namespace SK_OPTS_NS {
+              rg4567 = __lsx_vilvl_h(_57, _46),
+              ba4567 = __lsx_vilvh_h(_57, _46);
+ 
+-        *r = __lsx_vilvl_d(rg4567, rg0123);
+-        *g = __lsx_vilvh_d(rg4567, rg0123);
+-        *b = __lsx_vilvl_d(ba4567, ba0123);
+-        *a = __lsx_vilvh_d(ba4567, ba0123);
++        *r = (U16)__lsx_vilvl_d(rg4567, rg0123);
++        *g = (U16)__lsx_vilvh_d(rg4567, rg0123);
++        *b = (U16)__lsx_vilvl_d(ba4567, ba0123);
++        *a = (U16)__lsx_vilvh_d(ba4567, ba0123);
+     }
+ 
+     SI void store4(uint16_t* ptr, U16 r, U16 g, U16 b, U16 a) {
+-        auto rg0123 = __lsx_vilvl_h(g, r),      // r0 g0 r1 g1 r2 g2 r3 g3
+-             rg4567 = __lsx_vilvh_h(g, r),      // r4 g4 r5 g5 r6 g6 r7 g7
+-             ba0123 = __lsx_vilvl_h(a, b),
+-             ba4567 = __lsx_vilvh_h(a, b);
++        auto rg0123 = __lsx_vilvl_h((__m128i)g, (__m128i)r),      // r0 g0 r1 g1 r2 g2 r3 g3
++             rg4567 = __lsx_vilvh_h((__m128i)g, (__m128i)r),      // r4 g4 r5 g5 r6 g6 r7 g7
++             ba0123 = __lsx_vilvl_h((__m128i)a, (__m128i)b),
++             ba4567 = __lsx_vilvh_h((__m128i)a, (__m128i)b);
+ 
+         auto _01 =__lsx_vilvl_w(ba0123, rg0123),
+              _23 =__lsx_vilvh_w(ba0123, rg0123),
+@@ -1135,16 +1135,16 @@ namespace SK_OPTS_NS {
+                                                            sk_bit_cast<__m128i>(c)));
+     }
+ 
+-    SI F   min(F a, F b)        { return __lsx_vfmin_s(a,b);     }
+-    SI F   max(F a, F b)        { return __lsx_vfmax_s(a,b);     }
+-    SI I32 min(I32 a, I32 b)    { return __lsx_vmin_w(a,b);      }
+-    SI U32 min(U32 a, U32 b)    { return __lsx_vmin_wu(a,b);     }
+-    SI I32 max(I32 a, I32 b)    { return __lsx_vmax_w(a,b);      }
+-    SI U32 max(U32 a, U32 b)    { return __lsx_vmax_wu(a,b);     }
++    SI F   min(F a, F b)        { return __lsx_vfmin_s(a, b);     }
++    SI F   max(F a, F b)        { return __lsx_vfmax_s(a, b);     }
++    SI I32 min(I32 a, I32 b)    { return (I32)__lsx_vmin_w((__m128i)a, (__m128i)b);      }
++    SI U32 min(U32 a, U32 b)    { return (U32)__lsx_vmin_wu((__m128i)a, (__m128i)b);     }
++    SI I32 max(I32 a, I32 b)    { return (I32)__lsx_vmax_w((__m128i)a, (__m128i)b);      }
++    SI U32 max(U32 a, U32 b)    { return (U32)__lsx_vmax_wu((__m128i)a, (__m128i)b);     }
+ 
+     SI F   mad(F f, F m, F a)   { return __lsx_vfmadd_s(f, m, a);        }
+     SI F   nmad(F f, F m, F a)  { return __lsx_vfmadd_s(-f, m, a);      }
+-    SI F   abs_(F v)            { return (F)__lsx_vand_v((I32)v, (I32)(0-v));       }
++    SI F   abs_(F v)            { return (F)__lsx_vand_v((__m128i)(I32)v, (__m128i)(I32)(0-v));       }
+     SI I32 abs_(I32 v)          { return max(v, -v);                     }
+     SI F   rcp_approx (F v)     { return __lsx_vfrecip_s(v);             }
+     SI F   rcp_precise (F v)    { F e = rcp_approx(v); return e * nmad(v, e, F() + 2.0f); }
+@@ -1153,15 +1153,15 @@ namespace SK_OPTS_NS {
+ 
+     SI U32 iround(F v) {
+         F t = F() + 0.5f;
+-        return __lsx_vftintrz_w_s(v + t); }
++        return (U32)__lsx_vftintrz_w_s(v + t); }
+ 
+     SI U32 round(F v) {
+         F t = F() + 0.5f;
+-        return __lsx_vftintrz_w_s(v + t); }
++        return (U32)__lsx_vftintrz_w_s(v + t); }
+ 
+     SI U32 round(F v, F scale) {
+         F t = F() + 0.5f;
+-        return __lsx_vftintrz_w_s(mad(v, scale, t)); }
++        return (U32)__lsx_vftintrz_w_s(mad(v, scale, t)); }
+ 
+     SI U16 pack(U32 v) {
+         __m128i tmp = __lsx_vsat_wu(v, 15);
+@@ -1177,12 +1177,12 @@ namespace SK_OPTS_NS {
+     }
+ 
+     SI bool any(I32 c){
+-        v4i32 retv = (v4i32)__lsx_vmskltz_w(__lsx_vslt_wu(__lsx_vldi(0), c));
++        v4i32 retv = (v4i32)__lsx_vmskltz_w(__lsx_vslt_wu(__lsx_vldi(0), (__m128i)c));
+         return retv[0] != 0b0000;
+     }
+ 
+     SI bool all(I32 c){
+-        v4i32 retv = (v4i32)__lsx_vmskltz_w(__lsx_vslt_wu(__lsx_vldi(0), c));
++        v4i32 retv = (v4i32)__lsx_vmskltz_w(__lsx_vslt_wu(__lsx_vldi(0), (__m128i)c));
+         return retv[0] == 0b1111;
+     }
+ 
+@@ -1229,7 +1229,7 @@ namespace SK_OPTS_NS {
+     }
+ 
+     SI void store2(uint16_t* ptr, U16 r, U16 g) {
+-        U32 rg = __lsx_vilvl_h(widen_cast<__m128i>(g), widen_cast<__m128i>(r));
++        U32 rg = (U32)__lsx_vilvl_h(widen_cast<__m128i>(g), widen_cast<__m128i>(r));
+         __lsx_vst(rg, ptr, 0);
+     }
+ 
+@@ -3402,26 +3402,26 @@ SI void gradient_lookup(const SkRasterPipeline_GradientCtx* c, U32 idx, F t,
+     } else
+ #elif defined(SKRP_CPU_LASX)
+     if (c->stopCount <= 8) {
+-        fr = (__m256)__lasx_xvperm_w(__lasx_xvld(c->fs[0], 0), idx);
+-        br = (__m256)__lasx_xvperm_w(__lasx_xvld(c->bs[0], 0), idx);
+-        fg = (__m256)__lasx_xvperm_w(__lasx_xvld(c->fs[1], 0), idx);
+-        bg = (__m256)__lasx_xvperm_w(__lasx_xvld(c->bs[1], 0), idx);
+-        fb = (__m256)__lasx_xvperm_w(__lasx_xvld(c->fs[2], 0), idx);
+-        bb = (__m256)__lasx_xvperm_w(__lasx_xvld(c->bs[2], 0), idx);
+-        fa = (__m256)__lasx_xvperm_w(__lasx_xvld(c->fs[3], 0), idx);
+-        ba = (__m256)__lasx_xvperm_w(__lasx_xvld(c->bs[3], 0), idx);
++        fr = (__m256)__lasx_xvperm_w(__lasx_xvld(c->fs[0], 0), (__m256i)idx);
++        br = (__m256)__lasx_xvperm_w(__lasx_xvld(c->bs[0], 0), (__m256i)idx);
++        fg = (__m256)__lasx_xvperm_w(__lasx_xvld(c->fs[1], 0), (__m256i)idx);
++        bg = (__m256)__lasx_xvperm_w(__lasx_xvld(c->bs[1], 0), (__m256i)idx);
++        fb = (__m256)__lasx_xvperm_w(__lasx_xvld(c->fs[2], 0), (__m256i)idx);
++        bb = (__m256)__lasx_xvperm_w(__lasx_xvld(c->bs[2], 0), (__m256i)idx);
++        fa = (__m256)__lasx_xvperm_w(__lasx_xvld(c->fs[3], 0), (__m256i)idx);
++        ba = (__m256)__lasx_xvperm_w(__lasx_xvld(c->bs[3], 0), (__m256i)idx);
+     } else
+ #elif defined(SKRP_CPU_LSX)
+     if (c->stopCount <= 4) {
+         __m128i zero = __lsx_vldi(0);
+-        fr = (__m128)__lsx_vshuf_w(idx, zero, __lsx_vld(c->fs[0], 0));
+-        br = (__m128)__lsx_vshuf_w(idx, zero, __lsx_vld(c->bs[0], 0));
+-        fg = (__m128)__lsx_vshuf_w(idx, zero, __lsx_vld(c->fs[1], 0));
+-        bg = (__m128)__lsx_vshuf_w(idx, zero, __lsx_vld(c->bs[1], 0));
+-        fb = (__m128)__lsx_vshuf_w(idx, zero, __lsx_vld(c->fs[2], 0));
+-        bb = (__m128)__lsx_vshuf_w(idx, zero, __lsx_vld(c->bs[2], 0));
+-        fa = (__m128)__lsx_vshuf_w(idx, zero, __lsx_vld(c->fs[3], 0));
+-        ba = (__m128)__lsx_vshuf_w(idx, zero, __lsx_vld(c->bs[3], 0));
++        fr = (__m128)__lsx_vshuf_w((__m128i)idx, zero, __lsx_vld(c->fs[0], 0));
++        br = (__m128)__lsx_vshuf_w((__m128i)idx, zero, __lsx_vld(c->bs[0], 0));
++        fg = (__m128)__lsx_vshuf_w((__m128i)idx, zero, __lsx_vld(c->fs[1], 0));
++        bg = (__m128)__lsx_vshuf_w((__m128i)idx, zero, __lsx_vld(c->bs[1], 0));
++        fb = (__m128)__lsx_vshuf_w((__m128i)idx, zero, __lsx_vld(c->fs[2], 0));
++        bb = (__m128)__lsx_vshuf_w((__m128i)idx, zero, __lsx_vld(c->bs[2], 0));
++        fa = (__m128)__lsx_vshuf_w((__m128i)idx, zero, __lsx_vld(c->fs[3], 0));
++        ba = (__m128)__lsx_vshuf_w((__m128i)idx, zero, __lsx_vld(c->bs[3], 0));
+     } else
+ #endif
+     {
+@@ -5543,11 +5543,11 @@ SI I16 scaled_mult(I16 a, I16 b) {
+ #elif defined(SKRP_CPU_NEON)
+     return vqrdmulhq_s16(a, b);
+ #elif defined(SKRP_CPU_LASX)
+-    I16 res = __lasx_xvmuh_h(a, b);
+-    return __lasx_xvslli_h(res, 1);
++    I16 res = (I16)__lasx_xvmuh_h((__m256i)a, (__m256i)b);
++    return (I16)__lasx_xvslli_h((__m256i)res, 1);
+ #elif defined(SKRP_CPU_LSX)
+-    I16 res = __lsx_vmuh_h(a, b);
+-    return __lsx_vslli_h(res, 1);
++    I16 res = (I16)__lsx_vmuh_h((__m128i)a, (__m128i)b);
++    return (I16)__lsx_vslli_h(res, 1);
+ #else
+     const I32 roundingTerm = I32_(1 << 14);
+     return cast<I16>((cast<I32>(a) * cast<I32>(b) + roundingTerm) >> 15);
+@@ -5934,7 +5934,7 @@ SI void from_8888(U32 rgba, U16* r, U16* g, U16* b, U16* a) {
+         split(v, &_02,&_13);
+         __m256i tmp0 = __lasx_xvsat_wu(_02, 15);
+         __m256i tmp1 = __lasx_xvsat_wu(_13, 15);
+-        return __lasx_xvpickev_h(tmp1, tmp0);
++        return (U16)__lasx_xvpickev_h(tmp1, tmp0);
+     };
+ #elif defined(SKRP_CPU_LSX)
+     __m128i _01, _23, rg, ba;
+@@ -5944,10 +5944,10 @@ SI void from_8888(U32 rgba, U16* r, U16* g, U16* b, U16* a) {
+ 
+     __m128i mask_00ff = __lsx_vreplgr2vr_h(0xff);
+ 
+-    *r = __lsx_vand_v(rg, mask_00ff);
+-    *g = __lsx_vsrli_h(rg, 8);
+-    *b = __lsx_vand_v(ba, mask_00ff);
+-    *a = __lsx_vsrli_h(ba, 8);
++    *r = (U16)__lsx_vand_v(rg, mask_00ff);
++    *g = (U16)__lsx_vsrli_h(rg, 8);
++    *b = (U16)__lsx_vand_v(ba, mask_00ff);
++    *a = (U16)__lsx_vsrli_h(ba, 8);
+ #else
+     auto cast_U16 = [](U32 v) -> U16 {
+         return cast<U16>(v);
+@@ -5975,26 +5975,26 @@ SI void load_8888_(const uint32_t* ptr, U16* r, U16* g, U16* b, U16* a) {
+ SI void store_8888_(uint32_t* ptr, U16 r, U16 g, U16 b, U16 a) {
+ #if defined(SKRP_CPU_LSX)
+     __m128i mask = __lsx_vreplgr2vr_h(255);
+-    r = __lsx_vmin_hu(r, mask);
+-    g = __lsx_vmin_hu(g, mask);
+-    b = __lsx_vmin_hu(b, mask);
+-    a = __lsx_vmin_hu(a, mask);
++    r = (U16)__lsx_vmin_hu((__m128i)r, mask);
++    g = (U16)__lsx_vmin_hu((__m128i)g, mask);
++    b = (U16)__lsx_vmin_hu((__m128i)b, mask);
++    a = (U16)__lsx_vmin_hu((__m128i)a, mask);
+ 
+-    g = __lsx_vslli_h(g, 8);
++    g = (U16)__lsx_vslli_h(g, 8);
+     r = r | g;
+-    a = __lsx_vslli_h(a, 8);
++    a = (U16)__lsx_vslli_h(a, 8);
+     a = a | b;
+ 
+     __m128i r_lo = __lsx_vsllwil_wu_hu(r, 0);
+-    __m128i r_hi = __lsx_vexth_wu_hu(r);
++    __m128i r_hi = __lsx_vexth_wu_hu((__m128i)r);
+     __m128i a_lo = __lsx_vsllwil_wu_hu(a, 0);
+-    __m128i a_hi = __lsx_vexth_wu_hu(a);
++    __m128i a_hi = __lsx_vexth_wu_hu((__m128i)a);
+ 
+     a_lo = __lsx_vslli_w(a_lo, 16);
+     a_hi = __lsx_vslli_w(a_hi, 16);
+ 
+-    r = r_lo | a_lo;
+-    a = r_hi | a_hi;
++    r = (U16)(r_lo | a_lo);
++    a = (U16)(r_hi | a_hi);
+     store(ptr, join<U32>(r, a));
+ #else
+     r = min(r, 255);
+@@ -6560,8 +6560,8 @@ STAGE_GP(bilerp_clamp_8888, const SkRasterPipeline_GatherCtx* ctx) {
+     qy_lo = __lsx_vxor_v(qy_lo, temp);
+     qy_hi = __lsx_vxor_v(qy_hi, temp);
+ 
+-    I16 tx = __lsx_vpickev_h(qx_hi, qx_lo);
+-    I16 ty = __lsx_vpickev_h(qy_hi, qy_lo);
++    I16 tx = (I16)__lsx_vpickev_h(qx_hi, qx_lo);
++    I16 ty = (I16)__lsx_vpickev_h(qy_hi, qy_lo);
+ #else
+     I16 tx = cast<I16>(qx ^ 0x8000),
+         ty = cast<I16>(qy ^ 0x8000);
+-- 
+2.48.1
+
diff --git a/external/skia/0004-loong64-Honor-existing-LASX-LSX-settings.patch b/external/skia/0004-loong64-Honor-existing-LASX-LSX-settings.patch
new file mode 100644
index 000000000000..afc46fc1bea8
--- /dev/null
+++ b/external/skia/0004-loong64-Honor-existing-LASX-LSX-settings.patch
@@ -0,0 +1,35 @@
+From fd8f052e034476b2bd2260a0cfb84e547faeb818 Mon Sep 17 00:00:00 2001
+From: Bingwu Zhang <xtex@aosc.io>
+Date: Sat, 8 Feb 2025 12:05:04 +0800
+Subject: [PATCH 4/4] [loong64] Honor existing LASX/LSX settings
+X-Developer-Signature: v=1; a=openpgp-sha256; l=869; i=xtex@aosc.io;
+ h=from:subject; bh=Kx9QEBi2/PSAbavbNIZyRebIqDVHB+qj/P4JUxAoHRo=;
+ b=owGbwMvMwCW2U4Ij7wZL9ETG02pJDOnLZbKLZnjZFkUcFlApzvmW5viXPzSkv0fjxMozW8QbU
+ za+P7Wgo5SFQYyLQVZMkaXIsMGbVSedX3RZuSzMHFYmkCEMXJwCMJEPcxkZVn07eNZ/yuKW0ymM
+ i3KvdtqHTbQ87Scxs5f7wqO77pPufmBkuNL0yvjc36/i3Q3M3Pt41a6UPeOZY8oSt6xH9U5Z75/
+ dfAA=
+X-Developer-Key: i=xtex@aosc.io; a=openpgp;
+ fpr=7231804B052C670F15A6771DB918086ED8045B91
+
+Change-Id: I1e6183b987002ed09621b111a6012a0f186233f1
+Signed-off-by: Bingwu Zhang <xtex@aosc.io>
+---
+ src/opts/SkRasterPipeline_opts.h | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/src/opts/SkRasterPipeline_opts.h b/src/opts/SkRasterPipeline_opts.h
+index a0b0a3de8bd9..84650c34377a 100644
+--- a/src/opts/SkRasterPipeline_opts.h
++++ b/src/opts/SkRasterPipeline_opts.h
+@@ -69,7 +69,7 @@ using NoCtx = const void*;
+ 
+ #if defined(SKRP_CPU_SCALAR) || defined(SKRP_CPU_NEON) || defined(SKRP_CPU_HSW) || \
+         defined(SKRP_CPU_SKX) || defined(SKRP_CPU_AVX) || defined(SKRP_CPU_SSE41) || \
+-        defined(SKRP_CPU_SSE2)
++        defined(SKRP_CPU_SSE2) || defined(SKRP_CPU_LASX) || defined(SKRP_CPU_LSX)
+     // Honor the existing setting
+ #elif !defined(__clang__) && !defined(__GNUC__)
+     #define SKRP_CPU_SCALAR
+-- 
+2.48.1
+
diff --git a/external/skia/UnpackedTarball_skia.mk b/external/skia/UnpackedTarball_skia.mk
index 60481be5de23..fd36e575b0b4 100644
--- a/external/skia/UnpackedTarball_skia.mk
+++ b/external/skia/UnpackedTarball_skia.mk
@@ -41,6 +41,10 @@ skia_patches := \
     help-msvc-analyzer.patch \
     always_inline_and_multiversioning_conflict.patch.1 \
     windows-define-conflict.patch.1 \
+    0001-loong64-Explicitly-cast-vector-types-around-LSX-intr.patch \
+	0002-loong64-fix-some-bugs-in-type-conversion-on-Loongarc.patch \
+	0003-loong64-Cast-vector-types-around-LSX-intrinsics-in-S.patch \
+	0004-loong64-Honor-existing-LASX-LSX-settings.patch \
 
 $(eval $(call gb_UnpackedTarball_set_patchlevel,skia,1))
 
-- 
2.48.1

