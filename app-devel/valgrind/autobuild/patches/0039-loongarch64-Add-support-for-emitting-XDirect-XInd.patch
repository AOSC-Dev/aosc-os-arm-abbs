From 9b0edf8415977709649d3077feeb975454517717 Mon Sep 17 00:00:00 2001
From: Feiyang Chen <chenfeiyang@loongson.cn>
Date: Mon, 21 Mar 2022 16:12:15 +0800
Subject: [PATCH v4 039/123] loongarch64: Add support for emitting
 XDirect/XIndir/XAssisted instructions

---
 VEX/priv/host_loongarch64_defs.c | 378 +++++++++++++++++++++++++++++++
 VEX/priv/host_loongarch64_defs.h |  33 ++-
 2 files changed, 410 insertions(+), 1 deletion(-)

diff --git a/VEX/priv/host_loongarch64_defs.c b/VEX/priv/host_loongarch64_defs.c
index 4dbb4f833..0c2c471a1 100644
--- a/VEX/priv/host_loongarch64_defs.c
+++ b/VEX/priv/host_loongarch64_defs.c
@@ -969,6 +969,43 @@ LOONGARCH64Instr* LOONGARCH64Instr_Call ( HReg cond, Addr64 target,
    return i;
 }
 
+LOONGARCH64Instr* LOONGARCH64Instr_XDirect ( Addr64 dstGA,
+                                             LOONGARCH64AMode* amPC,
+                                             HReg cond, Bool toFastEP )
+{
+   LOONGARCH64Instr* i      = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                   = LAin_XDirect;
+   i->LAin.XDirect.dstGA    = dstGA;
+   i->LAin.XDirect.amPC     = amPC;
+   i->LAin.XDirect.cond     = cond;
+   i->LAin.XDirect.toFastEP = toFastEP;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_XIndir ( HReg dstGA, LOONGARCH64AMode* amPC,
+                                            HReg cond )
+{
+   LOONGARCH64Instr* i  = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag               = LAin_XIndir;
+   i->LAin.XIndir.dstGA = dstGA;
+   i->LAin.XIndir.amPC  = amPC;
+   i->LAin.XIndir.cond  = cond;
+   return i;
+}
+
+LOONGARCH64Instr* LOONGARCH64Instr_XAssisted ( HReg dstGA,
+                                               LOONGARCH64AMode* amPC,
+                                               HReg cond, IRJumpKind jk )
+{
+   LOONGARCH64Instr* i     = LibVEX_Alloc_inline(sizeof(LOONGARCH64Instr));
+   i->tag                  = LAin_XAssisted;
+   i->LAin.XAssisted.dstGA = dstGA;
+   i->LAin.XAssisted.amPC  = amPC;
+   i->LAin.XAssisted.cond  = cond;
+   i->LAin.XAssisted.jk    = jk;
+   return i;
+}
+
 
 /* -------- Pretty Print instructions ------------- */
 
@@ -1171,6 +1208,64 @@ static inline void ppCall ( HReg cond, Addr64 target,
       vex_printf(" }");
 }
 
+static inline void ppXDirect ( Addr64 dstGA, LOONGARCH64AMode* amPC,
+                               HReg cond, Bool toFastEP )
+{
+   vex_printf("(xDirect) ");
+   if (!hregIsInvalid(cond)) {
+      vex_printf("if (");
+      ppHRegLOONGARCH64(cond);
+      vex_printf(") { ");
+   }
+   vex_printf("li $t0, 0x%llx; ", (ULong)dstGA);
+   vex_printf("st.w $t0, ");
+   ppLOONGARCH64AMode(amPC);
+   vex_printf("; li $t0, $disp_cp_chain_me_to_%sEP; ",
+              toFastEP ? "fast" : "slow");
+   vex_printf("jirl $ra, $t0, 0");
+   if (!hregIsInvalid(cond))
+      vex_printf(" }");
+}
+
+static inline void ppXIndir ( HReg dstGA, LOONGARCH64AMode* amPC,
+                              HReg cond )
+{
+   vex_printf("(xIndir) ");
+   if (!hregIsInvalid(cond)) {
+      vex_printf("if (");
+      ppHRegLOONGARCH64(cond);
+      vex_printf(") { ");
+   }
+   vex_printf("st.w ");
+   ppHRegLOONGARCH64(dstGA);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(amPC);
+   vex_printf("; la $t0, disp_indir; ");
+   vex_printf("jirl $ra, $t0, 0");
+   if (!hregIsInvalid(cond))
+      vex_printf(" }");
+}
+
+static inline void ppXAssisted ( HReg dstGA, LOONGARCH64AMode* amPC,
+                                 HReg cond, IRJumpKind jk)
+{
+   vex_printf("(xAssisted) ");
+   if (!hregIsInvalid(cond)) {
+      vex_printf("if (");
+      ppHRegLOONGARCH64(cond);
+      vex_printf(") { ");
+   }
+   vex_printf("st.w ");
+   ppHRegLOONGARCH64(dstGA);
+   vex_printf(", ");
+   ppLOONGARCH64AMode(amPC);
+   vex_printf("; li.w $s8, IRJumpKind_to_TRCVAL(%d); ", (Int)jk);
+   vex_printf("la $t0, disp_assisted; ");
+   vex_printf("jirl $ra, $t0, 0");
+   if (!hregIsInvalid(cond))
+      vex_printf(" }");
+}
+
 void ppLOONGARCH64Instr ( const LOONGARCH64Instr* i, Bool mode64 )
 {
    vassert(mode64 == True);
@@ -1242,6 +1337,18 @@ void ppLOONGARCH64Instr ( const LOONGARCH64Instr* i, Bool mode64 )
          ppCall(i->LAin.Call.cond, i->LAin.Call.target,
                 i->LAin.Call.nArgRegs, i->LAin.Call.rloc);
          break;
+      case LAin_XDirect:
+         ppXDirect(i->LAin.XDirect.dstGA, i->LAin.XDirect.amPC,
+                   i->LAin.XDirect.cond, i->LAin.XDirect.toFastEP);
+         break;
+      case LAin_XIndir:
+         ppXIndir(i->LAin.XIndir.dstGA, i->LAin.XIndir.amPC,
+                  i->LAin.XIndir.cond);
+         break;
+      case LAin_XAssisted:
+         ppXAssisted(i->LAin.XAssisted.dstGA, i->LAin.XAssisted.amPC,
+                     i->LAin.XAssisted.cond, i->LAin.XAssisted.jk);
+         break;
       default:
          vpanic("ppLOONGARCH64Instr");
          break;
@@ -1373,6 +1480,31 @@ void getRegUsage_LOONGARCH64Instr ( HRegUsage* u, const LOONGARCH64Instr* i,
             of the allocator, but what the hell. */
          addHRegUse(u, HRmWrite, hregT0());
          break;
+      /* XDirect/XIndir/XAssisted are also a bit subtle.  They
+         conditionally exit the block.  Hence we only need to list (1)
+         the registers that they read, and (2) the registers that they
+         write in the case where the block is not exited.  (2) is
+         empty, hence only (1) is relevant here. */
+      case LAin_XDirect:
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.XDirect.amPC);
+         if (!hregIsInvalid(i->LAin.XDirect.cond))
+            addHRegUse(u, HRmRead, i->LAin.XDirect.cond);
+         addHRegUse(u, HRmWrite, hregT0()); /* unavail to RA */
+         break;
+      case LAin_XIndir:
+         addHRegUse(u, HRmRead, i->LAin.XIndir.dstGA);
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.XIndir.amPC);
+         if (!hregIsInvalid(i->LAin.XIndir.cond))
+            addHRegUse(u, HRmRead, i->LAin.XIndir.cond);
+         addHRegUse(u, HRmWrite, hregT0()); /* unavail to RA */
+         break;
+      case LAin_XAssisted:
+         addHRegUse(u, HRmRead, i->LAin.XAssisted.dstGA);
+         addRegUsage_LOONGARCH64AMode(u, i->LAin.XAssisted.amPC);
+         if (!hregIsInvalid(i->LAin.XAssisted.cond))
+            addHRegUse(u, HRmRead, i->LAin.XAssisted.cond);
+         addHRegUse(u, HRmWrite, hregT0()); /* unavail to RA */
+         break;
       default:
          ppLOONGARCH64Instr(i, mode64);
          vpanic("getRegUsage_LOONGARCH64Instr");
@@ -1466,6 +1598,28 @@ void mapRegs_LOONGARCH64Instr ( HRegRemap* m, LOONGARCH64Instr* i,
             mapReg(m, &i->LAin.Call.cond);
          /* Hardwires $r12. */
          break;
+      /* XDirect/XIndir/XAssisted are also a bit subtle.  They
+         conditionally exit the block.  Hence we only need to list (1)
+         the registers that they read, and (2) the registers that they
+         write in the case where the block is not exited.  (2) is
+         empty, hence only (1) is relevant here. */
+      case LAin_XDirect:
+         mapRegs_LOONGARCH64AMode(m, i->LAin.XDirect.amPC);
+         if (!hregIsInvalid(i->LAin.XDirect.cond))
+            mapReg(m, &i->LAin.XDirect.cond);
+         break;
+      case LAin_XIndir:
+         mapReg(m, &i->LAin.XIndir.dstGA);
+         mapRegs_LOONGARCH64AMode(m, i->LAin.XIndir.amPC);
+         if (!hregIsInvalid(i->LAin.XIndir.cond))
+            mapReg(m, &i->LAin.XIndir.cond);
+         break;
+      case LAin_XAssisted:
+         mapReg(m, &i->LAin.XAssisted.dstGA);
+         mapRegs_LOONGARCH64AMode(m, i->LAin.XAssisted.amPC);
+         if (!hregIsInvalid(i->LAin.XAssisted.cond))
+            mapReg(m, &i->LAin.XAssisted.cond);
+         break;
       default:
          ppLOONGARCH64Instr(i, mode64);
          vpanic("mapRegs_LOONGARCH64Instr");
@@ -2269,6 +2423,215 @@ static inline UInt* mkCall ( UInt* p, HReg cond, Addr64 target, RetLoc rloc )
    return p;
 }
 
+static inline UInt* mkXDirect ( UInt* p, Addr64 dstGA,
+                                LOONGARCH64AMode* amPC,
+                                HReg cond, Bool toFastEP,
+                                const void* disp_cp_chain_me_to_slowEP,
+                                const void* disp_cp_chain_me_to_fastEP )
+{
+   /* NB: what goes on here has to be very closely coordinated
+      with chainXDirect_LOONGARCH64 and unchainXDirect_LOONGARCH64 below. */
+   /* We're generating chain-me requests here, so we need to be
+      sure this is actually allowed -- no-redir translations
+      can't use chain-me's.  Hence: */
+   vassert(disp_cp_chain_me_to_slowEP != NULL);
+   vassert(disp_cp_chain_me_to_fastEP != NULL);
+
+   /* Use ptmp for backpatching conditional jumps. */
+   UInt* ptmp = NULL;
+
+   /* First off, if this is conditional, create a conditional
+      jump over the rest of it.  Or at least, leave a space for
+      it that we will shortly fill in. */
+   if (!hregIsInvalid(cond)) {
+      ptmp = p;
+      p++;
+   }
+
+   /* Update the guest PC.
+      $t0 = dstGA
+      st.d $t0, amPC
+    */
+   p = mkLoadImm(p, hregT0(), (ULong)dstGA);
+   p = mkStore(p, LAstore_ST_D, amPC, hregT0());
+
+   /* --- FIRST PATCHABLE BYTE follows --- */
+   /* VG_(disp_cp_chain_me_to_{slowEP,fastEP}) (where we're
+      calling to) backs up the return address, so as to find the
+      address of the first patchable byte.  So: don't change the
+      number of instructions (5) below. */
+   /*
+      la   $t0, VG_(disp_cp_chain_me_to_{slowEP,fastEP})
+      jirl $ra, $t0, 0
+    */
+   const void* disp_cp_chain_me = toFastEP ? disp_cp_chain_me_to_fastEP
+                                           : disp_cp_chain_me_to_slowEP;
+   p = mkLoadImm_EXACTLY4(p, hregT0(), (ULong)(Addr)disp_cp_chain_me);
+   *p++ = emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1);
+   /* --- END of PATCHABLE BYTES --- */
+
+   /* Fix up the conditional jump, if there was one. */
+   if (!hregIsInvalid(cond)) {
+      vassert(ptmp != NULL);
+      UInt offs = (UInt)(p - ptmp);
+      vassert(offs >= 8 && offs <= 11);
+      /* beq cond, $zero, offs */
+      *ptmp++ = emit_op_offs16_rj_rd(LAextra_BEQ, offs, iregEnc(cond), 0);
+   }
+
+   return p;
+}
+
+static inline UInt* mkXIndir ( UInt* p, HReg dstGA, LOONGARCH64AMode* amPC,
+                               HReg cond, const void* disp_cp_xindir )
+{
+   /* We're generating transfers that could lead indirectly to a
+      chain-me, so we need to be sure this is actually allowed --
+      no-redir translations are not allowed to reach normal
+      translations without going through the scheduler.  That means
+      no XDirects or XIndirs out from no-redir translations.
+      Hence: */
+   vassert(disp_cp_xindir != NULL);
+
+   /* Use ptmp for backpatching conditional jumps. */
+   UInt* ptmp = NULL;
+
+   /* First off, if this is conditional, create a conditional
+      jump over the rest of it. */
+   if (!hregIsInvalid(cond)) {
+      ptmp = p;
+      p++;
+   }
+
+   /* Update the guest PC.
+      or   $t0, dstGA, $zero
+      st.d $t0, amPC
+    */
+   *p++ = emit_op_rk_rj_rd(LAbin_OR, 0, iregEnc(dstGA), 12);
+   p = mkStore(p, LAstore_ST_D, amPC, hregT0());
+
+   /*
+      la   $t0, VG_(disp_cp_xindir)
+      jirl $ra, $t0, 0
+    */
+   p = mkLoadImm(p, hregT0(), (ULong)(Addr)disp_cp_xindir);
+   *p++ = emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1);
+
+   /* Fix up the conditional jump, if there was one. */
+   if (!hregIsInvalid(cond)) {
+      vassert(ptmp != NULL);
+      UInt offs = (UInt)(p - ptmp);
+      vassert(offs >= 5 && offs <= 8);
+      /* beq cond, $zero, offs */
+      *ptmp++ = emit_op_offs16_rj_rd(LAextra_BEQ, offs, iregEnc(cond), 0);
+   }
+
+   return p;
+}
+
+static inline UInt* mkXAssisted ( UInt* p, HReg dstGA, LOONGARCH64AMode* amPC,
+                                  HReg cond, IRJumpKind jk,
+                                  const void* disp_cp_xassisted )
+{
+   /* First off, if this is conditional, create a conditional jump
+      over the rest of it.  Or at least, leave a space for it that
+      we will shortly fill in. */
+   UInt* ptmp = NULL;
+   if (!hregIsInvalid(cond)) {
+      ptmp = p;
+      p++;
+   }
+
+   /* Update the guest PC.
+      or   $t0, dstGA, $zero
+      st.d $t0, amPC
+    */
+   *p++ = emit_op_rk_rj_rd(LAbin_OR, 0, iregEnc(dstGA), 12);
+   p = mkStore(p, LAstore_ST_D, amPC, hregT0());
+
+   /* li.w $s8, magic_number */
+   UInt trcval = 0;
+   switch (jk) {
+      case Ijk_Boring:
+         trcval = VEX_TRC_JMP_BORING;
+         break;
+      case Ijk_ClientReq:
+         trcval = VEX_TRC_JMP_CLIENTREQ;
+         break;
+      case Ijk_NoDecode:
+         trcval = VEX_TRC_JMP_NODECODE;
+         break;
+      case Ijk_InvalICache:
+         trcval = VEX_TRC_JMP_INVALICACHE;
+         break;
+      case Ijk_NoRedir:
+         trcval = VEX_TRC_JMP_NOREDIR;
+         break;
+      case Ijk_SigTRAP:
+         trcval = VEX_TRC_JMP_SIGTRAP;
+         break;
+      case Ijk_SigSEGV:
+         trcval = VEX_TRC_JMP_SIGSEGV;
+         break;
+      case Ijk_SigBUS:
+         trcval = VEX_TRC_JMP_SIGBUS;
+         break;
+      case Ijk_SigFPE_IntDiv:
+         trcval = VEX_TRC_JMP_SIGFPE_INTDIV;
+         break;
+      case Ijk_SigFPE_IntOvf:
+         trcval = VEX_TRC_JMP_SIGFPE_INTOVF;
+         break;
+      case Ijk_SigSYS:
+         trcval = VEX_TRC_JMP_SIGSYS;
+         break;
+      case Ijk_Sys_syscall:
+         trcval = VEX_TRC_JMP_SYS_SYSCALL;
+         break;
+      /* We don't expect to see the following being assisted.
+         case Ijk_Call:
+         case Ijk_Ret:
+         case Ijk_Yield:
+         case Ijk_EmWarn:
+         case Ijk_EmFail:
+         case Ijk_MapFail:
+         case Ijk_FlushDCache:
+         case Ijk_SigILL:
+         case Ijk_SigFPE:
+         case Ijk_Sys_int32:
+         case Ijk_Sys_int128:
+         case Ijk_Sys_int129:
+         case Ijk_Sys_int130:
+         case Ijk_Sys_int145:
+         case Ijk_Sys_int210:
+         case Ijk_Sys_sysenter:
+       */
+      default:
+         ppIRJumpKind(jk);
+         vpanic("emit_LOONGARCH64Instr.LAin_XAssisted: unexpected jump kind");
+   }
+   vassert(trcval != 0);
+   p = mkLoadImm(p, hregGSP(), trcval);
+
+   /*
+      la   $t0, VG_(disp_cp_xassisted)
+      jirl $ra, $t0, 0
+    */
+   p = mkLoadImm(p, hregT0(), (ULong)(Addr)disp_cp_xassisted);
+   *p++ = emit_op_offs16_rj_rd(LAextra_JIRL, 0, 12, 1);
+
+   /* Fix up the conditional jump, if there was one. */
+   if (!hregIsInvalid(cond)) {
+      vassert(ptmp != NULL);
+      UInt offs = (UInt)(p - ptmp);
+      vassert(offs >= 6 && offs <= 12);
+      /* beq cond, $zero, offs */
+      *ptmp++ = emit_op_offs16_rj_rd(LAextra_BEQ, offs, iregEnc(cond), 0);
+   }
+
+   return p;
+}
+
 /* Emit an instruction into buf and return the number of bytes used.
    Note that buf is not the insn's final place, and therefore it is
    imperative to emit position-independent code.  If the emitted
@@ -2363,6 +2726,21 @@ Int emit_LOONGARCH64Instr ( /*MB_MOD*/Bool* is_profInc,
          p = mkCall(p, i->LAin.Call.cond, i->LAin.Call.target,
                     i->LAin.Call.rloc);
          break;
+      case LAin_XDirect:
+         p = mkXDirect(p, i->LAin.XDirect.dstGA, i->LAin.XDirect.amPC,
+                       i->LAin.XDirect.cond, i->LAin.XDirect.toFastEP,
+                       disp_cp_chain_me_to_slowEP,
+                       disp_cp_chain_me_to_fastEP);
+         break;
+      case LAin_XIndir:
+         p = mkXIndir(p, i->LAin.XIndir.dstGA, i->LAin.XIndir.amPC,
+                      i->LAin.XIndir.cond, disp_cp_xindir);
+         break;
+      case LAin_XAssisted:
+         p = mkXAssisted(p, i->LAin.XAssisted.dstGA, i->LAin.XAssisted.amPC,
+                         i->LAin.XAssisted.cond, i->LAin.XAssisted.jk,
+                         disp_cp_xassisted);
+         break;
       default:
          p = NULL;
          break;
diff --git a/VEX/priv/host_loongarch64_defs.h b/VEX/priv/host_loongarch64_defs.h
index 8f6ce7091..7e634c06c 100644
--- a/VEX/priv/host_loongarch64_defs.h
+++ b/VEX/priv/host_loongarch64_defs.h
@@ -411,7 +411,12 @@ typedef enum {
 
    /* Call target (an absolute address), on given
       condition (which could be LAcc_AL). */
-   LAin_Call        /* call */
+   LAin_Call,       /* call */
+
+   /* The following 3 insns are mandated by translation chaining */
+   LAin_XDirect,    /* direct transfer to GA */
+   LAin_XIndir,     /* indirect transfer to GA */
+   LAin_XAssisted   /* assisted transfer to GA */
 } LOONGARCH64InstrTag;
 
 typedef struct {
@@ -517,6 +522,23 @@ typedef struct {
          UInt                 nArgRegs;
          RetLoc               rloc;
       } Call;
+      struct {
+         Addr64               dstGA;
+         LOONGARCH64AMode*    amPC;
+         HReg                 cond;
+         Bool                 toFastEP;
+      } XDirect;
+      struct {
+         HReg                 dstGA;
+         LOONGARCH64AMode*    amPC;
+         HReg                 cond;
+      } XIndir;
+      struct {
+         HReg                 dstGA;
+         LOONGARCH64AMode*    amPC;
+         HReg                 cond;
+         IRJumpKind           jk;
+      } XAssisted;
    } LAin;
 } LOONGARCH64Instr;
 
@@ -567,6 +589,15 @@ extern LOONGARCH64Instr* LOONGARCH64Instr_CMove     ( HReg cond, HReg r0, HReg r
                                                       HReg dst, Bool isInt );
 extern LOONGARCH64Instr* LOONGARCH64Instr_Call      ( HReg cond, Addr64 target,
                                                       UInt nArgRegs, RetLoc rloc );
+extern LOONGARCH64Instr* LOONGARCH64Instr_XDirect   ( Addr64 dstGA,
+                                                      LOONGARCH64AMode* amPC,
+                                                      HReg cond, Bool toFastEP );
+extern LOONGARCH64Instr* LOONGARCH64Instr_XIndir    ( HReg dstGA,
+                                                      LOONGARCH64AMode* amPC,
+                                                      HReg cond );
+extern LOONGARCH64Instr* LOONGARCH64Instr_XAssisted ( HReg dstGA,
+                                                      LOONGARCH64AMode* amPC,
+                                                      HReg cond, IRJumpKind jk );
 
 extern void ppLOONGARCH64Instr ( const LOONGARCH64Instr* i, Bool mode64 );
 
-- 
2.39.1

