From 04ef719caab8f567673a0201d3d8f9aaaee46383 Mon Sep 17 00:00:00 2001
From: Feiyang Chen <chenfeiyang@loongson.cn>
Date: Wed, 23 Mar 2022 16:47:34 +0800
Subject: [PATCH v4 096/123] loongarch64: Add support for client requests

---
 coregrind/m_main.c    |   5 +-
 include/valgrind.h.in | 457 ++++++++++++++++++++++++++++++++++++++----
 2 files changed, 423 insertions(+), 39 deletions(-)

diff --git a/coregrind/m_main.c b/coregrind/m_main.c
index 30eeb2133..adebe9d10 100644
--- a/coregrind/m_main.c
+++ b/coregrind/m_main.c
@@ -2521,7 +2521,10 @@ static void final_tidyup(ThreadId tid)
             offsetof(VexGuestS390XState, guest_r2),
             sizeof(VG_(threads)[tid].arch.vex.guest_r2));
 #  elif defined(VGA_loongarch64)
-   /* TODO */
+   VG_(threads)[tid].arch.vex.guest_R4 = to_run;
+   VG_TRACK(post_reg_write, Vg_CoreClientReq, tid,
+            offsetof(VexGuestLOONGARCH64State, guest_R4),
+            sizeof(VG_(threads)[tid].arch.vex.guest_R4));
 #else
    I_die_here : architecture missing in m_main.c
 #endif
diff --git a/include/valgrind.h.in b/include/valgrind.h.in
index 8b9dbf489..b330497f7 100644
--- a/include/valgrind.h.in
+++ b/include/valgrind.h.in
@@ -1140,29 +1140,60 @@ typedef
    OrigFn;
 
 #define __SPECIAL_INSTRUCTION_PREAMBLE                              \
-   do {                                                             \
-      /* TODO */                                                    \
-   } while (0)
+                       "srli.d $zero, $zero, 3  \n\t"               \
+                       "srli.d $zero, $zero, 13 \n\t"               \
+                       "srli.d $zero, $zero, 29 \n\t"               \
+                       "srli.d $zero, $zero, 19 \n\t"
 
 #define VALGRIND_DO_CLIENT_REQUEST_EXPR(                            \
         _zzq_default, _zzq_request,                                 \
         _zzq_arg1, _zzq_arg2, _zzq_arg3, _zzq_arg4, _zzq_arg5)      \
-   /* TODO */                                                       \
-   0
+   __extension__                                                    \
+   ({                                                               \
+      volatile unsigned long int _zzq_args[6];                      \
+      volatile unsigned long int _zzq_result;                       \
+      _zzq_args[0] = (unsigned long int)(_zzq_request);             \
+      _zzq_args[1] = (unsigned long int)(_zzq_arg1);                \
+      _zzq_args[2] = (unsigned long int)(_zzq_arg2);                \
+      _zzq_args[3] = (unsigned long int)(_zzq_arg3);                \
+      _zzq_args[4] = (unsigned long int)(_zzq_arg4);                \
+      _zzq_args[5] = (unsigned long int)(_zzq_arg5);                \
+      __asm__ volatile("move $a7, %1     \n\t" /*default*/          \
+                       "move $t0, %2     \n\t" /*ptr*/              \
+                       __SPECIAL_INSTRUCTION_PREAMBLE               \
+                       /* $a7 = client_request ( $t0 ) */           \
+                       "or $t1, $t1, $t1 \n\t"                      \
+                       "move %0, $a7     \n\t" /*result*/           \
+                       : "=r" (_zzq_result)                         \
+                       : "r" (_zzq_default), "r" (&_zzq_args[0])    \
+                       : "$a7", "$t0", "memory");                   \
+      _zzq_result;                                                  \
+   })
 
 #define VALGRIND_GET_NR_CONTEXT(_zzq_rlval)                         \
-   do {                                                             \
-      /* TODO */                                                    \
-   } while (0)
+   {                                                                \
+      volatile OrigFn* _zzq_orig = &(_zzq_rlval);                   \
+      volatile unsigned long int __addr;                            \
+      __asm__ volatile(__SPECIAL_INSTRUCTION_PREAMBLE               \
+                       /* $a7 = guest_NRADDR */                     \
+                       "or $t2, $t2, $t2 \n\t"                      \
+                       "move %0, $a7     \n\t" /*result*/           \
+                       : "=r" (__addr)                              \
+                       :                                            \
+                       : "$a7");                                    \
+      _zzq_orig->nraddr = __addr;                                   \
+   }
 
 #define VALGRIND_CALL_NOREDIR_T8                                    \
-   do {                                                             \
-      /* TODO */                                                    \
-   } while (0)
+                       __SPECIAL_INSTRUCTION_PREAMBLE               \
+                       /* call-noredir $t8 */                       \
+                       "or $t3, $t3, $t3 \n\t"
 
 #define VALGRIND_VEX_INJECT_IR()                                    \
    do {                                                             \
-      /* TODO */                                                    \
+      __asm__ volatile(__SPECIAL_INSTRUCTION_PREAMBLE               \
+                       "or $t4, $t4, $t4 \n\t"                      \
+                      );                                            \
    } while (0)
 
 #endif /* PLAT_loongarch64_linux */
@@ -6648,98 +6679,448 @@ typedef
 #if defined(PLAT_loongarch64_linux)
 
 /* These regs are trashed by the hidden call. */
-#define __CALLER_SAVED_REGS                                       \
-   do {                                                           \
-      /* TODO */                                                  \
-   } while (0)
+#define __CALLER_SAVED_REGS                                        \
+   "$ra", "$a0", "$a1", "$a2", "$a3", "$a4", "$a5", "$a6", "$a7",  \
+   "$t0", "$t1", "$t2", "$t3", "$t4", "$t5", "$t6", "$t7", "$t8",  \
+   "$f0",  "$f1",  "$f2",  "$f3",  "$f4",  "$f5",  "$f6",  "$f7",  \
+   "$f8",  "$f9",  "$f10", "$f11", "$f12", "$f13", "$f14", "$f15", \
+   "$f16", "$f17", "$f18", "$f19", "$f20", "$f21", "$f22", "$f23"
 
 /* $s0 is callee-saved, so we can use it to save and restore SP around
    the hidden call. */
-#define VALGRIND_ALIGN_STACK                                      \
-   do {                                                           \
-      /* TODO */                                                  \
-   } while (0)
-
-#define VALGRIND_RESTORE_STACK                                    \
-   do {                                                           \
-      /* TODO */                                                  \
-   } while (0)
+#define VALGRIND_ALIGN_STACK            \
+      "move      $s0, $sp         \n\t" \
+      "bstrins.d $sp, $zero, 3, 0 \n\t"
+#define VALGRIND_RESTORE_STACK          \
+      "move      $sp, $s0         \n\t"
 
 /* These CALL_FN_ macros assume that on loongarch64-linux,
    sizeof(unsigned long) == 8. */
 
 #define CALL_FN_W_v(lval, orig)                                   \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[1];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld.d $t8, %1, 0 \n\t"  /* target->t8 */                 \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0    \n\t"                                   \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_W(lval, orig, arg1)                             \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[2];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld.d $a0, %1, 8 \n\t"  /* arg1 */                       \
+         "ld.d $t8, %1, 0 \n\t"  /* target->t8 */                 \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0    \n\t"                                   \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_WW(lval, orig, arg1, arg2)                      \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[3];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld.d $a0, %1, 8  \n\t"  /* arg1 */                      \
+         "ld.d $a1, %1, 16 \n\t"  /* arg2 */                      \
+         "ld.d $t8, %1, 0  \n\t"  /* target->t8 */                \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_WWW(lval, orig, arg1, arg2, arg3)               \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[4];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld.d $a0, %1, 8  \n\t"  /* arg1 */                      \
+         "ld.d $a1, %1, 16 \n\t"  /* arg2 */                      \
+         "ld.d $a2, %1, 24 \n\t"  /* arg3 */                      \
+         "ld.d $t8, %1, 0  \n\t"  /* target->t8 */                \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_WWWW(lval, orig, arg1, arg2, arg3, arg4)        \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[5];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld.d $a0, %1, 8  \n\t"  /* arg1 */                      \
+         "ld.d $a1, %1, 16 \n\t"  /* arg2 */                      \
+         "ld.d $a2, %1, 24 \n\t"  /* arg3 */                      \
+         "ld.d $a3, %1, 32 \n\t"  /* arg4 */                      \
+         "ld.d $t8, %1, 0  \n\t"  /* target->t8 */                \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_5W(lval, orig, arg1, arg2, arg3, arg4, arg5)    \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[6];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld.d $a0, %1, 8  \n\t"  /* arg1 */                      \
+         "ld.d $a1, %1, 16 \n\t"  /* arg2 */                      \
+         "ld.d $a2, %1, 24 \n\t"  /* arg3 */                      \
+         "ld.d $a3, %1, 32 \n\t"  /* arg4 */                      \
+         "ld.d $a4, %1, 40 \n\t"  /* arg5 */                      \
+         "ld.d $t8, %1, 0  \n\t"  /* target->t8 */                \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_6W(lval, orig, arg1, arg2, arg3, arg4, arg5,    \
                                  arg6)                            \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[7];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld.d $a0, %1, 8  \n\t"  /* arg1 */                      \
+         "ld.d $a1, %1, 16 \n\t"  /* arg2 */                      \
+         "ld.d $a2, %1, 24 \n\t"  /* arg3 */                      \
+         "ld.d $a3, %1, 32 \n\t"  /* arg4 */                      \
+         "ld.d $a4, %1, 40 \n\t"  /* arg5 */                      \
+         "ld.d $a5, %1, 48 \n\t"  /* arg6 */                      \
+         "ld.d $t8, %1, 0  \n\t"  /* target->t8 */                \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_7W(lval, orig, arg1, arg2, arg3, arg4, arg5,    \
                                  arg6, arg7)                      \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[8];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld.d $a0, %1, 8  \n\t"  /* arg1 */                      \
+         "ld.d $a1, %1, 16 \n\t"  /* arg2 */                      \
+         "ld.d $a2, %1, 24 \n\t"  /* arg3 */                      \
+         "ld.d $a3, %1, 32 \n\t"  /* arg4 */                      \
+         "ld.d $a4, %1, 40 \n\t"  /* arg5 */                      \
+         "ld.d $a5, %1, 48 \n\t"  /* arg6 */                      \
+         "ld.d $a6, %1, 56 \n\t"  /* arg7 */                      \
+         "ld.d $t8, %1, 0  \n\t"  /* target->t8 */                \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_8W(lval, orig, arg1, arg2, arg3, arg4, arg5,    \
                                  arg6, arg7, arg8)                \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[9];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      _argvec[8] = (unsigned long)(arg8);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld.d $a0, %1, 8  \n\t"  /* arg1 */                      \
+         "ld.d $a1, %1, 16 \n\t"  /* arg2 */                      \
+         "ld.d $a2, %1, 24 \n\t"  /* arg3 */                      \
+         "ld.d $a3, %1, 32 \n\t"  /* arg4 */                      \
+         "ld.d $a4, %1, 40 \n\t"  /* arg5 */                      \
+         "ld.d $a5, %1, 48 \n\t"  /* arg6 */                      \
+         "ld.d $a6, %1, 56 \n\t"  /* arg7 */                      \
+         "ld.d $a7, %1, 64 \n\t"  /* arg8 */                      \
+         "ld.d $t8, %1, 0  \n\t"  /* target->t8 */                \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_9W(lval, orig, arg1, arg2, arg3, arg4, arg5,    \
                                  arg6, arg7, arg8, arg9)          \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[10];                         \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      _argvec[8] = (unsigned long)(arg8);                         \
+      _argvec[9] = (unsigned long)(arg9);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "addi.d $sp, $sp, -8 \n\t"                               \
+         "ld.d   $a0, %1, 72  \n\t"                               \
+         "st.d   $a0, $sp, 0  \n\t"  /* arg9 */                   \
+         "ld.d   $a0, %1, 8   \n\t"  /* arg1 */                   \
+         "ld.d   $a1, %1, 16  \n\t"  /* arg2 */                   \
+         "ld.d   $a2, %1, 24  \n\t"  /* arg3 */                   \
+         "ld.d   $a3, %1, 32  \n\t"  /* arg4 */                   \
+         "ld.d   $a4, %1, 40  \n\t"  /* arg5 */                   \
+         "ld.d   $a5, %1, 48  \n\t"  /* arg6 */                   \
+         "ld.d   $a6, %1, 56  \n\t"  /* arg7 */                   \
+         "ld.d   $a7, %1, 64  \n\t"  /* arg8 */                   \
+         "ld.d   $t8, %1, 0   \n\t"  /* target->t8 */             \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_10W(lval, orig, arg1, arg2, arg3, arg4, arg5,   \
                                   arg6, arg7, arg8, arg9, arg10)  \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[11];                         \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      _argvec[8] = (unsigned long)(arg8);                         \
+      _argvec[9] = (unsigned long)(arg9);                         \
+      _argvec[10] = (unsigned long)(arg10);                       \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "addi.d $sp, $sp, -16 \n\t"                              \
+         "ld.d   $a0, %1, 72   \n\t"                              \
+         "st.d   $a0, $sp, 0   \n\t"  /* arg9 */                  \
+         "ld.d   $a0, %1, 80   \n\t"                              \
+         "st.d   $a0, $sp, 8   \n\t"  /* arg10 */                 \
+         "ld.d   $a0, %1, 8    \n\t"  /* arg1 */                  \
+         "ld.d   $a1, %1, 16   \n\t"  /* arg2 */                  \
+         "ld.d   $a2, %1, 24   \n\t"  /* arg3 */                  \
+         "ld.d   $a3, %1, 32   \n\t"  /* arg4 */                  \
+         "ld.d   $a4, %1, 40   \n\t"  /* arg5 */                  \
+         "ld.d   $a5, %1, 48   \n\t"  /* arg6 */                  \
+         "ld.d   $a6, %1, 56   \n\t"  /* arg7 */                  \
+         "ld.d   $a7, %1, 64   \n\t"  /* arg8 */                  \
+         "ld.d   $t8, %1, 0    \n\t"  /* target->t8 */            \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_11W(lval, orig, arg1, arg2, arg3, arg4, arg5,   \
                                   arg6, arg7, arg8, arg9, arg10,  \
                                   arg11)                          \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[12];                         \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      _argvec[8] = (unsigned long)(arg8);                         \
+      _argvec[9] = (unsigned long)(arg9);                         \
+      _argvec[10] = (unsigned long)(arg10);                       \
+      _argvec[11] = (unsigned long)(arg11);                       \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "addi.d $sp, $sp, -24 \n\t"                              \
+         "ld.d   $a0, %1, 72   \n\t"                              \
+         "st.d   $a0, $sp, 0   \n\t"  /* arg9 */                  \
+         "ld.d   $a0, %1, 80   \n\t"                              \
+         "st.d   $a0, $sp, 8   \n\t"  /* arg10 */                 \
+         "ld.d   $a0, %1, 88   \n\t"                              \
+         "st.d   $a0, $sp, 16  \n\t"  /* arg11 */                 \
+         "ld.d   $a0, %1, 8    \n\t"  /* arg1 */                  \
+         "ld.d   $a1, %1, 16   \n\t"  /* arg2 */                  \
+         "ld.d   $a2, %1, 24   \n\t"  /* arg3 */                  \
+         "ld.d   $a3, %1, 32   \n\t"  /* arg4 */                  \
+         "ld.d   $a4, %1, 40   \n\t"  /* arg5 */                  \
+         "ld.d   $a5, %1, 48   \n\t"  /* arg6 */                  \
+         "ld.d   $a6, %1, 56   \n\t"  /* arg7 */                  \
+         "ld.d   $a7, %1, 64   \n\t"  /* arg8 */                  \
+         "ld.d   $t8, %1, 0    \n\t"  /* target->t8 */            \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #define CALL_FN_W_12W(lval, orig, arg1, arg2, arg3, arg4, arg5,   \
                                   arg6, arg7, arg8, arg9, arg10,  \
                                   arg11, arg12)                   \
    do {                                                           \
-      /* TODO */                                                  \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[13];                         \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      _argvec[8] = (unsigned long)(arg8);                         \
+      _argvec[9] = (unsigned long)(arg9);                         \
+      _argvec[10] = (unsigned long)(arg10);                       \
+      _argvec[11] = (unsigned long)(arg11);                       \
+      _argvec[12] = (unsigned long)(arg12);                       \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "addi.d $sp, $sp, -32 \n\t"                              \
+         "ld.d   $a0, %1, 72   \n\t"                              \
+         "st.d   $a0, $sp, 0   \n\t"  /* arg9 */                  \
+         "ld.d   $a0, %1, 80   \n\t"                              \
+         "st.d   $a0, $sp, 8   \n\t"  /* arg10 */                 \
+         "ld.d   $a0, %1, 88   \n\t"                              \
+         "st.d   $a0, $sp, 16  \n\t"  /* arg11 */                 \
+         "ld.d   $a0, %1, 96   \n\t"                              \
+         "st.d   $a0, $sp, 24  \n\t"  /* arg12 */                 \
+         "ld.d   $a0, %1, 8    \n\t"  /* arg1 */                  \
+         "ld.d   $a1, %1, 16   \n\t"  /* arg2 */                  \
+         "ld.d   $a2, %1, 24   \n\t"  /* arg3 */                  \
+         "ld.d   $a3, %1, 32   \n\t"  /* arg4 */                  \
+         "ld.d   $a4, %1, 40   \n\t"  /* arg5 */                  \
+         "ld.d   $a5, %1, 48   \n\t"  /* arg6 */                  \
+         "ld.d   $a6, %1, 56   \n\t"  /* arg7 */                  \
+         "ld.d   $a7, %1, 64   \n\t"  /* arg8 */                  \
+         "ld.d   $t8, %1, 0    \n\t"  /* target->t8 */            \
+         VALGRIND_CALL_NOREDIR_T8                                 \
+         VALGRIND_RESTORE_STACK                                   \
+         "move %0, $a0     \n\t"                                  \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "r" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "$s0"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
    } while (0)
 
 #endif /* PLAT_loongarch64_linux */
-- 
2.39.1

