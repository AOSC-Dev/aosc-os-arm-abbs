From d3dabe4688ce09edfc36b6993372c579bab24f25 Mon Sep 17 00:00:00 2001
From: "Dr. Chang Liu, PhD" <cl91tp@gmail.com>
Date: Tue, 26 Sep 2023 14:41:10 +0800
Subject: [PATCH] gsgpu: ttm_buffer_object::resv =>
 ttm_buffer_object::base.resv

---
 drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c      |  8 +++---
 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_drv.c  |  2 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_display.c |  2 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c     |  6 ++--
 drivers/gpu/drm/gsgpu/gpu/gsgpu_kms.c     |  2 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c      |  2 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c  | 24 ++++++++--------
 drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c   |  2 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c      | 34 +++++++++++------------
 9 files changed, 41 insertions(+), 41 deletions(-)

diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c
index 72a020c3d4be..fc70bf2c4908 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c
@@ -370,7 +370,7 @@ static int gsgpu_cs_bo_validate(struct gsgpu_cs_parser *p,
 	struct ttm_operation_ctx ctx = {
 		.interruptible = true,
 		.no_wait_gpu = false,
-		.resv = bo->tbo.resv,
+		.resv = bo->tbo.base.resv,
 		.flags = 0
 	};
 	uint32_t domain;
@@ -717,7 +717,7 @@ static int gsgpu_cs_sync_rings(struct gsgpu_cs_parser *p)
 	int r;
 
 	list_for_each_entry(e, &p->validated, tv.head) {
-		struct dma_resv *resv = e->robj->tbo.resv;
+		struct dma_resv *resv = e->robj->tbo.base.resv;
 		r = gsgpu_sync_resv(p->adev, &p->job->sync, resv, p->filp,
 				     gsgpu_bo_explicit_sync(e->robj));
 
@@ -850,7 +850,7 @@ static int gsgpu_cs_ib_vm_chunk(struct gsgpu_device *adev,
 		if (r)
 			return r;
 
-		r = dma_resv_reserve_shared(vm->root.base.bo->tbo.resv);
+		r = dma_resv_reserve_shared(vm->root.base.bo->tbo.base.resv);
 		if (r)
 			return r;
 	}
@@ -1514,7 +1514,7 @@ int gsgpu_cs_find_mapping(struct gsgpu_cs_parser *parser,
 	*map = mapping;
 
 	/* Double check that the BO is reserved by this CS */
-	if (READ_ONCE((*bo)->tbo.resv->lock.ctx) != &parser->ticket)
+	if (READ_ONCE((*bo)->tbo.base.resv->lock.ctx) != &parser->ticket)
 		return -EINVAL;
 
 	if (!((*bo)->flags & GSGPU_GEM_CREATE_VRAM_CONTIGUOUS)) {
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_drv.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_drv.c
index 7653206a616c..a64efbe4f73d 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_drv.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_drv.c
@@ -402,7 +402,7 @@ static void gsgpu_dc_do_flip(struct drm_crtc *crtc,
 	}
 
 	/* Wait for all fences on this FB */
-	WARN_ON(dma_resv_wait_timeout_rcu(abo->tbo.resv, true, false,
+	WARN_ON(dma_resv_wait_timeout_rcu(abo->tbo.base.resv, true, false,
 					  MAX_SCHEDULE_TIMEOUT) < 0);
 
 	gsgpu_bo_unreserve(abo);
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_display.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_display.c
index 1697cb923875..8cca8b100bf2 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_display.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_display.c
@@ -168,7 +168,7 @@ int gsgpu_display_crtc_page_flip_target(struct drm_crtc *crtc,
 		goto unpin;
 	}
 
-	r = dma_resv_get_fences_rcu(new_abo->tbo.resv, &work->excl,
+	r = dma_resv_get_fences_rcu(new_abo->tbo.base.resv, &work->excl,
 					      &work->shared_count,
 					      &work->shared);
 	if (unlikely(r != 0)) {
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c
index 1ff64b7967ce..795938dab699 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c
@@ -147,7 +147,7 @@ static int gsgpu_gem_object_open(struct drm_gem_object *obj,
 		return -EPERM;
 
 	if (abo->flags & GSGPU_GEM_CREATE_VM_ALWAYS_VALID &&
-	    abo->tbo.resv != vm->root.base.bo->tbo.resv)
+	    abo->tbo.base.resv != vm->root.base.bo->tbo.base.resv)
 		return -EPERM;
 
 	r = gsgpu_bo_reserve(abo, false);
@@ -298,7 +298,7 @@ int gsgpu_gem_create_ioctl(struct drm_device *dev, void *data,
 		if (r)
 			return r;
 
-		resv = vm->root.base.bo->tbo.resv;
+		resv = vm->root.base.bo->tbo.base.resv;
 	}
 
 	r = gsgpu_gem_object_create(adev, size, args->in.alignment,
@@ -482,7 +482,7 @@ int gsgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
 		return -ENOENT;
 	}
 	robj = gem_to_gsgpu_bo(gobj);
-	ret = dma_resv_wait_timeout_rcu(robj->tbo.resv, true, true,
+	ret = dma_resv_wait_timeout_rcu(robj->tbo.base.resv, true, true,
 						  timeout);
 
 	/* ret == 0 means not signaled,
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_kms.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_kms.c
index 1ade91e6a14b..f6f3c082e488 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_kms.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_kms.c
@@ -619,7 +619,7 @@ void gsgpu_driver_postclose_kms(struct drm_device *dev,
 	gsgpu_ctx_mgr_fini(&fpriv->ctx_mgr);
 
 	if (pasid)
-		gsgpu_pasid_free_delayed(pd->tbo.resv, pasid);
+		gsgpu_pasid_free_delayed(pd->tbo.base.resv, pasid);
 	gsgpu_bo_unref(&pd);
 
 	idr_for_each_entry(&fpriv->bo_list_handles, list, handle)
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c
index a6e3a2bf879f..407a6a6c35bd 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c
@@ -179,7 +179,7 @@ static void gsgpu_mn_invalidate_node(struct gsgpu_mn_node *node,
 		if (!gsgpu_ttm_tt_affect_userptr(bo->tbo.ttm, start, end))
 			continue;
 
-		r = dma_resv_wait_timeout_rcu(bo->tbo.resv,
+		r = dma_resv_wait_timeout_rcu(bo->tbo.base.resv,
 			true, false, MAX_SCHEDULE_TIMEOUT);
 		if (r <= 0)
 			DRM_ERROR("(%ld) failed to wait for user bo\n", r);
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c
index 93f966e7d01b..a4d600af27fc 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c
@@ -431,7 +431,7 @@ static int gsgpu_bo_do_create(struct gsgpu_device *adev,
 	if (unlikely(r != 0))
 		return r;
 
-	bo->gem_base.resv = bo->tbo.resv;
+	bo->gem_base.resv = bo->tbo.base.resv;
 
 	if (!gsgpu_gmc_vram_full_visible(&adev->gmc) &&
 	    bo->tbo.mem.mem_type == TTM_PL_VRAM &&
@@ -445,7 +445,7 @@ static int gsgpu_bo_do_create(struct gsgpu_device *adev,
 	    bo->tbo.mem.placement & TTM_PL_FLAG_VRAM) {
 		struct dma_fence *fence;
 
-		r = gsgpu_fill_buffer(bo, 0, bo->tbo.resv, &fence);
+		r = gsgpu_fill_buffer(bo, 0, bo->tbo.base.resv, &fence);
 		if (unlikely(r))
 			goto fail_unreserve;
 
@@ -469,7 +469,7 @@ static int gsgpu_bo_do_create(struct gsgpu_device *adev,
 
 fail_unreserve:
 	if (!bp->resv)
-		ww_mutex_unlock(&bo->tbo.resv->lock);
+		ww_mutex_unlock(&bo->tbo.base.resv->lock);
 	gsgpu_bo_unref(&bo);
 	return r;
 }
@@ -491,7 +491,7 @@ static int gsgpu_bo_create_shadow(struct gsgpu_device *adev,
 	bp.flags = GSGPU_GEM_CREATE_CPU_GTT_USWC |
 		GSGPU_GEM_CREATE_SHADOW;
 	bp.type = ttm_bo_type_kernel;
-	bp.resv = bo->tbo.resv;
+	bp.resv = bo->tbo.base.resv;
 
 	r = gsgpu_bo_do_create(adev, &bp, &bo->shadow);
 	if (!r) {
@@ -532,13 +532,13 @@ int gsgpu_bo_create(struct gsgpu_device *adev,
 
 	if ((flags & GSGPU_GEM_CREATE_SHADOW) && gsgpu_bo_need_backup(adev)) {
 		if (!bp->resv)
-			WARN_ON(dma_resv_lock((*bo_ptr)->tbo.resv,
+			WARN_ON(dma_resv_lock((*bo_ptr)->tbo.base.resv,
 							NULL));
 
 		r = gsgpu_bo_create_shadow(adev, bp->size, bp->byte_align, (*bo_ptr));
 
 		if (!bp->resv)
-			dma_resv_unlock((*bo_ptr)->tbo.resv);
+			dma_resv_unlock((*bo_ptr)->tbo.base.resv);
 
 		if (r)
 			gsgpu_bo_unref(bo_ptr);
@@ -580,7 +580,7 @@ int gsgpu_bo_backup_to_shadow(struct gsgpu_device *adev,
 	bo_addr = gsgpu_bo_gpu_offset(bo);
 	shadow_addr = gsgpu_bo_gpu_offset(bo->shadow);
 
-	r = dma_resv_reserve_shared(bo->tbo.resv);
+	r = dma_resv_reserve_shared(bo->tbo.base.resv);
 	if (r)
 		goto err;
 
@@ -662,7 +662,7 @@ int gsgpu_bo_restore_from_shadow(struct gsgpu_device *adev,
 	bo_addr = gsgpu_bo_gpu_offset(bo);
 	shadow_addr = gsgpu_bo_gpu_offset(bo->shadow);
 
-	r = dma_resv_reserve_shared(bo->tbo.resv);
+	r = dma_resv_reserve_shared(bo->tbo.base.resv);
 	if (r)
 		goto err;
 
@@ -702,7 +702,7 @@ int gsgpu_bo_kmap(struct gsgpu_bo *bo, void **ptr)
 		return 0;
 	}
 
-	r = dma_resv_wait_timeout_rcu(bo->tbo.resv, false, false,
+	r = dma_resv_wait_timeout_rcu(bo->tbo.base.resv, false, false,
 						MAX_SCHEDULE_TIMEOUT);
 	if (r < 0)
 		return r;
@@ -1055,7 +1055,7 @@ int gsgpu_bo_set_tiling_flags(struct gsgpu_bo *bo, u64 tiling_flags)
  */
 void gsgpu_bo_get_tiling_flags(struct gsgpu_bo *bo, u64 *tiling_flags)
 {
-	lockdep_assert_held(&bo->tbo.resv->lock.base);
+	lockdep_assert_held(&bo->tbo.base.resv->lock.base);
 
 	if (tiling_flags)
 		*tiling_flags = bo->tiling_flags;
@@ -1249,7 +1249,7 @@ vm_fault_t gsgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo)
 void gsgpu_bo_fence(struct gsgpu_bo *bo, struct dma_fence *fence,
 		     bool shared)
 {
-	struct dma_resv *resv = bo->tbo.resv;
+	struct dma_resv *resv = bo->tbo.base.resv;
 
 	if (shared)
 		dma_resv_add_shared_fence(resv, fence);
@@ -1272,7 +1272,7 @@ u64 gsgpu_bo_gpu_offset(struct gsgpu_bo *bo)
 	WARN_ON_ONCE(bo->tbo.mem.mem_type == TTM_PL_SYSTEM);
 	WARN_ON_ONCE(bo->tbo.mem.mem_type == TTM_PL_TT &&
 		     !gsgpu_gtt_mgr_has_gart_addr(&bo->tbo.mem));
-	WARN_ON_ONCE(!ww_mutex_is_locked(&bo->tbo.resv->lock) &&
+	WARN_ON_ONCE(!ww_mutex_is_locked(&bo->tbo.base.resv->lock) &&
 		     !bo->tbo.pin_count);
 	WARN_ON_ONCE(bo->tbo.mem.start == GSGPU_BO_INVALID_OFFSET);
 	WARN_ON_ONCE(bo->tbo.mem.mem_type == TTM_PL_VRAM &&
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c
index 767a7a28ae77..449214c38924 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c
@@ -190,7 +190,7 @@ static int gsgpu_gem_map_attach(struct dma_buf *dma_buf,
 		 * fences on the reservation object into a single exclusive
 		 * fence.
 		 */
-		r = __dma_resv_make_exclusive(bo->tbo.resv);
+		r = __dma_resv_make_exclusive(bo->tbo.base.resv);
 		if (r)
 			goto error_unreserve;
 	}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c
index 0fdee7317498..ec54da8330d2 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c
@@ -124,7 +124,7 @@ static void gsgpu_vm_bo_base_init(struct gsgpu_vm_bo_base *base,
 	if (bo->tbo.type == ttm_bo_type_kernel)
 		list_move(&base->vm_status, &vm->relocated);
 
-	if (bo->tbo.resv != vm->root.base.bo->tbo.resv)
+	if (bo->tbo.base.resv != vm->root.base.bo->tbo.base.resv)
 		return;
 
 	if (bo->preferred_domains &
@@ -349,7 +349,7 @@ static int gsgpu_vm_clear_bo(struct gsgpu_device *ldev,
 
 	ring = container_of(vm->entity.rq->sched, struct gsgpu_ring, sched);
 
-	r = dma_resv_reserve_shared(bo->tbo.resv);
+	r = dma_resv_reserve_shared(bo->tbo.base.resv);
 	if (r)
 		return r;
 
@@ -370,7 +370,7 @@ static int gsgpu_vm_clear_bo(struct gsgpu_device *ldev,
 	gsgpu_ring_pad_ib(ring, &job->ibs[0]);
 
 	WARN_ON(job->ibs[0].length_dw > 64);
-	r = gsgpu_sync_resv(ldev, &job->sync, bo->tbo.resv,
+	r = gsgpu_sync_resv(ldev, &job->sync, bo->tbo.base.resv,
 			     GSGPU_FENCE_OWNER_UNDEFINED, false);
 	if (r)
 		goto error_free;
@@ -452,7 +452,7 @@ static int gsgpu_vm_alloc_levels(struct gsgpu_device *ldev,
 	 * walk over the address space and allocate the page tables
 	 */
 	for (pt_idx = from; pt_idx <= to; ++pt_idx) {
-		struct dma_resv *resv = vm->root.base.bo->tbo.resv;
+		struct dma_resv *resv = vm->root.base.bo->tbo.base.resv;
 		struct gsgpu_vm_pt *entry = &parent->entries[pt_idx];
 		struct gsgpu_bo *pt;
 
@@ -827,7 +827,7 @@ static int gsgpu_vm_wait_pd(struct gsgpu_device *ldev, struct gsgpu_vm *vm,
 	int r;
 
 	gsgpu_sync_create(&sync);
-	gsgpu_sync_resv(ldev, &sync, vm->root.base.bo->tbo.resv, owner, false);
+	gsgpu_sync_resv(ldev, &sync, vm->root.base.bo->tbo.base.resv, owner, false);
 	r = gsgpu_sync_wait(&sync, true);
 	gsgpu_sync_free(&sync);
 
@@ -996,7 +996,7 @@ int gsgpu_vm_update_directories(struct gsgpu_device *ldev,
 				    sched);
 
 		gsgpu_ring_pad_ib(ring, params.ib);
-		gsgpu_sync_resv(ldev, &job->sync, root->tbo.resv,
+		gsgpu_sync_resv(ldev, &job->sync, root->tbo.base.resv,
 				 GSGPU_FENCE_OWNER_VM, false);
 		WARN_ON(params.ib->length_dw > ndw);
 		r = gsgpu_job_submit(job, &vm->entity, GSGPU_FENCE_OWNER_VM, &fence);
@@ -1295,12 +1295,12 @@ static int gsgpu_vm_bo_update_mapping(struct gsgpu_device *ldev,
 	if (r)
 		goto error_free;
 
-	r = gsgpu_sync_resv(ldev, &job->sync, vm->root.base.bo->tbo.resv,
+	r = gsgpu_sync_resv(ldev, &job->sync, vm->root.base.bo->tbo.base.resv,
 			     owner, false);
 	if (r)
 		goto error_free;
 
-	r = dma_resv_reserve_shared(vm->root.base.bo->tbo.resv);
+	r = dma_resv_reserve_shared(vm->root.base.bo->tbo.base.resv);
 	if (r)
 		goto error_free;
 
@@ -1448,7 +1448,7 @@ int gsgpu_vm_bo_update(struct gsgpu_device *ldev,
 			pages_addr = ttm->dma_address;
 		}
 
-		exclusive = dma_resv_get_excl(bo->tbo.resv);
+		exclusive = dma_resv_get_excl(bo->tbo.base.resv);
 	}
 
 	if (bo)
@@ -1460,7 +1460,7 @@ int gsgpu_vm_bo_update(struct gsgpu_device *ldev,
 		flags |= (bo->flags & GSGPU_GEM_CREATE_COMPRESSED_MASK)
 				  >> GSGPU_PTE_COMPRESSED_SHIFT;
 
-	if (clear || (bo && bo->tbo.resv == vm->root.base.bo->tbo.resv))
+	if (clear || (bo && bo->tbo.base.resv == vm->root.base.bo->tbo.base.resv))
 		last_update = &vm->last_update;
 	else
 		last_update = &bo_va->last_pt_update;
@@ -1491,7 +1491,7 @@ int gsgpu_vm_bo_update(struct gsgpu_device *ldev,
 	 * the evicted list so that it gets validated again on the
 	 * next command submission.
 	 */
-	if (bo && bo->tbo.resv == vm->root.base.bo->tbo.resv) {
+	if (bo && bo->tbo.base.resv == vm->root.base.bo->tbo.base.resv) {
 		u32 mem_type = bo->tbo.mem.mem_type;
 
 		if (!(bo->preferred_domains & gsgpu_mem_type_to_domain(mem_type)))
@@ -1606,10 +1606,10 @@ int gsgpu_vm_handle_moved(struct gsgpu_device *ldev,
 	spin_unlock(&vm->moved_lock);
 
 	list_for_each_entry_safe(bo_va, tmp, &moved, base.vm_status) {
-		struct dma_resv *resv = bo_va->base.bo->tbo.resv;
+		struct dma_resv *resv = bo_va->base.bo->tbo.base.resv;
 
 		/* Per VM BOs never need to bo cleared in the page tables */
-		if (resv == vm->root.base.bo->tbo.resv)
+		if (resv == vm->root.base.bo->tbo.base.resv)
 			clear = false;
 		/* Try to reserve the BO to avoid clearing its ptes */
 		else if (!gsgpu_vm_debug && dma_resv_trylock(resv))
@@ -1626,7 +1626,7 @@ int gsgpu_vm_handle_moved(struct gsgpu_device *ldev,
 			return r;
 		}
 
-		if (!clear && resv != vm->root.base.bo->tbo.resv)
+		if (!clear && resv != vm->root.base.bo->tbo.base.resv)
 			dma_resv_unlock(resv);
 
 	}
@@ -1689,7 +1689,7 @@ static void gsgpu_vm_bo_insert_map(struct gsgpu_device *ldev,
 	list_add(&mapping->list, &bo_va->invalids);
 	gsgpu_vm_it_insert(mapping, &vm->va);
 
-	if (bo && bo->tbo.resv == vm->root.base.bo->tbo.resv &&
+	if (bo && bo->tbo.base.resv == vm->root.base.bo->tbo.base.resv &&
 	    !bo_va->base.moved) {
 		spin_lock(&vm->moved_lock);
 		list_move(&bo_va->base.vm_status, &vm->moved);
@@ -2019,7 +2019,7 @@ void gsgpu_vm_bo_trace_cs(struct gsgpu_vm *vm, struct ww_acquire_ctx *ticket)
 			struct gsgpu_bo *bo;
 
 			bo = mapping->bo_va->base.bo;
-			if (READ_ONCE(bo->tbo.resv->lock.ctx) != ticket)
+			if (READ_ONCE(bo->tbo.base.resv->lock.ctx) != ticket)
 				continue;
 		}
 
@@ -2090,7 +2090,7 @@ void gsgpu_vm_bo_invalidate(struct gsgpu_device *ldev,
 		bool was_moved = bo_base->moved;
 
 		bo_base->moved = true;
-		if (evicted && bo->tbo.resv == vm->root.base.bo->tbo.resv) {
+		if (evicted && bo->tbo.base.resv == vm->root.base.bo->tbo.base.resv) {
 			if (bo->tbo.type == ttm_bo_type_kernel)
 				list_move(&bo_base->vm_status, &vm->evicted);
 			else
-- 
2.43.0

