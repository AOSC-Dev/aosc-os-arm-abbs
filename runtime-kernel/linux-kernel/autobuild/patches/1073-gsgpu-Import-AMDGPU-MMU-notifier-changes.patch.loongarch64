From 94bfc74fe55217a25d3885e19674fa95bdb34642 Mon Sep 17 00:00:00 2001
From: "Dr. Chang Liu, PhD" <cl91tp@gmail.com>
Date: Wed, 27 Sep 2023 17:30:06 +0800
Subject: [PATCH] gsgpu: Import AMDGPU MMU notifier changes

---
 drivers/gpu/drm/gsgpu/gpu/Makefile            |   2 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c          | 155 +++----
 drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c      |   3 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c         |  18 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_hmm.c         | 218 +++++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c          | 432 ------------------
 drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c      |  12 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c       |   4 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c         | 215 ++++-----
 drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c          |   4 +-
 drivers/gpu/drm/gsgpu/include/gsgpu.h         |   7 +-
 drivers/gpu/drm/gsgpu/include/gsgpu_bo_list.h |   2 +-
 drivers/gpu/drm/gsgpu/include/gsgpu_hmm.h     |  53 +++
 drivers/gpu/drm/gsgpu/include/gsgpu_mn.h      |  36 --
 drivers/gpu/drm/gsgpu/include/gsgpu_object.h  |   5 +-
 drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h     |   7 +-
 16 files changed, 442 insertions(+), 731 deletions(-)
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_hmm.c
 delete mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_hmm.h
 delete mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_mn.h

diff --git a/drivers/gpu/drm/gsgpu/gpu/Makefile b/drivers/gpu/drm/gsgpu/gpu/Makefile
index 3ff7c1936675..4f15a76aef2e 100644
--- a/drivers/gpu/drm/gsgpu/gpu/Makefile
+++ b/drivers/gpu/drm/gsgpu/gpu/Makefile
@@ -47,7 +47,7 @@ gsgpu-y += gsgpu_xdma.o
 gsgpu-y += gsgpu_job.o
 
 gsgpu-$(CONFIG_COMPAT) += gsgpu_ioc32.o
-gsgpu-$(CONFIG_MMU_NOTIFIER) += gsgpu_mn.o
+gsgpu-$(CONFIG_MMU_NOTIFIER) += gsgpu_hmm.o
 
 # add DC block
 gsgpu-y += gsgpu_dc_drv.o gsgpu_dc_crtc.o gsgpu_dc_hdmi.o \
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c
index 166104fe6f6b..e54af8dae0fe 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c
@@ -23,7 +23,6 @@ static int gsgpu_cs_user_fence_chunk(struct gsgpu_cs_parser *p,
 	p->uf_entry.priority = 0;
 	p->uf_entry.tv.bo = &p->uf_entry.robj->tbo;
 	p->uf_entry.tv.shared = true;
-	p->uf_entry.user_pages = NULL;
 
 	drm_gem_object_put_unlocked(gobj);
 
@@ -511,15 +510,13 @@ static int gsgpu_cs_list_validate(struct gsgpu_cs_parser *p,
 			return -EPERM;
 
 		/* Check if we have user pages and nobody bound the BO already */
-		if (gsgpu_ttm_tt_userptr_needs_pages(bo->tbo.ttm) &&
-		    lobj->user_pages) {
-			gsgpu_bo_placement_from_domain(bo,
-							GSGPU_GEM_DOMAIN_CPU);
+		if (gsgpu_ttm_tt_is_userptr(bo->tbo.ttm) &&
+		    lobj->user_invalidated && lobj->user_pages) {
+			gsgpu_bo_placement_from_domain(bo, GSGPU_GEM_DOMAIN_CPU);
 			r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
 			if (r)
 				return r;
-			gsgpu_ttm_tt_set_user_pages(bo->tbo.ttm,
-						     lobj->user_pages);
+			gsgpu_ttm_tt_set_user_pages(bo->tbo.ttm, lobj->user_pages);
 			binding_userptr = true;
 		}
 
@@ -539,14 +536,13 @@ static int gsgpu_cs_list_validate(struct gsgpu_cs_parser *p,
 }
 
 static int gsgpu_cs_parser_bos(struct gsgpu_cs_parser *p,
-				union drm_gsgpu_cs *cs)
+			       union drm_gsgpu_cs *cs)
 {
 	struct gsgpu_fpriv *fpriv = p->filp->driver_priv;
 	struct gsgpu_vm *vm = &fpriv->vm;
 	struct gsgpu_bo_list_entry *e;
 	struct list_head duplicates;
 	unsigned tries = 10;
-	int r;
 
 	INIT_LIST_HEAD(&p->validated);
 
@@ -568,8 +564,6 @@ static int gsgpu_cs_parser_bos(struct gsgpu_cs_parser *p,
 	}
 
 	gsgpu_bo_list_get_list(p->bo_list, &p->validated);
-	if (p->bo_list->first_userptr != p->bo_list->num_entries)
-		p->mn = gsgpu_mn_get(p->adev, GSGPU_MN_TYPE_GFX);
 
 	INIT_LIST_HEAD(&duplicates);
 	gsgpu_vm_get_pd_bo(&fpriv->vm, &p->validated, &p->vm_pd);
@@ -577,79 +571,45 @@ static int gsgpu_cs_parser_bos(struct gsgpu_cs_parser *p,
 	if (p->uf_entry.robj && !p->uf_entry.robj->parent)
 		list_add(&p->uf_entry.tv.head, &p->validated);
 
-	while (1) {
-		struct list_head need_pages;
-
-		r = ttm_eu_reserve_buffers(&p->ticket, &p->validated, true,
-					   &duplicates);
-		if (unlikely(r != 0)) {
-			if (r != -ERESTARTSYS)
-				DRM_ERROR("ttm_eu_reserve_buffers failed.\n");
-			goto error_free_pages;
-		}
-
-		INIT_LIST_HEAD(&need_pages);
-		gsgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {
-			struct gsgpu_bo *bo = e->robj;
-
-			if (gsgpu_ttm_tt_userptr_invalidated(bo->tbo.ttm,
-				 &e->user_invalidated) && e->user_pages) {
-
-				/* We acquired a page array, but somebody
-				 * invalidated it. Free it and try again
-				 */
-				release_pages(e->user_pages,
-					      bo->tbo.ttm->num_pages);
-				kvfree(e->user_pages);
-				e->user_pages = NULL;
-			}
-
-			if (gsgpu_ttm_tt_userptr_needs_pages(bo->tbo.ttm) &&
-			    !e->user_pages) {
-				list_del(&e->tv.head);
-				list_add(&e->tv.head, &need_pages);
-
-				gsgpu_bo_unreserve(e->robj);
-			}
+	/* Get userptr backing pages. If pages are updated after registered
+	 * in gsgpu_gem_userptr_ioctl(), gsgpu_cs_list_validate() will do
+	 * gsgpu_ttm_backend_bind() to flush and invalidate new pages
+	 */
+	gsgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {
+		struct gsgpu_bo *bo = ttm_to_gsgpu_bo(e->tv.bo);
+		bool userpage_invalidated = false;
+		int i;
+
+		e->user_pages = kvmalloc_array(bo->tbo.ttm->num_pages,
+					       sizeof(struct page *),
+					       GFP_KERNEL | __GFP_ZERO);
+		if (!e->user_pages) {
+			DRM_ERROR("calloc failure\n");
+			return -ENOMEM;
 		}
 
-		if (list_empty(&need_pages))
-			break;
-
-		/* Unreserve everything again. */
-		ttm_eu_backoff_reservation(&p->ticket, &p->validated);
-
-		/* We tried too many times, just abort */
-		if (!--tries) {
-			r = -EDEADLK;
-			DRM_ERROR("deadlock in %s\n", __func__);
-			goto error_free_pages;
+		r = gsgpu_ttm_tt_get_user_pages(bo, e->user_pages, &e->range);
+		if (r) {
+			kvfree(e->user_pages);
+			e->user_pages = NULL;
+			return r;
 		}
 
-		/* Fill the page arrays for all userptrs. */
-		list_for_each_entry(e, &need_pages, tv.head) {
-			struct ttm_tt *ttm = e->robj->tbo.ttm;
-
-			e->user_pages = kvmalloc_array(ttm->num_pages,
-							 sizeof(struct page *),
-							 GFP_KERNEL | __GFP_ZERO);
-			if (!e->user_pages) {
-				r = -ENOMEM;
-				DRM_ERROR("calloc failure in %s\n", __func__);
-				goto error_free_pages;
-			}
-
-			r = gsgpu_ttm_tt_get_user_pages(ttm, e->user_pages);
-			if (r) {
-				DRM_ERROR("gsgpu_ttm_tt_get_user_pages failed.\n");
-				kvfree(e->user_pages);
-				e->user_pages = NULL;
-				goto error_free_pages;
+		for (i = 0; i < bo->tbo.ttm->num_pages; i++) {
+			if (bo->tbo.ttm->pages[i] != e->user_pages[i]) {
+				userpage_invalidated = true;
+				break;
 			}
 		}
+		e->user_invalidated = userpage_invalidated;
+	}
 
-		/* And try again. */
-		list_splice(&need_pages, &p->validated);
+	r = ttm_eu_reserve_buffers(&p->ticket, &p->validated, true,
+				   &duplicates);
+	if (unlikely(r != 0)) {
+		if (r != -ERESTARTSYS)
+			DRM_ERROR("ttm_eu_reserve_buffers failed.\n");
+		goto out;
 	}
 
 	gsgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,
@@ -695,18 +655,7 @@ static int gsgpu_cs_parser_bos(struct gsgpu_cs_parser *p,
 error_validate:
 	if (r)
 		ttm_eu_backoff_reservation(&p->ticket, &p->validated);
-
-error_free_pages:
-
-	gsgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {
-		if (!e->user_pages)
-			continue;
-
-		release_pages(e->user_pages,
-			      e->robj->tbo.ttm->num_pages);
-		kvfree(e->user_pages);
-	}
-
+out:
 	return r;
 }
 
@@ -1065,7 +1014,6 @@ static int gsgpu_cs_submit(struct gsgpu_cs_parser *p,
 	struct gsgpu_bo_list_entry *e;
 	struct gsgpu_job *job;
 	uint64_t seq;
-
 	int r;
 
 	job = p->job;
@@ -1075,15 +1023,22 @@ static int gsgpu_cs_submit(struct gsgpu_cs_parser *p,
 	if (r)
 		goto error_unlock;
 
-	/* No memory allocation is allowed while holding the mn lock */
-	gsgpu_mn_lock(p->mn);
+	/* No memory allocation is allowed while holding the notifier lock.
+	 * The lock is held until gsgpu_cs_submit is finished and fence is
+	 * added to BOs.
+         */
+	mutex_lock(&p->adev->notifier_lock);
+
+	/* If userptr are invalidated after gsgpu_cs_parser_bos(), return
+	 * -EAGAIN, drmIoctl in libdrm will restart the gsgpu_cs_ioctl.
+	 */
 	gsgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {
 		struct gsgpu_bo *bo = e->robj;
-
-		if (gsgpu_ttm_tt_userptr_needs_pages(bo->tbo.ttm)) {
-			r = -ERESTARTSYS;
-			goto error_abort;
-		}
+		r |= !gsgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, &e->range);
+	}
+	if (r) {
+		r = -EAGAIN;
+		goto error_abort;
 	}
 
 	job->owner = p->filp;
@@ -1094,7 +1049,7 @@ static int gsgpu_cs_submit(struct gsgpu_cs_parser *p,
 		dma_fence_put(p->fence);
 		dma_fence_put(&job->base.s_fence->finished);
 		gsgpu_job_free(job);
-		gsgpu_mn_unlock(p->mn);
+		mutex_unlock(&p->adev->notifier_lock);
 		return r;
 	}
 
@@ -1120,14 +1075,14 @@ static int gsgpu_cs_submit(struct gsgpu_cs_parser *p,
 	gsgpu_ring_priority_get(ring, priority);
 
 	ttm_eu_fence_buffer_objects(&p->ticket, &p->validated, p->fence);
-	gsgpu_mn_unlock(p->mn);
+	mutex_unlock(&p->adev->notifier_lock);
 
 	return 0;
 
 error_abort:
 	dma_fence_put(&job->base.s_fence->finished);
 	job->base.s_fence = NULL;
-	gsgpu_mn_unlock(p->mn);
+	mutex_unlock(&p->adev->notifier_lock);
 
 error_unlock:
 	gsgpu_job_free(job);
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c
index 5fa6dd7b48bb..3f3b03004a26 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c
@@ -1279,8 +1279,7 @@ int gsgpu_device_init(struct gsgpu_device *adev,
 	mutex_init(&adev->gfx.gpu_clock_mutex);
 	mutex_init(&adev->srbm_mutex);
 	mutex_init(&adev->grbm_idx_mutex);
-	mutex_init(&adev->mn_lock);
-	hash_init(adev->mn_hash);
+	mutex_init(&adev->notifier_lock);
 	mutex_init(&adev->lock_reset);
 	spin_lock_init(&adev->dc_mmio_lock);
 
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c
index b5d551b2ea09..5be654e56a5f 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c
@@ -49,7 +49,7 @@ static void gsgpu_gem_object_free(struct drm_gem_object *gobj)
 	struct gsgpu_bo *robj = gem_to_gsgpu_bo(gobj);
 
 	if (robj) {
-		gsgpu_mn_unregister(robj);
+		gsgpu_hmm_unregister(robj);
 		gsgpu_bo_unref(&robj);
 	}
 }
@@ -366,7 +366,7 @@ int gsgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
 		goto release_object;
 
 	if (args->flags & GSGPU_GEM_USERPTR_REGISTER) {
-		r = gsgpu_mn_register(bo, args->addr);
+		r = gsgpu_hmm_register(bo, args->addr);
 		if (r)
 			goto release_object;
 	}
@@ -379,26 +379,24 @@ int gsgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
 
 		r = gsgpu_bo_reserve(bo, true);
 		if (r)
-			goto free_pages;
+			goto user_pages_done;
 
 		gsgpu_bo_placement_from_domain(bo, GSGPU_GEM_DOMAIN_GTT);
 		r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
 		gsgpu_bo_unreserve(bo);
 		if (r)
-			goto free_pages;
+			goto user_pages_done;
 	}
 
 	r = drm_gem_handle_create(filp, gobj, &handle);
-	/* drop reference from allocate - handle holds it now */
-	drm_gem_object_put_unlocked(gobj);
 	if (r)
-		return r;
+		goto user_pages_done;
 
 	args->handle = handle;
-	return 0;
 
-free_pages:
-	release_pages(bo->tbo.ttm->pages, bo->tbo.ttm->num_pages);
+user_pages_done:
+	if (args->flags & GSGPU_GEM_USERPTR_VALIDATE)
+		gsgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm);
 
 release_object:
 	drm_gem_object_put_unlocked(gobj);
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_hmm.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_hmm.c
new file mode 100644
index 000000000000..1768810772aa
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_hmm.c
@@ -0,0 +1,218 @@
+/*
+ * Copyright 2014 Advanced Micro Devices, Inc.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ */
+/*
+ * Authors:
+ *    Christian König <christian.koenig@amd.com>
+ */
+
+/**
+ * DOC: MMU Notifier
+ *
+ * For coherent userptr handling registers an MMU notifier to inform the driver
+ * about updates on the page tables of a process.
+ *
+ * When somebody tries to invalidate the page tables we block the update until
+ * all operations on the pages in question are completed, then those pages are
+ * marked as accessed and also dirty if it wasn't a read only access.
+ *
+ * New command submissions using the userptrs in question are delayed until all
+ * page table invalidation are completed and we once more see a coherent process
+ * address space.
+ */
+
+#include <linux/firmware.h>
+#include <linux/module.h>
+#include <drm/drm.h>
+
+#include "gsgpu.h"
+#include "gsgpu_hmm.h"
+
+#define MAX_WALK_BYTE	(2UL << 30)
+
+/**
+ * gsgpu_hmm_invalidate_gfx - callback to notify about mm change
+ *
+ * @mni: the range (mm) is about to update
+ * @range: details on the invalidation
+ * @cur_seq: Value to pass to mmu_interval_set_seq()
+ *
+ * Block for operations on BOs to finish and mark pages as accessed and
+ * potentially dirty.
+ */
+static bool gsgpu_hmm_invalidate_gfx(struct mmu_interval_notifier *mni,
+				     const struct mmu_notifier_range *range,
+				     unsigned long cur_seq)
+{
+	struct gsgpu_bo *bo = container_of(mni, struct gsgpu_bo, notifier);
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	long r;
+
+	if (!mmu_notifier_range_blockable(range))
+		return false;
+
+	mutex_lock(&adev->notifier_lock);
+
+	mmu_interval_set_seq(mni, cur_seq);
+
+	r = dma_resv_wait_timeout(bo->tbo.base.resv, DMA_RESV_USAGE_BOOKKEEP,
+				  false, MAX_SCHEDULE_TIMEOUT);
+	mutex_unlock(&adev->notifier_lock);
+	if (r <= 0)
+		DRM_ERROR("(%ld) failed to wait for user bo\n", r);
+	return true;
+}
+
+static const struct mmu_interval_notifier_ops gsgpu_hmm_gfx_ops = {
+	.invalidate = gsgpu_hmm_invalidate_gfx,
+};
+
+/**
+ * gsgpu_hmm_register - register a BO for notifier updates
+ *
+ * @bo: gsgpu buffer object
+ * @addr: userptr addr we should monitor
+ *
+ * Registers a mmu_notifier for the given BO at the specified address.
+ * Returns 0 on success, -ERRNO if anything goes wrong.
+ */
+int gsgpu_hmm_register(struct gsgpu_bo *bo, unsigned long addr)
+{
+	return mmu_interval_notifier_insert(&bo->notifier, current->mm, addr,
+					    gsgpu_bo_size(bo),
+					    &gsgpu_hmm_gfx_ops);
+}
+
+/**
+ * gsgpu_hmm_unregister - unregister a BO for notifier updates
+ *
+ * @bo: gsgpu buffer object
+ *
+ * Remove any registration of mmu notifier updates from the buffer object.
+ */
+void gsgpu_hmm_unregister(struct gsgpu_bo *bo)
+{
+	if (!bo->notifier.mm)
+		return;
+	mmu_interval_notifier_remove(&bo->notifier);
+	bo->notifier.mm = NULL;
+}
+
+int gsgpu_hmm_range_get_pages(struct mmu_interval_notifier *notifier,
+			      uint64_t start, uint64_t npages, bool readonly,
+			      void *owner, struct page **pages,
+			      struct hmm_range **phmm_range)
+{
+	struct hmm_range *hmm_range;
+	unsigned long end;
+	unsigned long timeout;
+	unsigned long i;
+	unsigned long *pfns;
+	int r = 0;
+
+	hmm_range = kzalloc(sizeof(*hmm_range), GFP_KERNEL);
+	if (unlikely(!hmm_range))
+		return -ENOMEM;
+
+	pfns = kvmalloc_array(npages, sizeof(*pfns), GFP_KERNEL);
+	if (unlikely(!pfns)) {
+		r = -ENOMEM;
+		goto out_free_range;
+	}
+
+	hmm_range->notifier = notifier;
+	hmm_range->default_flags = HMM_PFN_REQ_FAULT;
+	if (!readonly)
+		hmm_range->default_flags |= HMM_PFN_REQ_WRITE;
+	hmm_range->hmm_pfns = pfns;
+	hmm_range->start = start;
+	end = start + npages * PAGE_SIZE;
+	hmm_range->dev_private_owner = owner;
+
+	do {
+		hmm_range->end = min(hmm_range->start + MAX_WALK_BYTE, end);
+
+		pr_debug("hmm range: start = 0x%lx, end = 0x%lx",
+			 hmm_range->start, hmm_range->end);
+
+		/* Assuming 128MB takes maximum 1 second to fault page address */
+		timeout = max((hmm_range->end - hmm_range->start) >> 27, 1UL);
+		timeout *= HMM_RANGE_DEFAULT_TIMEOUT;
+		timeout = jiffies + msecs_to_jiffies(timeout);
+
+	retry:
+		hmm_range->notifier_seq = mmu_interval_read_begin(notifier);
+		r = hmm_range_fault(hmm_range);
+		if (unlikely(r)) {
+			/*
+			 * FIXME: This timeout should encompass the retry from
+			 * mmu_interval_read_retry() as well.
+			 */
+			if (r == -EBUSY && !time_after(jiffies, timeout))
+				goto retry;
+			goto out_free_pfns;
+		}
+
+		if (hmm_range->end == end)
+			break;
+		hmm_range->hmm_pfns += MAX_WALK_BYTE >> PAGE_SHIFT;
+		hmm_range->start = hmm_range->end;
+		schedule();
+	} while (hmm_range->end < end);
+
+	hmm_range->start = start;
+	hmm_range->hmm_pfns = pfns;
+
+	/*
+	 * Due to default_flags, all pages are HMM_PFN_VALID or
+	 * hmm_range_fault() fails. FIXME: The pages cannot be touched outside
+	 * the notifier_lock, and mmu_interval_read_retry() must be done first.
+	 */
+	for (i = 0; pages && i < npages; i++)
+		pages[i] = hmm_pfn_to_page(pfns[i]);
+
+	*phmm_range = hmm_range;
+
+	return 0;
+
+out_free_pfns:
+	kvfree(pfns);
+out_free_range:
+	kfree(hmm_range);
+
+	return r;
+}
+
+bool gsgpu_hmm_range_get_pages_done(struct hmm_range *hmm_range)
+{
+	bool r;
+
+	r = mmu_interval_read_retry(hmm_range->notifier,
+				    hmm_range->notifier_seq);
+	kvfree(hmm_range->hmm_pfns);
+	kfree(hmm_range);
+
+	return r;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c
deleted file mode 100644
index 3aedf500298b..000000000000
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c
+++ /dev/null
@@ -1,432 +0,0 @@
-#include <linux/firmware.h>
-#include <linux/module.h>
-#include <linux/mmu_notifier.h>
-#include <linux/interval_tree.h>
-#include <drm/drm.h>
-
-#include "gsgpu.h"
-
-/**
- * struct gsgpu_mn
- *
- * @adev: gsgpu device pointer
- * @mm: process address space
- * @mn: MMU notifier structure
- * @type: type of MMU notifier
- * @work: destruction work item
- * @node: hash table node to find structure by adev and mn
- * @lock: rw semaphore protecting the notifier nodes
- * @objects: interval tree containing gsgpu_mn_nodes
- * @read_lock: mutex for recursive locking of @lock
- * @recursion: depth of recursion
- *
- * Data for each gsgpu device and process address space.
- */
-struct gsgpu_mn {
-	/* constant after initialisation */
-	struct gsgpu_device	*adev;
-	struct mm_struct	*mm;
-	struct mmu_notifier	mn;
-	enum gsgpu_mn_type	type;
-
-	/* only used on destruction */
-	struct work_struct	work;
-
-	/* protected by adev->mn_lock */
-	struct hlist_node	node;
-
-	/* objects protected by lock */
-	struct rw_semaphore	lock;
-	struct rb_root_cached	objects;
-	struct mutex		read_lock;
-	atomic_t		recursion;
-};
-
-/**
- * struct gsgpu_mn_node
- *
- * @it: interval node defining start-last of the affected address range
- * @bos: list of all BOs in the affected address range
- *
- * Manages all BOs which are affected of a certain range of address space.
- */
-struct gsgpu_mn_node {
-	struct interval_tree_node	it;
-	struct list_head		bos;
-};
-
-/**
- * gsgpu_mn_destroy - destroy the MMU notifier
- *
- * @work: previously sheduled work item
- *
- * Lazy destroys the notifier from a work item
- */
-static void gsgpu_mn_destroy(struct work_struct *work)
-{
-	struct gsgpu_mn *amn = container_of(work, struct gsgpu_mn, work);
-	struct gsgpu_device *adev = amn->adev;
-	struct gsgpu_mn_node *node, *next_node;
-	struct gsgpu_bo *bo, *next_bo;
-
-	mutex_lock(&adev->mn_lock);
-	down_write(&amn->lock);
-	hash_del(&amn->node);
-	rbtree_postorder_for_each_entry_safe(node, next_node,
-					     &amn->objects.rb_root, it.rb) {
-		list_for_each_entry_safe(bo, next_bo, &node->bos, mn_list) {
-			bo->mn = NULL;
-			list_del_init(&bo->mn_list);
-		}
-		kfree(node);
-	}
-	up_write(&amn->lock);
-	mutex_unlock(&adev->mn_lock);
-	mmu_notifier_unregister_no_release(&amn->mn, amn->mm);
-	kfree(amn);
-}
-
-/**
- * gsgpu_mn_release - callback to notify about mm destruction
- *
- * @mn: our notifier
- * @mm: the mm this callback is about
- *
- * Shedule a work item to lazy destroy our notifier.
- */
-static void gsgpu_mn_release(struct mmu_notifier *mn,
-			      struct mm_struct *mm)
-{
-	struct gsgpu_mn *amn = container_of(mn, struct gsgpu_mn, mn);
-
-	INIT_WORK(&amn->work, gsgpu_mn_destroy);
-	schedule_work(&amn->work);
-}
-
-
-/**
- * gsgpu_mn_lock - take the write side lock for this notifier
- *
- * @mn: our notifier
- */
-void gsgpu_mn_lock(struct gsgpu_mn *mn)
-{
-	if (mn)
-		down_write(&mn->lock);
-}
-
-/**
- * gsgpu_mn_unlock - drop the write side lock for this notifier
- *
- * @mn: our notifier
- */
-void gsgpu_mn_unlock(struct gsgpu_mn *mn)
-{
-	if (mn)
-		up_write(&mn->lock);
-}
-
-/**
- * gsgpu_mn_read_lock - take the read side lock for this notifier
- *
- * @amn: our notifier
- */
-static int gsgpu_mn_read_lock(struct gsgpu_mn *amn, bool blockable)
-{
-	if (blockable)
-		mutex_lock(&amn->read_lock);
-	else if (!mutex_trylock(&amn->read_lock))
-		return -EAGAIN;
-
-	if (atomic_inc_return(&amn->recursion) == 1)
-		down_read_non_owner(&amn->lock);
-	mutex_unlock(&amn->read_lock);
-
-	return 0;
-}
-
-/**
- * gsgpu_mn_read_unlock - drop the read side lock for this notifier
- *
- * @amn: our notifier
- */
-static void gsgpu_mn_read_unlock(struct gsgpu_mn *amn)
-{
-	if (atomic_dec_return(&amn->recursion) == 0)
-		up_read_non_owner(&amn->lock);
-}
-
-/**
- * gsgpu_mn_invalidate_node - unmap all BOs of a node
- *
- * @node: the node with the BOs to unmap
- * @start: start of address range affected
- * @end: end of address range affected
- *
- * Block for operations on BOs to finish and mark pages as accessed and
- * potentially dirty.
- */
-static void gsgpu_mn_invalidate_node(struct gsgpu_mn_node *node,
-				      unsigned long start,
-				      unsigned long end)
-{
-	struct gsgpu_bo *bo;
-	long r;
-
-	list_for_each_entry(bo, &node->bos, mn_list) {
-
-		if (!gsgpu_ttm_tt_affect_userptr(bo->tbo.ttm, start, end))
-			continue;
-
-		r = dma_resv_wait_timeout_rcu(bo->tbo.base.resv,
-			true, false, MAX_SCHEDULE_TIMEOUT);
-		if (r <= 0)
-			DRM_ERROR("(%ld) failed to wait for user bo\n", r);
-
-		gsgpu_ttm_tt_mark_user_pages(bo->tbo.ttm);
-	}
-}
-
-/**
- * gsgpu_mn_invalidate_range_start_gfx - callback to notify about mm change
- *
- * @mn: our notifier
- * @mm: the mm this callback is about
- * @start: start of updated range
- * @end: end of updated range
- *
- * Block for operations on BOs to finish and mark pages as accessed and
- * potentially dirty.
- */
-static int gsgpu_mn_invalidate_range_start_gfx(struct mmu_notifier *mn,
-						 struct mm_struct *mm,
-						 unsigned long start,
-						 unsigned long end,
-						 bool blockable)
-{
-	struct gsgpu_mn *amn = container_of(mn, struct gsgpu_mn, mn);
-	struct interval_tree_node *it;
-
-	/* notification is exclusive, but interval is inclusive */
-	end -= 1;
-
-	/* TODO we should be able to split locking for interval tree and
-	 * gsgpu_mn_invalidate_node
-	 */
-	if (gsgpu_mn_read_lock(amn, blockable))
-		return -EAGAIN;
-
-	it = interval_tree_iter_first(&amn->objects, start, end);
-	while (it) {
-		struct gsgpu_mn_node *node;
-
-		if (!blockable) {
-			gsgpu_mn_read_unlock(amn);
-			return -EAGAIN;
-		}
-
-		node = container_of(it, struct gsgpu_mn_node, it);
-		it = interval_tree_iter_next(it, start, end);
-
-		gsgpu_mn_invalidate_node(node, start, end);
-	}
-
-	return 0;
-}
-
-/**
- * gsgpu_mn_invalidate_range_end - callback to notify about mm change
- *
- * @mn: our notifier
- * @mm: the mm this callback is about
- * @start: start of updated range
- * @end: end of updated range
- *
- * Release the lock again to allow new command submissions.
- */
-static void gsgpu_mn_invalidate_range_end(struct mmu_notifier *mn,
-					   struct mm_struct *mm,
-					   unsigned long start,
-					   unsigned long end)
-{
-	struct gsgpu_mn *amn = container_of(mn, struct gsgpu_mn, mn);
-
-	gsgpu_mn_read_unlock(amn);
-}
-
-static const struct mmu_notifier_ops gsgpu_mn_ops[] = {
-	[GSGPU_MN_TYPE_GFX] = {
-		.release = gsgpu_mn_release,
-		.invalidate_range_start = gsgpu_mn_invalidate_range_start_gfx,
-		.invalidate_range_end = gsgpu_mn_invalidate_range_end,
-	},
-};
-
-/* Low bits of any reasonable mm pointer will be unused due to struct
- * alignment. Use these bits to make a unique key from the mm pointer
- * and notifier type.
- */
-#define GSGPU_MN_KEY(mm, type) ((unsigned long)(mm) + (type))
-
-/**
- * gsgpu_mn_get - create notifier context
- *
- * @adev: gsgpu device pointer
- * @type: type of MMU notifier context
- *
- * Creates a notifier context for current->mm.
- */
-struct gsgpu_mn *gsgpu_mn_get(struct gsgpu_device *adev,
-				enum gsgpu_mn_type type)
-{
-	struct mm_struct *mm = current->mm;
-	struct gsgpu_mn *amn;
-	unsigned long key = GSGPU_MN_KEY(mm, type);
-	int r;
-
-	mutex_lock(&adev->mn_lock);
-	if (down_write_killable(&mm->mmap_sem)) {
-		mutex_unlock(&adev->mn_lock);
-		return ERR_PTR(-EINTR);
-	}
-
-	hash_for_each_possible(adev->mn_hash, amn, node, key)
-		if (GSGPU_MN_KEY(amn->mm, amn->type) == key)
-			goto release_locks;
-
-	amn = kzalloc(sizeof(*amn), GFP_KERNEL);
-	if (!amn) {
-		amn = ERR_PTR(-ENOMEM);
-		goto release_locks;
-	}
-
-	amn->adev = adev;
-	amn->mm = mm;
-	init_rwsem(&amn->lock);
-	amn->type = type;
-	amn->mn.ops = &gsgpu_mn_ops[type];
-	amn->objects = RB_ROOT_CACHED;
-	mutex_init(&amn->read_lock);
-	atomic_set(&amn->recursion, 0);
-
-	r = __mmu_notifier_register(&amn->mn, mm);
-	if (r)
-		goto free_amn;
-
-	hash_add(adev->mn_hash, &amn->node, GSGPU_MN_KEY(mm, type));
-
-release_locks:
-	up_write(&mm->mmap_sem);
-	mutex_unlock(&adev->mn_lock);
-
-	return amn;
-
-free_amn:
-	up_write(&mm->mmap_sem);
-	mutex_unlock(&adev->mn_lock);
-	kfree(amn);
-
-	return ERR_PTR(r);
-}
-
-/**
- * gsgpu_mn_register - register a BO for notifier updates
- *
- * @bo: gsgpu buffer object
- * @addr: userptr addr we should monitor
- *
- * Registers an MMU notifier for the given BO at the specified address.
- * Returns 0 on success, -ERRNO if anything goes wrong.
- */
-int gsgpu_mn_register(struct gsgpu_bo *bo, unsigned long addr)
-{
-	unsigned long end = addr + gsgpu_bo_size(bo) - 1;
-	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
-	enum gsgpu_mn_type type = GSGPU_MN_TYPE_GFX;
-	struct gsgpu_mn *amn;
-	struct gsgpu_mn_node *node = NULL, *new_node;
-	struct list_head bos;
-	struct interval_tree_node *it;
-
-	amn = gsgpu_mn_get(adev, type);
-	if (IS_ERR(amn))
-		return PTR_ERR(amn);
-
-	new_node = kmalloc(sizeof(*new_node), GFP_KERNEL);
-	if (!new_node)
-		return -ENOMEM;
-
-	INIT_LIST_HEAD(&bos);
-
-	down_write(&amn->lock);
-
-	while ((it = interval_tree_iter_first(&amn->objects, addr, end))) {
-		kfree(node);
-		node = container_of(it, struct gsgpu_mn_node, it);
-		interval_tree_remove(&node->it, &amn->objects);
-		addr = min(it->start, addr);
-		end = max(it->last, end);
-		list_splice(&node->bos, &bos);
-	}
-
-	if (!node)
-		node = new_node;
-	else
-		kfree(new_node);
-
-	bo->mn = amn;
-
-	node->it.start = addr;
-	node->it.last = end;
-	INIT_LIST_HEAD(&node->bos);
-	list_splice(&bos, &node->bos);
-	list_add(&bo->mn_list, &node->bos);
-
-	interval_tree_insert(&node->it, &amn->objects);
-
-	up_write(&amn->lock);
-
-	return 0;
-}
-
-/**
- * gsgpu_mn_unregister - unregister a BO for notifier updates
- *
- * @bo: gsgpu buffer object
- *
- * Remove any registration of MMU notifier updates from the buffer object.
- */
-void gsgpu_mn_unregister(struct gsgpu_bo *bo)
-{
-	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
-	struct gsgpu_mn *amn;
-	struct list_head *head;
-
-	mutex_lock(&adev->mn_lock);
-
-	amn = bo->mn;
-	if (amn == NULL) {
-		mutex_unlock(&adev->mn_lock);
-		return;
-	}
-
-	down_write(&amn->lock);
-
-	/* save the next list entry for later */
-	head = bo->mn_list.next;
-
-	bo->mn = NULL;
-	list_del_init(&bo->mn_list);
-
-	if (list_empty(head)) {
-		struct gsgpu_mn_node *node;
-
-		node = container_of(head, struct gsgpu_mn_node, bos);
-		interval_tree_remove(&node->it, &amn->objects);
-		kfree(node);
-	}
-
-	up_write(&amn->lock);
-	mutex_unlock(&adev->mn_lock);
-}
-
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c
index e019c9c353c2..92b301b7bd81 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c
@@ -687,7 +687,7 @@ int gsgpu_bo_kmap(struct gsgpu_bo *bo, void **ptr)
 	if (r < 0)
 		return r;
 
-	r = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.num_pages, &bo->kmap);
+	r = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.ttm->num_pages, &bo->kmap);
 	if (r)
 		return r;
 
@@ -818,7 +818,7 @@ int gsgpu_bo_pin_restricted(struct gsgpu_bo *bo, u32 domain,
 		ttm_bo_pin(&bo->tbo);
 
 		if (max_offset != 0) {
-			u64 domain_start = amdgpu_ttm_domain_start(adev, mem_type);
+			u64 domain_start = gsgpu_ttm_domain_start(adev, mem_type);
 			WARN_ON_ONCE(max_offset < (gsgpu_bo_gpu_offset(bo) - domain_start));
 		}
 
@@ -1136,7 +1136,7 @@ void gsgpu_bo_move_notify(struct ttm_buffer_object *bo,
 {
 	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->bdev);
 	struct gsgpu_bo *abo;
-	struct ttm_resource *old_mem = &bo->mem;
+	struct ttm_resource *old_mem = bo->resource;
 
 	if (!gsgpu_bo_is_gsgpu_bo(bo))
 		return;
@@ -1183,8 +1183,8 @@ vm_fault_t gsgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo)
 	if (bo->resource->mem_type != TTM_PL_VRAM)
 		return 0;
 
-	size = bo->mem.num_pages << PAGE_SHIFT;
-	offset = bo->mem.start << PAGE_SHIFT;
+	size = bo->ttm->num_pages << PAGE_SHIFT;
+	offset = bo->resource->start << PAGE_SHIFT;
 	if ((offset + size) <= adev->gmc.visible_vram_size)
 		return 0;
 
@@ -1207,7 +1207,7 @@ vm_fault_t gsgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo)
 	else if (unlikely(r))
 		return VM_FAULT_SIGBUS;
 
-	offset = bo->mem.start << PAGE_SHIFT;
+	offset = bo->resource->start << PAGE_SHIFT;
 	/* this should never happen */
 	if (bo->resource->mem_type == TTM_PL_VRAM &&
 	    (offset + size) > adev->gmc.visible_vram_size)
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c
index 2407398df5a3..ed4cfd124be6 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c
@@ -17,7 +17,7 @@ static const struct dma_buf_ops gsgpu_dmabuf_ops;
 struct sg_table *gsgpu_gem_prime_get_sg_table(struct drm_gem_object *obj)
 {
 	struct gsgpu_bo *bo = gem_to_gsgpu_bo(obj);
-	int npages = bo->tbo.num_pages;
+	int npages = bo->tbo.ttm->num_pages;
 
 	return drm_prime_pages_to_sg(bo->tbo.ttm->pages, npages);
 }
@@ -36,7 +36,7 @@ void *gsgpu_gem_prime_vmap(struct drm_gem_object *obj)
 	struct gsgpu_bo *bo = gem_to_gsgpu_bo(obj);
 	int ret;
 
-	ret = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.num_pages,
+	ret = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.ttm->num_pages,
 			  &bo->dma_buf_vmap);
 	if (ret)
 		return ERR_PTR(ret);
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c
index 929a99ea49a2..d3fcd0ab0c82 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c
@@ -9,6 +9,7 @@
 #include <linux/pagemap.h>
 #include <linux/debugfs.h>
 #include <linux/iommu.h>
+#include <linux/hmm.h>
 #include "gsgpu.h"
 #include "gsgpu_object.h"
 #include "gsgpu_trace.h"
@@ -28,6 +29,8 @@ static int gsgpu_ttm_backend_unbind(struct ttm_device *bdev,
 static int gsgpu_ttm_debugfs_init(struct gsgpu_device *adev);
 static void gsgpu_ttm_debugfs_fini(struct gsgpu_device *adev);
 
+#define ttm_to_gsgpu_ttm_tt(ptr)	container_of(ptr, struct gsgpu_ttm_tt, ttm)
+
 /**
  * gsgpu_evict_flags - Compute placement flags
  *
@@ -477,99 +480,94 @@ static unsigned long gsgpu_ttm_io_mem_pfn(struct ttm_buffer_object *bo,
 /*
  * TTM backend functions.
  */
-struct gsgpu_ttm_gup_task_list {
-	struct list_head	list;
-	struct task_struct	*task;
-};
-
 struct gsgpu_ttm_tt {
 	struct ttm_tt		ttm;
 	u64			offset;
 	uint64_t		userptr;
 	struct task_struct	*usertask;
 	uint32_t		userflags;
-	spinlock_t              guptasklock;
-	struct list_head        guptasks;
-	atomic_t		mmu_invalidations;
-	uint32_t		last_set_pages;
+	struct hmm_range        *ranges;
+	int                     nr_ranges;
 };
 
+/* Support Userptr pages cross max 16 vmas */
+#define MAX_NR_VMAS    (16)
+
 /**
- * gsgpu_ttm_tt_get_user_pages - Pin pages of memory pointed to by a USERPTR
- * pointer to memory
+ * gsgpu_ttm_tt_get_user_pages - get device accessible pages that back user
+ * memory and start HMM tracking CPU page table update
  *
- * Called by gsgpu_gem_userptr_ioctl() and gsgpu_cs_parser_bos().
- * This provides a wrapper around the get_user_pages() call to provide
- * device accessible pages that back user memory.
+ * Calling function must call gsgpu_ttm_tt_userptr_range_done() once and only
+ * once afterwards to stop HMM tracking
  */
-int gsgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)
+int gsgpu_ttm_tt_get_user_pages(struct gsgpu_bo *bo, struct page **pages,
+				struct hmm_range **range)
 {
-	struct gsgpu_ttm_tt *gtt = (void *)ttm;
-	struct mm_struct *mm = gtt->usertask->mm;
-	unsigned int flags = 0;
-	unsigned pinned = 0;
-	int r;
-
-	if (!mm) /* Happens during process shutdown */
-		return -ESRCH;
+	struct ttm_tt *ttm = bo->tbo.ttm;
+	struct gsgpu_ttm_tt *gtt = ttm_to_gsgpu_ttm_tt(ttm);
+	unsigned long start = gtt->userptr;
+	struct vm_area_struct *vma;
+	struct mm_struct *mm;
+	bool readonly;
+	int r = 0;
 
-	if (!(gtt->userflags & GSGPU_GEM_USERPTR_READONLY))
-		flags |= FOLL_WRITE;
+	/* Make sure get_user_pages_done() can cleanup gracefully */
+	*range = NULL;
 
-	mmap_read_lock(mm);
+	mm = bo->notifier.mm;
+	if (unlikely(!mm)) {
+		DRM_DEBUG_DRIVER("BO is not registered?\n");
+		return -EFAULT;
+	}
 
-	if (gtt->userflags & GSGPU_GEM_USERPTR_ANONONLY) {
-		/*
-		 * check that we only use anonymous memory to prevent problems
-		 * with writeback
-		 */
-		unsigned long end = gtt->userptr + ttm->num_pages * PAGE_SIZE;
-		struct vm_area_struct *vma;
+	if (!mmget_not_zero(mm)) /* Happens during process shutdown */
+		return -ESRCH;
 
-		vma = find_vma(mm, gtt->userptr);
-		if (!vma || vma->vm_file || vma->vm_end < end) {
-			mmap_read_unlock(mm);;
-			return -EPERM;
-		}
+	mmap_read_lock(mm);
+	vma = vma_lookup(mm, start);
+	if (unlikely(!vma)) {
+		r = -EFAULT;
+		goto out_unlock;
+	}
+	if (unlikely((gtt->userflags & GSGPU_GEM_USERPTR_ANONONLY) &&
+		     vma->vm_file)) {
+		r = -EPERM;
+		goto out_unlock;
 	}
 
-	/* loop enough times using contiguous pages of memory */
-	do {
-		unsigned num_pages = ttm->num_pages - pinned;
-		uint64_t userptr = gtt->userptr + pinned * PAGE_SIZE;
-		struct page **p = pages + pinned;
-		struct gsgpu_ttm_gup_task_list guptask;
-
-		guptask.task = current;
-		spin_lock(&gtt->guptasklock);
-		list_add(&guptask.list, &gtt->guptasks);
-		spin_unlock(&gtt->guptasklock);
+	readonly = gsgpu_ttm_tt_is_readonly(ttm);
+	r = gsgpu_hmm_range_get_pages(&bo->notifier, start, ttm->num_pages,
+				      readonly, NULL, pages, range);
+out_unlock:
+	mmap_read_unlock(mm);
+	if (r)
+		pr_debug("failed %d to get user pages 0x%lx\n", r, start);
 
-		if (mm == current->mm)
-			r = get_user_pages(userptr, num_pages, flags, p, NULL);
-		else
-			r = get_user_pages_remote(gtt->usertask,
-						  mm, userptr, num_pages,
-						  flags, p, NULL, NULL);
+	mmput(mm);
 
-		spin_lock(&gtt->guptasklock);
-		list_del(&guptask.list);
-		spin_unlock(&gtt->guptasklock);
+	return r;
+}
 
-		if (r < 0)
-			goto release_pages;
+/**
+ * gsgpu_ttm_tt_userptr_range_done - stop HMM track the CPU page table change
+ * Check if the pages backing this ttm range have been invalidated
+ *
+ * Returns: true if pages are still valid
+ */
+bool gsgpu_ttm_tt_get_user_pages_done(struct ttm_tt *ttm,
+				      struct hmm_range *range)
+{
+	struct gsgpu_ttm_tt *gtt = ttm_to_gsgpu_ttm_tt(ttm);
 
-		pinned += r;
+	if (!gtt || !gtt->userptr || !range)
+		return false;
 
-	} while (pinned < ttm->num_pages);
+	DRM_DEBUG_DRIVER("user_pages_done 0x%llx pages 0x%x\n",
+			 gtt->userptr, ttm->num_pages);
 
-	mmap_read_unlock(mm);;
-	return 0;
+	WARN_ONCE(!range->hmm_pfns, "No user pages to check\n");
 
-release_pages:
-	release_pages(pages, pinned);
-	mmap_read_unlock(mm);;
-	return r;
+	return !gsgpu_hmm_range_get_pages_done(range);
 }
 
 /**
@@ -581,16 +579,10 @@ int gsgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)
  */
 void gsgpu_ttm_tt_set_user_pages(struct ttm_tt *ttm, struct page **pages)
 {
-	struct gsgpu_ttm_tt *gtt = (void *)ttm;
 	unsigned i;
 
-	gtt->last_set_pages = atomic_read(&gtt->mmu_invalidations);
-	for (i = 0; i < ttm->num_pages; ++i) {
-		if (ttm->pages[i])
-			put_page(ttm->pages[i]);
-
+	for (i = 0; i < ttm->num_pages; ++i)
 		ttm->pages[i] = pages ? pages[i] : NULL;
-	}
 }
 
 /**
@@ -625,35 +617,33 @@ static int gsgpu_ttm_tt_pin_userptr(struct ttm_device *bdev,
 				    struct ttm_tt *ttm)
 {
 	struct gsgpu_device *adev = gsgpu_ttm_adev(bdev);
-	struct gsgpu_ttm_tt *gtt = (void *)ttm;
-	unsigned nents;
-	int r;
-
+	struct gsgpu_ttm_tt *gtt = ttm_to_gsgpu_ttm_tt(ttm);
 	int write = !(gtt->userflags & GSGPU_GEM_USERPTR_READONLY);
 	enum dma_data_direction direction = write ?
 		DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
+	int r;
 
 	/* Allocate an SG array and squash pages into it */
 	r = sg_alloc_table_from_pages(ttm->sg, ttm->pages, ttm->num_pages, 0,
-				      ttm->num_pages << PAGE_SHIFT,
+				      (u64)ttm->num_pages << PAGE_SHIFT,
 				      GFP_KERNEL);
 	if (r)
 		goto release_sg;
 
 	/* Map SG to device */
-	r = -ENOMEM;
-	nents = dma_map_sg(adev->dev, ttm->sg->sgl, ttm->sg->nents, direction);
-	if (nents != ttm->sg->nents)
+	r = dma_map_sgtable(adev->dev, ttm->sg, direction, 0);
+	if (r)
 		goto release_sg;
 
 	/* convert SG to linear array of pages and dma addresses */
-	drm_prime_sg_to_page_addr_arrays(ttm->sg, ttm->pages,
-					 gtt->ttm.dma_address, ttm->num_pages);
+	drm_prime_sg_to_dma_addr_array(ttm->sg, gtt->ttm.dma_address,
+				       ttm->num_pages);
 
 	return 0;
 
 release_sg:
 	kfree(ttm->sg);
+	ttm->sg = NULL;
 	return r;
 }
 
@@ -664,22 +654,17 @@ static void gsgpu_ttm_tt_unpin_userptr(struct ttm_device *bdev,
 				       struct ttm_tt *ttm)
 {
 	struct gsgpu_device *adev = gsgpu_ttm_adev(bdev);
-	struct gsgpu_ttm_tt *gtt = (void *)ttm;
-
+	struct gsgpu_ttm_tt *gtt = ttm_to_gsgpu_ttm_tt(ttm);
 	int write = !(gtt->userflags & GSGPU_GEM_USERPTR_READONLY);
 	enum dma_data_direction direction = write ?
 		DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
 
 	/* double check that we don't free the table twice */
-	if (!ttm->sg->sgl)
+	if (!ttm->sg || !ttm->sg->sgl)
 		return;
 
 	/* unmap the pages mapped to the device */
-	dma_unmap_sg(adev->dev, ttm->sg->sgl, ttm->sg->nents, direction);
-
-	/* mark the pages as dirty */
-	gsgpu_ttm_tt_mark_user_pages(ttm);
-
+	dma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);
 	sg_free_table(ttm->sg);
 }
 
@@ -784,7 +769,7 @@ int gsgpu_ttm_alloc_gart(struct ttm_buffer_object *bo)
 	gtt->offset = (u64)tmp->start << PAGE_SHIFT;
         r = gsgpu_ttm_gart_bind(adev, bo, flags);
 	if (unlikely(r)) {
-		ttm_resource_free(bo, tmp);
+		ttm_resource_free(bo, &tmp);
 		return r;
 	}
         ttm_resource_free(bo, &bo->resource);
@@ -989,11 +974,6 @@ int gsgpu_ttm_tt_set_userptr(struct ttm_tt *ttm, uint64_t addr,
 	gtt->usertask = current->group_leader;
 	get_task_struct(gtt->usertask);
 
-	spin_lock_init(&gtt->guptasklock);
-	INIT_LIST_HEAD(&gtt->guptasks);
-	atomic_set(&gtt->mmu_invalidations, 0);
-	gtt->last_set_pages = 0;
-
 	return 0;
 }
 
@@ -1022,7 +1002,6 @@ bool gsgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
 				 unsigned long end)
 {
 	struct gsgpu_ttm_tt *gtt = (void *)ttm;
-	struct gsgpu_ttm_gup_task_list *entry;
 	unsigned long size;
 
 	if (gtt == NULL || !gtt->userptr)
@@ -1035,48 +1014,20 @@ bool gsgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
 	if (gtt->userptr > end || gtt->userptr + size <= start)
 		return false;
 
-	/* Search the lists of tasks that hold this mapping and see
-	 * if current is one of them.  If it is return false.
-	 */
-	spin_lock(&gtt->guptasklock);
-	list_for_each_entry(entry, &gtt->guptasks, list) {
-		if (entry->task == current) {
-			spin_unlock(&gtt->guptasklock);
-			return false;
-		}
-	}
-	spin_unlock(&gtt->guptasklock);
-
-	atomic_inc(&gtt->mmu_invalidations);
-
 	return true;
 }
 
 /**
- * gsgpu_ttm_tt_userptr_invalidated - Has the ttm_tt object been invalidated?
+ * gsgpu_ttm_tt_is_userptr - Have the pages backing by userptr?
  */
-bool gsgpu_ttm_tt_userptr_invalidated(struct ttm_tt *ttm,
-				      int *last_invalidated)
-{
-	struct gsgpu_ttm_tt *gtt = (void *)ttm;
-	int prev_invalidated = *last_invalidated;
-
-	*last_invalidated = atomic_read(&gtt->mmu_invalidations);
-	return prev_invalidated != *last_invalidated;
-}
-
-/**
- * gsgpu_ttm_tt_userptr_needs_pages - Have the pages backing this ttm_tt object
- * been invalidated since the last time they've been set?
- */
-bool gsgpu_ttm_tt_userptr_needs_pages(struct ttm_tt *ttm)
+bool gsgpu_ttm_tt_is_userptr(struct ttm_tt *ttm)
 {
 	struct gsgpu_ttm_tt *gtt = (void *)ttm;
 
 	if (gtt == NULL || !gtt->userptr)
 		return false;
 
-	return atomic_read(&gtt->mmu_invalidations) != gtt->last_set_pages;
+	return true;
 }
 
 /**
@@ -1347,7 +1298,7 @@ static int gsgpu_ttm_fw_reserve_vram_init(struct gsgpu_device *adev)
 			bo->placements[i].lpfn = (offset + size) >> PAGE_SHIFT;
 		}
 
-		ttm_bo_mem_put(&bo->tbo, bo->tbo.resource);
+		ttm_resource_free(&bo->tbo, &bo->tbo.resource);
 		r = ttm_bo_mem_space(&bo->tbo, &bo->placement,
 				     &bo->tbo.resource, &ctx);
 		if (r)
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c
index 46c26aabe86e..5d16a04823ea 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c
@@ -1438,7 +1438,7 @@ int gsgpu_vm_bo_update(struct gsgpu_device *ldev,
 		nodes = NULL;
 		exclusive = NULL;
 	} else {
-		mem = &bo->tbo.mem;
+		mem = bo->tbo.resource;
 		nodes = mem->mm_node;
 		if (mem->mem_type == TTM_PL_TT) {
 			pages_addr = bo->tbo.ttm->dma_address;
@@ -1488,7 +1488,7 @@ int gsgpu_vm_bo_update(struct gsgpu_device *ldev,
 	 * next command submission.
 	 */
 	if (bo && bo->tbo.base.resv == vm->root.base.bo->tbo.base.resv) {
-		u32 mem_type = bo->tbo.mem.mem_type;
+		u32 mem_type = bo->tbo.resource->mem_type;
 
 		if (!(bo->preferred_domains & gsgpu_mem_type_to_domain(mem_type)))
 			list_add_tail(&bo_va->base.vm_status, &vm->evicted);
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu.h b/drivers/gpu/drm/gsgpu/include/gsgpu.h
index d0cb31c606fb..0cd5251ddec0 100644
--- a/drivers/gpu/drm/gsgpu/include/gsgpu.h
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu.h
@@ -9,6 +9,7 @@
 #include <linux/hashtable.h>
 #include <linux/dma-fence.h>
 #include <linux/pci.h>
+#include <linux/hmm.h>
 
 #include <drm/ttm/ttm_bo.h>
 #include <drm/ttm/ttm_tt.h>
@@ -33,7 +34,7 @@
 #include "gsgpu_sync.h"
 #include "gsgpu_ring.h"
 #include "gsgpu_vm.h"
-#include "gsgpu_mn.h"
+#include "gsgpu_hmm.h"
 #include "gsgpu_gmc.h"
 #include "gsgpu_dc.h"
 #include "gsgpu_gart.h"
@@ -766,7 +767,6 @@ struct gsgpu_cs_parser {
 	/* buffer objects */
 	struct ww_acquire_ctx		ticket;
 	struct gsgpu_bo_list		*bo_list;
-	struct gsgpu_mn		*mn;
 	struct gsgpu_bo_list_entry	vm_pd;
 	struct list_head		validated;
 	struct dma_fence		*fence;
@@ -1092,8 +1092,7 @@ struct gsgpu_device {
 
 	struct gsgpu_ip_block          ip_blocks[GSGPU_MAX_IP_NUM];
 	int				num_ip_blocks;
-	struct mutex	mn_lock;
-	DECLARE_HASHTABLE(mn_hash, 7);
+	struct mutex	notifier_lock;
 
 	/* tracking pinned memory */
 	atomic64_t vram_pin_size;
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_bo_list.h b/drivers/gpu/drm/gsgpu/include/gsgpu_bo_list.h
index f81479471068..169a9c42140d 100644
--- a/drivers/gpu/drm/gsgpu/include/gsgpu_bo_list.h
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_bo_list.h
@@ -15,7 +15,7 @@ struct gsgpu_bo_list_entry {
 	struct gsgpu_bo_va		*bo_va;
 	uint32_t			priority;
 	struct page			**user_pages;
-	int				user_invalidated;
+	bool				user_invalidated;
 };
 
 struct gsgpu_bo_list {
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_hmm.h b/drivers/gpu/drm/gsgpu/include/gsgpu_hmm.h
new file mode 100644
index 000000000000..bd8d0bf0cd0c
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_hmm.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright 2017 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: Christian König
+ */
+#ifndef __GSGPU_MN_H__
+#define __GSGPU_MN_H__
+
+#include <linux/types.h>
+#include <linux/hmm.h>
+#include <linux/rwsem.h>
+#include <linux/workqueue.h>
+#include <linux/interval_tree.h>
+#include <linux/mmu_notifier.h>
+
+int gsgpu_hmm_range_get_pages(struct mmu_interval_notifier *notifier,
+			       uint64_t start, uint64_t npages, bool readonly,
+			       void *owner, struct page **pages,
+			       struct hmm_range **phmm_range);
+bool gsgpu_hmm_range_get_pages_done(struct hmm_range *hmm_range);
+
+#if defined(CONFIG_HMM_MIRROR)
+int gsgpu_hmm_register(struct gsgpu_bo *bo, unsigned long addr);
+void gsgpu_hmm_unregister(struct gsgpu_bo *bo);
+#else
+static inline int gsgpu_hmm_register(struct gsgpu_bo *bo, unsigned long addr)
+{
+	DRM_WARN_ONCE("HMM_MIRROR kernel config option is not enabled, "
+		      "add CONFIG_ZONE_DEVICE=y in config file to fix this\n");
+	return -ENODEV;
+}
+static inline void gsgpu_hmm_unregister(struct gsgpu_bo *bo) {}
+#endif
+
+#endif
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_mn.h b/drivers/gpu/drm/gsgpu/include/gsgpu_mn.h
deleted file mode 100644
index f64e696fe663..000000000000
--- a/drivers/gpu/drm/gsgpu/include/gsgpu_mn.h
+++ /dev/null
@@ -1,36 +0,0 @@
-#ifndef __GSGPU_MN_H__
-#define __GSGPU_MN_H__
-
-/*
- * MMU Notifier
- */
-struct gsgpu_mn;
-
-enum gsgpu_mn_type {
-	GSGPU_MN_TYPE_GFX,
-	GSGPU_MN_TYPE_HSA,
-};
-
-#if defined(CONFIG_MMU_NOTIFIER)
-void gsgpu_mn_lock(struct gsgpu_mn *mn);
-void gsgpu_mn_unlock(struct gsgpu_mn *mn);
-struct gsgpu_mn *gsgpu_mn_get(struct gsgpu_device *adev,
-				enum gsgpu_mn_type type);
-int gsgpu_mn_register(struct gsgpu_bo *bo, unsigned long addr);
-void gsgpu_mn_unregister(struct gsgpu_bo *bo);
-#else
-static inline void gsgpu_mn_lock(struct gsgpu_mn *mn) {}
-static inline void gsgpu_mn_unlock(struct gsgpu_mn *mn) {}
-static inline struct gsgpu_mn *gsgpu_mn_get(struct gsgpu_device *adev,
-					      enum gsgpu_mn_type type)
-{
-	return NULL;
-}
-static inline int gsgpu_mn_register(struct gsgpu_bo *bo, unsigned long addr)
-{
-	return -ENODEV;
-}
-static inline void gsgpu_mn_unregister(struct gsgpu_bo *bo) {}
-#endif
-
-#endif /* __GSGPU_MN_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_object.h b/drivers/gpu/drm/gsgpu/include/gsgpu_object.h
index 27f62f50bb6f..76e16a058c47 100644
--- a/drivers/gpu/drm/gsgpu/include/gsgpu_object.h
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_object.h
@@ -60,7 +60,10 @@ struct gsgpu_bo {
 	struct gsgpu_bo		*shadow;
 
 	struct ttm_bo_kmap_obj		dma_buf_vmap;
-	struct gsgpu_mn		*mn;
+
+#ifdef CONFIG_MMU_NOTIFIER
+	struct mmu_interval_notifier	notifier;
+#endif
 
 	union {
 		struct list_head	mn_list;
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h b/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h
index ce291a95f82d..719946d39255 100644
--- a/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h
@@ -68,7 +68,10 @@ int gsgpu_fill_buffer(struct gsgpu_bo *bo,
 int gsgpu_ttm_alloc_gart(struct ttm_buffer_object *bo);
 int gsgpu_ttm_recover_gart(struct ttm_buffer_object *tbo);
 
-int gsgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages);
+int gsgpu_ttm_tt_get_user_pages(struct gsgpu_bo *bo, struct page **pages,
+				struct hmm_range **range);
+bool gsgpu_ttm_tt_get_user_pages_done(struct ttm_tt *ttm,
+				      struct hmm_range *range);
 void gsgpu_ttm_tt_set_user_pages(struct ttm_tt *ttm, struct page **pages);
 void gsgpu_ttm_tt_mark_user_pages(struct ttm_tt *ttm);
 int gsgpu_ttm_tt_set_userptr(struct ttm_tt *ttm, uint64_t addr,
@@ -79,7 +82,7 @@ bool gsgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
 				 unsigned long end);
 bool gsgpu_ttm_tt_userptr_invalidated(struct ttm_tt *ttm,
 				      int *last_invalidated);
-bool gsgpu_ttm_tt_userptr_needs_pages(struct ttm_tt *ttm);
+bool gsgpu_ttm_tt_is_userptr(struct ttm_tt *ttm);
 bool gsgpu_ttm_tt_is_readonly(struct ttm_tt *ttm);
 uint64_t gsgpu_ttm_tt_pte_flags(struct gsgpu_device *adev, struct ttm_tt *ttm,
 				struct ttm_resource *mem);
-- 
2.39.1

