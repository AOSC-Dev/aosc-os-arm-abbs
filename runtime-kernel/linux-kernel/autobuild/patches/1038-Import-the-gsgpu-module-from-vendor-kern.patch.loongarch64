From 5563d7f66c00cf8cde28f1909b85178712915345 Mon Sep 17 00:00:00 2001
From: "Dr. Chang Liu, PhD" <cl91tp@gmail.com>
Date: Fri, 22 Sep 2023 17:17:22 +0800
Subject: [PATCH] Import the gsgpu module from vendor kernel 4.19.190.8.14

---
 drivers/gpu/drm/Kconfig                       |   20 +
 drivers/gpu/drm/Makefile                      |    1 +
 drivers/gpu/drm/gsgpu/bridge/Makefile         |    8 +
 drivers/gpu/drm/gsgpu/bridge/bridge_phy.c     |  763 +++++
 drivers/gpu/drm/gsgpu/bridge/bridge_phy.h     |  292 ++
 drivers/gpu/drm/gsgpu/bridge/lt6711_drv.c     |   75 +
 drivers/gpu/drm/gsgpu/bridge/lt8619_drv.c     |  671 +++++
 drivers/gpu/drm/gsgpu/bridge/lt9721_drv.c     |  260 ++
 drivers/gpu/drm/gsgpu/bridge/lt9721_drv.h     |  105 +
 drivers/gpu/drm/gsgpu/bridge/ncs8805_drv.c    |  500 ++++
 drivers/gpu/drm/gsgpu/bridge/ncs8805_drv.h    |   83 +
 drivers/gpu/drm/gsgpu/gpu/Kconfig             |   17 +
 drivers/gpu/drm/gsgpu/gpu/Makefile            |   62 +
 drivers/gpu/drm/gsgpu/gpu/gsgpu_backlight.c   |  287 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_benchmark.c   |  221 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_bo_list.c     |  286 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_common.c      |  216 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_cp.c          |  162 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c          | 1529 ++++++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_ctx.c         |  473 +++
 .../gpu/drm/gsgpu/gpu/gsgpu_dc_connector.c    |  327 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_crtc.c     |  593 ++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_cursor.c   |  212 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_drv.c      | 1038 +++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_encoder.c  |   92 +
 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_hdmi.c     |  191 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_i2c.c      |  433 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_irq.c      |  906 ++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_plane.c    |  195 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_vbios.c    | 1147 ++++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_debugfs.c     |  588 ++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c      | 2172 ++++++++++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_display.c     |  434 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_drv.c         |  648 +++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_fb.c          |  363 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_fence.c       |  688 +++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_gart.c        |  367 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c         |  854 ++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_gfx.c         |  695 +++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_gtt_mgr.c     |  276 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_hw_sema.c     |  184 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_ib.c          |  322 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_ids.c         |  562 ++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_ih.c          |  441 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_ioc32.c       |   18 +
 drivers/gpu/drm/gsgpu/gpu/gsgpu_irq.c         |  489 ++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_job.c         |  216 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_kms.c         |  873 ++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_mmu.c         |  594 ++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c          |  433 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c      | 1323 +++++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_pm.c          |  194 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c       |  418 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_queue_mgr.c   |  227 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_ring.c        |  536 ++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_sa.c          |  355 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_sched.c       |   76 +
 drivers/gpu/drm/gsgpu/gpu/gsgpu_sync.c        |  389 +++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_test.c        |  226 ++
 .../gpu/drm/gsgpu/gpu/gsgpu_trace_points.c    |    6 +
 drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c         | 2396 +++++++++++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c          | 2567 +++++++++++++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_vm_it.c       |    9 +
 drivers/gpu/drm/gsgpu/gpu/gsgpu_vram_mgr.c    |  297 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_xdma.c        | 1199 ++++++++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_zip.c         |  213 ++
 drivers/gpu/drm/gsgpu/gpu/gsgpu_zip_meta.c    |  221 ++
 drivers/gpu/drm/gsgpu/include/gsgpu.h         | 1377 +++++++++
 .../gpu/drm/gsgpu/include/gsgpu_backlight.h   |   70 +
 drivers/gpu/drm/gsgpu/include/gsgpu_bo_list.h |   60 +
 drivers/gpu/drm/gsgpu/include/gsgpu_common.h  |    8 +
 drivers/gpu/drm/gsgpu/include/gsgpu_cp.h      |   26 +
 drivers/gpu/drm/gsgpu/include/gsgpu_dc.h      |  112 +
 .../drm/gsgpu/include/gsgpu_dc_connector.h    |   29 +
 drivers/gpu/drm/gsgpu/include/gsgpu_dc_crtc.h |   47 +
 .../gpu/drm/gsgpu/include/gsgpu_dc_encoder.h  |   15 +
 drivers/gpu/drm/gsgpu/include/gsgpu_dc_hdmi.h |   13 +
 drivers/gpu/drm/gsgpu/include/gsgpu_dc_i2c.h  |   49 +
 drivers/gpu/drm/gsgpu/include/gsgpu_dc_irq.h  |   93 +
 .../gpu/drm/gsgpu/include/gsgpu_dc_plane.h    |   43 +
 drivers/gpu/drm/gsgpu/include/gsgpu_dc_reg.h  |  167 ++
 .../gpu/drm/gsgpu/include/gsgpu_dc_resource.h |  194 ++
 .../gpu/drm/gsgpu/include/gsgpu_dc_vbios.h    |  203 ++
 drivers/gpu/drm/gsgpu/include/gsgpu_debugfs.h |   20 +
 drivers/gpu/drm/gsgpu/include/gsgpu_display.h |   10 +
 drivers/gpu/drm/gsgpu/include/gsgpu_drv.h     |   18 +
 drivers/gpu/drm/gsgpu/include/gsgpu_gart.h    |   51 +
 drivers/gpu/drm/gsgpu/include/gsgpu_gfx.h     |    8 +
 drivers/gpu/drm/gsgpu/include/gsgpu_gmc.h     |   93 +
 drivers/gpu/drm/gsgpu/include/gsgpu_hw_sema.h |   28 +
 drivers/gpu/drm/gsgpu/include/gsgpu_ids.h     |   62 +
 drivers/gpu/drm/gsgpu/include/gsgpu_ih.h      |   91 +
 drivers/gpu/drm/gsgpu/include/gsgpu_irq.h     |   83 +
 drivers/gpu/drm/gsgpu/include/gsgpu_job.h     |   50 +
 drivers/gpu/drm/gsgpu/include/gsgpu_mmu.h     |   49 +
 drivers/gpu/drm/gsgpu/include/gsgpu_mn.h      |   36 +
 drivers/gpu/drm/gsgpu/include/gsgpu_mode.h    |  178 ++
 drivers/gpu/drm/gsgpu/include/gsgpu_object.h  |  284 ++
 drivers/gpu/drm/gsgpu/include/gsgpu_pm.h      |    6 +
 drivers/gpu/drm/gsgpu/include/gsgpu_ring.h    |  249 ++
 drivers/gpu/drm/gsgpu/include/gsgpu_sched.h   |   10 +
 drivers/gpu/drm/gsgpu/include/gsgpu_shared.h  |   72 +
 drivers/gpu/drm/gsgpu/include/gsgpu_sync.h    |   36 +
 drivers/gpu/drm/gsgpu/include/gsgpu_trace.h   |  450 +++
 drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h     |   91 +
 drivers/gpu/drm/gsgpu/include/gsgpu_vm.h      |  267 ++
 drivers/gpu/drm/gsgpu/include/gsgpu_vm_it.h   |   36 +
 drivers/gpu/drm/gsgpu/include/gsgpu_xdma.h    |   10 +
 drivers/gpu/drm/gsgpu/include/gsgpu_zip.h     |   12 +
 .../gpu/drm/gsgpu/include/gsgpu_zip_meta.h    |   44 +
 include/drm/gsgpu_family_type.h               |   10 +
 include/uapi/drm/gsgpu_drm.h                  | 1003 +++++++
 112 files changed, 38897 insertions(+)
 create mode 100644 drivers/gpu/drm/gsgpu/bridge/Makefile
 create mode 100644 drivers/gpu/drm/gsgpu/bridge/bridge_phy.c
 create mode 100644 drivers/gpu/drm/gsgpu/bridge/bridge_phy.h
 create mode 100644 drivers/gpu/drm/gsgpu/bridge/lt6711_drv.c
 create mode 100644 drivers/gpu/drm/gsgpu/bridge/lt8619_drv.c
 create mode 100644 drivers/gpu/drm/gsgpu/bridge/lt9721_drv.c
 create mode 100644 drivers/gpu/drm/gsgpu/bridge/lt9721_drv.h
 create mode 100644 drivers/gpu/drm/gsgpu/bridge/ncs8805_drv.c
 create mode 100644 drivers/gpu/drm/gsgpu/bridge/ncs8805_drv.h
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/Kconfig
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/Makefile
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_backlight.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_benchmark.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_bo_list.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_common.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_cp.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_ctx.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_connector.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_crtc.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_cursor.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_drv.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_encoder.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_hdmi.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_i2c.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_irq.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_plane.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_vbios.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_debugfs.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_display.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_drv.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_fb.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_fence.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_gart.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_gfx.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_gtt_mgr.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_hw_sema.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_ib.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_ids.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_ih.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_ioc32.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_irq.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_job.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_kms.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_mmu.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_pm.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_queue_mgr.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_ring.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_sa.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_sched.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_sync.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_test.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_trace_points.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_vm_it.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_vram_mgr.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_xdma.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_zip.c
 create mode 100644 drivers/gpu/drm/gsgpu/gpu/gsgpu_zip_meta.c
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_backlight.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_bo_list.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_common.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_cp.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc_connector.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc_crtc.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc_encoder.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc_hdmi.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc_i2c.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc_irq.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc_plane.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc_reg.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc_resource.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_dc_vbios.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_debugfs.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_display.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_drv.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_gart.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_gfx.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_gmc.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_hw_sema.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_ids.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_ih.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_irq.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_job.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_mmu.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_mn.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_mode.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_object.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_pm.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_ring.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_sched.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_shared.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_sync.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_trace.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_vm.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_vm_it.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_xdma.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_zip.h
 create mode 100644 drivers/gpu/drm/gsgpu/include/gsgpu_zip_meta.h
 create mode 100644 include/drm/gsgpu_family_type.h
 create mode 100644 include/uapi/drm/gsgpu_drm.h

diff --git a/drivers/gpu/drm/Kconfig b/drivers/gpu/drm/Kconfig
index 3eee8636f847..ac571da0afb7 100644
--- a/drivers/gpu/drm/Kconfig
+++ b/drivers/gpu/drm/Kconfig
@@ -277,6 +277,26 @@ source "drivers/gpu/drm/i915/Kconfig"
 
 source "drivers/gpu/drm/kmb/Kconfig"
 
+config DRM_GSGPU
+	tristate "Loongosn Godson GPU"
+	depends on DRM && PCI && MMU
+	select FW_LOADER
+        select DRM_KMS_HELPER
+	select DRM_SCHED
+        select DRM_TTM
+	select POWER_SUPPLY
+	select HWMON
+	select BACKLIGHT_CLASS_DEVICE
+	select BACKLIGHT_LCD_SUPPORT
+	select INTERVAL_TREE
+	select CHASH
+	help
+	  Choose this option if you have a recent Godson graphics card.
+
+	  If M is selected, the module will be called gsgpu.
+
+source "drivers/gpu/drm/gsgpu/gpu/Kconfig"
+
 config DRM_VGEM
 	tristate "Virtual GEM provider"
 	depends on DRM && MMU
diff --git a/drivers/gpu/drm/Makefile b/drivers/gpu/drm/Makefile
index db0f3d3aff43..d7aa1e9721ab 100644
--- a/drivers/gpu/drm/Makefile
+++ b/drivers/gpu/drm/Makefile
@@ -186,6 +186,7 @@ obj-y			+= mxsfb/
 obj-$(CONFIG_DRM_PL111) += pl111/
 obj-$(CONFIG_DRM_TVE200) += tve200/
 obj-$(CONFIG_DRM_XEN) += xen/
+obj-$(CONFIG_DRM_GSGPU) += gsgpu/gpu/
 obj-$(CONFIG_DRM_VBOXVIDEO) += vboxvideo/
 obj-$(CONFIG_DRM_LIMA)  += lima/
 obj-$(CONFIG_DRM_PANFROST) += panfrost/
diff --git a/drivers/gpu/drm/gsgpu/bridge/Makefile b/drivers/gpu/drm/gsgpu/bridge/Makefile
new file mode 100644
index 000000000000..17b4b921552e
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/bridge/Makefile
@@ -0,0 +1,8 @@
+
+GSGPUPATH = $(RELATIVE_GSGPU_BRIDGE_PATH)
+
+BRIDGE_FILES = bridge_phy.o lt6711_drv.o lt9721_drv.o lt8619_drv.o ncs8805_drv.o
+
+GSGPU_BRIDGE = $(addprefix $(GSGPUPATH)/,$(BRIDGE_FILES))
+
+GSGPU_BRIDGE_FILES += $(GSGPU_BRIDGE)
diff --git a/drivers/gpu/drm/gsgpu/bridge/bridge_phy.c b/drivers/gpu/drm/gsgpu/bridge/bridge_phy.c
new file mode 100644
index 000000000000..f5541e433374
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/bridge/bridge_phy.c
@@ -0,0 +1,763 @@
+#include <drm/drm_atomic_helper.h>
+#include "gsgpu.h"
+#include "gsgpu_dc.h"
+#include "gsgpu_dc_connector.h"
+#include "gsgpu_dc_encoder.h"
+#include "gsgpu_dc_resource.h"
+#include "gsgpu_dc_vbios.h"
+#include "bridge_phy.h"
+#include "gsgpu_backlight.h"
+
+/**
+ * @section Bridge-phy connector functions
+ */
+static int bridge_phy_connector_get_modes(struct drm_connector *connector)
+{
+	struct edid *edid = NULL;
+	unsigned int count = 0;
+	struct gsgpu_device *adev = connector->dev->dev_private;
+	struct gsgpu_bridge_phy *phy =
+		adev->mode_info.encoders[connector->index]->bridge;
+	unsigned short used_method = phy->res->edid_method;
+
+	switch (used_method) {
+	case via_i2c:
+		edid = drm_get_edid(connector, &phy->li2c->adapter);
+		if (edid) {
+			drm_connector_update_edid_property(connector, edid);
+			count = drm_add_edid_modes(connector, edid);
+			kfree(edid);
+		}
+		break;
+	case via_encoder:
+		if (phy->ddc_funcs && phy->ddc_funcs->get_modes)
+			count = phy->ddc_funcs->get_modes(phy, connector);
+		break;
+	default:
+		break;
+	}
+
+	if (!count) {
+		count = drm_add_modes_noedid(connector, 1920, 1080);
+		drm_set_preferred_mode(connector, 1024, 768);
+		DRM_DEBUG_DRIVER("[Bridge_phy] Setting %s edid.\n",
+				 phy->res->chip_name);
+	}
+
+	return count;
+}
+
+static enum drm_mode_status
+bridge_phy_connector_mode_valid(struct drm_connector *connector,
+				struct drm_display_mode *mode)
+{
+	struct gsgpu_device *adev = connector->dev->dev_private;
+	struct gsgpu_bridge_phy *phy =
+		adev->mode_info.encoders[connector->index]->bridge;
+
+	if (phy->cfg_funcs && phy->cfg_funcs->mode_valid)
+		return phy->cfg_funcs->mode_valid(connector, mode);
+
+	return MODE_OK;
+}
+
+static struct drm_encoder *
+bridge_phy_connector_best_encoder(struct drm_connector *connector)
+{
+	int enc_id;
+
+	enc_id = connector->encoder_ids[0];
+	/* pick the encoder ids */
+	if (enc_id)
+		return drm_encoder_find(connector->dev, NULL, enc_id);
+
+	DRM_ERROR("Failed to get encoder %d\n", enc_id);
+	return NULL;
+}
+
+static struct drm_connector_helper_funcs bridge_phy_connector_helper_funcs = {
+	.get_modes = bridge_phy_connector_get_modes,
+	.mode_valid = bridge_phy_connector_mode_valid,
+	.best_encoder = bridge_phy_connector_best_encoder,
+};
+
+static enum drm_connector_status
+bridge_phy_connector_detect(struct drm_connector *connector, bool force)
+{
+	return connector_status_connected;
+}
+
+static const struct drm_connector_funcs bridge_phy_connector_funcs = {
+	.dpms = drm_helper_connector_dpms,
+	.detect = bridge_phy_connector_detect,
+	.fill_modes = drm_helper_probe_single_connector_modes,
+	.destroy = drm_connector_cleanup,
+	.reset = drm_atomic_helper_connector_reset,
+	.atomic_duplicate_state = drm_atomic_helper_connector_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_connector_destroy_state,
+	.late_register = gsgpu_backlight_register
+};
+
+/**
+ * @section Bridge-phy core functions
+ */
+static void bridge_phy_enable(struct drm_bridge *bridge)
+{
+	struct gsgpu_bridge_phy *phy = to_bridge_phy(bridge);
+
+	DRM_DEBUG("[Bridge_phy] [%s] enable\n", phy->res->chip_name);
+	if (phy->cfg_funcs && phy->cfg_funcs->afe_high)
+		phy->cfg_funcs->afe_high(phy);
+	if (phy->cfg_funcs && phy->cfg_funcs->afe_set_tx)
+		phy->cfg_funcs->afe_set_tx(phy, TRUE);
+	if (phy->cfg_funcs && phy->cfg_funcs->hdmi_audio)
+		phy->cfg_funcs->hdmi_audio(phy);
+}
+
+static void bridge_phy_disable(struct drm_bridge *bridge)
+{
+	struct gsgpu_bridge_phy *phy = to_bridge_phy(bridge);
+
+	DRM_DEBUG("[Bridge_phy] [%s] disable\n", phy->res->chip_name);
+	if (phy->cfg_funcs && phy->cfg_funcs->afe_low)
+		phy->cfg_funcs->afe_low(phy);
+	if (phy->cfg_funcs && phy->cfg_funcs->afe_set_tx)
+		phy->cfg_funcs->afe_set_tx(phy, FALSE);
+}
+
+static int __bridge_phy_mode_set(struct gsgpu_bridge_phy *phy,
+				 const struct drm_display_mode *mode,
+				 const struct drm_display_mode *adj_mode)
+{
+	if (phy->mode_config.input_mode.gen_sync)
+		DRM_DEBUG("[Bridge_phy] [%s] bridge_phy gen_sync\n",
+				phy->res->chip_name);
+	if (phy->cfg_funcs && phy->cfg_funcs->mode_set_pre)
+		phy->cfg_funcs->mode_set_pre(&phy->bridge, mode, adj_mode);
+	if (phy->cfg_funcs && phy->cfg_funcs->mode_set)
+		phy->cfg_funcs->mode_set(phy, mode, adj_mode);
+	if (phy->cfg_funcs && phy->cfg_funcs->mode_set_post)
+		phy->cfg_funcs->mode_set_post(&phy->bridge, mode, adj_mode);
+
+	return 0;
+}
+
+void bridge_phy_mode_set(struct gsgpu_bridge_phy *phy,
+				struct drm_display_mode *mode,
+				struct drm_display_mode *adj_mode)
+{
+	if (!phy)
+		return;
+
+	DRM_DEBUG("[Bridge_phy] [%s] mode set\n", phy->res->chip_name);
+	drm_mode_debug_printmodeline(mode);
+
+	__bridge_phy_mode_set(phy, mode, adj_mode);
+}
+
+static int bridge_phy_attach(struct drm_bridge *bridge)
+{
+	struct gsgpu_bridge_phy *phy = to_bridge_phy(bridge);
+	struct gsgpu_connector *lconnector;
+	int link_index = phy->display_pipe_index;
+	int ret;
+
+	DRM_DEBUG("[Bridge_phy] %s attach\n", phy->res->chip_name);
+	if (!bridge->encoder) {
+		DRM_ERROR("Parent encoder object not found\n");
+		return -ENODEV;
+	}
+
+	lconnector = kzalloc(sizeof(*lconnector), GFP_KERNEL);
+	if (!lconnector)
+		return -ENOMEM;
+
+	ret = drm_connector_init(bridge->dev, &lconnector->base,
+				 &bridge_phy_connector_funcs,
+				 phy->connector_type);
+	if (ret) {
+		DRM_ERROR("[Bridge_phy] %s Failed to initialize connector\n",
+				phy->res->chip_name);
+		return ret;
+	}
+
+	lconnector->connector_id = link_index;
+	phy->connector = &lconnector->base;
+	phy->adev->mode_info.connectors[link_index] = lconnector;
+	phy->connector->dpms = DRM_MODE_DPMS_OFF;
+
+	drm_connector_helper_add(&lconnector->base,
+				 &bridge_phy_connector_helper_funcs);
+
+	if (link_index == 0) {
+		lconnector->irq_source_i2c = DC_IRQ_SOURCE_I2C0;
+		lconnector->irq_source_hpd = DC_IRQ_SOURCE_HPD_HDMI0_NULL;
+	} else if (link_index == 1) {
+		lconnector->irq_source_i2c = DC_IRQ_SOURCE_I2C1;
+		lconnector->irq_source_hpd = DC_IRQ_SOURCE_HPD_HDMI1_NULL;
+	}
+
+	switch (phy->res->hotplug) {
+	case IRQ:
+		phy->connector->polled = DRM_CONNECTOR_POLL_HPD;
+		if (link_index == 0)
+			lconnector->irq_source_hpd = DC_IRQ_SOURCE_HPD_HDMI0;
+		else if (link_index == 1)
+			lconnector->irq_source_hpd = DC_IRQ_SOURCE_HPD_HDMI1;
+		break;
+	case FORCE_ON:
+		phy->connector->polled = 0;
+		break;
+	case POLLING:
+	default:
+		phy->connector->polled = DRM_CONNECTOR_POLL_CONNECT |
+					 DRM_CONNECTOR_POLL_DISCONNECT;
+		break;
+	}
+
+	DRM_DEBUG_DRIVER("[Bridge_phy] %s Set connector poll=0x%x.\n",
+			 phy->res->chip_name, phy->connector->polled);
+
+	return ret;
+}
+
+static const struct drm_bridge_funcs bridge_funcs = {
+	.enable = bridge_phy_enable,
+	.disable = bridge_phy_disable,
+	.attach = bridge_phy_attach,
+};
+
+static int bridge_phy_bind(struct gsgpu_bridge_phy *phy)
+{
+	int ret;
+
+	phy->bridge.funcs = &bridge_funcs;
+	drm_bridge_add(&phy->bridge);
+	ret = drm_bridge_attach(phy->encoder, &phy->bridge, NULL);
+	if (ret) {
+		DRM_ERROR("[Bridge_phy] %s Failed to attach phy ret %d\n",
+			  phy->res->chip_name, ret);
+		return ret;
+	}
+
+	DRM_INFO("[Bridge_phy] %s encoder-%d be attach to this bridge.\n",
+		 phy->res->chip_name, phy->encoder->index);
+
+	return 0;
+}
+
+/**
+ * @section Bridge-phy helper functions
+ */
+void bridge_phy_reg_mask_seq(struct gsgpu_bridge_phy *phy,
+			     const struct reg_mask_seq *seq, size_t seq_size)
+{
+	unsigned int i;
+	struct regmap *regmap;
+
+	regmap = phy->phy_regmap;
+	for (i = 0; i < seq_size; i++)
+		regmap_update_bits(regmap, seq[i].reg, seq[i].mask, seq[i].val);
+}
+
+void bridge_phy_reg_update_bits(struct gsgpu_bridge_phy *phy, unsigned int reg,
+				unsigned int mask, unsigned int val)
+{
+	unsigned int reg_val;
+
+	regmap_read(phy->phy_regmap, reg, &reg_val);
+	val ? (reg_val |= (mask)) : (reg_val &= ~(mask));
+	regmap_write(phy->phy_regmap, reg, reg_val);
+}
+
+int bridge_phy_reg_dump(struct gsgpu_bridge_phy *phy, size_t start,
+			size_t count)
+{
+	u8 *buf;
+	int ret;
+	unsigned int i;
+
+	buf = kzalloc(count, GFP_KERNEL);
+	if (IS_ERR(buf)) {
+		ret = PTR_ERR(buf);
+		return -ENOMEM;
+	}
+	ret = regmap_raw_read(phy->phy_regmap, start, buf, count);
+	for (i = 0; i < count; i++)
+		pr_info("[%lx]=%02x", start + i, buf[i]);
+
+	kfree(buf);
+	return ret;
+}
+
+static char *get_encoder_chip_name(int encoder_obj)
+{
+	switch (encoder_obj) {
+	case ENCODER_OBJECT_ID_NONE:
+		return "none";
+	case ENCODER_OBJECT_ID_EDP_NCS8803:
+		return "ncs8803";
+	case ENCODER_OBJECT_ID_EDP_NCS8805:
+		return "ncs8805";
+	case ENCODER_OBJECT_ID_EDP_LT9721:
+		return "lt9721";
+	case ENCODER_OBJECT_ID_EDP_LT6711:
+		return "lt6711";
+	case ENCODER_OBJECT_ID_LVDS_LT8619:
+		return "lt8619";
+	case ENCODER_OBJECT_ID_VGA_TRANSPARENT:
+		return "vga";
+	case ENCODER_OBJECT_ID_DVI_TRANSPARENT:
+		return "dvi";
+	case ENCODER_OBJECT_ID_HDMI_TRANSPARENT:
+		return "hdmi";
+	case ENCODER_OBJECT_ID_EDP_TRANSPARENT:
+		return "edp";
+	default:
+		DRM_WARN("No ext encoder chip 0x%x.\n", encoder_obj);
+		return "Unknown";
+	}
+}
+
+static bool bridge_phy_check_feature(const struct gsgpu_bridge_phy *phy,
+				     u32 feature)
+{
+	return phy->feature & feature;
+}
+
+static int bridge_phy_add_hpd_funcs(struct gsgpu_bridge_phy *phy, void *funcs)
+{
+	phy->hpd_funcs = (struct bridge_phy_hpd_funcs *)funcs;
+
+	return 0;
+}
+
+static int bridge_phy_add_ddc_funcs(struct gsgpu_bridge_phy *phy, void *funcs)
+{
+	phy->ddc_funcs = (struct bridge_phy_ddc_funcs *)funcs;
+
+	return 0;
+}
+
+static int bridge_phy_add_hdmi_aux_funcs(struct gsgpu_bridge_phy *phy,
+					 void *funcs)
+{
+	phy->hdmi_aux_funcs = (struct bridge_phy_hdmi_aux_funcs *)funcs;
+
+	return 0;
+}
+
+static void  bridge_phy_add_helper_funcs(struct gsgpu_bridge_phy *phy,
+					 struct bridge_phy_helper *helper)
+{
+	u32 feature, check_feature;
+
+	feature = phy->feature;
+	phy->helper = helper;
+
+	DRM_DEBUG_DRIVER("[Bridge_phy] %s features=%#x, add helper funcs\n",
+			 phy->res->chip_name, feature);
+
+	check_feature = SUPPORT_HPD;
+	if (bridge_phy_check_feature(phy, check_feature))
+		bridge_phy_add_hpd_funcs(phy, helper->hpd_funcs);
+
+	check_feature = SUPPORT_DDC;
+	if (bridge_phy_check_feature(phy, check_feature))
+		bridge_phy_add_ddc_funcs(phy, helper->ddc_funcs);
+
+	check_feature = SUPPORT_HDMI_AUX;
+	if (bridge_phy_check_feature(phy, check_feature))
+		bridge_phy_add_hdmi_aux_funcs(phy, helper->hdmi_aux_funcs);
+}
+
+static int bridge_phy_register_irq_num(struct gsgpu_bridge_phy *phy)
+{
+	int ret = -1;
+	char irq_name[NAME_SIZE_MAX];
+	struct gsgpu_dc_bridge *res;
+
+	res = phy->res;
+	phy->irq_num = -1;
+
+	if (res->adev->chip == dev_7a2000 || res->adev->chip == dev_2k2000)
+		return ret;
+
+	if (phy->res->hotplug != IRQ)
+		return ret;
+
+	if (res->gpio_placement)
+		res->irq_gpio += LS7A_GPIO_OFFSET;
+
+	ret = gpio_is_valid(res->irq_gpio);
+	if (!ret)
+		goto error_gpio_valid;
+	sprintf(irq_name, "%s-irq", res->chip_name);
+
+	ret = gpio_request(res->irq_gpio, irq_name);
+	if (ret)
+		goto error_gpio_req;
+	ret = gpio_direction_input(res->irq_gpio);
+	if (ret)
+		goto error_gpio_cfg;
+
+	phy->irq_num = gpio_to_irq(res->irq_gpio);
+	if (phy->irq_num < 0) {
+		ret = phy->irq_num;
+		DRM_ERROR("GPIO %d has no interrupt\n", res->irq_gpio);
+		return ret;
+	}
+
+	DRM_DEBUG("[Bridge_phy] %s register irq num %d.\n", res->chip_name,
+		  phy->irq_num);
+	return 0;
+
+error_gpio_cfg:
+	DRM_ERROR("Failed to config gpio %d free it %d\n", res->irq_gpio, ret);
+	gpio_free(res->irq_gpio);
+error_gpio_req:
+	DRM_ERROR("Failed to request gpio %d, %d\n", res->irq_gpio, ret);
+error_gpio_valid:
+	DRM_ERROR("Invalid gpio %d, %d\n", res->irq_gpio, ret);
+	return ret;
+}
+
+static int bridge_phy_register_irq_handle(struct gsgpu_bridge_phy *phy)
+{
+	int ret = -1;
+	irqreturn_t (*irq_handler)(int irq, void *dev);
+	irqreturn_t (*isr_thread)(int irq, void *dev);
+
+	if (phy->adev->chip == dev_7a2000 || phy->adev->chip == dev_2k2000)
+		return ret;
+
+	if (phy->res->hotplug != IRQ)
+		return ret;
+
+	if (phy->irq_num <= 0) {
+		phy->connector->polled = DRM_CONNECTOR_POLL_CONNECT |
+					 DRM_CONNECTOR_POLL_DISCONNECT;
+		return ret;
+	}
+
+	if (phy->hpd_funcs && phy->hpd_funcs->isr_thread) {
+		irq_handler = phy->hpd_funcs->irq_handler;
+		isr_thread = phy->hpd_funcs->isr_thread;
+	}
+
+	ret = devm_request_threaded_irq(
+		&phy->i2c_phy->dev, phy->irq_num, irq_handler, isr_thread,
+		IRQF_ONESHOT | IRQF_SHARED | IRQF_TRIGGER_HIGH,
+		phy->res->chip_name, phy);
+	if (ret)
+		goto error_irq;
+
+	DRM_DEBUG_DRIVER("[Bridge_phy] %s register irq handler succeed %d.\n",
+			 phy->res->chip_name, phy->irq_num);
+	return 0;
+
+error_irq:
+	gpio_free(phy->res->irq_gpio);
+	DRM_ERROR("Failed to request irq handler for irq %d.\n", phy->irq_num);
+	return ret;
+}
+
+static int bridge_phy_hw_reset(struct gsgpu_bridge_phy *phy)
+{
+	if (phy->cfg_funcs && phy->cfg_funcs->hw_reset)
+		phy->cfg_funcs->hw_reset(phy);
+
+	return 0;
+}
+
+static int bridge_phy_misc_init(struct gsgpu_bridge_phy *phy)
+{
+	if (phy->helper && phy->helper->misc_funcs
+			&& phy->helper->misc_funcs->debugfs_init)
+		phy->helper->misc_funcs->debugfs_init(phy);
+
+	return 0;
+}
+
+static int bridge_phy_regmap_init(struct gsgpu_bridge_phy *phy)
+{
+	int ret;
+	struct regmap *regmap;
+
+	if (!phy->helper)
+		return 0;
+
+	mutex_init(&phy->ddc_status.ddc_bus_mutex);
+	atomic_set(&phy->irq_status, 0);
+
+	if (!phy->helper->regmap_cfg)
+		return 0;
+
+	regmap = devm_regmap_init_i2c(phy->li2c->ddc_client,
+				      phy->helper->regmap_cfg);
+	if (IS_ERR(regmap)) {
+		ret = PTR_ERR(regmap);
+		return -ret;
+	}
+	phy->phy_regmap = regmap;
+
+	return 0;
+}
+
+static int bridge_phy_chip_id_verify(struct gsgpu_bridge_phy *phy)
+{
+	int ret;
+	char str[NAME_SIZE_MAX] = "";
+
+	if (phy->helper && phy->helper->misc_funcs
+			&& phy->helper->misc_funcs->chip_id_verify) {
+		ret = phy->helper->misc_funcs->chip_id_verify(phy, str);
+		if (!ret)
+			DRM_ERROR("Failed to verify chip %s, return [%s]\n",
+				  phy->res->chip_name, str);
+		strncpy(phy->res->vendor_str, str, NAME_SIZE_MAX - 1);
+		return ret;
+	}
+
+	return -ENODEV;
+}
+
+static int bridge_phy_video_config(struct gsgpu_bridge_phy *phy)
+{
+	if (phy->cfg_funcs && phy->cfg_funcs->video_input_cfg)
+		phy->cfg_funcs->video_input_cfg(phy);
+	if (phy->cfg_funcs && phy->cfg_funcs->video_output_cfg)
+		phy->cfg_funcs->video_output_cfg(phy);
+
+	return 0;
+}
+
+static int bridge_phy_hdmi_config(struct gsgpu_bridge_phy *phy)
+{
+	if (phy->cfg_funcs && phy->cfg_funcs->hdmi_audio)
+		phy->cfg_funcs->hdmi_audio(phy);
+	if (phy->cfg_funcs && phy->cfg_funcs->hdmi_csc)
+		phy->cfg_funcs->hdmi_csc(phy);
+	if (phy->cfg_funcs && phy->cfg_funcs->hdmi_hdcp_init)
+		phy->cfg_funcs->hdmi_hdcp_init(phy);
+
+	return 0;
+}
+
+static int bridge_phy_sw_init(struct gsgpu_bridge_phy *phy)
+{
+	bridge_phy_chip_id_verify(phy);
+	if (phy->cfg_funcs && phy->cfg_funcs->sw_reset)
+		phy->cfg_funcs->sw_reset(phy);
+	if (phy->cfg_funcs && phy->cfg_funcs->reg_init)
+		phy->cfg_funcs->reg_init(phy);
+	if (phy->cfg_funcs && phy->cfg_funcs->sw_enable)
+		phy->cfg_funcs->sw_enable(phy);
+	bridge_phy_video_config(phy);
+	bridge_phy_hdmi_config(phy);
+
+	DRM_DEBUG_DRIVER("[Bridge_phy] %s sw init completed\n",
+			 phy->res->chip_name);
+
+	return 0;
+}
+
+static int bridge_phy_encoder_obj_select(struct gsgpu_dc_bridge *dc_bridge)
+{
+	int ret = 0;
+
+	switch (dc_bridge->encoder_obj) {
+	case ENCODER_OBJECT_ID_EDP_LT6711:
+		ret = bridge_phy_lt6711_init(dc_bridge);
+		break;
+	case ENCODER_OBJECT_ID_EDP_LT9721:
+		ret = bridge_phy_lt9721_init(dc_bridge);
+		break;
+	case ENCODER_OBJECT_ID_LVDS_LT8619:
+		ret = bridge_phy_lt8619_init(dc_bridge);
+		break;
+	case ENCODER_OBJECT_ID_EDP_NCS8805:
+		ret = bridge_phy_ncs8805_init(dc_bridge);
+		break;
+	case ENCODER_OBJECT_ID_NONE:
+	case ENCODER_OBJECT_ID_VGA_TRANSPARENT:
+	case ENCODER_OBJECT_ID_HDMI_TRANSPARENT:
+	case ENCODER_OBJECT_ID_EDP_TRANSPARENT:
+	default:
+		ret = 0;
+		DRM_DEBUG_DRIVER("No matching chip! Skip bridge phy init\n");
+		break;
+	}
+
+	return ret;
+}
+
+static int bridge_phy_init(struct gsgpu_bridge_phy *phy)
+{
+	bridge_phy_hw_reset(phy);
+	bridge_phy_regmap_init(phy);
+	bridge_phy_register_irq_num(phy);
+
+	bridge_phy_misc_init(phy);
+	bridge_phy_bind(phy);
+	bridge_phy_sw_init(phy);
+	bridge_phy_register_irq_handle(phy);
+	DRM_INFO("[Bridge_phy] %s init finish.\n", phy->res->chip_name);
+
+	return 0;
+}
+
+/**
+ * @section Bridge-phy interface
+ */
+int bridge_phy_register(struct gsgpu_bridge_phy *phy,
+			const struct bridge_phy_cfg_funcs *cfg_funcs,
+			u32 feature, struct bridge_phy_helper *helper)
+{
+	phy->feature = feature;
+	if (cfg_funcs)
+		phy->cfg_funcs = cfg_funcs;
+
+	if (helper)
+		bridge_phy_add_helper_funcs(phy, helper);
+
+	bridge_phy_init(phy);
+	DRM_DEBUG_DRIVER("bridge phy register success!\n");
+
+	return 0;
+}
+
+struct gsgpu_bridge_phy *bridge_phy_alloc(struct gsgpu_dc_bridge *dc_bridge)
+{
+	struct gsgpu_bridge_phy *bridge_phy;
+	int index = dc_bridge->display_pipe_index;
+	struct gsgpu_dc_connector *connector =
+			dc_bridge->adev->dc->link_info[index].connector;
+
+	bridge_phy = kzalloc(sizeof(*bridge_phy), GFP_KERNEL);
+	if (IS_ERR(bridge_phy)) {
+		DRM_ERROR("Failed to alloc gsgpu bridge phy!\n");
+		return NULL;
+	}
+
+	bridge_phy->display_pipe_index = dc_bridge->display_pipe_index;
+	bridge_phy->bridge.driver_private = bridge_phy;
+	bridge_phy->adev = dc_bridge->adev;
+	bridge_phy->res = dc_bridge;
+	bridge_phy->encoder = &dc_bridge->adev->mode_info.encoders[index]->base;
+	bridge_phy->li2c = dc_bridge->adev->i2c[index];
+	bridge_phy->connector_type = connector->resource->type;
+	dc_bridge->adev->mode_info.encoders[index]->bridge = bridge_phy;
+
+	return bridge_phy;
+}
+
+struct gsgpu_dc_bridge
+*dc_bridge_construct(struct gsgpu_dc *dc,
+		     struct encoder_resource *encoder_res,
+		     struct connector_resource *connector_res)
+{
+	struct gsgpu_dc_bridge *dc_bridge;
+	const char *chip_name;
+	u32 link;
+
+	if (IS_ERR_OR_NULL(dc) ||
+	    IS_ERR_OR_NULL(encoder_res) ||
+	    IS_ERR_OR_NULL(connector_res))
+		return NULL;
+
+	dc_bridge = kzalloc(sizeof(*dc_bridge), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(dc_bridge))
+		return NULL;
+
+	link = encoder_res->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	dc_bridge->adev = dc->adev;
+	dc_bridge->dc = dc;
+	dc_bridge->display_pipe_index = link;
+
+	dc_bridge->encoder_obj = encoder_res->chip;
+	chip_name = get_encoder_chip_name(dc_bridge->encoder_obj);
+	snprintf(dc_bridge->chip_name, NAME_SIZE_MAX, "%s", chip_name);
+	dc_bridge->i2c_bus_num = encoder_res->i2c_id;
+	dc_bridge->i2c_dev_addr = encoder_res->chip_addr;
+
+	dc_bridge->hotplug = connector_res->hotplug;
+	dc_bridge->edid_method = connector_res->edid_method;
+	dc_bridge->gpio_placement = connector_res->gpio_placement;
+	dc_bridge->irq_gpio = connector_res->irq_gpio;
+
+	switch (dc_bridge->encoder_obj) {
+	case ENCODER_OBJECT_ID_EDP_LT6711:
+	case ENCODER_OBJECT_ID_EDP_LT9721:
+	case ENCODER_OBJECT_ID_EDP_NCS8805:
+		dc->link_info[link].encoder->has_ext_encoder = true;
+		break;
+	default:
+		dc->link_info[link].encoder->has_ext_encoder = false;
+		break;
+	}
+
+	DRM_INFO("Encoder Parse: #0x%02x-%s type:%s hotplug:%s.\n",
+		 dc_bridge->encoder_obj, dc_bridge->chip_name,
+		 encoder_type_to_str(encoder_res->type),
+		 hotplug_to_str(dc_bridge->hotplug));
+	DRM_INFO("Encoder Parse: config_type:%s, edid_method:%s.\n",
+		 encoder_config_to_str(encoder_res->config_type),
+		 edid_method_to_str(dc_bridge->edid_method));
+
+	return dc_bridge;
+}
+
+int gsgpu_dc_bridge_init(struct gsgpu_device *adev, int link_index)
+{
+	struct gsgpu_dc_bridge *dc_bridge =
+					adev->dc->link_info[link_index].bridge;
+	int ret;
+
+	if (link_index >= 2)
+		return -1;
+
+	ret = bridge_phy_encoder_obj_select(dc_bridge);
+	if (ret)
+		return ret;
+
+	return ret;
+}
+
+void gsgpu_bridge_suspend(struct gsgpu_device *adev)
+{
+	struct gsgpu_bridge_phy *phy;
+	int i;
+
+	for (i = 0; i < 2; i++) {
+		phy = adev->mode_info.encoders[i]->bridge;
+		if (phy && phy->cfg_funcs && phy->cfg_funcs->suspend) {
+			phy->cfg_funcs->suspend(phy);
+			DRM_INFO("[Bridge_phy] %s suspend completed.\n",
+					phy->res->chip_name);
+		}
+	}
+}
+
+void gsgpu_bridge_resume(struct gsgpu_device *adev)
+{
+	struct gsgpu_bridge_phy *phy = NULL;
+	int i;
+
+	for (i = 0; i < 2; i++) {
+		phy = adev->mode_info.encoders[i]->bridge;
+		if (phy) {
+			if (phy->cfg_funcs &&  phy->cfg_funcs->resume)
+				phy->cfg_funcs->resume(phy);
+			else {
+				bridge_phy_hw_reset(phy);
+				bridge_phy_sw_init(phy);
+			}
+			DRM_INFO("[Bridge_phy] %s resume completed.\n",
+					phy->res->chip_name);
+		}
+	}
+}
diff --git a/drivers/gpu/drm/gsgpu/bridge/bridge_phy.h b/drivers/gpu/drm/gsgpu/bridge/bridge_phy.h
new file mode 100644
index 000000000000..653020ae9d57
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/bridge/bridge_phy.h
@@ -0,0 +1,292 @@
+#ifndef __BRIDGE_PHY_H__
+#define __BRIDGE_PHY_H__
+
+#include <linux/types.h>
+#include <linux/i2c.h>
+#include <linux/gpio.h>
+#include <linux/pwm.h>
+#include <linux/regmap.h>
+#include <linux/hdmi.h>
+#include <drm/drm_edid.h>
+#include <drm/drm_bridge.h>
+#include <drm/drm_connector.h>
+#include <video/videomode.h>
+#include "gsgpu.h"
+
+/* MISC: 0x00~-0x0f */
+#define ENCODER_OBJECT_ID_NONE 0x00
+/* VGA: 0x10~-0x1f */
+#define ENCODER_OBJECT_ID_VGA_TRANSPARENT 0x1F
+/* DVI: 0x20~-0x2f */
+#define ENCODER_OBJECT_ID_DVI_TRANSPARENT 0x2f
+/* HDMI: 0x30~-0x3f */
+#define ENCODER_OBJECT_ID_HDMI_IT66121 0x30
+#define ENCODER_OBJECT_ID_HDMI_LT8618 0x32
+#define ENCODER_OBJECT_ID_HDMI_MS7210 0x33
+#define ENCODER_OBJECT_ID_HDMI_TRANSPARENT 0x3F
+/* EDP: 0x40~-0x4f */
+#define ENCODER_OBJECT_ID_EDP_NCS8805 0x40
+#define ENCODER_OBJECT_ID_EDP_NCS8803 0x41
+#define ENCODER_OBJECT_ID_EDP_LT9721 0x42
+#define ENCODER_OBJECT_ID_EDP_LT6711 0x43
+#define ENCODER_OBJECT_ID_EDP_TRANSPARENT 0x4F
+/* HDMI to LVDS */
+#define ENCODER_OBJECT_ID_LVDS_LT8619 0x50
+#define ENCODER_OBJECT_ID_MAX 0xFF
+
+#define NAME_SIZE_MAX 50U
+#define LS7A_GPIO_OFFSET 16U
+
+#define to_bridge_phy(drm_bridge)                                            \
+	container_of(drm_bridge, struct gsgpu_bridge_phy, bridge)
+
+struct gsgpu_bridge_phy;
+
+struct gsgpu_dc_bridge {
+	struct gsgpu_device *adev;
+	struct gsgpu_dc *dc;
+	struct list_head node;
+	int display_pipe_index;
+	int encoder_obj;
+	char chip_name[NAME_SIZE_MAX];
+	char vendor_str[NAME_SIZE_MAX];
+	unsigned int i2c_bus_num;
+	unsigned short i2c_dev_addr;
+	unsigned short hotplug;
+	unsigned short edid_method;
+	unsigned int irq_gpio;
+	unsigned int gpio_placement;
+};
+
+enum encoder_type {
+	encoder_none,
+	encoder_dac,
+	encoder_tmds,
+	encoder_lvds,
+	encoder_tvdac,
+	encoder_virtual,
+	encoder_dsi,
+	encoder_dpmst,
+	encoder_dpi,
+};
+
+enum hpd_status {
+	hpd_status_plug_off = 0,
+	hpd_status_plug_on = 1,
+};
+
+enum int_type {
+	interrupt_all = 0,
+	interrupt_hpd = 1,
+	interrupt_max = 0xff,
+};
+
+struct reg_mask_seq {
+	unsigned int reg;
+	unsigned int mask;
+	unsigned int val;
+};
+
+#define encoder_type_to_str(index)\
+	(index == encoder_none ? "none" :\
+	(index == encoder_dac ? "dac" :\
+	(index == encoder_tmds ? "tmds" :\
+	(index == encoder_lvds ? "lvds" :\
+	(index == encoder_virtual ? "virtual" :\
+	(index == encoder_dsi ? "dsi" :\
+	(index == encoder_dpmst ? "dpmst" :\
+	(index == encoder_dpi ? "dpi" :\
+	"other"))))))))
+
+#define hotplug_to_str(index)\
+	(index == FORCE_ON ? "connected" :\
+	(index == POLLING ? "polling" :\
+	(index == IRQ ? "irq" :\
+	"Unknown")))
+
+enum encoder_config {
+	encoder_transparent = 0,
+	encoder_os_config, /* vbios_config */
+	encoder_bios_config,
+	encoder_timing_filling, /* legacy */
+	encoder_kernel_driver,  /* Driver */
+	encoder_type_max = 0xffffffff,
+} __packed;
+
+#define encoder_config_to_str(index)\
+	(index == encoder_transparent ? "transparent" :\
+	(index == encoder_os_config ? "os" :\
+	(index == encoder_bios_config ? "bios" :\
+	(index == encoder_timing_filling ? "timing" :\
+	(index == encoder_kernel_driver ? "kernel" :\
+	"Unknown")))))
+
+#define edid_method_to_str(index)\
+	(index == via_null ? "null" :\
+	(index == via_i2c ? "i2c" :\
+	(index == via_vbios ? "vbios" :\
+	(index == via_encoder ? "encoder" :\
+	"Unknown"))))
+
+enum bridge_phy_feature {
+	SUPPORT_HPD = BIT(0),
+	SUPPORT_DDC = BIT(1),
+	SUPPORT_HDMI_AUX = BIT(2),
+};
+
+static const char *const feature_str[] = {
+	[SUPPORT_HPD] = "HPD",
+	[SUPPORT_DDC] = "DDC",
+};
+
+enum input_signal_sample_type {
+	SDR_CLK = 0,
+	DDR_CLK,
+};
+
+struct ddc_status {
+	struct mutex ddc_bus_mutex;
+	bool ddc_bus_idle;
+	bool ddc_bus_error;
+	bool ddc_fifo_empty;
+};
+
+struct bridge_phy_mode_config {
+	bool edid_read;
+	u8 edid_buf[256];
+	union hdmi_infoframe hdmi_frame;
+	struct {
+		enum input_signal_sample_type input_signal_type;
+		bool gen_sync;
+		struct drm_display_mode *mode;
+		struct videomode vmode;
+	} input_mode;
+};
+
+struct bridge_phy_cfg_funcs {
+	int (*reg_init)(struct gsgpu_bridge_phy *phy);
+	int (*hw_reset)(struct gsgpu_bridge_phy *phy);
+	int (*sw_enable)(struct gsgpu_bridge_phy *phy);
+	int (*sw_reset)(struct gsgpu_bridge_phy *phy);
+	int (*suspend)(struct gsgpu_bridge_phy *phy);
+	int (*resume)(struct gsgpu_bridge_phy *phy);
+	void (*prepare)(struct gsgpu_bridge_phy *phy);
+	void (*commit)(struct gsgpu_bridge_phy *phy);
+	int (*backlight_ctrl)(struct gsgpu_bridge_phy *phy, int mode);
+	int (*video_input_cfg)(struct gsgpu_bridge_phy *phy);
+	int (*video_input_check)(struct gsgpu_bridge_phy *phy);
+	int (*video_output_cfg)(struct gsgpu_bridge_phy *phy);
+	int (*video_output_timing)(struct gsgpu_bridge_phy *phy,
+				   const struct drm_display_mode *mode);
+	int (*hdmi_output_mode)(struct gsgpu_bridge_phy *phy);
+	int (*hdmi_audio)(struct gsgpu_bridge_phy *phy);
+	int (*hdmi_csc)(struct gsgpu_bridge_phy *phy);
+	int (*hdmi_hdcp_init)(struct gsgpu_bridge_phy *phy);
+	int (*afe_high)(struct gsgpu_bridge_phy *phy);
+	int (*afe_low)(struct gsgpu_bridge_phy *phy);
+	int (*afe_set_tx)(struct gsgpu_bridge_phy *phy, bool enable);
+	int (*mode_set_pre)(struct drm_bridge *bridge,
+			    const struct drm_display_mode *mode,
+			    const struct drm_display_mode *adj_mode);
+	int (*mode_set)(struct gsgpu_bridge_phy *phy,
+			const struct drm_display_mode *mode,
+			const struct drm_display_mode *adj_mode);
+	int (*mode_set_post)(struct drm_bridge *bridge,
+			     const struct drm_display_mode *mode,
+			     const struct drm_display_mode *adj_mode);
+	enum drm_mode_status (*mode_valid)(struct drm_connector *connector,
+					   struct drm_display_mode *mode);
+};
+
+struct bridge_phy_misc_funcs {
+	bool (*chip_id_verify)(struct gsgpu_bridge_phy *phy, char *id);
+	int (*debugfs_init)(struct gsgpu_bridge_phy *phy);
+	void (*dpms_ctrl)(struct gsgpu_bridge_phy *phy, int mode);
+};
+
+struct bridge_phy_hpd_funcs {
+	enum hpd_status (*get_hpd_status)(struct gsgpu_bridge_phy *phy);
+	int (*int_enable)(struct gsgpu_bridge_phy *phy,
+			  enum int_type interrut);
+	int (*int_disable)(struct gsgpu_bridge_phy *phy,
+			   enum int_type interrut);
+	int (*int_clear)(struct gsgpu_bridge_phy *phy, enum int_type interrut);
+	irqreturn_t (*irq_handler)(int irq, void *dev);
+	irqreturn_t (*isr_thread)(int irq, void *dev);
+};
+
+struct bridge_phy_ddc_funcs {
+	int (*ddc_fifo_fetch)(struct gsgpu_bridge_phy *phy, u8 *buf, u8 block,
+			      size_t len, size_t offset);
+	int (*ddc_fifo_abort)(struct gsgpu_bridge_phy *phy);
+	int (*ddc_fifo_clear)(struct gsgpu_bridge_phy *phy);
+	int (*get_edid_block)(void *data, u8 *buf, unsigned int block,
+			      size_t len);
+	int (*get_modes)(struct gsgpu_bridge_phy *phy,
+			 struct drm_connector *connector);
+};
+
+struct bridge_phy_hdmi_aux_funcs {
+	int (*set_gcp_avmute)(struct gsgpu_bridge_phy *phy, bool enable,
+			      bool blue_screen);
+	int (*set_avi_infoframe)(struct gsgpu_bridge_phy *phy,
+				 const struct drm_display_mode *mode);
+	int (*set_hdcp)(struct gsgpu_bridge_phy *phy);
+};
+
+struct bridge_phy_helper {
+	const struct regmap_config *regmap_cfg;
+	struct bridge_phy_misc_funcs *misc_funcs;
+	struct bridge_phy_hpd_funcs *hpd_funcs;
+	struct bridge_phy_ddc_funcs *ddc_funcs;
+	struct bridge_phy_hdmi_aux_funcs *hdmi_aux_funcs;
+};
+
+struct gsgpu_bridge_phy {
+	int display_pipe_index;
+	struct drm_bridge bridge;
+	struct drm_encoder *encoder;
+	struct drm_connector *connector;
+	enum drm_connector_status status;
+
+	struct gsgpu_dc_bridge *res;
+	struct gsgpu_device *adev;
+	struct gsgpu_dc_i2c *li2c;
+	struct i2c_client *i2c_phy;
+	struct regmap *phy_regmap;
+
+	void *priv;
+	u8 chip_version;
+	u32 feature;
+	u32 connector_type;
+
+	u8 sys_status;
+	int irq_num;
+	atomic_t irq_status;
+	struct ddc_status ddc_status;
+	struct bridge_phy_mode_config mode_config;
+	struct bridge_phy_helper *helper;
+	const struct bridge_phy_cfg_funcs *cfg_funcs;
+	const struct bridge_phy_hpd_funcs *hpd_funcs;
+	const struct bridge_phy_ddc_funcs *ddc_funcs;
+	const struct bridge_phy_hdmi_aux_funcs *hdmi_aux_funcs;
+};
+
+struct gsgpu_dc_bridge
+*dc_bridge_construct(struct gsgpu_dc *dc,
+		     struct encoder_resource *encoder_res,
+		     struct connector_resource *connector_res);
+int gsgpu_dc_bridge_init(struct gsgpu_device *adev, int link_index);
+struct gsgpu_bridge_phy *bridge_phy_alloc(struct gsgpu_dc_bridge *dc_bridge);
+int bridge_phy_register(struct gsgpu_bridge_phy *phy,
+			const struct bridge_phy_cfg_funcs *cfg_funcs,
+			u32 feature, struct bridge_phy_helper *helper);
+
+int bridge_phy_lt6711_init(struct gsgpu_dc_bridge *dc_bridge);
+int bridge_phy_lt9721_init(struct gsgpu_dc_bridge *res);
+int bridge_phy_lt8619_init(struct gsgpu_dc_bridge *dc_bridge);
+int bridge_phy_ncs8805_init(struct gsgpu_dc_bridge *dc_bridge);
+void bridge_phy_mode_set(struct gsgpu_bridge_phy *phy,
+				struct drm_display_mode *mode,
+				struct drm_display_mode *adj_mode);
+#endif /* __BRIDGE_PHY_H__ */
diff --git a/drivers/gpu/drm/gsgpu/bridge/lt6711_drv.c b/drivers/gpu/drm/gsgpu/bridge/lt6711_drv.c
new file mode 100644
index 000000000000..ce438c73d498
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/bridge/lt6711_drv.c
@@ -0,0 +1,75 @@
+
+#include "gsgpu.h"
+#include "gsgpu_dc.h"
+#include "gsgpu_dc_vbios.h"
+#include "bridge_phy.h"
+
+static enum drm_mode_status lt6711_mode_valid(struct drm_connector *connector,
+					      struct drm_display_mode *mode)
+{
+	if (mode->hdisplay < 1920)
+		return MODE_BAD;
+	if (mode->vdisplay < 1080)
+		return MODE_BAD;
+
+	return MODE_OK;
+}
+
+static int lt6711_get_modes(struct gsgpu_bridge_phy *phy,
+			    struct drm_connector *connector)
+{
+	struct gsgpu_dc_i2c *i2c = phy->li2c;
+	struct edid *edid;
+	unsigned int count = 0;
+
+	edid = drm_get_edid(connector, &i2c->adapter);
+	if (edid) {
+		drm_connector_update_edid_property(connector, edid);
+		count = drm_add_edid_modes(connector, edid);
+		kfree(edid);
+	} else {
+		DRM_ERROR("LT6711 edid is invalid.\n");
+	}
+
+	return count;
+}
+
+static enum hpd_status lt6711_get_hpd_status(struct gsgpu_bridge_phy *phy)
+{
+	return hpd_status_plug_on;
+}
+
+static const struct bridge_phy_cfg_funcs lt6711_cfg_funcs = {
+	.mode_valid = lt6711_mode_valid,
+};
+
+static struct bridge_phy_ddc_funcs lt6711_ddc_funcs = {
+	.get_modes = lt6711_get_modes,
+};
+
+static struct bridge_phy_hpd_funcs lt6711_hpd_funcs = {
+	.get_hpd_status = lt6711_get_hpd_status,
+};
+
+static struct bridge_phy_helper lt6711_helper_funcs = {
+	.ddc_funcs = &lt6711_ddc_funcs,
+	.hpd_funcs = &lt6711_hpd_funcs,
+};
+
+int bridge_phy_lt6711_init(struct gsgpu_dc_bridge *dc_bridge)
+{
+	struct gsgpu_bridge_phy *lt6711_phy;
+	int ret = -1;
+	u32 feature;
+
+	feature = SUPPORT_DDC | SUPPORT_HPD;
+	lt6711_phy = bridge_phy_alloc(dc_bridge);
+	ret = bridge_phy_register(lt6711_phy, &lt6711_cfg_funcs, feature,
+				  &lt6711_helper_funcs);
+	return ret;
+}
+
+int bridge_phy_lt6711_remove(struct gsgpu_dc_bridge *phy)
+{
+	return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/bridge/lt8619_drv.c b/drivers/gpu/drm/gsgpu/bridge/lt8619_drv.c
new file mode 100644
index 000000000000..8e551aae483d
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/bridge/lt8619_drv.c
@@ -0,0 +1,671 @@
+#include <linux/delay.h>
+#include <linux/types.h>
+#include <linux/i2c.h>
+#include <linux/regmap.h>
+
+#include <loongson-pch.h>
+#include "gsgpu.h"
+#include "gsgpu_dc.h"
+#include "gsgpu_dc_vbios.h"
+#include "bridge_phy.h"
+
+#define LT8619_REG_START                        0x0000
+#define LT8619_REG_END                          0x80FE
+#define LT8619_REG_PAGE_SELECT                  0xFF
+#define EDID_DATA_SIZE                          256
+#define WAIT_TIMES                              10000
+
+/* REG 0x6000 */
+#define LT8619_REG_CHIP_ID                      0x6000
+
+/* REG 0x6004 */
+
+/* Analog register clock enable(Address range: 0x6080 ~ 0x60FE) */
+#define ANALOG_REG_CLK_EN_BIT                   BIT(7)
+/* Control register clock enable(Address range: 0x6010 ~ 0x607F) */
+#define CONTROL_REG_CLK_EN_BIT                  BIT(6)
+/* HDMI register clock enable(Address range: 0x8000 ~ 0x809F) */
+#define HDMI_REG_CLK_EN_BIT                     BIT(5)
+/* HDCP register clock enable(Address range: 0x80A0 ~ 0x80CF) */
+#define HDCP_REG_CLK_EN_BIT                     BIT(4)
+/* LVDS PLL lock detect module clock enable */
+#define LVDS_PLL_LOCK_DETECT_CLK_EN_BIT         BIT(1)
+
+/* REG 0x6006 */
+#define LVDS_TX_CLK_EN_BIT                      BIT(7)
+#define BT_TX_CLK_EN_BIT                        BIT(6)
+#define EDID_SHADOW_CLK_EN_BIT                  BIT(5)
+#define DUAL_PORT_CLK_EN_BIT                    BIT(4)
+#define CEC_CTL_SYS_CLK_EN_BIT                  BIT(3)
+#define VIDEO_CHECK_SYS_CLK_EN_BIT              BIT(2)
+#define VIDEO_CHECK_PIX_CLK_EN_BIT              BIT(1)
+#define INTERRUPT_PROCESS_SYS_CLK_EN_BIT        BIT(0)
+
+/* REG 0x600A */
+#define LVDS_PLL_LOCK_DETECT_CTL_SW_RST         BIT(6)
+
+/* REG 0x600C */
+#define EDID_SHADOW_SW_RST_BIT                  BIT(6)
+#define VIDEO_CHK_SW_RST_BIT                    BIT(2)
+
+/* REG 0x600D */
+#define LVDS_TX_CTL_SW_RST                      BIT(2)
+#define BT_TX_CTL_SW_RST                        BIT(1)
+#define BT_TX_AFIFO_SW_RST                      BIT(0)
+
+/* REG 0x600E */
+#define HDMI_RX_CALIBRATION_BIT                 BIT(7)
+#define HDMI_RX_PLL_BIT                         BIT(6)
+#define HDMI_RX_PI0_BIT                         BIT(5)
+#define HDMI_RX_PI1_BIT                         BIT(4)
+#define HDMI_RX_PI2_BIT                         BIT(3)
+#define HDMI_RX_AUDIO_PLL_BIT                   BIT(2)
+#define LVDS_PLL_SW_RST_BIT                     BIT(1)
+#define LVDS_TX_CLK_GEN_RESET                   BIT(0)
+
+/* REG 0x60A0 */
+/* 1 = Power down, 0 = Power up */
+#define LVDS_PLL_PD                             BIT(7)
+/* 1 = Adaptive BW tracking PLL, 0 = Second order passive LPF PLL */
+#define LVDS_PLL_ADAPRT_BW_EN                   BIT(6)
+/* 1 = High BW, 0 = Low BW */
+#define LVDS_PLL_CP_CUR_SEL                     BIT(5)
+/* This bit controls the operation of PLL locking EN */
+#define LVDS_PLL_LOCK_EN                        BIT(4)
+
+/* Reference clock selection:
+ * 1 = Pixel clock as reference, 0 = Double clock as reference
+ */
+#define LVDS_PLL_PIX_CLK_SEL                    BIT(3)
+/* Pixel clock selection: 1 = 2 x lvds clock, 0 = 1 x lvds clock */
+#define LVDS_PLL_DUAL_MODE_EN                   BIT(2)
+/* Pixel clock selection: 1 = 4 x lvds clock, 0 = 2 x lvds clock */
+#define LVDS_PLL_HALF_MODE_EN                   BIT(1)
+/* BT clock clock selection */
+#define LVDS_PLL_DOUB_MODE_EN                   BIT(0)
+
+/* REG 0x60A8 */
+#define RGB888_TTL_EN_BIT                       BIT(3)
+
+/* 1 = Input is HDMI, 0 = Input is DVI */
+#define RX_IS_HDMI_BIT                          BIT(1)
+/* 1 = Hsync is detected and is stable, 0 = No hsync detected or not stable */
+#define RX_HDMI_HSYNC_STABLE_BIT                BIT(0)
+
+/* Set to enable clock data recovery bandwidth adaptation */
+#define CDR_BW_ADAP_EN_BIT                      BIT(3)
+
+/* REG 0x8044 */
+/* Clock stable status indicator */
+#define RX_CLK_STABLE_BIT                       BIT(3)
+
+/* REG 0x8071 */
+/* Packet byte 1 of AVI information
+ * {Y1, Y0} of {0,Y1,Y0,A0,B1,B0,S1,S0}:
+ */
+#define COLOR_SPACE_MASK                        GENMASK(6, 5)
+#define CSC_RGB                                 0x00
+#define CSC_YCBCR_422                           0x20
+#define CSC_YCBCR_444                           0x40
+#define CSC_FUTURE                              0x60
+
+/* TMDS clock frequency, unit: kHz
+ * +------+--------+--------+--------+
+ * | REG  | 0x8044 | 0x8045 | 0x8046 |
+ * +------+--------+--------+--------+
+ * | bits |  18:16 |  15:8  |  7:0   |
+ * +------+--------+--------+--------+
+ */
+
+/* REG 0x8087 */
+#define LVDS_PLL_LOCKED_BIT                     BIT(5)
+#define RX_PLL_LOCKED_BIT                       BIT(4)
+
+struct lt8619_timing {
+	int polarity;
+	int h_front_porch;
+	int h_back_porch;
+	int h_total;
+	int h_active;
+	int v_front_porch;
+	int v_back_porch;
+	int v_total;
+	int v_active;
+	int v_sync;
+	int h_sync;
+};
+
+static const struct regmap_range_cfg lt8619_ranges[] = {
+	{
+		.name = "lt8619_register_range",
+		.range_min = LT8619_REG_START,
+		.range_max = LT8619_REG_END,
+		.selector_reg = LT8619_REG_PAGE_SELECT,
+		.selector_mask = 0xFF,
+		.selector_shift = 0,
+		.window_start = 0,
+		.window_len = 0x100,
+	},
+};
+
+static const struct regmap_config lt8619_regmap_config = {
+	.reg_bits = 8,
+	.val_bits = 8,
+	.reg_stride = 1,
+	.max_register = LT8619_REG_END,
+	.ranges = lt8619_ranges,
+	.num_ranges = ARRAY_SIZE(lt8619_ranges),
+	.fast_io = false,
+};
+
+static void lt8619_read_revision(struct gsgpu_bridge_phy *phy)
+{
+	unsigned int chip_id = 0;
+	int ret;
+
+	ret = regmap_bulk_read(phy->phy_regmap,
+			       LT8619_REG_CHIP_ID, &chip_id, 4);
+	if (ret) {
+		DRM_ERROR("failed to read revision: %d\n", ret);
+		return;
+	}
+
+	DRM_DEV_INFO(&phy->li2c->ddc_client->dev, "LT8619 vision: 0x%x\n",
+		     chip_id);
+}
+
+static int lt8619_config_rgb888_phy(struct gsgpu_bridge_phy *phy, bool enable)
+{
+	int ret;
+	unsigned int val;
+
+	val = enable ? RGB888_TTL_EN_BIT : 0;
+
+	ret = regmap_update_bits(phy->phy_regmap, 0x60A8,
+				 RGB888_TTL_EN_BIT, val);
+
+	DRM_DEBUG_DRIVER("%s: reg=0x%x, val=0x%x\n", __func__, 0x60A8, val);
+
+	return ret;
+}
+
+static void lt8619_edid_shadow_clk_enable(struct gsgpu_bridge_phy *phy)
+{
+	unsigned int val;
+
+	regmap_read(phy->phy_regmap, 0x6006, &val);
+
+	if (val & EDID_SHADOW_CLK_EN_BIT)
+		DRM_DEBUG_DRIVER("edid shadow clk is enabled\n");
+	else
+		DRM_DEBUG_DRIVER("edid shadow clk is not enabled\n");
+
+	regmap_update_bits(phy->phy_regmap, 0x6006,
+			   EDID_SHADOW_CLK_EN_BIT,
+			   EDID_SHADOW_CLK_EN_BIT);
+}
+
+static void lt8619_lvds_tx_clk_enable(struct gsgpu_bridge_phy *phy)
+{
+	regmap_update_bits(phy->phy_regmap, 0x6006,
+			   LVDS_TX_CLK_EN_BIT,
+			   LVDS_TX_CLK_EN_BIT);
+
+	regmap_update_bits(phy->phy_regmap, 0x6006,
+			   BT_TX_CLK_EN_BIT,
+			   BT_TX_CLK_EN_BIT);
+
+	DRM_DEBUG_DRIVER("LVDS TX controller module clock enabled\n");
+}
+
+static int lt8619_enable_sys_clk(struct gsgpu_bridge_phy *phy, bool is_lvds)
+{
+	u32 val = ANALOG_REG_CLK_EN_BIT |
+		  CONTROL_REG_CLK_EN_BIT |
+		  HDMI_REG_CLK_EN_BIT |
+		  HDCP_REG_CLK_EN_BIT;
+	int ret;
+
+	if (is_lvds)
+		val |= LVDS_PLL_LOCK_DETECT_CLK_EN_BIT;
+
+	ret = regmap_write(phy->phy_regmap, 0x6004, val);
+
+	return ret;
+}
+
+static int lt8619_enable_cdr_bandwidth_adaptation(struct gsgpu_bridge_phy *phy)
+{
+	int ret;
+
+	ret = regmap_update_bits(phy->phy_regmap, 0x802C,
+				 CDR_BW_ADAP_EN_BIT,
+				 CDR_BW_ADAP_EN_BIT);
+
+	return ret;
+}
+
+static void lt8619_wait_rx_pll_locked(struct gsgpu_bridge_phy *phy)
+{
+	unsigned int val = 0;
+	unsigned int count = 0;
+
+	do {
+		regmap_read(phy->phy_regmap, 0x8087, &val);
+		count++;
+		udelay(5);
+	} while (((val & RX_PLL_LOCKED_BIT) == 0) && (count < WAIT_TIMES));
+
+	if (count >= WAIT_TIMES)
+		DRM_ERROR("HDMI RX PLL IS NOT LOCKED");
+	else
+		DRM_DEBUG_DRIVER("HDMI RX PLL LOCKED\n");
+}
+
+/*
+ * When HDMI signal is stable, LVDS PLL lock status needs to be detected.
+ * If it is not locked, LVDS PLL needs to be reset.
+ */
+static void lt8619_wait_hdmi_stable(struct gsgpu_bridge_phy *phy)
+{
+	unsigned int val = 0;
+	unsigned int count = 0;
+
+	do {
+		regmap_read(phy->phy_regmap, 0x8044, &val);
+		count++;
+		mdelay(50);
+	} while ((!(val & RX_CLK_STABLE_BIT)) && (count < 5));
+
+	DRM_DEBUG_DRIVER("HDMI clock signal stabled\n");
+
+	lt8619_wait_rx_pll_locked(phy);
+
+	count = 0;
+	DRM_DEBUG_DRIVER("Wait HDMI HSync detect and stable\n");
+
+	do {
+		regmap_read(phy->phy_regmap, 0x8013, &val);
+		count++;
+		mdelay(50);
+	} while ((!(val & RX_HDMI_HSYNC_STABLE_BIT)) && (count < 5));
+
+	DRM_DEBUG_DRIVER("HDMI HSync stabled\n");
+}
+
+/* TMDS clock frequency indicator */
+static void lt8619_read_hdmi_clock_frequency(struct gsgpu_bridge_phy *phy,
+					     unsigned int *pfreq)
+{
+	unsigned int up, mid, low;
+	unsigned int freq;
+
+	regmap_read(phy->phy_regmap, 0x8044, &up);
+	regmap_read(phy->phy_regmap, 0x8045, &mid);
+	regmap_read(phy->phy_regmap, 0x8046, &low);
+
+	freq = ((up & 0x07) << 16) + (mid << 8) + low;
+
+	if (pfreq)
+		*pfreq = freq;
+	else
+		DRM_DEBUG_DRIVER("HDMI clock frequency: %dkHz\n", freq);
+}
+
+/* LVDS TX controller module soft reset */
+static void lt8619_lvds_tx_ctl_soft_reset(struct gsgpu_bridge_phy *phy)
+{
+	regmap_update_bits(phy->phy_regmap, 0x600D, LVDS_TX_CTL_SW_RST, 0);
+	mdelay(100);
+	regmap_update_bits(phy->phy_regmap, 0x600D,
+			   LVDS_TX_CTL_SW_RST,
+			   LVDS_TX_CTL_SW_RST);
+	mdelay(100);
+	DRM_DEBUG_DRIVER("LVDS TX controller module soft reset finished\n");
+}
+
+static void lt8619_edid_shadow_soft_reset(struct gsgpu_bridge_phy *phy)
+{
+	regmap_update_bits(phy->phy_regmap, 0x600C, EDID_SHADOW_SW_RST_BIT, 0);
+	mdelay(100);
+	regmap_update_bits(phy->phy_regmap, 0x600C,
+			   EDID_SHADOW_SW_RST_BIT,
+			   EDID_SHADOW_SW_RST_BIT);
+	mdelay(100);
+	DRM_DEBUG_DRIVER("%s\n", __func__);
+}
+
+/* Video check logic soft reset */
+static void lt8619_vid_chk_soft_reset(struct gsgpu_bridge_phy *phy)
+{
+	regmap_update_bits(phy->phy_regmap, 0x600C, VIDEO_CHK_SW_RST_BIT, 0);
+	mdelay(100);
+	regmap_update_bits(phy->phy_regmap, 0x600C,
+			   VIDEO_CHK_SW_RST_BIT,
+			   VIDEO_CHK_SW_RST_BIT);
+	mdelay(100);
+}
+
+/*
+ * Read HDMI Timing information
+ */
+static ssize_t lt8619_read_hdmi_timings(struct gsgpu_bridge_phy *phy,
+					struct lt8619_timing *buf)
+{
+	unsigned int pos = 0;
+	unsigned int high, low;
+	unsigned int polarity;
+	unsigned int h_front_porch, h_back_porch, h_total, h_active;
+	unsigned int v_front_porch, v_back_porch, v_total, v_active;
+	unsigned int v_sync, h_sync;
+
+	/* horizontal sync width */
+	regmap_read(phy->phy_regmap, 0x6014, &high);
+	regmap_read(phy->phy_regmap, 0x6015, &low);
+	h_sync = (high << 8) + low;
+	/* horizontal back porch */
+	regmap_read(phy->phy_regmap, 0x6018, &high);
+	regmap_read(phy->phy_regmap, 0x6019, &low);
+	h_back_porch = (high << 8) + low;
+	/* horizontal front porch */
+	regmap_read(phy->phy_regmap, 0x601A, &high);
+	regmap_read(phy->phy_regmap, 0x601B, &low);
+	h_front_porch = (high << 8) + low;
+	/* horizontal total */
+	regmap_read(phy->phy_regmap, 0x601C, &high);
+	regmap_read(phy->phy_regmap, 0x601D, &low);
+	h_total = (high << 8) + low;
+	/* horizontal active */
+	regmap_read(phy->phy_regmap, 0x6020, &high);
+	regmap_read(phy->phy_regmap, 0x6021, &low);
+	h_active = (high << 8) + low;
+
+	/* vertical total */
+	regmap_read(phy->phy_regmap, 0x601E, &high);
+	regmap_read(phy->phy_regmap, 0x601F, &low);
+	v_total = (high << 8) + low;
+	/* vertical active */
+	regmap_read(phy->phy_regmap, 0x6022, &high);
+	regmap_read(phy->phy_regmap, 0x6023, &low);
+	v_active = (high << 8) + low;
+	/* vertical back porch */
+	regmap_read(phy->phy_regmap, 0x6016, &v_back_porch);
+	/* vertical front porch */
+	regmap_read(phy->phy_regmap, 0x6017, &v_front_porch);
+	/* vertical sync width */
+	regmap_read(phy->phy_regmap, 0x6013, &v_sync);
+
+	/* The vsync polarity and hsync polarity */
+	regmap_read(phy->phy_regmap, 0x6024, &polarity);
+
+	if (buf) {
+		buf->polarity = polarity;
+		buf->h_front_porch = h_front_porch;
+		buf->h_back_porch = h_back_porch;
+		buf->h_total = h_total;
+		buf->h_active = h_active;
+		buf->v_front_porch = v_front_porch;
+		buf->v_back_porch = v_back_porch;
+		buf->v_total = v_total;
+		buf->v_active = v_active;
+		buf->v_sync = v_sync;
+		buf->h_sync = h_sync;
+		DRM_DEBUG_DRIVER("h_front_porch-%d h_back_porch-%d h_total-%d\n",
+				 h_front_porch, h_back_porch, h_total);
+		DRM_DEBUG_DRIVER("h_active-%d h_sync-%d\n", h_active, h_sync);
+		DRM_DEBUG_DRIVER("v_front_porch-%d v_back_porch-%d v_total-%d\n",
+				 v_front_porch, v_back_porch, v_total);
+		DRM_DEBUG_DRIVER("v_active-%d v_sync-%d\n", v_active, v_sync);
+	} else {
+		DRM_DEBUG_DRIVER("%ux%u\n", h_active, v_active);
+		DRM_DEBUG_DRIVER("%u, %u, %u, %u\n",
+				 h_front_porch, h_back_porch, h_total, h_sync);
+		DRM_DEBUG_DRIVER("%u, %u, %u, %u\n",
+				 v_front_porch, v_back_porch, v_total, v_sync);
+	}
+
+	return pos;
+}
+
+static void lt8619_turn_on_lvds(struct gsgpu_bridge_phy *phy)
+{
+	/* bit2 = 0 => turn on LVDS C */
+	regmap_write(phy->phy_regmap, 0x60BA, 0x18);
+
+	/* bit2 = 0 => turn on LVDS D */
+	regmap_write(phy->phy_regmap, 0x60C0, 0x18);
+}
+
+static void lt8619_turn_off_lvds(struct gsgpu_bridge_phy *phy)
+{
+	/* bit2= 1 => turn off LVDS C */
+	regmap_write(phy->phy_regmap, 0x60BA, 0x44);
+
+	/* bit2= 1 => turn off LVDS D */
+	regmap_write(phy->phy_regmap, 0x60C0, 0x44);
+}
+
+static void lt8619_hdmi_rx_reset(struct gsgpu_bridge_phy *phy)
+{
+	regmap_write(phy->phy_regmap, 0x600E, 0xBF);
+	regmap_write(phy->phy_regmap, 0x6009, 0xFD);
+	mdelay(50);
+
+	regmap_write(phy->phy_regmap, 0x600E, 0xFF);
+	regmap_write(phy->phy_regmap, 0x6009, 0xFF);
+	mdelay(50);
+
+	regmap_write(phy->phy_regmap, 0x600E, 0xC7);
+	regmap_write(phy->phy_regmap, 0x6009, 0x0F);
+	mdelay(100);
+
+	regmap_write(phy->phy_regmap, 0x600E, 0xFF);
+	mdelay(100);
+	regmap_write(phy->phy_regmap, 0x6009, 0x8F);
+	mdelay(100);
+	regmap_write(phy->phy_regmap, 0x6009, 0xFF);
+	mdelay(100);
+}
+
+/* LVDS PLL lock detect control logic Soft Reset */
+static void lt8619_lvds_pll_lock_ctl_soft_reset(struct gsgpu_bridge_phy *phy)
+{
+	regmap_update_bits(phy->phy_regmap, 0x600A,
+			   LVDS_PLL_LOCK_DETECT_CTL_SW_RST, 0);
+	mdelay(100);
+	regmap_update_bits(phy->phy_regmap, 0x600A,
+			   LVDS_PLL_LOCK_DETECT_CTL_SW_RST,
+			   LVDS_PLL_LOCK_DETECT_CTL_SW_RST);
+}
+
+/* LVDS PLL Soft Reset */
+static void lt8619_lvds_pll_soft_reset(struct gsgpu_bridge_phy *phy)
+{
+	regmap_update_bits(phy->phy_regmap, 0x600E, LVDS_PLL_SW_RST_BIT, 0);
+	mdelay(100);
+	regmap_update_bits(phy->phy_regmap, 0x600E, LVDS_PLL_SW_RST_BIT,
+						LVDS_PLL_SW_RST_BIT);
+
+	DRM_DEBUG_DRIVER("%s\n", __func__);
+}
+
+static void lt8619_wait_lvds_pll_locked(struct gsgpu_bridge_phy *phy)
+{
+	unsigned int val = 0;
+	unsigned int val1 = 0;
+	unsigned int count = 0;
+
+	regmap_read(phy->phy_regmap, 0x8087, &val);
+	while ((val & 0x20) == 0x00) {
+		regmap_read(phy->phy_regmap, 0x600E, &val1);
+		regmap_write(phy->phy_regmap, 0x600E, val1 & 0xFD);
+		mdelay(50);
+		regmap_write(phy->phy_regmap, 0x600E, 0xFF);
+		count++;
+
+		if (count > 50)
+			break;
+	}
+
+	if (count >= 50)
+		DRM_ERROR("LVDS PLL is NOT locked\n");
+	else
+		DRM_DEBUG_DRIVER("LVDS PLL locked\n");
+}
+
+static void lt8619_config_lvds_pll(struct gsgpu_bridge_phy *phy)
+{
+	/* LVDS TX controller module clock enable */
+	regmap_write(phy->phy_regmap, 0x60A0,
+		     LVDS_PLL_ADAPRT_BW_EN |
+		     LVDS_PLL_LOCK_EN |
+		     LVDS_PLL_PIX_CLK_SEL);
+}
+
+static void lt8619_config_color_space(struct gsgpu_bridge_phy *phy)
+{
+	unsigned int temp_csc;
+
+	regmap_read(phy->phy_regmap, 0x8071, &temp_csc);
+
+	/* if the color space is not RGB, we need convert it */
+	if ((temp_csc & COLOR_SPACE_MASK) == CSC_YCBCR_422) {
+		/* enable YCbCr to RGB clk */
+		regmap_write(phy->phy_regmap, 0x6007, 0x8C);
+		/* YUV422 to YUV444 enable */
+		regmap_write(phy->phy_regmap, 0x6052, 0x01);
+		/* 0x40:YUV to RGB enable; */
+		regmap_write(phy->phy_regmap, 0x6053, 0x40 + 0x30);
+		DRM_DEBUG_DRIVER("HDMI Color Space is YUV422\n");
+	} else if ((temp_csc & COLOR_SPACE_MASK) == CSC_YCBCR_444) {
+		/* enable YCbCr to RGB clk */
+		regmap_write(phy->phy_regmap, 0x6007, 0x8C);
+		/* YUV444 */
+		regmap_write(phy->phy_regmap, 0x6052, 0x00);
+		/* 0x40:YUV to RGB enable; */
+		regmap_write(phy->phy_regmap, 0x6053, 0x40 + 0x30);
+		DRM_DEBUG_DRIVER("HDMI Color Space is YUV444\n");
+	} else if ((temp_csc & COLOR_SPACE_MASK) == CSC_RGB) {
+		/* 0x00: bypass ColorSpace conversion */
+		regmap_write(phy->phy_regmap, 0x6007, 0x80);
+		regmap_write(phy->phy_regmap, 0x6052, 0x00);
+		regmap_write(phy->phy_regmap, 0x6053, 0x00);
+		DRM_DEBUG_DRIVER("HDMI Color Space is RGB");
+	}
+}
+
+static int lt8619_lvds_config(struct gsgpu_bridge_phy *phy)
+{
+	int ret;
+
+	regmap_write(phy->phy_regmap, 0x6059, 0x40);
+
+	regmap_write(phy->phy_regmap, 0x60A4, 0x01);
+	regmap_write(phy->phy_regmap, 0x605C, 0x01);
+
+	/* LVDS channel output current settings */
+	regmap_write(phy->phy_regmap, 0x60B0, 0x66);
+	regmap_write(phy->phy_regmap, 0x60B1, 0x66);
+	regmap_write(phy->phy_regmap, 0x60B2, 0x66);
+	regmap_write(phy->phy_regmap, 0x60B3, 0x66);
+	regmap_write(phy->phy_regmap, 0x60B4, 0x66);
+
+	regmap_write(phy->phy_regmap, 0x60B5, 0x41);
+	regmap_write(phy->phy_regmap, 0x60B6, 0x41);
+	regmap_write(phy->phy_regmap, 0x60B7, 0x41);
+	regmap_write(phy->phy_regmap, 0x60B8, 0x4c);
+	regmap_write(phy->phy_regmap, 0x60B9, 0x41);
+
+	regmap_write(phy->phy_regmap, 0x60BB, 0x41);
+	regmap_write(phy->phy_regmap, 0x60BC, 0x41);
+	regmap_write(phy->phy_regmap, 0x60BD, 0x41);
+	regmap_write(phy->phy_regmap, 0x60BE, 0x4c);
+	regmap_write(phy->phy_regmap, 0x60BF, 0x41);
+
+	ret = regmap_write(phy->phy_regmap, 0x6080, 0x08);
+	if (ret)
+		return ret;
+
+	ret = regmap_write(phy->phy_regmap, 0x6089, 0x88);
+	if (ret)
+		return ret;
+
+	ret = regmap_write(phy->phy_regmap, 0x608B, 0x90);
+	if (ret)
+		return ret;
+
+	regmap_write(phy->phy_regmap, 0x60A1, 0xb0);
+	regmap_write(phy->phy_regmap, 0x60A2, 0x10);
+
+	return 0;
+}
+
+void lt8619_bridge_enable(struct gsgpu_bridge_phy *phy)
+{
+	lt8619_hdmi_rx_reset(phy);
+	lt8619_vid_chk_soft_reset(phy);
+
+	lt8619_edid_shadow_soft_reset(phy);
+	lt8619_edid_shadow_clk_enable(phy);
+	lt8619_enable_cdr_bandwidth_adaptation(phy);
+
+	lt8619_wait_hdmi_stable(phy);
+	lt8619_read_hdmi_timings(phy, NULL);
+	lt8619_read_hdmi_clock_frequency(phy, NULL);
+
+	lt8619_config_color_space(phy);
+	lt8619_config_lvds_pll(phy);
+
+	lt8619_lvds_pll_lock_ctl_soft_reset(phy);
+	lt8619_lvds_pll_soft_reset(phy);
+	lt8619_lvds_tx_ctl_soft_reset(phy);
+	lt8619_config_rgb888_phy(phy, false);
+	lt8619_lvds_config(phy);
+
+	/*
+	 * When HDMI signal is stable, then we wait LVDS PLL locked.
+	 * If it is not locked, LVDS PLL needs to be reset.
+	 */
+	lt8619_wait_lvds_pll_locked(phy);
+	lt8619_turn_on_lvds(phy);
+}
+
+static int lt8619_reg_init(struct gsgpu_bridge_phy *phy)
+{
+	lt8619_enable_sys_clk(phy, true);
+	lt8619_lvds_tx_clk_enable(phy);
+
+	return 0;
+}
+
+static const struct bridge_phy_cfg_funcs lt8619_cfg_funcs = {
+	.reg_init = lt8619_reg_init,
+};
+
+int bridge_phy_lt8619_init(struct gsgpu_dc_bridge *dc_bridge)
+{
+	struct regmap *regmap;
+	int ret;
+
+	struct gsgpu_bridge_phy *lt8619_phy = bridge_phy_alloc(dc_bridge);
+	struct gsgpu_dc_i2c *li2c = lt8619_phy->li2c;
+	struct i2c_client *client = li2c->ddc_client;
+
+	lt8619_phy->cfg_funcs = &lt8619_cfg_funcs;
+
+	regmap = devm_regmap_init_i2c(client, &lt8619_regmap_config);
+	if (IS_ERR(regmap)) {
+		DRM_ERROR("Failed to regmap: %d\n", ret);
+		return -EINVAL;
+	}
+
+	lt8619_phy->phy_regmap = regmap;
+
+	lt8619_read_revision(lt8619_phy);
+	lt8619_enable_sys_clk(lt8619_phy, true);
+	lt8619_lvds_tx_clk_enable(lt8619_phy);
+
+	DRM_INFO("[Bridge_phy] lt8619 init finish\n");
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/bridge/lt9721_drv.c b/drivers/gpu/drm/gsgpu/bridge/lt9721_drv.c
new file mode 100644
index 000000000000..41f77af47738
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/bridge/lt9721_drv.c
@@ -0,0 +1,260 @@
+#include "gsgpu.h"
+#include "gsgpu_dc.h"
+#include "gsgpu_dc_vbios.h"
+#include "lt9721_drv.h"
+
+static bool lt9721_register_volatile(struct device *dev, u32 reg)
+{
+	switch (reg) {
+	case LT9721_REG_PAGE_SELECT:
+		return false;
+	default:
+		return true;
+	}
+}
+
+static bool lt9721_register_readable(struct device *dev, u32 reg)
+{
+	switch (reg) {
+	case 0x00:
+		return false;
+	case LT9721_REG_PAGE_SELECT:
+	default:
+		return true;
+	}
+}
+
+static const struct regmap_range lt9721_rw_regs_range[] = {
+	regmap_reg_range(LT9721_REG_START, LT9721_REG_END),
+};
+
+static const struct regmap_range lt9721_ro_regs_range[] = {
+	regmap_reg_range(LT9721_REG_CHIP_VERSION_BASE,
+			LT9721_REG_CHIP_VERSION_BASE + CHIP_VERSION_LEN),
+};
+
+static const struct regmap_range lt9721_vo_regs_range[] = {
+	regmap_reg_range(LT9721_REG_START, LT9721_REG_END),
+};
+
+static const struct regmap_range_cfg lt9721_regmap_range_cfg[] = {
+	{
+		.name = "lt9721_registers",
+		.range_min = LT9721_REG_START,
+		.range_max = LT9721_REG_END,
+		.window_start = LT9721_REG_START,
+		.window_len = LT9721_REG_PAGE,
+		.selector_reg = LT9721_REG_PAGE_SELECT,
+		.selector_mask = 0xFF,
+		.selector_shift = 0,
+	},
+};
+
+static bool lt9721_chip_id_verify(struct gsgpu_bridge_phy *phy, char *str)
+{
+	struct lt9721_device *lt9721_dev;
+	u8 version_val[2];
+
+	regmap_bulk_read(phy->phy_regmap, LT9721_REG_CHIP_VERSION_BASE,
+				version_val, CHIP_VERSION_LEN);
+	if (version_val[0] != 0x16) {
+		DRM_ERROR("Invalid lt9721 chip version {%02x}\n", version_val[0]);
+		strcpy(str, "Unknown");
+		return false;
+	}
+
+	phy->chip_version = version_val[1];
+	strncpy(str, version_val, ARRAY_SIZE(version_val));
+
+	lt9721_dev = phy->priv;
+	if (version_val[1] == 0x06)
+		lt9721_dev->ver = LT9721_VER_1;
+	else
+		lt9721_dev->ver = LT9721_VER_Unknown;
+
+	DRM_INFO("Get chip version: LT9721_VER_U%d\n", lt9721_dev->ver);
+	return true;
+}
+
+static enum hpd_status lt9721_get_hpd_status(struct gsgpu_bridge_phy *phy)
+{
+	u32 val;
+
+	regmap_read(phy->phy_regmap, LT9721_REG_LINK_STATUS, &val);
+
+	if (test_bit(LINK_STATUS_OUTPUT_DC_POS, (unsigned long *)&val) ==
+			LINK_STATUS_STABLE) {
+		return hpd_status_plug_on;
+	}
+
+	return hpd_status_plug_off;
+}
+
+static struct bridge_phy_misc_funcs lt9721_misc_funcs = {
+	.chip_id_verify = lt9721_chip_id_verify,
+};
+
+static struct bridge_phy_hpd_funcs lt9721_hpd_funcs = {
+	.get_hpd_status = lt9721_get_hpd_status,
+};
+
+static const struct regmap_access_table lt9721_read_table = {
+	.yes_ranges = lt9721_rw_regs_range,
+	.n_yes_ranges = ARRAY_SIZE(lt9721_rw_regs_range),
+};
+
+static const struct regmap_access_table lt9721_write_table = {
+	.yes_ranges = lt9721_rw_regs_range,
+	.n_yes_ranges = ARRAY_SIZE(lt9721_rw_regs_range),
+	.no_ranges = lt9721_ro_regs_range,
+	.n_no_ranges = ARRAY_SIZE(lt9721_ro_regs_range),
+};
+
+static const struct regmap_config lt9721_regmap_config = {
+	.reg_bits = 8,
+	.val_bits = 8,
+	.reg_stride = 1,
+	.max_register = LT9721_REG_END,
+	.ranges = lt9721_regmap_range_cfg,
+	.num_ranges = ARRAY_SIZE(lt9721_regmap_range_cfg),
+
+	.fast_io = false,
+	.cache_type = REGCACHE_RBTREE,
+
+	.volatile_reg = lt9721_register_volatile,
+	.readable_reg = lt9721_register_readable,
+	.rd_table = &lt9721_read_table,
+	.wr_table = &lt9721_write_table,
+};
+
+int lt9721_hdmi_rx_cdr(struct gsgpu_bridge_phy *phy)
+{
+	regmap_multi_reg_write(phy->phy_regmap, lt9721_HDMIRxCDR,
+			ARRAY_SIZE(lt9721_HDMIRxCDR));
+	return 0;
+}
+
+int lt9721_rx_pll(struct gsgpu_bridge_phy *phy)
+{
+	regmap_multi_reg_write(phy->phy_regmap, lt9711_rx_pll_cfg,
+				ARRAY_SIZE(lt9711_rx_pll_cfg));
+
+	return 0;
+}
+
+static int lt9721_tx_phy(struct gsgpu_bridge_phy *phy)
+{
+	regmap_multi_reg_write(phy->phy_regmap, lt9721_tx_phy_cfg,
+					ARRAY_SIZE(lt9721_tx_phy_cfg));
+	return 0;
+}
+
+static int lt9721_tx_pll_cfg(struct gsgpu_bridge_phy *phy)
+{
+	regmap_multi_reg_write(phy->phy_regmap, lt9721_pll_cfg_seq,
+					ARRAY_SIZE(lt9721_pll_cfg_seq));
+	return 0;
+}
+
+int lt9721_hdmi_format(struct gsgpu_bridge_phy *phy)
+{
+	regmap_write(phy->phy_regmap, 0x8817, 0x08);
+	regmap_write(phy->phy_regmap, 0x8818, 0x20);
+	regmap_write(phy->phy_regmap, 0x881d, 0x36);
+	regmap_write(phy->phy_regmap, 0x881f, 0x4e);
+	regmap_write(phy->phy_regmap, 0x8820, 0x66);
+
+	return 0;
+}
+
+static struct bridge_phy_helper lt9721_helper_funcs = {
+	.regmap_cfg = &lt9721_regmap_config,
+	.misc_funcs = &lt9721_misc_funcs,
+	.hpd_funcs = &lt9721_hpd_funcs,
+};
+
+static int lt9721_bl_ctrl(struct gsgpu_bridge_phy *phy, int mode)
+{
+	if (mode == DRM_MODE_DPMS_ON) {
+		regmap_write(phy->phy_regmap, 0x7044, 0x0c);
+		regmap_write(phy->phy_regmap, 0x80d1, 0x0c);
+		regmap_write(phy->phy_regmap, 0x80d3, 0x00);
+		regmap_write(phy->phy_regmap, 0x80d3, 0x30);
+		regmap_write(phy->phy_regmap, 0x80c8, 0x20);
+		regmap_write(phy->phy_regmap, 0x80c9, 0x00);
+		regmap_write(phy->phy_regmap, 0x80ca, 0x27);
+		regmap_write(phy->phy_regmap, 0x80cb, 0x10);
+		regmap_write(phy->phy_regmap, 0x80cc, 0x00);
+		regmap_write(phy->phy_regmap, 0x80cd, 0x27);
+		regmap_write(phy->phy_regmap, 0x80ce, 0x10);
+	} else {
+		regmap_write(phy->phy_regmap, 0x80d1, 0x04);
+		regmap_write(phy->phy_regmap, 0x80cc, 0x00);
+		regmap_write(phy->phy_regmap, 0x80cd, 0x00);
+		regmap_write(phy->phy_regmap, 0x80cd, 0x00);
+		regmap_write(phy->phy_regmap, 0x80ce, 0x00);
+	}
+
+	return 0;
+}
+
+static int lt9721_mode_set(struct gsgpu_bridge_phy *phy,
+				const struct drm_display_mode *mode,
+				const struct drm_display_mode *adj_mode)
+{
+	lt9721_hdmi_format(phy);
+	regmap_write(phy->phy_regmap, 0x80d1, 0x0c);
+	regmap_write(phy->phy_regmap, 0x881e, 0x30);
+
+	return 0;
+}
+
+static int lt9721_video_input_cfg(struct gsgpu_bridge_phy *phy)
+{
+	lt9721_tx_phy(phy);
+	lt9721_tx_pll_cfg(phy);
+
+	regmap_write(phy->phy_regmap, 0x7021, 0x0d);
+	regmap_write(phy->phy_regmap, 0x7021, 0x0f);
+
+	regmap_write(phy->phy_regmap, 0x881e, 0x20);
+	return 0;
+}
+
+static int lt9721_video_output_cfg(struct gsgpu_bridge_phy *phy)
+{
+	lt9721_hdmi_rx_cdr(phy);
+	lt9721_rx_pll(phy);
+	return 0;
+}
+
+static const struct bridge_phy_cfg_funcs lt9721_cfg_funcs = {
+	.backlight_ctrl = lt9721_bl_ctrl,
+	.video_input_cfg = lt9721_video_input_cfg,
+	.video_output_cfg = lt9721_video_output_cfg,
+};
+
+int bridge_phy_lt9721_init(struct gsgpu_dc_bridge *res)
+{
+	struct gsgpu_bridge_phy *lt9721_phy;
+	struct lt9721_device *lt9721_dev;
+	u32 feature;
+	int ret;
+
+	feature = SUPPORT_HPD | SUPPORT_DDC | SUPPORT_HDMI_AUX;
+	lt9721_phy = bridge_phy_alloc(res);
+
+	lt9721_dev = kmalloc(sizeof(struct lt9721_device), GFP_KERNEL);
+	if (IS_ERR(lt9721_dev))
+		return PTR_ERR(lt9721_dev);
+
+	lt9721_phy->priv = lt9721_dev;
+
+	ret = bridge_phy_register(lt9721_phy, &lt9721_cfg_funcs, feature,
+					&lt9721_helper_funcs);
+
+	lt9721_bl_ctrl(lt9721_phy, DRM_MODE_DPMS_ON);
+	lt9721_mode_set(lt9721_phy, NULL, NULL);
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/bridge/lt9721_drv.h b/drivers/gpu/drm/gsgpu/bridge/lt9721_drv.h
new file mode 100644
index 000000000000..743eda9a23f6
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/bridge/lt9721_drv.h
@@ -0,0 +1,105 @@
+#ifndef __LT9721_H__
+#define __LT9721_H__
+
+#include "bridge_phy.h"
+
+typedef  unsigned char u8;
+typedef  unsigned int u32;
+
+#define LT9721_CHIP_NAME    "LT9721"
+#define LT9721_CHIP_ADDR    0X66
+
+#define LT9721_REG_START    0x0000
+#define LT9721_REG_END      0x90FE
+#define LT9721_REG_PAGE     0x100
+#define LT9721_REG_PAGE_SELECT  0xFF
+
+#define LT9721_REG_LINK_STATUS  0x9006
+#define LINK_STATUS_OUTPUT_DC_POS 3U
+#define LINK_STATUS_STABLE 1U
+
+/* General Registers */
+#define LT9721_REG_CHIP_VERSION_BASE 0x6000
+#define CHIP_VERSION_LEN 2U
+#define LT9721_REG_CHIP_VERSION(x) (LT9721_REG_CHIP_VERSION_BASE + (x))
+
+enum lt9721_chip_version {
+	LT9721_VER_Unknown = 0,
+	LT9721_VER_1,
+};
+
+enum lt9721_pll_level {
+	LT9721_PLL_LEVEL_LOW = 0,
+	LT9721_PLL_LEVEL_MIDDLE,
+	LT9721_PLL_LEVEL_HIGH,
+};
+
+struct lt9721_device {
+	enum lt9721_chip_version ver;
+	enum lt9721_pll_level pll_level;
+};
+
+static const struct reg_sequence lt9721_pll_cfg_seq[] = {
+	{ 0x8040, 0x22 },
+	{ 0x8041, 0x36 },
+	{ 0x8042, 0x00 },
+	{ 0x8043, 0x80 },
+	{ 0x701c, 0x18 },
+	{ 0x701d, 0x42 },
+	{ 0x701e, 0x00 },
+	{ 0x701e, 0x01 },
+	{ 0x701f, 0x3b },
+	{ 0x804a, 0x0e },
+	{ 0x804b, 0x10 },
+	{ 0x8045, 0x83 },
+	{ 0x601e, 0xbf },
+	{ 0x601e, 0xff },
+	{ 0x701f, 0x3b },
+	{ 0x8044, 0x41 },
+	{ 0x8045, 0x03 },
+	{ 0x8046, 0x0a },
+	{ 0x8048, 0x0a },
+	{ 0x8040, 0x22 },
+};
+
+static const struct reg_sequence lt9711_rx_pll_cfg[] = {
+	{ 0x7016, 0x30 },
+	{ 0x7018, 0x03 },
+	{ 0x7019, 0x1e },
+	{ 0x701a, 0x24 },
+};
+
+static const struct reg_sequence lt9721_HDMIRxCDR[] = {
+	{ 0x9005, 0x00 },
+	{ 0x901d, 0x04 },
+	{ 0x901e, 0x1f },
+	{ 0x901f, 0x20 },
+	{ 0x9024, 0xe0 },
+	{ 0x9025, 0xf0 },
+	{ 0x9027, 0x01 },
+	{ 0x902b, 0x40 },
+	{ 0x902c, 0x65 },
+};
+
+static const struct reg_sequence lt9721_tx_phy_cfg[] = {
+	{ 0x7021, 0x0d },
+	{ 0x7021, 0x0f },
+	{ 0x7022, 0x77 },
+	{ 0x7023, 0x77 },
+	{ 0x7024, 0x80 },
+	{ 0x7025, 0x00 },
+	{ 0x7026, 0x80 },
+	{ 0x7027, 0x00 },
+	{ 0x7028, 0x80 },
+	{ 0x7029, 0x00 },
+	{ 0x702a, 0x80 },
+	{ 0x702b, 0x00 },
+	{ 0x702c, 0xb0 },
+	{ 0x702c, 0xf0 },
+	{ 0x702f, 0x70 },
+	{ 0x7030, 0x24 },
+	{ 0x7031, 0xFC },
+	{ 0x8030, 0x0e },
+};
+
+#endif
diff --git a/drivers/gpu/drm/gsgpu/bridge/ncs8805_drv.c b/drivers/gpu/drm/gsgpu/bridge/ncs8805_drv.c
new file mode 100644
index 000000000000..c606beef7fda
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/bridge/ncs8805_drv.c
@@ -0,0 +1,500 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2021 Loongson Technology Co., Ltd.
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ */
+
+#include <linux/types.h>
+#include <drm/drm_encoder.h>
+#include <drm/drm_atomic_helper.h>
+#include <linux/i2c.h>
+#include <linux/pwm.h>
+#include <linux/delay.h>
+#include <drm/drm_edid.h>
+#include "bridge_phy.h"
+#include "ncs8805_drv.h"
+#include "gsgpu_backlight.h"
+#include "gsgpu_dc_vbios.h"
+#include "gsgpu_dc_i2c.h"
+#include "gsgpu_dc.h"
+#include "gsgpu.h"
+
+static bool legacy_handle_flags = true;
+static struct ncs_resources  *ncs_res;
+
+static struct display_mode  mode_current = {
+	.mode.htotal = 2200,
+	.mode.vtotal = 1125,
+	.mode.hdisplay = 1920,
+	.mode.vdisplay = 1080,
+};
+
+/*
+ * Function as i2c_master_send.
+ * Returns negative errno, else the number of messages executed.
+ */
+static bool ncs8805_read_bytes(struct gsgpu_bridge_phy *phy, uint8_t addr,
+		uint8_t offset, uint8_t *buf)
+{
+	struct gsgpu_dc_i2c *i2c = phy->li2c;
+	uint8_t ret, w_buf, r_buf;
+	struct i2c_msg msgs[] = {
+		{
+			.flags = 0,
+			.addr = addr,
+			.len = 1,
+			.buf = &w_buf,
+		},
+		{
+			.flags = I2C_M_RD,
+			.addr = addr,
+			.len = 1,
+			.buf = &r_buf,
+		}
+	};
+
+	r_buf = 0;
+	w_buf = offset;
+
+	ret = i2c_transfer(&i2c->adapter, msgs, 2);
+	if (ret != 2) {
+		DRM_DEBUG("[%s] i2c-0x%02x read reg 0x%02x failed\n",
+				PHY_NAME, addr, offset);
+		return false;
+	}
+	*buf = r_buf;
+	return true;
+}
+
+/*Function as i2c_master_send */
+static bool ncs8805_write_bytes(struct gsgpu_bridge_phy *phy, uint8_t addr,
+		uint8_t offset, uint8_t val)
+{
+	struct gsgpu_dc_i2c *i2c = phy->li2c;
+	uint8_t  ret, w_buf[2];
+	struct i2c_msg msg = {
+		.flags = 0,
+		.addr = addr,
+		.len = 2,
+		.buf = w_buf,
+	};
+
+	w_buf[0] = offset;
+	w_buf[1] = val;
+
+	ret = i2c_transfer(&i2c->adapter, &msg, 1);
+	if (ret != 1) {
+		DRM_DEBUG("[%s] i2c-0x%02x write reg 0x%02x failed\n",
+				PHY_NAME, addr, offset);
+		return false;
+	}
+	return true;
+}
+
+static void ncs8805_rx_timing_get(struct gsgpu_bridge_phy *phy,
+		struct rx_timing *timing)
+{
+	uint8_t addr = NCS8805_SLAVE1_ADDR; /* 7 bit addr*/
+	uint8_t reg_low, reg_upper;
+
+	/* hdisplay */
+	ncs8805_read_bytes(phy, addr, 0xec, &reg_low);
+	/* 0:Hsync active high;  1:Hsync active low*/
+	timing->hpolarity = reg_low >> 7;
+
+	ncs8805_read_bytes(phy, addr, 0xe0, &reg_upper);
+	ncs8805_read_bytes(phy, addr, 0xe1, &reg_low);
+	timing->htotal = (reg_upper<<8) | reg_low;
+
+	ncs8805_read_bytes(phy, addr, 0xe4, &reg_upper);
+	ncs8805_read_bytes(phy, addr, 0xe5, &reg_low);
+	timing->hdisplay = (reg_upper<<8) | reg_low;
+
+	ncs8805_read_bytes(phy, addr, 0xe2, &reg_upper);
+	ncs8805_read_bytes(phy, addr, 0xe3, &reg_low);
+	timing->hsync_start = timing->htotal - ((reg_upper<<8)|reg_low);
+
+	ncs8805_read_bytes(phy, addr, 0xed, &reg_low);
+	timing->hsync_end = timing->hsync_start + reg_low;
+
+	/* vdisplay */
+	ncs8805_read_bytes(phy, addr, 0xee, &reg_low);
+	/* 0:Hsync active high;  1:Hsync active low*/
+	timing->vpolarity = reg_low >> 7;
+
+	ncs8805_read_bytes(phy, addr, 0xe6, &reg_upper);
+	ncs8805_read_bytes(phy, addr, 0xe7, &reg_low);
+	timing->vtotal = (reg_upper<<8) | reg_low;
+
+	ncs8805_read_bytes(phy, addr, 0xea, &reg_upper);
+	ncs8805_read_bytes(phy, addr, 0xeb, &reg_low);
+	timing->vdisplay = (reg_upper<<8) | reg_low;
+
+	ncs8805_read_bytes(phy, addr, 0xe8, &reg_upper);
+	ncs8805_read_bytes(phy, addr, 0xe9, &reg_low);
+	timing->vsync_start = timing->vtotal - ((reg_upper<<8)|reg_low);
+
+	ncs8805_read_bytes(phy, addr, 0xef, &reg_low);
+	timing->vsync_end = timing->vsync_start + reg_low;
+
+	DRM_DEBUG("[%s] htotal:%d, hdisplay:%d, hsync_start:%d, hsync_end:%d.\n",
+			PHY_NAME, timing->htotal, timing->hdisplay,
+			timing->hsync_start, timing->hsync_end);
+	DRM_DEBUG("[%s] vtotal:%d, vdisplay:%d, vsync_start:%d, vsync_end:%d.\n",
+			PHY_NAME, timing->vtotal, timing->vdisplay,
+			timing->vsync_start, timing->vsync_end);
+}
+
+static int ncs8805_rx_timing_wait(struct gsgpu_bridge_phy *phy)
+{
+	struct rx_timing timing;
+	uint8_t ret, times_active = 10;
+
+	do {
+		if (!(--times_active)) {
+			DRM_ERROR("[%s] rx timing failed. timeout!.\n", PHY_NAME);
+			break;
+		}
+		msleep(30);
+		ncs8805_rx_timing_get(phy, &timing);
+		DRM_DEBUG("[%s] rx timing, times_active=%d, %dx%d.\n", PHY_NAME,
+				times_active, timing.hdisplay, timing.vdisplay);
+		ret = (abs(timing.htotal - mode_current.mode.htotal) <= 2)
+			&& (abs(timing.hdisplay - mode_current.mode.hdisplay) <= 2)
+			&& (abs(timing.hsync_start - mode_current.mode.hsync_start) <= 2)
+			&& (abs(timing.hsync_end - mode_current.mode.hsync_end) <= 2)
+			&& (abs(timing.vtotal - mode_current.mode.vtotal) <= 2)
+			&& (abs(timing.vdisplay - mode_current.mode.vdisplay) <= 2)
+			&& (abs(timing.vsync_start - mode_current.mode.vsync_start) <= 2)
+			&& (abs(timing.vsync_end - mode_current.mode.vsync_end) <= 2);
+	} while (!ret);
+
+	if (ret)
+		DRM_DEBUG("[%s] rx timing complete.\n", PHY_NAME);
+
+	return ret;
+}
+
+static int ncs8805_get_resolution_index(uint16_t hdisplay, uint16_t vdisplay)
+{
+	int index = -1;
+
+	for (index = 0; index < ncs_res->resolution_number; index++) {
+		if (ncs_res->resolution_list[index].hdisplay == hdisplay
+				&& ncs_res->resolution_list[index].vdisplay == vdisplay)
+			return index;
+	}
+
+	if (index == -1)
+		DRM_DEBUG("[%s] detect auto_mvid_list failed. not find %dx%d.\n",
+				PHY_NAME, hdisplay, vdisplay);
+
+	return index;
+}
+
+static int ncs8805_auto_mvid_detect(struct gsgpu_bridge_phy *phy)
+{
+	int index, ret = 0;
+	uint8_t reg_hig, reg_mid, reg_low;
+	struct ncs_resolution_cfg *cfg;
+	uint8_t times_active = 10;
+
+	index = ncs8805_get_resolution_index(mode_current.mode.hdisplay,
+			mode_current.mode.vdisplay);
+	if (index == -1)
+		return ret;
+	cfg = &ncs_res->resolution_list[index];
+
+	do {
+		if (!(--times_active)) {
+			DRM_ERROR("[%s] mvid detect failed. timeout!.\n", PHY_NAME);
+			break;
+		}
+		msleep(30);
+		reg_hig = reg_mid = reg_low = 0;
+		ncs8805_read_bytes(phy, NCS8805_SLAVE1_ADDR,
+				NCS8805_REG_NVID_HIG_ADDR, &reg_hig);
+		ncs8805_read_bytes(phy, NCS8805_SLAVE1_ADDR,
+				NCS8805_REG_NVID_MID_ADDR, &reg_mid);
+		ncs8805_read_bytes(phy, NCS8805_SLAVE1_ADDR,
+				NCS8805_REG_NVID_LOW_ADDR, &reg_low);
+		if ((reg_hig <= cfg->lcd_p.auto_mvid_hig_max
+					&& reg_hig >= cfg->lcd_p.auto_mvid_hig_min)
+				&& (reg_mid <= cfg->lcd_p.auto_mvid_mid_max
+					&& reg_mid >= cfg->lcd_p.auto_mvid_mid_min)
+				&& (reg_low <= cfg->lcd_p.auto_mvid_low_max
+					&& reg_low >= cfg->lcd_p.auto_mvid_low_min)) {
+			ret = 1;
+		}
+		DRM_DEBUG("[%s] auto mvid detect, times_active=%d, reg=0x%x,0x%x,0x%x.\n",
+				PHY_NAME, times_active, reg_hig, reg_mid, reg_low);
+	} while (!ret);
+	if (ret)
+		DRM_DEBUG("[%s] auto mvid detect complete.\n", PHY_NAME);
+
+	return ret;
+}
+
+static int ncs8805_dp_lock_wait(struct gsgpu_bridge_phy *phy)
+{
+	int ret = 0;
+	uint8_t reg_f, reg_g;
+	uint8_t times_active = 10;
+
+	do {
+		if (!(--times_active)) {
+			DRM_ERROR("[%s] dp lock failed. timeout!.\n", PHY_NAME);
+			break;
+		}
+		msleep(30);
+		ncs8805_read_bytes(phy, NCS8805_SLAVE1_ADDR, 0x81, &reg_f);
+		ncs8805_read_bytes(phy, NCS8805_SLAVE1_ADDR, 0x83, &reg_g);
+		if (reg_f == 0x77 && reg_g == 0x81)
+			ret = 1;
+		DRM_DEBUG("[%s] dp lock, times_active=%d, reg=0x%x,0x%x.\n",
+				PHY_NAME, times_active, reg_f, reg_g);
+	} while (!ret);
+	if (ret)
+		DRM_DEBUG("[%s] dp lock complete.\n", PHY_NAME);
+
+	return ret;
+}
+
+static int ncs8805_tx_timing_wait(struct gsgpu_bridge_phy *phy)
+{
+	uint8_t addr, reg, ret;
+	uint8_t times_active = 10;
+
+	addr = NCS8805_SLAVE1_ADDR;
+	do {
+		if (!(--times_active)) {
+			DRM_ERROR("[%s] tx timing failed. timeout!.\n", PHY_NAME);
+			break;
+		}
+		msleep(30);
+		ncs8805_read_bytes(phy, addr, 0x81, &reg);
+		ret = (reg != 0x7) && (reg != 0x77);
+		DRM_DEBUG("[%s] tx timing, times_active=%d, reg=0x%x.\n",
+				PHY_NAME, times_active, reg);
+	} while (ret);
+
+	if (!ret)
+		DRM_DEBUG("[%s] tx timing complete.\n", PHY_NAME);
+
+	return !ret;
+}
+
+static void ncs8805_working_ctrl(struct gsgpu_bridge_phy *phy, uint8_t enable)
+{
+	uint8_t addr, reg;
+	bool ret = false;
+	struct gsgpu_backlight *ls_bl = NULL;
+
+	ls_bl = phy->adev->mode_info.backlights[1];
+	addr = NCS8805_SLAVE1_ADDR;
+
+	if (!phy)
+		return;
+
+	if (!ncs8805_read_bytes(phy, addr, 0x71, &reg))
+		return;
+
+	if (enable == NCS8805_WORKING_DISABLE) {
+		/* 1:enable  idle */
+		ret = ncs8805_write_bytes(phy, addr, 0x71, reg|(0x1<<1));
+	} else {
+		if (!ls_bl || !ls_bl->hw_enabled) {
+			DRM_INFO("[%s] backlight off or not register,so working mode not set.\n",
+					PHY_NAME);
+			return;
+		} else {
+			if (!legacy_handle_flags) {
+				if (ncs8805_tx_timing_wait(phy)
+						&& ncs8805_rx_timing_wait(phy)
+						&& ncs8805_dp_lock_wait(phy)
+						&& ncs8805_auto_mvid_detect(phy))
+					/* 0:disable idle */
+					ret = ncs8805_write_bytes(phy, addr, 0x71, reg & (~(0x1<<1)));
+			} else {
+				ret = ncs8805_write_bytes(phy, addr, 0x71, reg & (~(0x1<<1)));
+			}
+		}
+	}
+	if (ret) {
+		DRM_INFO("[%s] Enter %s mode.\n", PHY_NAME,
+				enable == NCS8805_WORKING_DISABLE ? "idle" : "working");
+	} else {
+		DRM_ERROR("[%s] Enter %s mode failed.\n", PHY_NAME,
+				enable == NCS8805_WORKING_DISABLE ? "idle" : "working");
+	}
+}
+
+static bool ncs8805_mode_config(struct gsgpu_bridge_phy *phy,
+		struct display_mode *d_mode)
+{
+	int index, i;
+	struct ncs_resolution_cfg *cfg;
+
+	DRM_DEBUG("[%s] mode config %dx%d.\n", PHY_NAME,
+			d_mode->mode.hdisplay, d_mode->mode.vdisplay);
+
+	index = ncs8805_get_resolution_index(d_mode->mode.hdisplay,
+			d_mode->mode.vdisplay);
+	if (index == -1)
+		return false;
+	cfg = &ncs_res->resolution_list[index];
+
+	ncs8805_working_ctrl(phy, NCS8805_WORKING_DISABLE);
+	for (i = 0; i < cfg->reg_number; i++) {
+		DRM_DEBUG("[%s] write reg:<%02d> addr=0x%02x, reg=0x%02x, val=0x%02x.\n",
+				PHY_NAME, i, cfg->reg_list[i].addr,
+				cfg->reg_list[i].offset, cfg->reg_list[i].val);
+		if (!ncs8805_write_bytes(phy, cfg->reg_list[i].addr,
+					cfg->reg_list[i].offset, cfg->reg_list[i].val)) {
+			DRM_ERROR("[%s] write reg failed: addr=0x%x, reg=0x%x, val=0x%x.\n",
+					PHY_NAME, cfg->reg_list[i].addr,
+					cfg->reg_list[i].offset, cfg->reg_list[i].val);
+		}
+	}
+
+	DRM_DEBUG("[%s] %dx%d: ncs8805 config reg num:%d.\n",
+			PHY_NAME, d_mode->mode.hdisplay, d_mode->mode.vdisplay, i);
+	return true;
+}
+
+static int ncs8805_bl_ctrl(struct gsgpu_bridge_phy *phy, int mode)
+{
+	struct gsgpu_backlight *ls_bl = NULL;
+	ls_bl =  phy->adev->mode_info.backlights[1];
+
+	if (!ls_bl) {
+		DRM_ERROR("[%s] loongson backlight not register or not present.\n", PHY_NAME);
+		return 0;
+	}
+
+	if (mode == DRM_MODE_DPMS_ON) {
+		/*fix display abnormal when automatic switching screen.*/
+		if (!legacy_handle_flags)
+			ncs8805_mode_config(phy, &mode_current);
+		/* Edp power on */
+		BACKLIGHT_DEFAULT_METHOD_FORCE_OPEN(ls_bl);
+		DRM_DEBUG("[%s] backlight power on.\n", PHY_NAME);
+		/* Exit idle mode */
+		ncs8805_working_ctrl(phy, NCS8805_WORKING_ENABLE);
+	} else {
+		/* Enter idle mode */
+		ncs8805_working_ctrl(phy, NCS8805_WORKING_DISABLE);
+		/* Edp power dowm */
+		BACKLIGHT_DEFAULT_METHOD_FORCE_CLOSE(ls_bl);
+		DRM_DEBUG("[%s] backlight power off.\n", PHY_NAME);
+	}
+	return 0;
+}
+
+
+void ncs8805_prepare(struct gsgpu_bridge_phy *phy)
+{
+	ncs8805_working_ctrl(phy, NCS8805_WORKING_DISABLE);
+}
+
+static int ncs8805_mode_set(struct gsgpu_bridge_phy *phy,
+		const struct drm_display_mode *mode,
+		const struct drm_display_mode *adj_mode)
+{
+	if (!phy)
+		return 1;
+
+	if (!memcmp(mode, &mode_current.mode, sizeof(struct drm_display_mode)))
+		return 0;
+
+	memcpy(&mode_current.mode, mode, sizeof(struct drm_display_mode));
+	if (!legacy_handle_flags)
+		ncs8805_mode_config(phy, &mode_current);
+
+	return 0;
+}
+
+void ncs8805_commit(struct gsgpu_bridge_phy *phy)
+{
+	ncs8805_working_ctrl(phy, NCS8805_WORKING_ENABLE);
+}
+
+static struct bridge_phy_cfg_funcs ncs8805_cfg_funcs = {
+	.backlight_ctrl = ncs8805_bl_ctrl,
+	.prepare = ncs8805_prepare,
+	.commit = ncs8805_commit,
+	.mode_set = ncs8805_mode_set,
+};
+
+static int ncs8805_get_modes(struct gsgpu_bridge_phy *phy,
+			     struct drm_connector *connector)
+{
+	struct edid *edid;
+	unsigned int count = 0;
+	int size = sizeof(u8) * EDID_LENGTH * 2;
+
+	edid = kmalloc(size, GFP_KERNEL);
+	if (edid) {
+		memcpy(edid, ncs_res->edid, size);
+		drm_connector_update_edid_property(connector, edid);
+		count = drm_add_edid_modes(connector, edid);
+		kfree(edid);
+	} else {
+		DRM_ERROR("[%s] resources is invalid.\n", PHY_NAME);
+	}
+	return count;
+}
+
+static struct bridge_phy_ddc_funcs ncs8805_ddc_funcs = {
+	.get_modes = ncs8805_get_modes,
+};
+
+static struct bridge_phy_helper ncs8805_helper_funcs = {
+	.ddc_funcs = &ncs8805_ddc_funcs,
+};
+
+static bool ncs8805_resources_valid_check(struct ext_encoder_resources *encoder_res)
+{
+	if (gsgpu_vbios_checksum(encoder_res->data, encoder_res->data_size)
+			== encoder_res->data_checksum)
+		return true;
+	return false;
+}
+
+int bridge_phy_ncs8805_init(struct gsgpu_dc_bridge *dc_bridge)
+{
+	struct gsgpu_bridge_phy *ncs8805_phy;
+	struct ext_encoder_resources  *ext_resource;
+	u32 feature;
+	int ret = -1;
+
+	if (gsgpu_vbios_version(dc_bridge->dc->vbios) >= VBIOS_VERSION_V1_1) {
+		legacy_handle_flags = false;
+		ext_resource = dc_get_vbios_resource(dc_bridge->dc->vbios, 1, GSGPU_RESOURCE_EXT_ENCODER);
+		if (ncs8805_resources_valid_check(ext_resource)) {
+			ncs_res = (struct ncs_resources *)ext_resource->data;
+			DRM_INFO("[%s] resources version: %d.%d.%d\n", PHY_NAME,
+					(ncs_res->version >> 16) & 0xFF,
+					(ncs_res->version >> 8) & 0xFF,
+					(ncs_res->version >> 0) & 0xFF);
+		} else {
+			DRM_ERROR("[%s] resources is invalid.\n", PHY_NAME);
+			return ret;
+		}
+	}
+
+	feature = SUPPORT_DDC | SUPPORT_HPD;
+	ncs8805_phy = bridge_phy_alloc(dc_bridge);
+	ret = bridge_phy_register(ncs8805_phy, &ncs8805_cfg_funcs, feature,
+			&ncs8805_helper_funcs);
+
+	return ret;
+}
+
+int bridge_phy_ncs8805_remove(struct gsgpu_dc_bridge *phy)
+{
+	return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/bridge/ncs8805_drv.h b/drivers/gpu/drm/gsgpu/bridge/ncs8805_drv.h
new file mode 100644
index 000000000000..1a03ace3b87e
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/bridge/ncs8805_drv.h
@@ -0,0 +1,83 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2021 Loongson Technology Co., Ltd.
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ */
+
+#ifndef __NCS8805_H__
+#define __NCS8805_H__
+
+#include <drm/drm_modes.h>
+
+#define PHY_NAME  "NCS8805"
+
+#define NCS8805_SLAVE1_ADDR  (0x70)
+#define NCS8805_SLAVE2_ADDR  (0x75)
+#define NCS8805_REG_NVID_HIG_ADDR  (0x8a)
+#define NCS8805_REG_NVID_MID_ADDR  (0x8b)
+#define NCS8805_REG_NVID_LOW_ADDR  (0x8c)
+
+#define NCS8805_WORKING_ENABLE   (1)
+#define NCS8805_WORKING_DISABLE  (0)
+
+#define NCS8805_RESET_ENABLE   (1)
+#define NCS8805_RESET_DISABLE  (0)
+
+#define REG_NUM_MAX        (80)
+#define RESOLUTION_NUM_MAX  (8)
+
+/* NCS8805 resource */
+struct ncs_reg_cell {
+	unsigned char addr;    /* 7bit i2c addr */
+	unsigned char offset;  /* offset. */
+	unsigned char val;     /* reg val. */
+} __attribute__((packed));
+
+struct ncs_lcd_parameter {
+	unsigned char auto_mvid_hig_max;  /* reg 0x8a */
+	unsigned char auto_mvid_hig_min;  /* reg 0x8a */
+	unsigned char auto_mvid_mid_max;  /* reg 0x8b */
+	unsigned char auto_mvid_mid_min;  /* reg 0x8b */
+	unsigned char auto_mvid_low_max;  /* reg 0x8c */
+	unsigned char auto_mvid_low_min;  /* reg 0x8c */
+} __attribute__((packed));
+
+struct ncs_resolution_cfg {
+	unsigned int hdisplay;
+	unsigned int vdisplay;
+	struct ncs_lcd_parameter lcd_p;
+	unsigned int reg_number;
+	struct ncs_reg_cell  reg_list[REG_NUM_MAX];
+} __attribute__((packed));
+
+struct ncs_resources {
+	/* major[23:16], minor[15:8], revision[7:0] */
+	unsigned int version;
+	unsigned char edid[EDID_LENGTH * 2];
+	unsigned int resolution_number;
+	struct ncs_resolution_cfg  resolution_list[RESOLUTION_NUM_MAX];
+} __attribute__((packed));
+
+/* NCS8805 struct  */
+struct rx_timing {
+	int hdisplay;
+	int hsync_start;
+	int hsync_end;
+	int htotal;
+	int hpolarity;
+
+	int vdisplay;
+	int vsync_start;
+	int vsync_end;
+	int vtotal;
+	int vpolarity;
+};
+
+struct display_mode {
+	struct drm_display_mode  mode;
+};
+
+#endif
diff --git a/drivers/gpu/drm/gsgpu/gpu/Kconfig b/drivers/gpu/drm/gsgpu/gpu/Kconfig
new file mode 100644
index 000000000000..5a31c3f7e0a8
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/Kconfig
@@ -0,0 +1,17 @@
+config DRM_GSGPU_USERPTR
+	bool "Always enable userptr write support"
+	depends on DRM_GSGPU
+	select MMU_NOTIFIER
+	default y
+	help
+	  This option selects CONFIG_MMU_NOTIFIER if it isn't already
+	  selected to enabled full userptr support.
+
+config DRM_GSGPU_GART_DEBUGFS
+	bool "Allow GART access through debugfs"
+	depends on DRM_GSGPU
+	depends on DEBUG_FS
+	default n
+	help
+	  Selecting this option creates a debugfs file to inspect the mapped
+	  pages. Uses more memory for housekeeping, enable only for debugging.
diff --git a/drivers/gpu/drm/gsgpu/gpu/Makefile b/drivers/gpu/drm/gsgpu/gpu/Makefile
new file mode 100644
index 000000000000..3ff7c1936675
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/Makefile
@@ -0,0 +1,62 @@
+
+FULL_LS_PATH=$(src)/..
+
+FULL_BRIDGE_PATH = $(FULL_LS_PATH)/bridge
+
+ccflags-y := -I$(FULL_LS_PATH)/include \
+	-I$(FULL_BRIDGE_PATH)/ \
+	-I$(FULL_LS_PATH)/include \
+
+CFLAGS_gsgpu_fence.o += -O
+gsgpu-y := gsgpu_drv.o
+
+# add KMS driver
+gsgpu-y += gsgpu_device.o gsgpu_kms.o \
+	gsgpu_fence.o gsgpu_ttm.o gsgpu_object.o gsgpu_gart.o \
+	gsgpu_display.o gsgpu_cp.o \
+	gsgpu_fb.o gsgpu_gem.o gsgpu_ring.o \
+	gsgpu_cs.o gsgpu_benchmark.o gsgpu_test.o \
+	gsgpu_trace_points.o \
+	gsgpu_sa.o gsgpu_zip_meta.o \
+	gsgpu_prime.o gsgpu_vm.o gsgpu_vm_it.o gsgpu_ib.o \
+	gsgpu_bo_list.o gsgpu_ctx.o gsgpu_sync.o \
+	gsgpu_gtt_mgr.o gsgpu_vram_mgr.o \
+	gsgpu_queue_mgr.o gsgpu_sched.o gsgpu_debugfs.o \
+	gsgpu_ids.o gsgpu_hw_sema.o gsgpu_pm.o
+
+gsgpu-y += gsgpu_common.o
+
+# add MMU block
+gsgpu-y +=  gsgpu_mmu.o
+
+# add ZIP block
+gsgpu-y += gsgpu_zip.o
+
+# add IH block
+gsgpu-y += \
+	gsgpu_irq.o \
+	gsgpu_ih.o
+
+# add GFX block
+gsgpu-y += gsgpu_gfx.o
+
+# add XDMA block
+gsgpu-y += gsgpu_xdma.o
+
+# GPU scheduler
+gsgpu-y += gsgpu_job.o
+
+gsgpu-$(CONFIG_COMPAT) += gsgpu_ioc32.o
+gsgpu-$(CONFIG_MMU_NOTIFIER) += gsgpu_mn.o
+
+# add DC block
+gsgpu-y += gsgpu_dc_drv.o gsgpu_dc_crtc.o gsgpu_dc_hdmi.o \
+	  gsgpu_dc_irq.o gsgpu_dc_plane.o gsgpu_dc_connector.o \
+	  gsgpu_dc_encoder.o gsgpu_dc_i2c.o  gsgpu_dc_cursor.o \
+	  gsgpu_dc_vbios.o gsgpu_backlight.o
+
+RELATIVE_GSGPU_BRIDGE_PATH = ../bridge
+include $(FULL_BRIDGE_PATH)/Makefile
+gsgpu-y += $(GSGPU_BRIDGE_FILES)
+
+obj-$(CONFIG_DRM_GSGPU)+= gsgpu.o
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_backlight.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_backlight.c
new file mode 100644
index 000000000000..a003bf708d50
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_backlight.c
@@ -0,0 +1,287 @@
+// SPDX-License-Identifier: GPL-2.0+
+#include <linux/types.h>
+#include <linux/gpio.h>
+#include <linux/pwm.h>
+#include "gsgpu.h"
+#include "gsgpu_dc_vbios.h"
+#include "gsgpu_backlight.h"
+#include "bridge_phy.h"
+
+static bool gsgpu_backlight_get_hw_status(struct gsgpu_backlight *ls_bl)
+{
+	return (gpio_get_value(GPIO_LCD_VDD) && gpio_get_value(GPIO_LCD_EN)
+		&& pwm_is_enabled(ls_bl->pwm));
+}
+
+static void gsgpu_backlight_enable(struct gsgpu_backlight *ls_bl)
+{
+	struct gsgpu_device *adev = ls_bl->driver_private;
+	struct gsgpu_bridge_phy *phy =
+		adev->mode_info.encoders[ls_bl->display_pipe_index]->bridge;
+
+	if (IS_ERR(ls_bl->pwm))
+		return;
+
+	if (phy && phy->cfg_funcs && phy->cfg_funcs->backlight_ctrl)
+		phy->cfg_funcs->backlight_ctrl(phy, DRM_MODE_DPMS_ON);
+	else {
+		BACKLIGHT_DEFAULT_METHOD_OPEN(ls_bl);
+		DRM_DEBUG("%s set backlight enable.\n",
+			 phy ? phy->res->chip_name : "");
+	}
+
+	ls_bl->device->props.power = FB_BLANK_UNBLANK;
+}
+
+static void gsgpu_backlight_disable(struct gsgpu_backlight *ls_bl)
+{
+	struct gsgpu_device *adev = ls_bl->driver_private;
+	struct gsgpu_bridge_phy *phy =
+		adev->mode_info.encoders[ls_bl->display_pipe_index]->bridge;
+
+	if (IS_ERR(ls_bl->pwm))
+		return;
+
+	if (phy && phy->cfg_funcs && phy->cfg_funcs->backlight_ctrl)
+		phy->cfg_funcs->backlight_ctrl(phy, DRM_MODE_DPMS_OFF);
+	else {
+		BACKLIGHT_DEFAULT_METHOD_CLOSE(ls_bl);
+		DRM_DEBUG("%s set backlight disable.\n",
+			 phy ? phy->res->chip_name : "");
+	}
+
+	ls_bl->device->props.power = !FB_BLANK_UNBLANK;
+}
+
+static void gsgpu_backlight_power(struct gsgpu_backlight *ls_bl, bool enable)
+{
+	DRM_DEBUG("Request backlight power: %s->%s.\n",
+		  ls_bl->hw_enabled ? "open" : "close",
+		  enable ? "open" : "close");
+
+	if (enable && !ls_bl->hw_enabled) {
+		if (ls_bl->enable)
+			ls_bl->enable(ls_bl);
+	} else if (!enable && ls_bl->hw_enabled) {
+		if (ls_bl->disable)
+			ls_bl->disable(ls_bl);
+	}
+}
+
+static int gsgpu_backlight_update(struct backlight_device *bd)
+{
+	struct gsgpu_backlight *ls_bl = bl_get_data(bd);
+
+	DRM_DEBUG("Request bl update: %s->%s, level:%d->%d.\n",
+		  ls_bl->hw_enabled ? "open" : "close",
+		  bd->props.power == FB_BLANK_UNBLANK ? "open" : "close",
+		  ls_bl->level, bd->props.brightness);
+
+	if (ls_bl->hw_enabled != (bd->props.power == FB_BLANK_UNBLANK))
+		ls_bl->power(ls_bl, bd->props.power == FB_BLANK_UNBLANK);
+
+	if (ls_bl->level != bd->props.brightness) {
+		ls_bl->level = bd->props.brightness;
+		ls_bl->set_brightness(ls_bl, ls_bl->level);
+	}
+
+	return 0;
+}
+
+static int gsgpu_backlight_get_brightness(struct backlight_device *bd)
+{
+	struct gsgpu_backlight *ls_bl = bl_get_data(bd);
+
+	if (ls_bl->get_brightness)
+		return ls_bl->get_brightness(ls_bl);
+
+	return -ENOEXEC;
+}
+
+static const struct backlight_ops gsgpu_backlight_ops = {
+	.update_status  = gsgpu_backlight_update,
+	.get_brightness = gsgpu_backlight_get_brightness,
+};
+
+static unsigned int gsgpu_backlight_get(struct gsgpu_backlight *ls_bl)
+{
+	u16 duty_ns, period_ns;
+	u32 level;
+
+	if (IS_ERR(ls_bl->pwm))
+		return 0;
+
+	period_ns = ls_bl->pwm_period;
+	duty_ns = pwm_get_duty_cycle(ls_bl->pwm);
+
+	level = DIV_ROUND_UP((duty_ns * ls_bl->max), period_ns);
+	level = clamp(level, ls_bl->min, ls_bl->max);
+
+	return level;
+}
+
+static void gsgpu_backlight_set(struct gsgpu_backlight *ls_bl,
+		unsigned int level)
+{
+	unsigned int period_ns;
+	unsigned int duty_ns;
+
+	if (IS_ERR(ls_bl->pwm))
+		return;
+
+	level = clamp(level, ls_bl->min, ls_bl->max);
+	period_ns = ls_bl->pwm_period;
+	duty_ns = DIV_ROUND_UP((level * period_ns), ls_bl->max);
+
+	DRM_DEBUG("Set backlight: level=%d, 0x%x/0x%x ns.\n",
+		  level, duty_ns, period_ns);
+
+	pwm_config(ls_bl->pwm, duty_ns, period_ns);
+}
+
+static int gsgpu_backlight_hw_request_init(struct gsgpu_backlight *ls_bl)
+{
+	int ret = 0;
+	bool pwm_enable_default;
+
+	ls_bl->pwm = pwm_request(ls_bl->pwm_id, "Loongson_bl");
+	if (IS_ERR(ls_bl->pwm)) {
+		DRM_ERROR("Failed to get the pwm chip\n");
+		ls_bl->pwm = NULL;
+		goto ERROR_PWM;
+	}
+
+	pwm_enable_default = pwm_is_enabled(ls_bl->pwm);
+	/* pwm init.*/
+	pwm_disable(ls_bl->pwm);
+	pwm_set_polarity(ls_bl->pwm, ls_bl->pwm_polarity);
+	gsgpu_backlight_set(ls_bl, ls_bl->level);
+	if (pwm_enable_default)
+		pwm_enable(ls_bl->pwm);
+
+	ret = gpio_request(GPIO_LCD_VDD, "GPIO_VDD");
+	if (ret) {
+		DRM_ERROR("EN request error!\n");
+		goto ERROR_VDD;
+	}
+
+	ret = gpio_request(GPIO_LCD_EN, "GPIO_EN");
+	if (ret) {
+		DRM_ERROR("VDD request error!\n");
+		goto ERROR_EN;
+	}
+
+	/* gpio init */
+	gpio_direction_output(GPIO_LCD_VDD, 1);
+	gpio_direction_output(GPIO_LCD_EN, 1);
+
+	return ret;
+
+ERROR_EN:
+	gpio_free(GPIO_LCD_VDD);
+ERROR_VDD:
+	pwm_free(ls_bl->pwm);
+ERROR_PWM:
+	return -ENODEV;
+}
+
+static struct gsgpu_backlight
+*gsgpu_backlight_init(struct gsgpu_device *adev, int index)
+{
+	struct gsgpu_backlight *ls_bl = NULL;
+	struct backlight_properties props;
+	struct pwm_resource *pwm_res;
+	int ret = 0;
+
+	ls_bl = kzalloc(sizeof(struct gsgpu_backlight), GFP_KERNEL);
+	if (IS_ERR(ls_bl)) {
+		DRM_ERROR("Failed to alloc backlight.\n");
+		return NULL;
+	}
+
+	ls_bl->min = BL_MIN_LEVEL;
+	ls_bl->max = BL_MAX_LEVEL;
+	ls_bl->level = BL_DEF_LEVEL;
+	ls_bl->driver_private = adev;
+	ls_bl->display_pipe_index = index;
+	ls_bl->get_brightness = gsgpu_backlight_get;
+	ls_bl->set_brightness = gsgpu_backlight_set;
+	ls_bl->enable = gsgpu_backlight_enable;
+	ls_bl->disable = gsgpu_backlight_disable;
+	ls_bl->power = gsgpu_backlight_power;
+
+	pwm_res = dc_get_vbios_resource(adev->dc->vbios,
+					index, GSGPU_RESOURCE_PWM);
+	ls_bl->pwm_id = pwm_res->pwm;
+	/* 0:low start, 1:high start */
+	ls_bl->pwm_polarity = pwm_res->polarity;
+	ls_bl->pwm_period = pwm_res->peroid;
+
+	DRM_INFO("pwm: id=%d, period=%dns, polarity=%d.\n",
+		 ls_bl->pwm_id, ls_bl->pwm_period, ls_bl->pwm_polarity);
+
+	ret = gsgpu_backlight_hw_request_init(ls_bl);
+	if (ret)
+		goto ERROR_HW;
+
+	memset(&props, 0, sizeof(props));
+	props.type = BACKLIGHT_RAW;
+	props.power = FB_BLANK_UNBLANK;
+	props.max_brightness = ls_bl->max;
+	props.brightness = ls_bl->level;
+
+	ls_bl->device = backlight_device_register("gsgpu-bl",
+			adev->mode_info.connectors[index]->base.kdev,
+			ls_bl, &gsgpu_backlight_ops, &props);
+	if (IS_ERR(ls_bl->device)) {
+		DRM_ERROR("Failed to register backlight.\n");
+		goto ERROR_REG;
+	}
+
+	DRM_INFO("register gsgpu backlight_%d completed.\n", index);
+
+	adev->mode_info.backlights[index] = ls_bl;
+
+	return ls_bl;
+
+ERROR_HW:
+ERROR_REG:
+	kfree(ls_bl);
+
+	return NULL;
+}
+
+int gsgpu_backlight_register(struct drm_connector *connector)
+{
+	struct gsgpu_backlight *ls_bl;
+	struct gsgpu_device *adev = connector->dev->dev_private;
+	int ret = 0;
+	bool ls_bl_status = false;
+
+	switch (connector->connector_type) {
+	case DRM_MODE_CONNECTOR_eDP:
+	case DRM_MODE_CONNECTOR_LVDS:
+		break;
+	default:
+		return ret;
+	}
+
+	ls_bl = gsgpu_backlight_init(adev, connector->index);
+	if (!ls_bl)
+		return -ENXIO;
+
+	ls_bl_status = gsgpu_backlight_get_hw_status(ls_bl);
+	if (ls_bl_status) {
+		ls_bl->hw_enabled = true;
+		ls_bl->power(ls_bl, true);
+	} else {
+		ls_bl->hw_enabled = false;
+		ls_bl->power(ls_bl, false);
+	}
+
+	DRM_INFO("backlight power status: %s->%s.\n",
+		 ls_bl_status ? "on" : "off",
+		 ls_bl->hw_enabled ? "on" : "off");
+
+	return ret;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_benchmark.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_benchmark.c
new file mode 100644
index 000000000000..7b850f761dba
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_benchmark.c
@@ -0,0 +1,221 @@
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+
+#define GSGPU_BENCHMARK_ITERATIONS 1024
+#define GSGPU_BENCHMARK_COMMON_MODES_N 17
+
+static int gsgpu_benchmark_do_move(struct gsgpu_device *adev, unsigned size,
+				    uint64_t saddr, uint64_t daddr, int n)
+{
+	unsigned long start_jiffies;
+	unsigned long end_jiffies;
+	struct dma_fence *fence = NULL;
+	int i, r;
+
+	start_jiffies = jiffies;
+	for (i = 0; i < n; i++) {
+		struct gsgpu_ring *ring = adev->mman.buffer_funcs_ring;
+		r = gsgpu_copy_buffer(ring, saddr, daddr, size, NULL, &fence,
+				       false, false);
+		if (r)
+			goto exit_do_move;
+		r = dma_fence_wait(fence, false);
+		if (r)
+			goto exit_do_move;
+		dma_fence_put(fence);
+	}
+	end_jiffies = jiffies;
+	r = jiffies_to_msecs(end_jiffies - start_jiffies);
+
+exit_do_move:
+	if (fence)
+		dma_fence_put(fence);
+	return r;
+}
+
+
+static void gsgpu_benchmark_log_results(int n, unsigned size,
+					 unsigned int time,
+					 unsigned sdomain, unsigned ddomain,
+					 char *kind)
+{
+	unsigned int throughput = (n * (size >> 10)) / time;
+	DRM_INFO("gsgpu: %s %u bo moves of %u kB from"
+		 " %d to %d in %u ms, throughput: %u Mb/s or %u MB/s\n",
+		 kind, n, size >> 10, sdomain, ddomain, time,
+		 throughput * 8, throughput);
+}
+
+static void gsgpu_benchmark_move(struct gsgpu_device *adev, unsigned size,
+				  unsigned sdomain, unsigned ddomain)
+{
+	struct gsgpu_bo *dobj = NULL;
+	struct gsgpu_bo *sobj = NULL;
+	struct gsgpu_bo_param bp;
+	uint64_t saddr, daddr;
+	int r, n;
+	int time;
+
+	memset(&bp, 0, sizeof(bp));
+	bp.size = size;
+	bp.byte_align = PAGE_SIZE;
+	bp.domain = sdomain;
+	bp.flags = 0;
+	bp.type = ttm_bo_type_kernel;
+	bp.resv = NULL;
+	n = GSGPU_BENCHMARK_ITERATIONS;
+	r = gsgpu_bo_create(adev, &bp, &sobj);
+	if (r) {
+		goto out_cleanup;
+	}
+	r = gsgpu_bo_reserve(sobj, false);
+	if (unlikely(r != 0))
+		goto out_cleanup;
+	r = gsgpu_bo_pin(sobj, sdomain);
+	if (r) {
+		gsgpu_bo_unreserve(sobj);
+		goto out_cleanup;
+	}
+	r = gsgpu_ttm_alloc_gart(&sobj->tbo);
+	gsgpu_bo_unreserve(sobj);
+	if (r) {
+		goto out_cleanup;
+	}
+	saddr = gsgpu_bo_gpu_offset(sobj);
+	bp.domain = ddomain;
+	r = gsgpu_bo_create(adev, &bp, &dobj);
+	if (r) {
+		goto out_cleanup;
+	}
+	r = gsgpu_bo_reserve(dobj, false);
+	if (unlikely(r != 0))
+		goto out_cleanup;
+	r = gsgpu_bo_pin(dobj, ddomain);
+	if (r) {
+		gsgpu_bo_unreserve(sobj);
+		goto out_cleanup;
+	}
+	r = gsgpu_ttm_alloc_gart(&dobj->tbo);
+	gsgpu_bo_unreserve(dobj);
+	if (r) {
+		goto out_cleanup;
+	}
+	daddr = gsgpu_bo_gpu_offset(dobj);
+
+	if (adev->mman.buffer_funcs) {
+		time = gsgpu_benchmark_do_move(adev, size, saddr, daddr, n);
+		if (time < 0)
+			goto out_cleanup;
+		if (time > 0)
+			gsgpu_benchmark_log_results(n, size, time,
+						     sdomain, ddomain, "dma");
+	}
+
+out_cleanup:
+	/* Check error value now. The value can be overwritten when clean up.*/
+	if (r) {
+		DRM_ERROR("Error while benchmarking BO move.\n");
+	}
+
+	if (sobj) {
+		r = gsgpu_bo_reserve(sobj, true);
+		if (likely(r == 0)) {
+			gsgpu_bo_unpin(sobj);
+			gsgpu_bo_unreserve(sobj);
+		}
+		gsgpu_bo_unref(&sobj);
+	}
+	if (dobj) {
+		r = gsgpu_bo_reserve(dobj, true);
+		if (likely(r == 0)) {
+			gsgpu_bo_unpin(dobj);
+			gsgpu_bo_unreserve(dobj);
+		}
+		gsgpu_bo_unref(&dobj);
+	}
+}
+
+void gsgpu_benchmark(struct gsgpu_device *adev, int test_number)
+{
+	int i;
+	static const int common_modes[GSGPU_BENCHMARK_COMMON_MODES_N] = {
+		640 * 480 * 4,
+		720 * 480 * 4,
+		800 * 600 * 4,
+		848 * 480 * 4,
+		1024 * 768 * 4,
+		1152 * 768 * 4,
+		1280 * 720 * 4,
+		1280 * 800 * 4,
+		1280 * 854 * 4,
+		1280 * 960 * 4,
+		1280 * 1024 * 4,
+		1440 * 900 * 4,
+		1400 * 1050 * 4,
+		1680 * 1050 * 4,
+		1600 * 1200 * 4,
+		1920 * 1080 * 4,
+		1920 * 1200 * 4
+	};
+
+	switch (test_number) {
+	case 1:
+		/* simple test, VRAM to GTT and GTT to VRAM */
+		gsgpu_benchmark_move(adev, 1024*1024, GSGPU_GEM_DOMAIN_GTT,
+				      GSGPU_GEM_DOMAIN_VRAM);
+		gsgpu_benchmark_move(adev, 1024*1024, GSGPU_GEM_DOMAIN_VRAM,
+				      GSGPU_GEM_DOMAIN_GTT);
+		break;
+	case 2:
+		/* simple test, VRAM to VRAM */
+		gsgpu_benchmark_move(adev, 1024*1024, GSGPU_GEM_DOMAIN_VRAM,
+				      GSGPU_GEM_DOMAIN_VRAM);
+		break;
+	case 3:
+		/* GTT to VRAM, buffer size sweep, powers of 2 */
+		for (i = 1; i <= 16384; i <<= 1)
+			gsgpu_benchmark_move(adev, i * GSGPU_GPU_PAGE_SIZE,
+					      GSGPU_GEM_DOMAIN_GTT,
+					      GSGPU_GEM_DOMAIN_VRAM);
+		break;
+	case 4:
+		/* VRAM to GTT, buffer size sweep, powers of 2 */
+		for (i = 1; i <= 16384; i <<= 1)
+			gsgpu_benchmark_move(adev, i * GSGPU_GPU_PAGE_SIZE,
+					      GSGPU_GEM_DOMAIN_VRAM,
+					      GSGPU_GEM_DOMAIN_GTT);
+		break;
+	case 5:
+		/* VRAM to VRAM, buffer size sweep, powers of 2 */
+		for (i = 1; i <= 16384; i <<= 1)
+			gsgpu_benchmark_move(adev, i * GSGPU_GPU_PAGE_SIZE,
+					      GSGPU_GEM_DOMAIN_VRAM,
+					      GSGPU_GEM_DOMAIN_VRAM);
+		break;
+	case 6:
+		/* GTT to VRAM, buffer size sweep, common modes */
+		for (i = 0; i < GSGPU_BENCHMARK_COMMON_MODES_N; i++)
+			gsgpu_benchmark_move(adev, common_modes[i],
+					      GSGPU_GEM_DOMAIN_GTT,
+					      GSGPU_GEM_DOMAIN_VRAM);
+		break;
+	case 7:
+		/* VRAM to GTT, buffer size sweep, common modes */
+		for (i = 0; i < GSGPU_BENCHMARK_COMMON_MODES_N; i++)
+			gsgpu_benchmark_move(adev, common_modes[i],
+					      GSGPU_GEM_DOMAIN_VRAM,
+					      GSGPU_GEM_DOMAIN_GTT);
+		break;
+	case 8:
+		/* VRAM to VRAM, buffer size sweep, common modes */
+		for (i = 0; i < GSGPU_BENCHMARK_COMMON_MODES_N; i++)
+			gsgpu_benchmark_move(adev, common_modes[i],
+					      GSGPU_GEM_DOMAIN_VRAM,
+					      GSGPU_GEM_DOMAIN_VRAM);
+		break;
+
+	default:
+		DRM_ERROR("Unknown benchmark\n");
+	}
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_bo_list.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_bo_list.c
new file mode 100644
index 000000000000..007b3e98a010
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_bo_list.c
@@ -0,0 +1,286 @@
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include "gsgpu_trace.h"
+
+#define GSGPU_BO_LIST_MAX_PRIORITY	32u
+#define GSGPU_BO_LIST_NUM_BUCKETS	(GSGPU_BO_LIST_MAX_PRIORITY + 1)
+
+static void gsgpu_bo_list_free_rcu(struct rcu_head *rcu)
+{
+	struct gsgpu_bo_list *list = container_of(rcu, struct gsgpu_bo_list,
+						   rhead);
+
+	kvfree(list);
+}
+
+static void gsgpu_bo_list_free(struct kref *ref)
+{
+	struct gsgpu_bo_list *list = container_of(ref, struct gsgpu_bo_list,
+						   refcount);
+	struct gsgpu_bo_list_entry *e;
+
+	gsgpu_bo_list_for_each_entry(e, list)
+		gsgpu_bo_unref(&e->robj);
+
+	call_rcu(&list->rhead, gsgpu_bo_list_free_rcu);
+}
+
+int gsgpu_bo_list_create(struct gsgpu_device *adev, struct drm_file *filp,
+			  struct drm_gsgpu_bo_list_entry *info,
+			  unsigned num_entries, struct gsgpu_bo_list **result)
+{
+	unsigned last_entry = 0, first_userptr = num_entries;
+	struct gsgpu_bo_list_entry *array;
+	struct gsgpu_bo_list *list;
+	uint64_t total_size = 0;
+	size_t size;
+	unsigned i;
+	int r;
+
+	if (num_entries > (SIZE_MAX - sizeof(struct gsgpu_bo_list))
+				/ sizeof(struct gsgpu_bo_list_entry))
+		return -EINVAL;
+
+	size = sizeof(struct gsgpu_bo_list);
+	size += num_entries * sizeof(struct gsgpu_bo_list_entry);
+	list = kvmalloc(size, GFP_KERNEL);
+	if (!list)
+		return -ENOMEM;
+
+	kref_init(&list->refcount);
+
+	array = gsgpu_bo_list_array_entry(list, 0);
+	memset(array, 0, num_entries * sizeof(struct gsgpu_bo_list_entry));
+
+	for (i = 0; i < num_entries; ++i) {
+		struct gsgpu_bo_list_entry *entry;
+		struct drm_gem_object *gobj;
+		struct gsgpu_bo *bo;
+		struct mm_struct *usermm;
+
+		gobj = drm_gem_object_lookup(filp, info[i].bo_handle);
+		if (!gobj) {
+			r = -ENOENT;
+			goto error_free;
+		}
+
+		bo = gsgpu_bo_ref(gem_to_gsgpu_bo(gobj));
+		drm_gem_object_put_unlocked(gobj);
+
+		usermm = gsgpu_ttm_tt_get_usermm(bo->tbo.ttm);
+		if (usermm) {
+			if (usermm != current->mm) {
+				gsgpu_bo_unref(&bo);
+				r = -EPERM;
+				goto error_free;
+			}
+			entry = &array[--first_userptr];
+		} else {
+			entry = &array[last_entry++];
+		}
+
+		entry->robj = bo;
+		entry->priority = min(info[i].bo_priority,
+				      GSGPU_BO_LIST_MAX_PRIORITY);
+		entry->tv.bo = &entry->robj->tbo;
+		entry->tv.shared = !entry->robj->prime_shared_count;
+
+		total_size += gsgpu_bo_size(entry->robj);
+		trace_gsgpu_bo_list_set(list, entry->robj);
+	}
+
+	list->first_userptr = first_userptr;
+	list->num_entries = num_entries;
+
+	trace_gsgpu_cs_bo_status(list->num_entries, total_size);
+
+	*result = list;
+	return 0;
+
+error_free:
+	while (i--)
+		gsgpu_bo_unref(&array[i].robj);
+	kvfree(list);
+	return r;
+
+}
+
+static void gsgpu_bo_list_destroy(struct gsgpu_fpriv *fpriv, int id)
+{
+	struct gsgpu_bo_list *list;
+
+	mutex_lock(&fpriv->bo_list_lock);
+	list = idr_remove(&fpriv->bo_list_handles, id);
+	mutex_unlock(&fpriv->bo_list_lock);
+	if (list)
+		kref_put(&list->refcount, gsgpu_bo_list_free);
+}
+
+int gsgpu_bo_list_get(struct gsgpu_fpriv *fpriv, int id,
+		       struct gsgpu_bo_list **result)
+{
+	rcu_read_lock();
+	*result = idr_find(&fpriv->bo_list_handles, id);
+
+	if (*result && kref_get_unless_zero(&(*result)->refcount)) {
+		rcu_read_unlock();
+		return 0;
+	}
+
+	rcu_read_unlock();
+	return -ENOENT;
+}
+
+void gsgpu_bo_list_get_list(struct gsgpu_bo_list *list,
+			     struct list_head *validated)
+{
+	/* This is based on the bucket sort with O(n) time complexity.
+	 * An item with priority "i" is added to bucket[i]. The lists are then
+	 * concatenated in descending order.
+	 */
+	struct list_head bucket[GSGPU_BO_LIST_NUM_BUCKETS];
+	struct gsgpu_bo_list_entry *e;
+	unsigned i;
+
+	for (i = 0; i < GSGPU_BO_LIST_NUM_BUCKETS; i++)
+		INIT_LIST_HEAD(&bucket[i]);
+
+	/* Since buffers which appear sooner in the relocation list are
+	 * likely to be used more often than buffers which appear later
+	 * in the list, the sort mustn't change the ordering of buffers
+	 * with the same priority, i.e. it must be stable.
+	 */
+	gsgpu_bo_list_for_each_entry(e, list) {
+		unsigned priority = e->priority;
+
+		if (!e->robj->parent)
+			list_add_tail(&e->tv.head, &bucket[priority]);
+
+		e->user_pages = NULL;
+	}
+
+	/* Connect the sorted buckets in the output list. */
+	for (i = 0; i < GSGPU_BO_LIST_NUM_BUCKETS; i++)
+		list_splice(&bucket[i], validated);
+}
+
+void gsgpu_bo_list_put(struct gsgpu_bo_list *list)
+{
+	kref_put(&list->refcount, gsgpu_bo_list_free);
+}
+
+int gsgpu_bo_create_list_entry_array(struct drm_gsgpu_bo_list_in *in,
+				      struct drm_gsgpu_bo_list_entry **info_param)
+{
+	const void __user *uptr = u64_to_user_ptr(in->bo_info_ptr);
+	const uint32_t info_size = sizeof(struct drm_gsgpu_bo_list_entry);
+	struct drm_gsgpu_bo_list_entry *info;
+	int r;
+
+	info = kvmalloc_array(in->bo_number, info_size, GFP_KERNEL);
+	if (!info)
+		return -ENOMEM;
+
+	/* copy the handle array from userspace to a kernel buffer */
+	r = -EFAULT;
+	if (likely(info_size == in->bo_info_size)) {
+		unsigned long bytes = in->bo_number *
+			in->bo_info_size;
+
+		if (copy_from_user(info, uptr, bytes))
+			goto error_free;
+
+	} else {
+		unsigned long bytes = min(in->bo_info_size, info_size);
+		unsigned i;
+
+		memset(info, 0, in->bo_number * info_size);
+		for (i = 0; i < in->bo_number; ++i) {
+			if (copy_from_user(&info[i], uptr, bytes))
+				goto error_free;
+
+			uptr += in->bo_info_size;
+		}
+	}
+
+	*info_param = info;
+	return 0;
+
+error_free:
+	kvfree(info);
+	return r;
+}
+
+int gsgpu_bo_list_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *filp)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_fpriv *fpriv = filp->driver_priv;
+	union drm_gsgpu_bo_list *args = data;
+	uint32_t handle = args->in.list_handle;
+	struct drm_gsgpu_bo_list_entry *info = NULL;
+	struct gsgpu_bo_list *list, *old;
+	int r;
+
+	r = gsgpu_bo_create_list_entry_array(&args->in, &info);
+	if (r)
+		goto error_free;
+
+	switch (args->in.operation) {
+	case GSGPU_BO_LIST_OP_CREATE:
+		r = gsgpu_bo_list_create(adev, filp, info, args->in.bo_number,
+					  &list);
+		if (r)
+			goto error_free;
+
+		mutex_lock(&fpriv->bo_list_lock);
+		r = idr_alloc(&fpriv->bo_list_handles, list, 1, 0, GFP_KERNEL);
+		mutex_unlock(&fpriv->bo_list_lock);
+		if (r < 0) {
+			gsgpu_bo_list_put(list);
+			return r;
+		}
+
+		handle = r;
+		break;
+
+	case GSGPU_BO_LIST_OP_DESTROY:
+		gsgpu_bo_list_destroy(fpriv, handle);
+		handle = 0;
+		break;
+
+	case GSGPU_BO_LIST_OP_UPDATE:
+		r = gsgpu_bo_list_create(adev, filp, info, args->in.bo_number,
+					  &list);
+		if (r)
+			goto error_free;
+
+		mutex_lock(&fpriv->bo_list_lock);
+		old = idr_replace(&fpriv->bo_list_handles, list, handle);
+		mutex_unlock(&fpriv->bo_list_lock);
+
+		if (IS_ERR(old)) {
+			gsgpu_bo_list_put(list);
+			r = PTR_ERR(old);
+			goto error_free;
+		}
+
+		gsgpu_bo_list_put(old);
+		break;
+
+	default:
+		r = -EINVAL;
+		goto error_free;
+	}
+
+	memset(args, 0, sizeof(*args));
+	args->out.list_handle = handle;
+	kvfree(info);
+
+	return 0;
+
+error_free:
+	if (info)
+		kvfree(info);
+	return r;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_common.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_common.c
new file mode 100644
index 000000000000..895a7607f2b3
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_common.c
@@ -0,0 +1,216 @@
+#include <linux/slab.h>
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include "gsgpu_ih.h"
+#include "gsgpu_common.h"
+#include "gsgpu_mmu.h"
+#include "gsgpu_zip.h"
+#include "gsgpu_gfx.h"
+#include "gsgpu_xdma.h"
+#include "gsgpu_dc_reg.h"
+
+static u32 gsgpu_get_clk(struct gsgpu_device *adev)
+{
+	DRM_DEBUG_DRIVER("%s Not implemented\n", __func__);
+
+	return 0;
+}
+
+static void gsgpu_vga_set_state(struct gsgpu_device *adev, bool state)
+{
+	return;
+/* TODO: In this function we should be enable\disable GPU&DC */
+/*	u32 conf_reg;
+	u32 i;
+
+	for (i = 0; i < adev->mode_info.num_crtc; i++) {
+		conf_reg = dc_readl(adev, CURRENT_REG(DC_CRTC_CFG_REG, i));
+		if (state)
+			conf_reg |= CRTC_CFG_ENABLE;
+		else
+			conf_reg &= ~CRTC_CFG_ENABLE;
+		dc_writel(adev, CURRENT_REG(DC_CRTC_CFG_REG, i), conf_reg);
+	}
+*/
+}
+
+static bool gsgpu_read_bios_from_rom(struct gsgpu_device *adev,
+				  u8 *bios, u32 length_bytes)
+{
+	DRM_DEBUG_DRIVER("%s Not implemented\n", __func__);
+
+	return true;
+}
+
+static int gsgpu_read_register(struct gsgpu_device *adev, u32 se_num,
+			    u32 sh_num, u32 reg_offset, u32 *value)
+{
+	DRM_DEBUG_DRIVER("%s Not implemented\n", __func__);
+	*value = 0;
+	return 0;
+}
+
+static int gsgpu_gpu_pci_config_reset(struct gsgpu_device *adev)
+{
+	u32 i;
+
+	dev_info(adev->dev, "GPU pci config reset\n");
+
+	/* disable BM */
+	pci_clear_master(adev->pdev);
+	/* reset */
+	gsgpu_device_pci_config_reset(adev);
+
+	udelay(100);
+
+	/* wait for asic to come out of reset */
+	for (i = 0; i < adev->usec_timeout; i++) {
+		if (1) {
+			/* enable BM */
+			pci_set_master(adev->pdev);
+			adev->has_hw_reset = true;
+			return 0;
+		}
+		udelay(1);
+	}
+	return -EINVAL;
+}
+
+static int gsgpu_reset(struct gsgpu_device *adev)
+{
+	int r;
+
+	/*XXX Set pcie config regs not Need*/
+	return 0;
+
+	r = gsgpu_gpu_pci_config_reset(adev);
+
+	return r;
+}
+
+static bool gsgpu_need_full_reset(struct gsgpu_device *adev)
+{
+	switch (adev->family_type) {
+	case CHIP_LG100:
+	default:
+		/* change this when we support soft reset */
+		return true;
+	}
+}
+
+static const struct gsgpu_asic_funcs gsgpu_asic_funcs = {
+	.read_bios_from_rom = &gsgpu_read_bios_from_rom,
+	.read_register = &gsgpu_read_register,
+	.reset = &gsgpu_reset,
+	.set_vga_state = &gsgpu_vga_set_state,
+	.get_clk = &gsgpu_get_clk,
+	.need_full_reset = &gsgpu_need_full_reset,
+};
+
+static int gsgpu_common_early_init(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	DRM_DEBUG_DRIVER("%s Not implemented\n", __func__);
+
+	adev->asic_funcs = &gsgpu_asic_funcs;
+
+	return 0;
+}
+
+static int gsgpu_common_late_init(void *handle)
+{
+	return 0;
+}
+
+static int gsgpu_common_sw_init(void *handle)
+{
+	return 0;
+}
+
+static int gsgpu_common_sw_fini(void *handle)
+{
+	return 0;
+}
+
+static int gsgpu_common_hw_init(void *handle)
+{
+	return 0;
+}
+
+static int gsgpu_common_hw_fini(void *handle)
+{
+	return 0;
+}
+
+static int gsgpu_common_suspend(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	return gsgpu_common_hw_fini(adev);
+}
+
+static int gsgpu_common_resume(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	return gsgpu_common_hw_init(adev);
+}
+
+static bool gsgpu_common_is_idle(void *handle)
+{
+	return true;
+}
+
+static int gsgpu_common_wait_for_idle(void *handle)
+{
+	return 0;
+}
+
+static int gsgpu_common_soft_reset(void *handle)
+{
+	return 0;
+}
+
+static const struct gsgpu_ip_funcs gsgpu_common_ip_funcs = {
+	.name = "gsgpu_common",
+	.early_init = gsgpu_common_early_init,
+	.late_init = gsgpu_common_late_init,
+	.sw_init = gsgpu_common_sw_init,
+	.sw_fini = gsgpu_common_sw_fini,
+	.hw_init = gsgpu_common_hw_init,
+	.hw_fini = gsgpu_common_hw_fini,
+	.suspend = gsgpu_common_suspend,
+	.resume = gsgpu_common_resume,
+	.is_idle = gsgpu_common_is_idle,
+	.wait_for_idle = gsgpu_common_wait_for_idle,
+	.soft_reset = gsgpu_common_soft_reset,
+};
+
+static const struct gsgpu_ip_block_version gsgpu_common_ip_block = {
+	.type = GSGPU_IP_BLOCK_TYPE_COMMON,
+	.major = 1,
+	.minor = 0,
+	.rev = 0,
+	.funcs = &gsgpu_common_ip_funcs,
+};
+
+int gsgpu_set_ip_blocks(struct gsgpu_device *adev)
+{
+	switch (adev->family_type) {
+	case CHIP_LG100:
+		gsgpu_device_ip_block_add(adev, &gsgpu_common_ip_block);
+		gsgpu_device_ip_block_add(adev, &mmu_ip_block);
+		gsgpu_device_ip_block_add(adev, &zip_ip_block);
+		gsgpu_device_ip_block_add(adev, &gsgpu_ih_ip_block);
+		gsgpu_device_ip_block_add(adev, &dc_ip_block);
+		gsgpu_device_ip_block_add(adev, &gfx_ip_block);
+		gsgpu_device_ip_block_add(adev, &xdma_ip_block);
+		break;
+	default:
+		/* FIXME: not supported yet */
+		return -EINVAL;
+	}
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_cp.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_cp.c
new file mode 100644
index 000000000000..73528f7c8cd3
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_cp.c
@@ -0,0 +1,162 @@
+#include <linux/firmware.h>
+#include "gsgpu.h"
+#include "gsgpu_cp.h"
+
+#define MAJOR_SHIFT			0
+#define MINOR_SHIFT			8
+#define REVISION_SHIFT			16
+#define FAMILY_SHIFT			8
+#define MAJOR_MASK			0xff
+#define MINOR_MASK			0xff
+#define REVISION_MASK			0xff
+#define HWINF_MASK			0xfff
+#define FAMILY_MASK			0xf
+
+static int gsgpu_cp_wait_for_idle(struct gsgpu_device *adev)
+{
+	if (gsgpu_cp_wait_done(adev) == true)
+		return 0;
+
+	return -ETIMEDOUT;
+}
+
+int gsgpu_cp_enable(struct gsgpu_device *adev, bool enable)
+{
+	int i;
+	u32 tmp = RREG32(GSGPU_EC_CTRL);
+
+	if (enable) {
+		tmp |= 1;
+	} else {
+		tmp &= ~1;
+		for (i = 0; i < adev->gfx.num_gfx_rings; i++)
+			adev->gfx.gfx_ring[i].ready = false;
+	}
+	WREG32(GSGPU_EC_CTRL, tmp);
+	mdelay(100);
+	if (enable)
+		return gsgpu_cp_wait_for_idle(adev);
+	else
+		return 0;
+}
+
+static void gsgpu_free_microcode(struct gsgpu_device *adev)
+{
+	release_firmware(adev->gfx.cp_fw);
+	adev->gfx.cp_fw = NULL;
+}
+
+static int gsgpu_init_microcode(struct gsgpu_device *adev)
+{
+	const char *chip_name;
+	char fw_name[30];
+	int err;
+
+	DRM_DEBUG("\n");
+
+	switch (adev->family_type) {
+	case CHIP_LG100:
+		chip_name = "lg100";
+		break;
+	default:
+		BUG();
+	}
+
+	snprintf(fw_name, sizeof(fw_name), "loongson/%s_cp.bin", chip_name);
+	err = request_firmware(&adev->gfx.cp_fw, fw_name, adev->dev);
+	if (err)
+		goto out;
+	adev->gfx.cp_fw_version = 0;
+	adev->gfx.cp_feature_version = 0;
+
+out:
+	if (err) {
+		dev_err(adev->dev,
+			"gfx8: Failed to load firmware \"%s\"\n",
+			fw_name);
+		release_firmware(adev->gfx.cp_fw);
+		adev->gfx.cp_fw = NULL;
+	}
+	return err;
+}
+
+int gsgpu_cp_gfx_load_microcode(struct gsgpu_device *adev)
+{
+	const __le32 *fw_data;
+	unsigned i, fw_size, fw_wptr;
+
+	if (!adev->gfx.cp_fw)
+		return -EINVAL;
+
+	gsgpu_cp_enable(adev, false);
+
+	/* CP */
+	fw_data = (const __le32 *)(adev->gfx.cp_fw->data);
+	fw_size = adev->gfx.cp_fw->size;
+
+	if (fw_size > 0x10000)
+		return -EINVAL;
+
+	for (i = 0; i < fw_size; i += 4)
+		WREG32(GSGPU_FW_WPORT, le32_to_cpup(fw_data++));
+
+	fw_wptr = RREG32(GSGPU_FW_WPTR);
+	if (fw_size != fw_wptr)
+		return -EINVAL;
+
+	return 0;
+}
+
+static void gsgpu_get_version(struct gsgpu_device *adev)
+{
+	u32 cp_fw_version;
+	u32 cp_feature_version;
+	u32 hw_inf;
+	u32 hw_version;
+	u8 fw_major;
+	u8 fw_minor;
+	u8 fw_revision;
+
+	cp_fw_version = RREG32(GSGPU_FW_VERSION_OFFSET);
+	fw_major = (cp_fw_version >> MAJOR_SHIFT & MAJOR_MASK);
+	fw_minor = (cp_fw_version >> MINOR_SHIFT) & MINOR_MASK;
+	fw_revision = (cp_fw_version >> REVISION_SHIFT) & REVISION_MASK;
+	cp_feature_version = RREG32(GSGPU_HW_FEATURE_OFFSET);
+	hw_inf = RREG32(GSGPU_HWINF);
+	hw_inf &= HWINF_MASK;
+	hw_version = hw_inf;
+	adev->gfx.cp_fw_version = cp_fw_version;
+	adev->gfx.cp_feature_version = cp_feature_version;
+	DRM_INFO("GPU Family: LG%x00 series LG%x, Feature:0x%08x", (hw_inf >> FAMILY_SHIFT) & FAMILY_MASK, hw_version, cp_feature_version);
+	DRM_INFO("Firmware Version: %d.%d.%d", fw_major, fw_minor, fw_revision);
+}
+
+int gsgpu_cp_init(struct gsgpu_device *adev)
+{
+	int r;
+
+	r = gsgpu_init_microcode(adev);
+	if (r) {
+		DRM_ERROR("Failed to load gfx firmware!\n");
+		return r;
+	}
+
+	r = gsgpu_cp_gfx_load_microcode(adev);
+	if (r)
+		return r;
+
+	r = gsgpu_cp_enable(adev, true);
+	if (r)
+		return r;
+
+	gsgpu_get_version(adev);
+
+	return 0;
+}
+
+int gsgpu_cp_fini(struct gsgpu_device *adev)
+{
+	gsgpu_free_microcode(adev);
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c
new file mode 100644
index 000000000000..3bc59bab103e
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_cs.c
@@ -0,0 +1,1529 @@
+#include <linux/pagemap.h>
+#include <linux/sync_file.h>
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include <drm/drm_syncobj.h>
+#include "gsgpu.h"
+#include "gsgpu_vm_it.h"
+#include "gsgpu_trace.h"
+#include "gsgpu_gmc.h"
+
+static int gsgpu_cs_user_fence_chunk(struct gsgpu_cs_parser *p,
+				      struct drm_gsgpu_cs_chunk_fence *data,
+				      uint32_t *offset)
+{
+	struct drm_gem_object *gobj;
+	unsigned long size;
+	int r;
+
+	gobj = drm_gem_object_lookup(p->filp, data->handle);
+	if (gobj == NULL)
+		return -EINVAL;
+
+	p->uf_entry.robj = gsgpu_bo_ref(gem_to_gsgpu_bo(gobj));
+	p->uf_entry.priority = 0;
+	p->uf_entry.tv.bo = &p->uf_entry.robj->tbo;
+	p->uf_entry.tv.shared = true;
+	p->uf_entry.user_pages = NULL;
+
+	drm_gem_object_put_unlocked(gobj);
+
+	size = gsgpu_bo_size(p->uf_entry.robj);
+	if (size != PAGE_SIZE || (data->offset + 8) > size) {
+		r = -EINVAL;
+		goto error_unref;
+	}
+
+	if (gsgpu_ttm_tt_get_usermm(p->uf_entry.robj->tbo.ttm)) {
+		r = -EINVAL;
+		goto error_unref;
+	}
+
+	*offset = data->offset;
+
+	return 0;
+
+error_unref:
+	gsgpu_bo_unref(&p->uf_entry.robj);
+	return r;
+}
+
+static int gsgpu_cs_bo_handles_chunk(struct gsgpu_cs_parser *p,
+				      struct drm_gsgpu_bo_list_in *data)
+{
+	int r;
+	struct drm_gsgpu_bo_list_entry *info = NULL;
+
+	r = gsgpu_bo_create_list_entry_array(data, &info);
+	if (r)
+		return r;
+
+	r = gsgpu_bo_list_create(p->adev, p->filp, info, data->bo_number,
+				  &p->bo_list);
+	if (r)
+		goto error_free;
+
+	kvfree(info);
+	return 0;
+
+error_free:
+	if (info)
+		kvfree(info);
+
+	return r;
+}
+
+static int gsgpu_cs_parser_init(struct gsgpu_cs_parser *p, union drm_gsgpu_cs *cs)
+{
+	struct gsgpu_fpriv *fpriv = p->filp->driver_priv;
+	struct gsgpu_vm *vm = &fpriv->vm;
+	uint64_t *chunk_array_user;
+	uint64_t *chunk_array;
+	unsigned size, num_ibs = 0;
+	uint32_t uf_offset = 0;
+	int i;
+	int ret;
+
+	if (cs->in.num_chunks == 0)
+		return 0;
+
+	chunk_array = kmalloc_array(cs->in.num_chunks, sizeof(uint64_t), GFP_KERNEL);
+	if (!chunk_array)
+		return -ENOMEM;
+
+	p->ctx = gsgpu_ctx_get(fpriv, cs->in.ctx_id);
+	if (!p->ctx) {
+		ret = -EINVAL;
+		goto free_chunk;
+	}
+
+	mutex_lock(&p->ctx->lock);
+
+	/* skip guilty context job */
+	if (atomic_read(&p->ctx->guilty) == 1) {
+		ret = -ECANCELED;
+		goto free_chunk;
+	}
+
+	/* get chunks */
+	chunk_array_user = u64_to_user_ptr(cs->in.chunks);
+	if (copy_from_user(chunk_array, chunk_array_user,
+			   sizeof(uint64_t)*cs->in.num_chunks)) {
+		ret = -EFAULT;
+		goto free_chunk;
+	}
+
+	p->nchunks = cs->in.num_chunks;
+	p->chunks = kmalloc_array(p->nchunks, sizeof(struct gsgpu_cs_chunk),
+			    GFP_KERNEL);
+	if (!p->chunks) {
+		ret = -ENOMEM;
+		goto free_chunk;
+	}
+
+	for (i = 0; i < p->nchunks; i++) {
+		struct drm_gsgpu_cs_chunk __user **chunk_ptr = NULL;
+		struct drm_gsgpu_cs_chunk user_chunk;
+		uint32_t __user *cdata;
+
+		chunk_ptr = u64_to_user_ptr(chunk_array[i]);
+		if (copy_from_user(&user_chunk, chunk_ptr,
+				       sizeof(struct drm_gsgpu_cs_chunk))) {
+			ret = -EFAULT;
+			i--;
+			goto free_partial_kdata;
+		}
+		p->chunks[i].chunk_id = user_chunk.chunk_id;
+		p->chunks[i].length_dw = user_chunk.length_dw;
+
+		size = p->chunks[i].length_dw;
+		cdata = u64_to_user_ptr(user_chunk.chunk_data);
+
+		p->chunks[i].kdata = kvmalloc_array(size, sizeof(uint32_t), GFP_KERNEL);
+		if (p->chunks[i].kdata == NULL) {
+			ret = -ENOMEM;
+			i--;
+			goto free_partial_kdata;
+		}
+		size *= sizeof(uint32_t);
+		if (copy_from_user(p->chunks[i].kdata, cdata, size)) {
+			ret = -EFAULT;
+			goto free_partial_kdata;
+		}
+
+		switch (p->chunks[i].chunk_id) {
+		case GSGPU_CHUNK_ID_IB:
+			++num_ibs;
+			break;
+
+		case GSGPU_CHUNK_ID_FENCE:
+			size = sizeof(struct drm_gsgpu_cs_chunk_fence);
+			if (p->chunks[i].length_dw * sizeof(uint32_t) < size) {
+				ret = -EINVAL;
+				goto free_partial_kdata;
+			}
+
+			ret = gsgpu_cs_user_fence_chunk(p, p->chunks[i].kdata,
+							 &uf_offset);
+			if (ret)
+				goto free_partial_kdata;
+
+			break;
+
+		case GSGPU_CHUNK_ID_BO_HANDLES:
+			size = sizeof(struct drm_gsgpu_bo_list_in);
+			if (p->chunks[i].length_dw * sizeof(uint32_t) < size) {
+				ret = -EINVAL;
+				goto free_partial_kdata;
+			}
+
+			ret = gsgpu_cs_bo_handles_chunk(p, p->chunks[i].kdata);
+			if (ret)
+				goto free_partial_kdata;
+
+			break;
+
+		case GSGPU_CHUNK_ID_DEPENDENCIES:
+		case GSGPU_CHUNK_ID_SYNCOBJ_IN:
+		case GSGPU_CHUNK_ID_SYNCOBJ_OUT:
+			break;
+
+		default:
+			ret = -EINVAL;
+			goto free_partial_kdata;
+		}
+	}
+
+	ret = gsgpu_job_alloc(p->adev, num_ibs, &p->job, vm);
+	if (ret)
+		goto free_all_kdata;
+
+	if (p->ctx->vram_lost_counter != p->job->vram_lost_counter) {
+		ret = -ECANCELED;
+		goto free_all_kdata;
+	}
+
+	if (p->uf_entry.robj)
+		p->job->uf_addr = uf_offset;
+	kfree(chunk_array);
+
+	/* Use this opportunity to fill in task info for the vm */
+	gsgpu_vm_set_task_info(vm);
+
+	return 0;
+
+free_all_kdata:
+	i = p->nchunks - 1;
+free_partial_kdata:
+	for (; i >= 0; i--)
+		kvfree(p->chunks[i].kdata);
+	kfree(p->chunks);
+	p->chunks = NULL;
+	p->nchunks = 0;
+free_chunk:
+	kfree(chunk_array);
+
+	return ret;
+}
+
+/* Convert microseconds to bytes. */
+static u64 us_to_bytes(struct gsgpu_device *adev, s64 us)
+{
+	if (us <= 0 || !adev->mm_stats.log2_max_MBps)
+		return 0;
+
+	/* Since accum_us is incremented by a million per second, just
+	 * multiply it by the number of MB/s to get the number of bytes.
+	 */
+	return us << adev->mm_stats.log2_max_MBps;
+}
+
+static s64 bytes_to_us(struct gsgpu_device *adev, u64 bytes)
+{
+	if (!adev->mm_stats.log2_max_MBps)
+		return 0;
+
+	return bytes >> adev->mm_stats.log2_max_MBps;
+}
+
+/* Returns how many bytes TTM can move right now. If no bytes can be moved,
+ * it returns 0. If it returns non-zero, it's OK to move at least one buffer,
+ * which means it can go over the threshold once. If that happens, the driver
+ * will be in debt and no other buffer migrations can be done until that debt
+ * is repaid.
+ *
+ * This approach allows moving a buffer of any size (it's important to allow
+ * that).
+ *
+ * The currency is simply time in microseconds and it increases as the clock
+ * ticks. The accumulated microseconds (us) are converted to bytes and
+ * returned.
+ */
+static void gsgpu_cs_get_threshold_for_moves(struct gsgpu_device *adev,
+					      u64 *max_bytes,
+					      u64 *max_vis_bytes)
+{
+	s64 time_us, increment_us;
+	u64 free_vram, total_vram, used_vram;
+
+	/* Allow a maximum of 200 accumulated ms. This is basically per-IB
+	 * throttling.
+	 *
+	 * It means that in order to get full max MBps, at least 5 IBs per
+	 * second must be submitted and not more than 200ms apart from each
+	 * other.
+	 */
+	const s64 us_upper_bound = 200000;
+
+	if (!adev->mm_stats.log2_max_MBps) {
+		*max_bytes = 0;
+		*max_vis_bytes = 0;
+		return;
+	}
+
+	total_vram = adev->gmc.real_vram_size - atomic64_read(&adev->vram_pin_size);
+	used_vram = gsgpu_vram_mgr_usage(&adev->mman.bdev.man[TTM_PL_VRAM]);
+	free_vram = used_vram >= total_vram ? 0 : total_vram - used_vram;
+
+	spin_lock(&adev->mm_stats.lock);
+
+	/* Increase the amount of accumulated us. */
+	time_us = ktime_to_us(ktime_get());
+	increment_us = time_us - adev->mm_stats.last_update_us;
+	adev->mm_stats.last_update_us = time_us;
+	adev->mm_stats.accum_us = min(adev->mm_stats.accum_us + increment_us,
+						us_upper_bound);
+
+	/* This prevents the short period of low performance when the VRAM
+	 * usage is low and the driver is in debt or doesn't have enough
+	 * accumulated us to fill VRAM quickly.
+	 *
+	 * The situation can occur in these cases:
+	 * - a lot of VRAM is freed by userspace
+	 * - the presence of a big buffer causes a lot of evictions
+	 *   (solution: split buffers into smaller ones)
+	 *
+	 * If 128 MB or 1/8th of VRAM is free, start filling it now by setting
+	 * accum_us to a positive number.
+	 */
+	if (free_vram >= 128 * 1024 * 1024 || free_vram >= total_vram / 8) {
+		s64 min_us;
+
+		/* Be more aggresive on dGPUs. Try to fill a portion of free
+		 * VRAM now.
+		 */
+		if (!(adev->flags & GSGPU_IS_APU))
+			min_us = bytes_to_us(adev, free_vram / 4);
+		else
+			min_us = 0; /* Reset accum_us on APUs. */
+
+		adev->mm_stats.accum_us = max(min_us, adev->mm_stats.accum_us);
+	}
+
+	/* This is set to 0 if the driver is in debt to disallow (optional)
+	 * buffer moves.
+	 */
+	*max_bytes = us_to_bytes(adev, adev->mm_stats.accum_us);
+
+	/* Do the same for visible VRAM if half of it is free */
+	if (!gsgpu_gmc_vram_full_visible(&adev->gmc)) {
+		u64 total_vis_vram = adev->gmc.visible_vram_size;
+		u64 used_vis_vram =
+			gsgpu_vram_mgr_vis_usage(&adev->mman.bdev.man[TTM_PL_VRAM]);
+
+		if (used_vis_vram < total_vis_vram) {
+			u64 free_vis_vram = total_vis_vram - used_vis_vram;
+			adev->mm_stats.accum_us_vis = min(adev->mm_stats.accum_us_vis +
+							  increment_us, us_upper_bound);
+
+			if (free_vis_vram >= total_vis_vram / 2)
+				adev->mm_stats.accum_us_vis =
+					max(bytes_to_us(adev, free_vis_vram / 2),
+					    adev->mm_stats.accum_us_vis);
+		}
+
+		*max_vis_bytes = us_to_bytes(adev, adev->mm_stats.accum_us_vis);
+	} else {
+		*max_vis_bytes = 0;
+	}
+
+	spin_unlock(&adev->mm_stats.lock);
+}
+
+/* Report how many bytes have really been moved for the last command
+ * submission. This can result in a debt that can stop buffer migrations
+ * temporarily.
+ */
+void gsgpu_cs_report_moved_bytes(struct gsgpu_device *adev, u64 num_bytes,
+				  u64 num_vis_bytes)
+{
+	spin_lock(&adev->mm_stats.lock);
+	adev->mm_stats.accum_us -= bytes_to_us(adev, num_bytes);
+	adev->mm_stats.accum_us_vis -= bytes_to_us(adev, num_vis_bytes);
+	spin_unlock(&adev->mm_stats.lock);
+}
+
+static int gsgpu_cs_bo_validate(struct gsgpu_cs_parser *p,
+				 struct gsgpu_bo *bo)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	struct ttm_operation_ctx ctx = {
+		.interruptible = true,
+		.no_wait_gpu = false,
+		.resv = bo->tbo.resv,
+		.flags = 0
+	};
+	uint32_t domain;
+	int r;
+
+	if (bo->pin_count)
+		return 0;
+
+	/* Don't move this buffer if we have depleted our allowance
+	 * to move it. Don't move anything if the threshold is zero.
+	 */
+	if (p->bytes_moved < p->bytes_moved_threshold) {
+		if (!gsgpu_gmc_vram_full_visible(&adev->gmc) &&
+		    (bo->flags & GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)) {
+			/* And don't move a CPU_ACCESS_REQUIRED BO to limited
+			 * visible VRAM if we've depleted our allowance to do
+			 * that.
+			 */
+			if (p->bytes_moved_vis < p->bytes_moved_vis_threshold)
+				domain = bo->preferred_domains;
+			else
+				domain = bo->allowed_domains;
+		} else {
+			domain = bo->preferred_domains;
+		}
+	} else {
+		domain = bo->allowed_domains;
+	}
+
+retry:
+	gsgpu_bo_placement_from_domain(bo, domain);
+	r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+
+	p->bytes_moved += ctx.bytes_moved;
+	if (!gsgpu_gmc_vram_full_visible(&adev->gmc) &&
+	    gsgpu_bo_in_cpu_visible_vram(bo))
+		p->bytes_moved_vis += ctx.bytes_moved;
+
+	if (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {
+		domain = bo->allowed_domains;
+		goto retry;
+	}
+
+	return r;
+}
+
+/* Last resort, try to evict something from the current working set */
+static bool gsgpu_cs_try_evict(struct gsgpu_cs_parser *p,
+				struct gsgpu_bo *validated)
+{
+	uint32_t domain = validated->allowed_domains;
+	struct ttm_operation_ctx ctx = { true, false };
+	int r;
+
+	if (!p->evictable)
+		return false;
+
+	for (; &p->evictable->tv.head != &p->validated;
+	     p->evictable = list_prev_entry(p->evictable, tv.head)) {
+
+		struct gsgpu_bo_list_entry *candidate = p->evictable;
+		struct gsgpu_bo *bo = candidate->robj;
+		struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+		bool update_bytes_moved_vis;
+		uint32_t other;
+
+		/* If we reached our current BO we can forget it */
+		if (candidate->robj == validated)
+			break;
+
+		/* We can't move pinned BOs here */
+		if (bo->pin_count)
+			continue;
+
+		other = gsgpu_mem_type_to_domain(bo->tbo.mem.mem_type);
+
+		/* Check if this BO is in one of the domains we need space for */
+		if (!(other & domain))
+			continue;
+
+		/* Check if we can move this BO somewhere else */
+		other = bo->allowed_domains & ~domain;
+		if (!other)
+			continue;
+
+		/* Good we can try to move this BO somewhere else */
+		update_bytes_moved_vis =
+				!gsgpu_gmc_vram_full_visible(&adev->gmc) &&
+				gsgpu_bo_in_cpu_visible_vram(bo);
+		gsgpu_bo_placement_from_domain(bo, other);
+		r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+		p->bytes_moved += ctx.bytes_moved;
+		if (update_bytes_moved_vis)
+			p->bytes_moved_vis += ctx.bytes_moved;
+
+		if (unlikely(r))
+			break;
+
+		p->evictable = list_prev_entry(p->evictable, tv.head);
+		list_move(&candidate->tv.head, &p->validated);
+
+		return true;
+	}
+
+	return false;
+}
+
+static int gsgpu_cs_validate(void *param, struct gsgpu_bo *bo)
+{
+	struct gsgpu_cs_parser *p = param;
+	int r;
+
+	do {
+		r = gsgpu_cs_bo_validate(p, bo);
+	} while (r == -ENOMEM && gsgpu_cs_try_evict(p, bo));
+	if (r)
+		return r;
+
+	if (bo->shadow)
+		r = gsgpu_cs_bo_validate(p, bo->shadow);
+
+	return r;
+}
+
+static int gsgpu_cs_list_validate(struct gsgpu_cs_parser *p,
+			    struct list_head *validated)
+{
+	struct ttm_operation_ctx ctx = { true, false };
+	struct gsgpu_bo_list_entry *lobj;
+	int r;
+
+	list_for_each_entry(lobj, validated, tv.head) {
+		struct gsgpu_bo *bo = lobj->robj;
+		bool binding_userptr = false;
+		struct mm_struct *usermm;
+
+		usermm = gsgpu_ttm_tt_get_usermm(bo->tbo.ttm);
+		if (usermm && usermm != current->mm)
+			return -EPERM;
+
+		/* Check if we have user pages and nobody bound the BO already */
+		if (gsgpu_ttm_tt_userptr_needs_pages(bo->tbo.ttm) &&
+		    lobj->user_pages) {
+			gsgpu_bo_placement_from_domain(bo,
+							GSGPU_GEM_DOMAIN_CPU);
+			r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+			if (r)
+				return r;
+			gsgpu_ttm_tt_set_user_pages(bo->tbo.ttm,
+						     lobj->user_pages);
+			binding_userptr = true;
+		}
+
+		if (p->evictable == lobj)
+			p->evictable = NULL;
+
+		r = gsgpu_cs_validate(p, bo);
+		if (r)
+			return r;
+
+		if (binding_userptr) {
+			kvfree(lobj->user_pages);
+			lobj->user_pages = NULL;
+		}
+	}
+	return 0;
+}
+
+static int gsgpu_cs_parser_bos(struct gsgpu_cs_parser *p,
+				union drm_gsgpu_cs *cs)
+{
+	struct gsgpu_fpriv *fpriv = p->filp->driver_priv;
+	struct gsgpu_vm *vm = &fpriv->vm;
+	struct gsgpu_bo_list_entry *e;
+	struct list_head duplicates;
+	unsigned tries = 10;
+	int r;
+
+	INIT_LIST_HEAD(&p->validated);
+
+	/* p->bo_list could already be assigned if GSGPU_CHUNK_ID_BO_HANDLES is present */
+	if (cs->in.bo_list_handle) {
+		if (p->bo_list)
+			return -EINVAL;
+
+		r = gsgpu_bo_list_get(fpriv, cs->in.bo_list_handle,
+				       &p->bo_list);
+		if (r)
+			return r;
+	} else if (!p->bo_list) {
+		/* Create a empty bo_list when no handle is provided */
+		r = gsgpu_bo_list_create(p->adev, p->filp, NULL, 0,
+					  &p->bo_list);
+		if (r)
+			return r;
+	}
+
+	gsgpu_bo_list_get_list(p->bo_list, &p->validated);
+	if (p->bo_list->first_userptr != p->bo_list->num_entries)
+		p->mn = gsgpu_mn_get(p->adev, GSGPU_MN_TYPE_GFX);
+
+	INIT_LIST_HEAD(&duplicates);
+	gsgpu_vm_get_pd_bo(&fpriv->vm, &p->validated, &p->vm_pd);
+
+	if (p->uf_entry.robj && !p->uf_entry.robj->parent)
+		list_add(&p->uf_entry.tv.head, &p->validated);
+
+	while (1) {
+		struct list_head need_pages;
+
+		r = ttm_eu_reserve_buffers(&p->ticket, &p->validated, true,
+					   &duplicates);
+		if (unlikely(r != 0)) {
+			if (r != -ERESTARTSYS)
+				DRM_ERROR("ttm_eu_reserve_buffers failed.\n");
+			goto error_free_pages;
+		}
+
+		INIT_LIST_HEAD(&need_pages);
+		gsgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {
+			struct gsgpu_bo *bo = e->robj;
+
+			if (gsgpu_ttm_tt_userptr_invalidated(bo->tbo.ttm,
+				 &e->user_invalidated) && e->user_pages) {
+
+				/* We acquired a page array, but somebody
+				 * invalidated it. Free it and try again
+				 */
+				release_pages(e->user_pages,
+					      bo->tbo.ttm->num_pages);
+				kvfree(e->user_pages);
+				e->user_pages = NULL;
+			}
+
+			if (gsgpu_ttm_tt_userptr_needs_pages(bo->tbo.ttm) &&
+			    !e->user_pages) {
+				list_del(&e->tv.head);
+				list_add(&e->tv.head, &need_pages);
+
+				gsgpu_bo_unreserve(e->robj);
+			}
+		}
+
+		if (list_empty(&need_pages))
+			break;
+
+		/* Unreserve everything again. */
+		ttm_eu_backoff_reservation(&p->ticket, &p->validated);
+
+		/* We tried too many times, just abort */
+		if (!--tries) {
+			r = -EDEADLK;
+			DRM_ERROR("deadlock in %s\n", __func__);
+			goto error_free_pages;
+		}
+
+		/* Fill the page arrays for all userptrs. */
+		list_for_each_entry(e, &need_pages, tv.head) {
+			struct ttm_tt *ttm = e->robj->tbo.ttm;
+
+			e->user_pages = kvmalloc_array(ttm->num_pages,
+							 sizeof(struct page *),
+							 GFP_KERNEL | __GFP_ZERO);
+			if (!e->user_pages) {
+				r = -ENOMEM;
+				DRM_ERROR("calloc failure in %s\n", __func__);
+				goto error_free_pages;
+			}
+
+			r = gsgpu_ttm_tt_get_user_pages(ttm, e->user_pages);
+			if (r) {
+				DRM_ERROR("gsgpu_ttm_tt_get_user_pages failed.\n");
+				kvfree(e->user_pages);
+				e->user_pages = NULL;
+				goto error_free_pages;
+			}
+		}
+
+		/* And try again. */
+		list_splice(&need_pages, &p->validated);
+	}
+
+	gsgpu_cs_get_threshold_for_moves(p->adev, &p->bytes_moved_threshold,
+					  &p->bytes_moved_vis_threshold);
+	p->bytes_moved = 0;
+	p->bytes_moved_vis = 0;
+	p->evictable = list_last_entry(&p->validated,
+				       struct gsgpu_bo_list_entry,
+				       tv.head);
+
+	r = gsgpu_vm_validate_pt_bos(p->adev, &fpriv->vm,
+				      gsgpu_cs_validate, p);
+	if (r) {
+		DRM_ERROR("gsgpu_vm_validate_pt_bos() failed.\n");
+		goto error_validate;
+	}
+
+	r = gsgpu_cs_list_validate(p, &duplicates);
+	if (r) {
+		DRM_ERROR("gsgpu_cs_list_validate(duplicates) failed.\n");
+		goto error_validate;
+	}
+
+	r = gsgpu_cs_list_validate(p, &p->validated);
+	if (r) {
+		DRM_ERROR("gsgpu_cs_list_validate(validated) failed.\n");
+		goto error_validate;
+	}
+
+	gsgpu_cs_report_moved_bytes(p->adev, p->bytes_moved,
+				     p->bytes_moved_vis);
+
+	gsgpu_bo_list_for_each_entry(e, p->bo_list)
+		e->bo_va = gsgpu_vm_bo_find(vm, e->robj);
+
+	if (!r && p->uf_entry.robj) {
+		struct gsgpu_bo *uf = p->uf_entry.robj;
+
+		r = gsgpu_ttm_alloc_gart(&uf->tbo);
+		p->job->uf_addr += gsgpu_bo_gpu_offset(uf);
+	}
+
+error_validate:
+	if (r)
+		ttm_eu_backoff_reservation(&p->ticket, &p->validated);
+
+error_free_pages:
+
+	gsgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {
+		if (!e->user_pages)
+			continue;
+
+		release_pages(e->user_pages,
+			      e->robj->tbo.ttm->num_pages);
+		kvfree(e->user_pages);
+	}
+
+	return r;
+}
+
+static int gsgpu_cs_sync_rings(struct gsgpu_cs_parser *p)
+{
+	struct gsgpu_bo_list_entry *e;
+	int r;
+
+	list_for_each_entry(e, &p->validated, tv.head) {
+		struct reservation_object *resv = e->robj->tbo.resv;
+		r = gsgpu_sync_resv(p->adev, &p->job->sync, resv, p->filp,
+				     gsgpu_bo_explicit_sync(e->robj));
+
+		if (r)
+			return r;
+	}
+	return 0;
+}
+
+/**
+ * cs_parser_fini() - clean parser states
+ * @parser:	parser structure holding parsing context.
+ * @error:	error number
+ *
+ * If error is set than unvalidate buffer, otherwise just free memory
+ * used by parsing context.
+ **/
+static void gsgpu_cs_parser_fini(struct gsgpu_cs_parser *parser, int error,
+				  bool backoff)
+{
+	unsigned i;
+
+	if (error && backoff)
+		ttm_eu_backoff_reservation(&parser->ticket,
+					   &parser->validated);
+
+	for (i = 0; i < parser->num_post_dep_syncobjs; i++)
+		drm_syncobj_put(parser->post_dep_syncobjs[i]);
+	kfree(parser->post_dep_syncobjs);
+
+	dma_fence_put(parser->fence);
+
+	if (parser->ctx) {
+		mutex_unlock(&parser->ctx->lock);
+		gsgpu_ctx_put(parser->ctx);
+	}
+	if (parser->bo_list)
+		gsgpu_bo_list_put(parser->bo_list);
+
+	for (i = 0; i < parser->nchunks; i++)
+		kvfree(parser->chunks[i].kdata);
+	kfree(parser->chunks);
+	if (parser->job)
+		gsgpu_job_free(parser->job);
+	gsgpu_bo_unref(&parser->uf_entry.robj);
+}
+
+static int gsgpu_bo_vm_update_pte(struct gsgpu_cs_parser *p)
+{
+	struct gsgpu_fpriv *fpriv = p->filp->driver_priv;
+	struct gsgpu_device *adev = p->adev;
+	struct gsgpu_vm *vm = &fpriv->vm;
+	struct gsgpu_bo_list_entry *e;
+	struct gsgpu_bo_va *bo_va;
+	struct gsgpu_bo *bo;
+	int r;
+
+	r = gsgpu_vm_clear_freed(adev, vm, NULL);
+	if (r)
+		return r;
+
+	r = gsgpu_vm_bo_update(adev, fpriv->prt_va, false);
+	if (r)
+		return r;
+
+	r = gsgpu_sync_fence(adev, &p->job->sync,
+			      fpriv->prt_va->last_pt_update, false);
+	if (r)
+		return r;
+
+	gsgpu_bo_list_for_each_entry(e, p->bo_list) {
+		struct dma_fence *f;
+
+		/* ignore duplicates */
+		bo = e->robj;
+		if (!bo)
+			continue;
+
+		bo_va = e->bo_va;
+		if (bo_va == NULL)
+			continue;
+
+		r = gsgpu_vm_bo_update(adev, bo_va, false);
+		if (r)
+			return r;
+
+		f = bo_va->last_pt_update;
+		r = gsgpu_sync_fence(adev, &p->job->sync, f, false);
+		if (r)
+			return r;
+	}
+
+	r = gsgpu_vm_handle_moved(adev, vm);
+	if (r)
+		return r;
+
+	r = gsgpu_vm_update_directories(adev, vm);
+	if (r)
+		return r;
+
+	r = gsgpu_sync_fence(adev, &p->job->sync, vm->last_update, false);
+	if (r)
+		return r;
+
+	if (gsgpu_vm_debug) {
+		/* Invalidate all BOs to test for userspace bugs */
+		gsgpu_bo_list_for_each_entry(e, p->bo_list) {
+			/* ignore duplicates */
+			if (!e->robj)
+				continue;
+
+			gsgpu_vm_bo_invalidate(adev, e->robj, false);
+		}
+	}
+
+	return r;
+}
+
+static int gsgpu_cs_ib_vm_chunk(struct gsgpu_device *adev,
+				 struct gsgpu_cs_parser *p)
+{
+	struct gsgpu_fpriv *fpriv = p->filp->driver_priv;
+	struct gsgpu_vm *vm = &fpriv->vm;
+	int r;
+
+	if (p->job->vm) {
+		p->job->vm_pd_addr = gsgpu_bo_gpu_offset(vm->root.base.bo);
+
+		r = gsgpu_bo_vm_update_pte(p);
+		if (r)
+			return r;
+
+		r = reservation_object_reserve_shared(vm->root.base.bo->tbo.resv);
+		if (r)
+			return r;
+	}
+
+	return gsgpu_cs_sync_rings(p);
+}
+
+static int gsgpu_cs_ib_fill(struct gsgpu_device *adev,
+			     struct gsgpu_cs_parser *parser)
+{
+	struct gsgpu_fpriv *fpriv = parser->filp->driver_priv;
+	struct gsgpu_vm *vm = &fpriv->vm;
+	int i, j;
+	int r;
+
+	for (i = 0, j = 0; i < parser->nchunks && j < parser->job->num_ibs; i++) {
+		struct gsgpu_cs_chunk *chunk;
+		struct gsgpu_ib *ib;
+		struct drm_gsgpu_cs_chunk_ib *chunk_ib;
+		struct gsgpu_ring *ring;
+
+		chunk = &parser->chunks[i];
+		ib = &parser->job->ibs[j];
+		chunk_ib = (struct drm_gsgpu_cs_chunk_ib *)chunk->kdata;
+
+		if (chunk->chunk_id != GSGPU_CHUNK_ID_IB)
+			continue;
+
+		r = gsgpu_queue_mgr_map(adev, &parser->ctx->queue_mgr, chunk_ib->ip_type,
+					 chunk_ib->ip_instance, chunk_ib->ring, &ring);
+		if (r)
+			return r;
+
+		if (chunk_ib->flags & GSGPU_IB_FLAG_PREAMBLE)
+			parser->job->preamble_status |=
+				GSGPU_PREAMBLE_IB_PRESENT;
+
+		if (parser->ring && parser->ring != ring)
+			return -EINVAL;
+
+		parser->ring = ring;
+
+		r =  gsgpu_ib_get(adev, vm,
+					ring->funcs->parse_cs ? chunk_ib->ib_bytes : 0,
+					ib);
+		if (r) {
+			DRM_ERROR("Failed to get ib !\n");
+			return r;
+		}
+
+		ib->gpu_addr = chunk_ib->va_start;
+		ib->length_dw = chunk_ib->ib_bytes / 4;
+		ib->flags = chunk_ib->flags;
+
+		j++;
+	}
+
+	return gsgpu_ctx_wait_prev_fence(parser->ctx, parser->ring->idx);
+}
+
+static int gsgpu_cs_process_fence_dep(struct gsgpu_cs_parser *p,
+				       struct gsgpu_cs_chunk *chunk)
+{
+	struct gsgpu_fpriv *fpriv = p->filp->driver_priv;
+	unsigned num_deps;
+	int i, r;
+	struct drm_gsgpu_cs_chunk_dep *deps;
+
+	deps = (struct drm_gsgpu_cs_chunk_dep *)chunk->kdata;
+	num_deps = chunk->length_dw * 4 /
+		sizeof(struct drm_gsgpu_cs_chunk_dep);
+
+	for (i = 0; i < num_deps; ++i) {
+		struct gsgpu_ring *ring;
+		struct gsgpu_ctx *ctx;
+		struct dma_fence *fence;
+
+		ctx = gsgpu_ctx_get(fpriv, deps[i].ctx_id);
+		if (ctx == NULL)
+			return -EINVAL;
+
+		r = gsgpu_queue_mgr_map(p->adev, &ctx->queue_mgr,
+					 deps[i].ip_type,
+					 deps[i].ip_instance,
+					 deps[i].ring, &ring);
+		if (r) {
+			gsgpu_ctx_put(ctx);
+			return r;
+		}
+
+		fence = gsgpu_ctx_get_fence(ctx, ring,
+					     deps[i].handle);
+		if (IS_ERR(fence)) {
+			r = PTR_ERR(fence);
+			gsgpu_ctx_put(ctx);
+			return r;
+		} else if (fence) {
+			r = gsgpu_sync_fence(p->adev, &p->job->sync, fence,
+					true);
+			dma_fence_put(fence);
+			gsgpu_ctx_put(ctx);
+			if (r)
+				return r;
+		}
+	}
+	return 0;
+}
+
+static int gsgpu_syncobj_lookup_and_add_to_sync(struct gsgpu_cs_parser *p,
+						 uint32_t handle)
+{
+	int r;
+	struct dma_fence *fence;
+	r = drm_syncobj_find_fence(p->filp, handle, &fence);
+	if (r)
+		return r;
+
+	r = gsgpu_sync_fence(p->adev, &p->job->sync, fence, true);
+	dma_fence_put(fence);
+
+	return r;
+}
+
+static int gsgpu_cs_process_syncobj_in_dep(struct gsgpu_cs_parser *p,
+					    struct gsgpu_cs_chunk *chunk)
+{
+	unsigned num_deps;
+	int i, r;
+	struct drm_gsgpu_cs_chunk_sem *deps;
+
+	deps = (struct drm_gsgpu_cs_chunk_sem *)chunk->kdata;
+	num_deps = chunk->length_dw * 4 /
+		sizeof(struct drm_gsgpu_cs_chunk_sem);
+
+	for (i = 0; i < num_deps; ++i) {
+		r = gsgpu_syncobj_lookup_and_add_to_sync(p, deps[i].handle);
+		if (r)
+			return r;
+	}
+	return 0;
+}
+
+static int gsgpu_cs_process_syncobj_out_dep(struct gsgpu_cs_parser *p,
+					     struct gsgpu_cs_chunk *chunk)
+{
+	unsigned num_deps;
+	int i;
+	struct drm_gsgpu_cs_chunk_sem *deps;
+	deps = (struct drm_gsgpu_cs_chunk_sem *)chunk->kdata;
+	num_deps = chunk->length_dw * 4 /
+		sizeof(struct drm_gsgpu_cs_chunk_sem);
+
+	p->post_dep_syncobjs = kmalloc_array(num_deps,
+					     sizeof(struct drm_syncobj *),
+					     GFP_KERNEL);
+	p->num_post_dep_syncobjs = 0;
+
+	if (!p->post_dep_syncobjs)
+		return -ENOMEM;
+
+	for (i = 0; i < num_deps; ++i) {
+		p->post_dep_syncobjs[i] = drm_syncobj_find(p->filp, deps[i].handle);
+		if (!p->post_dep_syncobjs[i])
+			return -EINVAL;
+		p->num_post_dep_syncobjs++;
+	}
+	return 0;
+}
+
+static int gsgpu_cs_dependencies(struct gsgpu_device *adev,
+				  struct gsgpu_cs_parser *p)
+{
+	int i, r;
+
+	for (i = 0; i < p->nchunks; ++i) {
+		struct gsgpu_cs_chunk *chunk;
+
+		chunk = &p->chunks[i];
+
+		if (chunk->chunk_id == GSGPU_CHUNK_ID_DEPENDENCIES) {
+			r = gsgpu_cs_process_fence_dep(p, chunk);
+			if (r)
+				return r;
+		} else if (chunk->chunk_id == GSGPU_CHUNK_ID_SYNCOBJ_IN) {
+			r = gsgpu_cs_process_syncobj_in_dep(p, chunk);
+			if (r)
+				return r;
+		} else if (chunk->chunk_id == GSGPU_CHUNK_ID_SYNCOBJ_OUT) {
+			r = gsgpu_cs_process_syncobj_out_dep(p, chunk);
+			if (r)
+				return r;
+		}
+	}
+
+	return 0;
+}
+
+static void gsgpu_cs_post_dependencies(struct gsgpu_cs_parser *p)
+{
+	int i;
+
+	for (i = 0; i < p->num_post_dep_syncobjs; ++i)
+		drm_syncobj_replace_fence(p->post_dep_syncobjs[i], p->fence);
+}
+
+static int gsgpu_cs_submit(struct gsgpu_cs_parser *p,
+			    union drm_gsgpu_cs *cs)
+{
+	struct gsgpu_fpriv *fpriv = p->filp->driver_priv;
+	struct gsgpu_ring *ring = p->ring;
+	struct drm_sched_entity *entity = &p->ctx->rings[ring->idx].entity;
+	enum drm_sched_priority priority;
+	struct gsgpu_bo_list_entry *e;
+	struct gsgpu_job *job;
+	uint64_t seq;
+
+	int r;
+
+	job = p->job;
+	p->job = NULL;
+
+	r = drm_sched_job_init(&job->base, entity, p->filp);
+	if (r)
+		goto error_unlock;
+
+	/* No memory allocation is allowed while holding the mn lock */
+	gsgpu_mn_lock(p->mn);
+	gsgpu_bo_list_for_each_userptr_entry(e, p->bo_list) {
+		struct gsgpu_bo *bo = e->robj;
+
+		if (gsgpu_ttm_tt_userptr_needs_pages(bo->tbo.ttm)) {
+			r = -ERESTARTSYS;
+			goto error_abort;
+		}
+	}
+
+	job->owner = p->filp;
+	p->fence = dma_fence_get(&job->base.s_fence->finished);
+
+	r = gsgpu_ctx_add_fence(p->ctx, ring, p->fence, &seq);
+	if (r) {
+		dma_fence_put(p->fence);
+		dma_fence_put(&job->base.s_fence->finished);
+		gsgpu_job_free(job);
+		gsgpu_mn_unlock(p->mn);
+		return r;
+	}
+
+	gsgpu_cs_post_dependencies(p);
+
+	if ((job->preamble_status & GSGPU_PREAMBLE_IB_PRESENT) &&
+	    !p->ctx->preamble_presented) {
+		job->preamble_status |= GSGPU_PREAMBLE_IB_PRESENT_FIRST;
+		p->ctx->preamble_presented = true;
+	}
+
+	cs->out.handle = seq;
+	job->uf_sequence = seq;
+
+	gsgpu_job_free_resources(job);
+
+	trace_gsgpu_cs_ioctl(job);
+	gsgpu_vm_bo_trace_cs(&fpriv->vm, &p->ticket);
+	priority = job->base.s_priority;
+	drm_sched_entity_push_job(&job->base, entity);
+
+	ring = to_gsgpu_ring(entity->rq->sched);
+	gsgpu_ring_priority_get(ring, priority);
+
+	ttm_eu_fence_buffer_objects(&p->ticket, &p->validated, p->fence);
+	gsgpu_mn_unlock(p->mn);
+
+	return 0;
+
+error_abort:
+	dma_fence_put(&job->base.s_fence->finished);
+	job->base.s_fence = NULL;
+	gsgpu_mn_unlock(p->mn);
+
+error_unlock:
+	gsgpu_job_free(job);
+	return r;
+}
+
+int gsgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	union drm_gsgpu_cs *cs = data;
+	struct gsgpu_cs_parser parser = {};
+	bool reserved_buffers = false;
+	int i, r;
+
+	if (!adev->accel_working)
+		return -EBUSY;
+
+	parser.adev = adev;
+	parser.filp = filp;
+
+	r = gsgpu_cs_parser_init(&parser, data);
+	if (r) {
+		DRM_ERROR("Failed to initialize parser !\n");
+		goto out;
+	}
+
+	r = gsgpu_cs_ib_fill(adev, &parser);
+	if (r)
+		goto out;
+
+	r = gsgpu_cs_parser_bos(&parser, data);
+	if (r) {
+		if (r == -ENOMEM)
+			DRM_ERROR("Not enough memory for command submission!\n");
+		else if (r != -ERESTARTSYS)
+			DRM_ERROR("Failed to process the buffer list %d!\n", r);
+		goto out;
+	}
+
+	reserved_buffers = true;
+
+	r = gsgpu_cs_dependencies(adev, &parser);
+	if (r) {
+		DRM_ERROR("Failed in the dependencies handling %d!\n", r);
+		goto out;
+	}
+
+	for (i = 0; i < parser.job->num_ibs; i++)
+		trace_gsgpu_cs(&parser, i);
+
+	r = gsgpu_cs_ib_vm_chunk(adev, &parser);
+	if (r)
+		goto out;
+
+	r = gsgpu_cs_submit(&parser, cs);
+
+out:
+	gsgpu_cs_parser_fini(&parser, r, reserved_buffers);
+	return r;
+}
+
+/**
+ * gsgpu_cs_wait_ioctl - wait for a command submission to finish
+ *
+ * @dev: drm device
+ * @data: data from userspace
+ * @filp: file private
+ *
+ * Wait for the command submission identified by handle to finish.
+ */
+int gsgpu_cs_wait_ioctl(struct drm_device *dev, void *data,
+			 struct drm_file *filp)
+{
+	union drm_gsgpu_wait_cs *wait = data;
+	struct gsgpu_device *adev = dev->dev_private;
+	unsigned long timeout = gsgpu_gem_timeout(wait->in.timeout);
+	struct gsgpu_ring *ring = NULL;
+	struct gsgpu_ctx *ctx;
+	struct dma_fence *fence;
+	long r;
+
+	ctx = gsgpu_ctx_get(filp->driver_priv, wait->in.ctx_id);
+	if (ctx == NULL)
+		return -EINVAL;
+
+	r = gsgpu_queue_mgr_map(adev, &ctx->queue_mgr,
+				 wait->in.ip_type, wait->in.ip_instance,
+				 wait->in.ring, &ring);
+	if (r) {
+		gsgpu_ctx_put(ctx);
+		return r;
+	}
+
+	fence = gsgpu_ctx_get_fence(ctx, ring, wait->in.handle);
+	if (IS_ERR(fence))
+		r = PTR_ERR(fence);
+	else if (fence) {
+		r = dma_fence_wait_timeout(fence, true, timeout);
+		if (r > 0 && fence->error)
+			r = fence->error;
+		dma_fence_put(fence);
+	} else
+		r = 1;
+
+	gsgpu_ctx_put(ctx);
+	if (r < 0)
+		return r;
+
+	memset(wait, 0, sizeof(*wait));
+	wait->out.status = (r == 0);
+
+	return 0;
+}
+
+/**
+ * gsgpu_cs_get_fence - helper to get fence from drm_gsgpu_fence
+ *
+ * @adev: gsgpu device
+ * @filp: file private
+ * @user: drm_gsgpu_fence copied from user space
+ */
+static struct dma_fence *gsgpu_cs_get_fence(struct gsgpu_device *adev,
+					     struct drm_file *filp,
+					     struct drm_gsgpu_fence *user)
+{
+	struct gsgpu_ring *ring;
+	struct gsgpu_ctx *ctx;
+	struct dma_fence *fence;
+	int r;
+
+	ctx = gsgpu_ctx_get(filp->driver_priv, user->ctx_id);
+	if (ctx == NULL)
+		return ERR_PTR(-EINVAL);
+
+	r = gsgpu_queue_mgr_map(adev, &ctx->queue_mgr, user->ip_type,
+				 user->ip_instance, user->ring, &ring);
+	if (r) {
+		gsgpu_ctx_put(ctx);
+		return ERR_PTR(r);
+	}
+
+	fence = gsgpu_ctx_get_fence(ctx, ring, user->seq_no);
+	gsgpu_ctx_put(ctx);
+
+	return fence;
+}
+
+int gsgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,
+				    struct drm_file *filp)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	union drm_gsgpu_fence_to_handle *info = data;
+	struct dma_fence *fence;
+	struct drm_syncobj *syncobj;
+	struct sync_file *sync_file;
+	int fd, r;
+
+	fence = gsgpu_cs_get_fence(adev, filp, &info->in.fence);
+	if (IS_ERR(fence))
+		return PTR_ERR(fence);
+
+	switch (info->in.what) {
+	case GSGPU_FENCE_TO_HANDLE_GET_SYNCOBJ:
+		r = drm_syncobj_create(&syncobj, 0, fence);
+		dma_fence_put(fence);
+		if (r)
+			return r;
+		r = drm_syncobj_get_handle(filp, syncobj, &info->out.handle);
+		drm_syncobj_put(syncobj);
+		return r;
+
+	case GSGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD:
+		r = drm_syncobj_create(&syncobj, 0, fence);
+		dma_fence_put(fence);
+		if (r)
+			return r;
+		r = drm_syncobj_get_fd(syncobj, (int *)&info->out.handle);
+		drm_syncobj_put(syncobj);
+		return r;
+
+	case GSGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD:
+		fd = get_unused_fd_flags(O_CLOEXEC);
+		if (fd < 0) {
+			dma_fence_put(fence);
+			return fd;
+		}
+
+		sync_file = sync_file_create(fence);
+		dma_fence_put(fence);
+		if (!sync_file) {
+			put_unused_fd(fd);
+			return -ENOMEM;
+		}
+
+		fd_install(fd, sync_file->file);
+		info->out.handle = fd;
+		return 0;
+
+	default:
+		return -EINVAL;
+	}
+}
+
+/**
+ * gsgpu_cs_wait_all_fence - wait on all fences to signal
+ *
+ * @adev: gsgpu device
+ * @filp: file private
+ * @wait: wait parameters
+ * @fences: array of drm_gsgpu_fence
+ */
+static int gsgpu_cs_wait_all_fences(struct gsgpu_device *adev,
+				     struct drm_file *filp,
+				     union drm_gsgpu_wait_fences *wait,
+				     struct drm_gsgpu_fence *fences)
+{
+	uint32_t fence_count = wait->in.fence_count;
+	unsigned int i;
+	long r = 1;
+
+	for (i = 0; i < fence_count; i++) {
+		struct dma_fence *fence;
+		unsigned long timeout = gsgpu_gem_timeout(wait->in.timeout_ns);
+
+		fence = gsgpu_cs_get_fence(adev, filp, &fences[i]);
+		if (IS_ERR(fence))
+			return PTR_ERR(fence);
+		else if (!fence)
+			continue;
+
+		r = dma_fence_wait_timeout(fence, true, timeout);
+		dma_fence_put(fence);
+		if (r < 0)
+			return r;
+
+		if (r == 0)
+			break;
+
+		if (fence->error)
+			return fence->error;
+	}
+
+	memset(wait, 0, sizeof(*wait));
+	wait->out.status = (r > 0);
+
+	return 0;
+}
+
+/**
+ * gsgpu_cs_wait_any_fence - wait on any fence to signal
+ *
+ * @adev: gsgpu device
+ * @filp: file private
+ * @wait: wait parameters
+ * @fences: array of drm_gsgpu_fence
+ */
+static int gsgpu_cs_wait_any_fence(struct gsgpu_device *adev,
+				    struct drm_file *filp,
+				    union drm_gsgpu_wait_fences *wait,
+				    struct drm_gsgpu_fence *fences)
+{
+	unsigned long timeout = gsgpu_gem_timeout(wait->in.timeout_ns);
+	uint32_t fence_count = wait->in.fence_count;
+	uint32_t first = ~0;
+	struct dma_fence **array;
+	unsigned int i;
+	long r;
+
+	/* Prepare the fence array */
+	array = kcalloc(fence_count, sizeof(struct dma_fence *), GFP_KERNEL);
+
+	if (array == NULL)
+		return -ENOMEM;
+
+	for (i = 0; i < fence_count; i++) {
+		struct dma_fence *fence;
+
+		fence = gsgpu_cs_get_fence(adev, filp, &fences[i]);
+		if (IS_ERR(fence)) {
+			r = PTR_ERR(fence);
+			goto err_free_fence_array;
+		} else if (fence) {
+			array[i] = fence;
+		} else { /* NULL, the fence has been already signaled */
+			r = 1;
+			first = i;
+			goto out;
+		}
+	}
+
+	r = dma_fence_wait_any_timeout(array, fence_count, true, timeout,
+				       &first);
+	if (r < 0)
+		goto err_free_fence_array;
+
+out:
+	memset(wait, 0, sizeof(*wait));
+	wait->out.status = (r > 0);
+	wait->out.first_signaled = first;
+
+	if (first < fence_count && array[first])
+		r = array[first]->error;
+	else
+		r = 0;
+
+err_free_fence_array:
+	for (i = 0; i < fence_count; i++)
+		dma_fence_put(array[i]);
+	kfree(array);
+
+	return r;
+}
+
+/**
+ * gsgpu_cs_wait_fences_ioctl - wait for multiple command submissions to finish
+ *
+ * @dev: drm device
+ * @data: data from userspace
+ * @filp: file private
+ */
+int gsgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *filp)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	union drm_gsgpu_wait_fences *wait = data;
+	uint32_t fence_count = wait->in.fence_count;
+	struct drm_gsgpu_fence *fences_user;
+	struct drm_gsgpu_fence *fences;
+	int r;
+
+	/* Get the fences from userspace */
+	fences = kmalloc_array(fence_count, sizeof(struct drm_gsgpu_fence),
+			GFP_KERNEL);
+	if (fences == NULL)
+		return -ENOMEM;
+
+	fences_user = u64_to_user_ptr(wait->in.fences);
+	if (copy_from_user(fences, fences_user,
+		sizeof(struct drm_gsgpu_fence) * fence_count)) {
+		r = -EFAULT;
+		goto err_free_fences;
+	}
+
+	if (wait->in.wait_all)
+		r = gsgpu_cs_wait_all_fences(adev, filp, wait, fences);
+	else
+		r = gsgpu_cs_wait_any_fence(adev, filp, wait, fences);
+
+err_free_fences:
+	kfree(fences);
+
+	return r;
+}
+
+/**
+ * gsgpu_cs_find_bo_va - find bo_va for VM address
+ *
+ * @parser: command submission parser context
+ * @addr: VM address
+ * @bo: resulting BO of the mapping found
+ *
+ * Search the buffer objects in the command submission context for a certain
+ * virtual memory address. Returns allocation structure when found, NULL
+ * otherwise.
+ */
+int gsgpu_cs_find_mapping(struct gsgpu_cs_parser *parser,
+			   uint64_t addr, struct gsgpu_bo **bo,
+			   struct gsgpu_bo_va_mapping **map)
+{
+	struct gsgpu_fpriv *fpriv = parser->filp->driver_priv;
+	struct ttm_operation_ctx ctx = { false, false };
+	struct gsgpu_vm *vm = &fpriv->vm;
+	struct gsgpu_bo_va_mapping *mapping;
+	int r;
+
+	addr /= GSGPU_GPU_PAGE_SIZE;
+
+	mapping = gsgpu_vm_bo_lookup_mapping(vm, addr);
+	if (!mapping || !mapping->bo_va || !mapping->bo_va->base.bo)
+		return -EINVAL;
+
+	*bo = mapping->bo_va->base.bo;
+	*map = mapping;
+
+	/* Double check that the BO is reserved by this CS */
+	if (READ_ONCE((*bo)->tbo.resv->lock.ctx) != &parser->ticket)
+		return -EINVAL;
+
+	if (!((*bo)->flags & GSGPU_GEM_CREATE_VRAM_CONTIGUOUS)) {
+		(*bo)->flags |= GSGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+		gsgpu_bo_placement_from_domain(*bo, (*bo)->allowed_domains);
+		r = ttm_bo_validate(&(*bo)->tbo, &(*bo)->placement, &ctx);
+		if (r)
+			return r;
+	}
+
+	return gsgpu_ttm_alloc_gart(&(*bo)->tbo);
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ctx.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ctx.c
new file mode 100644
index 000000000000..03a8ca5821d5
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ctx.c
@@ -0,0 +1,473 @@
+#include <drm/drmP.h>
+#include <drm/drm_auth.h>
+#include "gsgpu.h"
+#include "gsgpu_sched.h"
+
+static int gsgpu_ctx_priority_permit(struct drm_file *filp,
+				      enum drm_sched_priority priority)
+{
+	/* NORMAL and below are accessible by everyone */
+	if (priority <= DRM_SCHED_PRIORITY_NORMAL)
+		return 0;
+
+	if (capable(CAP_SYS_NICE))
+		return 0;
+
+	if (drm_is_current_master(filp))
+		return 0;
+
+	return -EACCES;
+}
+
+static int gsgpu_ctx_init(struct gsgpu_device *adev,
+			   enum drm_sched_priority priority,
+			   struct drm_file *filp,
+			   struct gsgpu_ctx *ctx)
+{
+	unsigned i, j;
+	int r;
+
+	if (priority < 0 || priority >= DRM_SCHED_PRIORITY_MAX)
+		return -EINVAL;
+
+	r = gsgpu_ctx_priority_permit(filp, priority);
+	if (r)
+		return r;
+
+	memset(ctx, 0, sizeof(*ctx));
+	ctx->adev = adev;
+	kref_init(&ctx->refcount);
+	spin_lock_init(&ctx->ring_lock);
+	ctx->fences = kcalloc(gsgpu_sched_jobs * GSGPU_MAX_RINGS,
+			      sizeof(struct dma_fence *), GFP_KERNEL);
+	if (!ctx->fences)
+		return -ENOMEM;
+
+	mutex_init(&ctx->lock);
+
+	for (i = 0; i < GSGPU_MAX_RINGS; ++i) {
+		ctx->rings[i].sequence = 1;
+		ctx->rings[i].fences = &ctx->fences[gsgpu_sched_jobs * i];
+	}
+
+	ctx->reset_counter = atomic_read(&adev->gpu_reset_counter);
+	ctx->reset_counter_query = ctx->reset_counter;
+	ctx->vram_lost_counter = atomic_read(&adev->vram_lost_counter);
+	ctx->init_priority = priority;
+	ctx->override_priority = DRM_SCHED_PRIORITY_UNSET;
+
+	/* create context entity for each ring */
+	for (i = 0; i < adev->num_rings; i++) {
+		struct gsgpu_ring *ring = adev->rings[i];
+		struct drm_sched_rq *rq;
+
+		rq = &ring->sched.sched_rq[priority];
+
+		r = drm_sched_entity_init(&ctx->rings[i].entity,
+					  &rq, 1, &ctx->guilty);
+		if (r)
+			goto failed;
+	}
+
+	r = gsgpu_queue_mgr_init(adev, &ctx->queue_mgr);
+	if (r)
+		goto failed;
+
+	return 0;
+
+failed:
+	for (j = 0; j < i; j++)
+		drm_sched_entity_destroy(&ctx->rings[j].entity);
+	kfree(ctx->fences);
+	ctx->fences = NULL;
+	return r;
+}
+
+static void gsgpu_ctx_fini(struct kref *ref)
+{
+	struct gsgpu_ctx *ctx = container_of(ref, struct gsgpu_ctx, refcount);
+	struct gsgpu_device *adev = ctx->adev;
+	unsigned i, j;
+
+	if (!adev)
+		return;
+
+	for (i = 0; i < GSGPU_MAX_RINGS; ++i)
+		for (j = 0; j < gsgpu_sched_jobs; ++j)
+			dma_fence_put(ctx->rings[i].fences[j]);
+	kfree(ctx->fences);
+	ctx->fences = NULL;
+
+	gsgpu_queue_mgr_fini(adev, &ctx->queue_mgr);
+
+	mutex_destroy(&ctx->lock);
+
+	kfree(ctx);
+}
+
+static int gsgpu_ctx_alloc(struct gsgpu_device *adev,
+			    struct gsgpu_fpriv *fpriv,
+			    struct drm_file *filp,
+			    enum drm_sched_priority priority,
+			    uint32_t *id)
+{
+	struct gsgpu_ctx_mgr *mgr = &fpriv->ctx_mgr;
+	struct gsgpu_ctx *ctx;
+	int r;
+
+	ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+
+	mutex_lock(&mgr->lock);
+	r = idr_alloc(&mgr->ctx_handles, ctx, 1, 0, GFP_KERNEL);
+	if (r < 0) {
+		mutex_unlock(&mgr->lock);
+		kfree(ctx);
+		return r;
+	}
+
+	*id = (uint32_t)r;
+	r = gsgpu_ctx_init(adev, priority, filp, ctx);
+	if (r) {
+		idr_remove(&mgr->ctx_handles, *id);
+		*id = 0;
+		kfree(ctx);
+	}
+	mutex_unlock(&mgr->lock);
+	return r;
+}
+
+static void gsgpu_ctx_do_release(struct kref *ref)
+{
+	struct gsgpu_ctx *ctx;
+	u32 i;
+
+	ctx = container_of(ref, struct gsgpu_ctx, refcount);
+
+	for (i = 0; i < ctx->adev->num_rings; i++) {
+		drm_sched_entity_destroy(&ctx->rings[i].entity);
+	}
+
+	gsgpu_ctx_fini(ref);
+}
+
+static int gsgpu_ctx_free(struct gsgpu_fpriv *fpriv, uint32_t id)
+{
+	struct gsgpu_ctx_mgr *mgr = &fpriv->ctx_mgr;
+	struct gsgpu_ctx *ctx;
+
+	mutex_lock(&mgr->lock);
+	ctx = idr_remove(&mgr->ctx_handles, id);
+	if (ctx)
+		kref_put(&ctx->refcount, gsgpu_ctx_do_release);
+	mutex_unlock(&mgr->lock);
+	return ctx ? 0 : -EINVAL;
+}
+
+static int gsgpu_ctx_query(struct gsgpu_device *adev,
+			    struct gsgpu_fpriv *fpriv, uint32_t id,
+			    union drm_gsgpu_ctx_out *out)
+{
+	struct gsgpu_ctx *ctx;
+	struct gsgpu_ctx_mgr *mgr;
+	unsigned reset_counter;
+
+	if (!fpriv)
+		return -EINVAL;
+
+	mgr = &fpriv->ctx_mgr;
+	mutex_lock(&mgr->lock);
+	ctx = idr_find(&mgr->ctx_handles, id);
+	if (!ctx) {
+		mutex_unlock(&mgr->lock);
+		return -EINVAL;
+	}
+
+	/* TODO: these two are always zero */
+	out->state.flags = 0x0;
+	out->state.hangs = 0x0;
+
+	/* determine if a GPU reset has occured since the last call */
+	reset_counter = atomic_read(&adev->gpu_reset_counter);
+	/* TODO: this should ideally return NO, GUILTY, or INNOCENT. */
+	if (ctx->reset_counter_query == reset_counter)
+		out->state.reset_status = GSGPU_CTX_NO_RESET;
+	else
+		out->state.reset_status = GSGPU_CTX_UNKNOWN_RESET;
+	ctx->reset_counter_query = reset_counter;
+
+	mutex_unlock(&mgr->lock);
+	return 0;
+}
+
+static int gsgpu_ctx_query2(struct gsgpu_device *adev,
+	struct gsgpu_fpriv *fpriv, uint32_t id,
+	union drm_gsgpu_ctx_out *out)
+{
+	struct gsgpu_ctx *ctx;
+	struct gsgpu_ctx_mgr *mgr;
+
+	if (!fpriv)
+		return -EINVAL;
+
+	mgr = &fpriv->ctx_mgr;
+	mutex_lock(&mgr->lock);
+	ctx = idr_find(&mgr->ctx_handles, id);
+	if (!ctx) {
+		mutex_unlock(&mgr->lock);
+		return -EINVAL;
+	}
+
+	out->state.flags = 0x0;
+	out->state.hangs = 0x0;
+
+	if (ctx->reset_counter != atomic_read(&adev->gpu_reset_counter))
+		out->state.flags |= GSGPU_CTX_QUERY2_FLAGS_RESET;
+
+	if (ctx->vram_lost_counter != atomic_read(&adev->vram_lost_counter))
+		out->state.flags |= GSGPU_CTX_QUERY2_FLAGS_VRAMLOST;
+
+	if (atomic_read(&ctx->guilty))
+		out->state.flags |= GSGPU_CTX_QUERY2_FLAGS_GUILTY;
+
+	mutex_unlock(&mgr->lock);
+	return 0;
+}
+
+int gsgpu_ctx_ioctl(struct drm_device *dev, void *data,
+		     struct drm_file *filp)
+{
+	int r;
+	uint32_t id;
+	enum drm_sched_priority priority;
+
+	union drm_gsgpu_ctx *args = data;
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_fpriv *fpriv = filp->driver_priv;
+
+	r = 0;
+	id = args->in.ctx_id;
+	priority = gsgpu_to_sched_priority(args->in.priority);
+
+	/* For backwards compatibility reasons, we need to accept
+	 * ioctls with garbage in the priority field */
+	if (priority == DRM_SCHED_PRIORITY_INVALID)
+		priority = DRM_SCHED_PRIORITY_NORMAL;
+
+	switch (args->in.op) {
+	case GSGPU_CTX_OP_ALLOC_CTX:
+		r = gsgpu_ctx_alloc(adev, fpriv, filp, priority, &id);
+		args->out.alloc.ctx_id = id;
+		break;
+	case GSGPU_CTX_OP_FREE_CTX:
+		r = gsgpu_ctx_free(fpriv, id);
+		break;
+	case GSGPU_CTX_OP_QUERY_STATE:
+		r = gsgpu_ctx_query(adev, fpriv, id, &args->out);
+		break;
+	case GSGPU_CTX_OP_QUERY_STATE2:
+		r = gsgpu_ctx_query2(adev, fpriv, id, &args->out);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return r;
+}
+
+struct gsgpu_ctx *gsgpu_ctx_get(struct gsgpu_fpriv *fpriv, uint32_t id)
+{
+	struct gsgpu_ctx *ctx;
+	struct gsgpu_ctx_mgr *mgr;
+
+	if (!fpriv)
+		return NULL;
+
+	mgr = &fpriv->ctx_mgr;
+
+	mutex_lock(&mgr->lock);
+	ctx = idr_find(&mgr->ctx_handles, id);
+	if (ctx)
+		kref_get(&ctx->refcount);
+	mutex_unlock(&mgr->lock);
+	return ctx;
+}
+
+int gsgpu_ctx_put(struct gsgpu_ctx *ctx)
+{
+	if (ctx == NULL)
+		return -EINVAL;
+
+	kref_put(&ctx->refcount, gsgpu_ctx_do_release);
+	return 0;
+}
+
+int gsgpu_ctx_add_fence(struct gsgpu_ctx *ctx, struct gsgpu_ring *ring,
+			      struct dma_fence *fence, uint64_t *handler)
+{
+	struct gsgpu_ctx_ring *cring = &ctx->rings[ring->idx];
+	uint64_t seq = cring->sequence;
+	unsigned idx = 0;
+	struct dma_fence *other = NULL;
+
+	idx = seq & (gsgpu_sched_jobs - 1);
+	other = cring->fences[idx];
+	if (other)
+		BUG_ON(!dma_fence_is_signaled(other));
+
+	dma_fence_get(fence);
+
+	spin_lock(&ctx->ring_lock);
+	cring->fences[idx] = fence;
+	cring->sequence++;
+	spin_unlock(&ctx->ring_lock);
+
+	dma_fence_put(other);
+	if (handler)
+		*handler = seq;
+
+	return 0;
+}
+
+struct dma_fence *gsgpu_ctx_get_fence(struct gsgpu_ctx *ctx,
+				       struct gsgpu_ring *ring, uint64_t seq)
+{
+	struct gsgpu_ctx_ring *cring = &ctx->rings[ring->idx];
+	struct dma_fence *fence;
+
+	spin_lock(&ctx->ring_lock);
+
+	if (seq == ~0ull)
+		seq = ctx->rings[ring->idx].sequence - 1;
+
+	if (seq >= cring->sequence) {
+		spin_unlock(&ctx->ring_lock);
+		return ERR_PTR(-EINVAL);
+	}
+
+
+	if (seq + gsgpu_sched_jobs < cring->sequence) {
+		spin_unlock(&ctx->ring_lock);
+		return NULL;
+	}
+
+	fence = dma_fence_get(cring->fences[seq & (gsgpu_sched_jobs - 1)]);
+	spin_unlock(&ctx->ring_lock);
+
+	return fence;
+}
+
+void gsgpu_ctx_priority_override(struct gsgpu_ctx *ctx,
+				  enum drm_sched_priority priority)
+{
+	int i;
+	struct gsgpu_device *adev = ctx->adev;
+	struct drm_sched_entity *entity;
+	struct gsgpu_ring *ring;
+	enum drm_sched_priority ctx_prio;
+
+	ctx->override_priority = priority;
+
+	ctx_prio = (ctx->override_priority == DRM_SCHED_PRIORITY_UNSET) ?
+			ctx->init_priority : ctx->override_priority;
+
+	for (i = 0; i < adev->num_rings; i++) {
+		ring = adev->rings[i];
+		entity = &ctx->rings[i].entity;
+
+		drm_sched_entity_set_priority(entity, ctx_prio);
+	}
+}
+
+int gsgpu_ctx_wait_prev_fence(struct gsgpu_ctx *ctx, unsigned ring_id)
+{
+	struct gsgpu_ctx_ring *cring = &ctx->rings[ring_id];
+	unsigned idx = cring->sequence & (gsgpu_sched_jobs - 1);
+	struct dma_fence *other = cring->fences[idx];
+
+	if (other) {
+		signed long r;
+		r = dma_fence_wait(other, true);
+		if (r < 0) {
+			if (r != -ERESTARTSYS)
+				DRM_ERROR("Error (%ld) waiting for fence!\n", r);
+
+			return r;
+		}
+	}
+
+	return 0;
+}
+
+void gsgpu_ctx_mgr_init(struct gsgpu_ctx_mgr *mgr)
+{
+	mutex_init(&mgr->lock);
+	idr_init(&mgr->ctx_handles);
+}
+
+void gsgpu_ctx_mgr_entity_flush(struct gsgpu_ctx_mgr *mgr)
+{
+	struct gsgpu_ctx *ctx;
+	struct idr *idp;
+	uint32_t id, i;
+	long max_wait = MAX_WAIT_SCHED_ENTITY_Q_EMPTY;
+
+	idp = &mgr->ctx_handles;
+
+	mutex_lock(&mgr->lock);
+	idr_for_each_entry(idp, ctx, id) {
+
+		if (!ctx->adev) {
+			mutex_unlock(&mgr->lock);
+			return;
+		}
+
+		for (i = 0; i < ctx->adev->num_rings; i++) {
+			max_wait = drm_sched_entity_flush(&ctx->rings[i].entity,
+							  max_wait);
+		}
+	}
+	mutex_unlock(&mgr->lock);
+}
+
+void gsgpu_ctx_mgr_entity_fini(struct gsgpu_ctx_mgr *mgr)
+{
+	struct gsgpu_ctx *ctx;
+	struct idr *idp;
+	uint32_t id, i;
+
+	idp = &mgr->ctx_handles;
+
+	idr_for_each_entry(idp, ctx, id) {
+
+		if (!ctx->adev)
+			return;
+
+		for (i = 0; i < ctx->adev->num_rings; i++) {
+			if (kref_read(&ctx->refcount) == 1)
+				drm_sched_entity_fini(&ctx->rings[i].entity);
+			else
+				DRM_ERROR("ctx %p is still alive\n", ctx);
+		}
+	}
+}
+
+void gsgpu_ctx_mgr_fini(struct gsgpu_ctx_mgr *mgr)
+{
+	struct gsgpu_ctx *ctx;
+	struct idr *idp;
+	uint32_t id;
+
+	gsgpu_ctx_mgr_entity_fini(mgr);
+
+	idp = &mgr->ctx_handles;
+
+	idr_for_each_entry(idp, ctx, id) {
+		if (kref_put(&ctx->refcount, gsgpu_ctx_fini) != 1)
+			DRM_ERROR("ctx %p is still alive\n", ctx);
+	}
+
+	idr_destroy(&mgr->ctx_handles);
+	mutex_destroy(&mgr->lock);
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_connector.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_connector.c
new file mode 100644
index 000000000000..16856cc1b27b
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_connector.c
@@ -0,0 +1,327 @@
+#include <drm/drm_atomic_helper.h>
+#include <drm/drm_crtc_helper.h>
+
+#include "gsgpu_dc_connector.h"
+#include "gsgpu_dc_crtc.h"
+#include "gsgpu_dc_encoder.h"
+#include "gsgpu_dc_irq.h"
+#include "gsgpu_dc_i2c.h"
+#include "gsgpu_dc_vbios.h"
+#include "gsgpu_backlight.h"
+
+static struct drm_encoder *best_encoder(struct drm_connector *connector)
+{
+	int enc_id = connector->encoder_ids[0];
+	struct drm_mode_object *obj;
+	struct drm_encoder *encoder;
+
+	DRM_DEBUG_DRIVER("Finding the best encoder\n");
+
+	/* pick the encoder ids */
+	if (enc_id) {
+		obj = drm_mode_object_find(connector->dev, NULL, enc_id, DRM_MODE_OBJECT_ENCODER);
+		if (!obj) {
+			DRM_ERROR("Couldn't find a matching encoder for our connector\n");
+			return NULL;
+		}
+		encoder = obj_to_encoder(obj);
+		return encoder;
+	}
+
+	DRM_ERROR("No encoder id\n");
+
+	return NULL;
+}
+
+static int gsgpu_dc_connector_get_modes(struct drm_connector *connector)
+{
+	struct drm_device *dev = connector->dev;
+	struct gsgpu_device *lgdev = dev->dev_private;
+	struct gsgpu_dc_i2c *i2c = lgdev->i2c[connector->index];
+	struct edid *edid = NULL;
+	int ret;
+
+	edid = drm_get_edid(connector, &i2c->adapter);
+
+	if (edid) {
+		INIT_LIST_HEAD(&connector->probed_modes); /*empty probed_modes*/
+		drm_connector_update_edid_property(connector, edid);
+		ret = drm_add_edid_modes(connector, edid);
+		kfree(edid);
+	} else {
+		ret = drm_add_modes_noedid(connector, 1920, 1080);
+		drm_set_preferred_mode(connector, 1024, 768);
+	}
+
+	return ret;
+}
+
+static const struct drm_connector_helper_funcs dc_connector_helper_funcs = {
+	.get_modes = gsgpu_dc_connector_get_modes,
+	.best_encoder = best_encoder
+};
+
+static bool is_connected(struct drm_connector *connector)
+{
+	struct gsgpu_device *adev = connector->dev->dev_private;
+	struct gsgpu_dc_i2c *i2c = adev->i2c[connector->index];
+	unsigned char start = 0x0;
+	struct i2c_adapter *adapter;
+	struct i2c_msg msgs = {
+		.addr = DDC_ADDR,
+		.flags = I2C_M_RD,
+		.len = 1,
+		.buf = &start,
+	};
+
+	if (!i2c)
+		return false;
+
+	adapter = &i2c->adapter;
+	if (i2c_transfer(adapter, &msgs, 1) != 1) {
+		DRM_DEBUG_KMS("display-%d not connect\n", connector->index);
+		return false;
+	}
+
+	return true;
+}
+
+static enum drm_connector_status
+gsgpu_2k2000_detect(struct drm_connector *connector)
+{
+	struct gsgpu_device *adev = connector->dev->dev_private;
+	enum drm_connector_status status = connector_status_disconnected;
+	u32 reg_val = dc_readl(adev, DC_HDMI_HOTPLUG_STATUS);
+
+	switch (connector->index) {
+	case 0:
+		if (reg_val & 0x1)
+			status = connector_status_connected;
+		break;
+	case 1:
+		if (is_connected(connector))
+			status = connector_status_connected;
+		else
+			status = connector_status_disconnected;
+		break;
+	}
+
+	return status;
+}
+
+static enum drm_connector_status
+gsgpu_7a2000_detect(struct drm_connector *connector)
+{
+	struct gsgpu_device *adev = connector->dev->dev_private;
+	enum drm_connector_status status = connector_status_disconnected;
+	u32 reg_val = dc_readl(adev, DC_HDMI_HOTPLUG_STATUS);
+
+	switch (connector->index) {
+	case 0:
+		if (adev->vga_hpd_status == connector_status_unknown)
+			status = connector_status_unknown;
+
+		if (reg_val & 0x1)
+			status = connector_status_connected;
+		else if (status != adev->vga_hpd_status)
+			status = connector_status_connected;
+		break;
+	case 1:
+		if (reg_val & 0x2)
+			status = connector_status_connected;
+		break;
+	}
+
+	return status;
+}
+
+static enum drm_connector_status
+gsgpu_dc_connector_detect(struct drm_connector *connector, bool force)
+{
+	struct gsgpu_device *adev = connector->dev->dev_private;
+	enum drm_connector_status status = connector_status_disconnected;
+
+	if (connector->polled == 0)
+		status = connector_status_connected;
+	else if (connector->polled == (DRM_CONNECTOR_POLL_CONNECT
+				      | DRM_CONNECTOR_POLL_DISCONNECT)) {
+		if (is_connected(connector))
+			status = connector_status_connected;
+		else
+			status = connector_status_disconnected;
+	} else if (connector->polled == DRM_CONNECTOR_POLL_HPD) {
+		switch (adev->chip) {
+		case dev_7a2000:
+			status = gsgpu_7a2000_detect(connector);
+			break;
+		case dev_2k2000:
+			status = gsgpu_2k2000_detect(connector);
+			break;
+		}
+	}
+
+	return status;
+}
+
+static void gsgpu_dc_connector_funcs_reset(struct drm_connector *connector)
+{
+	struct dc_connector_state *state =
+		to_dc_connector_state(connector->state);
+
+	if (connector->state)
+		__drm_atomic_helper_connector_destroy_state(connector->state);
+
+	kfree(state);
+
+	state = kzalloc(sizeof(*state), GFP_KERNEL);
+
+	if (state) {
+		state->max_bpc = 8;
+		__drm_atomic_helper_connector_reset(connector, &state->base);
+	}
+}
+
+static void gsgpu_dc_connector_destroy(struct drm_connector *connector)
+{
+	drm_connector_unregister(connector);
+	drm_connector_cleanup(connector);
+	kfree(connector);
+}
+
+struct drm_connector_state *
+gsgpu_dc_connector_atomic_duplicate_state(struct drm_connector *connector)
+{
+	struct dc_connector_state *state =
+		to_dc_connector_state(connector->state);
+
+	struct dc_connector_state *new_state =
+			kmemdup(state, sizeof(*state), GFP_KERNEL);
+
+	if (new_state) {
+		__drm_atomic_helper_connector_duplicate_state(connector,
+							      &new_state->base);
+		new_state->max_bpc = state->max_bpc;
+		return &new_state->base;
+	}
+
+	return NULL;
+}
+
+static const struct drm_connector_funcs gsgpu_dc_connector_funcs = {
+	.detect = gsgpu_dc_connector_detect,
+	.reset = gsgpu_dc_connector_funcs_reset,
+	.fill_modes = drm_helper_probe_single_connector_modes,
+	.destroy = gsgpu_dc_connector_destroy,
+	.atomic_duplicate_state = gsgpu_dc_connector_atomic_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_connector_destroy_state,
+	.late_register = gsgpu_backlight_register
+};
+
+struct gsgpu_dc_connector *dc_connector_construct(struct gsgpu_dc *dc, struct connector_resource *resource)
+{
+	struct gsgpu_dc_connector *connector;
+	u32 link;
+
+	if (IS_ERR_OR_NULL(dc) || IS_ERR_OR_NULL(resource))
+		return NULL;
+
+	connector = kzalloc(sizeof(*connector), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(connector))
+		return NULL;
+
+	connector->dc = dc;
+	connector->resource = resource;
+
+	link = connector->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	list_add_tail(&connector->node, &dc->connector_list);
+
+	return connector;
+}
+
+int gsgpu_dc_connector_init(struct gsgpu_device *adev, uint32_t link_index)
+{
+	struct gsgpu_dc_connector *dc_connector;
+	struct gsgpu_connector *lconnector;
+	int res = 0;
+
+	DRM_DEBUG_DRIVER("%s()\n", __func__);
+
+	if (adev->dc->link_info[link_index].encoder->has_ext_encoder)
+		return 0;
+
+	if (link_index >= 2)
+		return -1;
+
+	lconnector = kzalloc(sizeof(*lconnector), GFP_KERNEL);
+	if (!lconnector)
+		return -ENOMEM;
+
+	dc_connector = adev->dc->link_info[link_index].connector;
+	res = drm_connector_init(adev->ddev, &lconnector->base,
+				 &gsgpu_dc_connector_funcs,
+				 dc_connector->resource->type);
+	if (res) {
+		DRM_ERROR("connector_init failed\n");
+		lconnector->connector_id = -1;
+		goto out_free;
+	}
+
+	drm_connector_helper_add(&lconnector->base, &dc_connector_helper_funcs);
+
+	adev->mode_info.connectors[link_index] = lconnector;
+	lconnector->connector_id = link_index;
+	lconnector->base.dpms = DRM_MODE_DPMS_OFF;
+
+	mutex_init(&lconnector->hpd_lock);
+
+	switch (adev->chip) {
+	case dev_7a2000:
+		if (link_index == 0) {
+			lconnector->irq_source_i2c = DC_IRQ_SOURCE_I2C0;
+			lconnector->irq_source_hpd = DC_IRQ_SOURCE_HPD_HDMI0;
+			lconnector->irq_source_vga_hpd = DC_IRQ_SOURCE_HPD_VGA;
+		} else if (link_index == 1) {
+			lconnector->irq_source_i2c = DC_IRQ_SOURCE_I2C1;
+			lconnector->irq_source_hpd = DC_IRQ_SOURCE_HPD_HDMI1;
+		}
+		if (dc_connector->resource->type == DRM_MODE_CONNECTOR_VGA)
+			dc_connector->resource->hotplug = POLLING;
+		break;
+	case dev_2k2000:
+		if (link_index == 0) {
+			lconnector->irq_source_i2c = DC_IRQ_SOURCE_I2C0;
+			lconnector->irq_source_hpd = DC_IRQ_SOURCE_HPD_HDMI0;
+		} else if (link_index == 1) {
+			lconnector->irq_source_i2c = DC_IRQ_SOURCE_I2C1;
+			lconnector->irq_source_hpd = DC_IRQ_SOURCE_HPD_HDMI1_NULL;
+		}
+		break;
+	}
+
+	switch (dc_connector->resource->hotplug) {
+	case IRQ:
+		lconnector->base.polled = DRM_CONNECTOR_POLL_HPD;
+		break;
+	case POLLING:
+	default:
+		lconnector->base.polled = DRM_CONNECTOR_POLL_CONNECT |
+					  DRM_CONNECTOR_POLL_DISCONNECT;
+		break;
+	case FORCE_ON:
+		lconnector->base.polled = 0;
+		break;
+	}
+
+	drm_connector_register(&lconnector->base);
+
+out_free:
+	if (res) {
+		kfree(lconnector);
+	}
+
+	return res;
+}
+
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_crtc.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_crtc.c
new file mode 100644
index 000000000000..522b670fd936
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_crtc.c
@@ -0,0 +1,593 @@
+#include <drm/drm_atomic_helper.h>
+#include <gsgpu.h>
+#include "gsgpu_dc.h"
+#include "gsgpu_dc_resource.h"
+#include "gsgpu_dc_plane.h"
+#include "gsgpu_dc_crtc.h"
+#include "gsgpu_dc_hdmi.h"
+#include "gsgpu_dc_reg.h"
+
+static unsigned int
+cal_freq(unsigned int pixclock_khz, struct pixel_clock *pll_config)
+{
+	unsigned int pstdiv, loopc, frefc;
+	unsigned long a, b, c;
+	unsigned long min = 50;
+
+	for (pstdiv = 1; pstdiv < 64; pstdiv++) {
+		a = (unsigned long)pixclock_khz * pstdiv;
+		for (frefc = 3; frefc < 6; frefc++) {
+			for (loopc = 24; loopc < 161; loopc++) {
+				if ((loopc < 12 * frefc) ||
+				    (loopc > 32 * frefc))
+					continue;
+
+				b = 100000L * loopc / frefc;
+				c = (a > b) ? (a * 10000 / b - 10000) :
+					(b * 10000 / a - 10000);
+				if (c < min) {
+					min = c;
+					pll_config->l2_div = pstdiv;
+					pll_config->l1_loopc = loopc;
+					pll_config->l1_frefc = frefc;
+				}
+			}
+		}
+	}
+
+	if (min < 50)
+		return 1;
+
+	return 0;
+}
+
+static u32 dc_io_rreg(void *base, u32 offset)
+{
+	return readl(base + offset);
+}
+
+static void dc_io_wreg(void *base, u32 offset, u32 val)
+{
+	writel(val, base + offset);
+}
+
+static bool config_pll(struct gsgpu_device *adev, u32 clock, unsigned long pll_reg)
+{
+	u32 val;
+	u32 count = 0;
+	struct pixel_clock pll_cfg = {0};
+
+	void __iomem *io_base = adev->io_base;
+	if (io_base == NULL)
+		return false;
+
+	cal_freq(clock, &pll_cfg);
+
+	/* clear sel_pll_out0 */
+	val = dc_io_rreg(io_base, pll_reg + 0x4);
+	val &= ~(1UL << 8);
+	dc_io_wreg(io_base, pll_reg + 0x4, val);
+
+	/* set pll_pd */
+	val = dc_io_rreg(io_base, pll_reg + 0x4);
+	val |= (1UL << 13);
+	dc_io_wreg(io_base, pll_reg + 0x4, val);
+
+	/* clear set_pll_param */
+	val = dc_io_rreg(io_base, pll_reg + 0x4);
+	val &= ~(1UL << 11);
+	dc_io_wreg(io_base, pll_reg + 0x4, val);
+
+	/* clear old value & config new value */
+	val = dc_io_rreg(io_base, pll_reg + 0x4);
+	val &= ~(0x7fUL << 0);
+	val |= (pll_cfg.l1_frefc << 0); /* refc */
+	dc_io_wreg(io_base, pll_reg + 0x4, val);
+	val = dc_io_rreg(io_base, pll_reg + 0x0);
+	val &= ~(0x7fUL << 0);
+	val |= (pll_cfg.l2_div << 0); /* div */
+	val &= ~(0x1ffUL << 21);
+	val |= (pll_cfg.l1_loopc << 21); /* loopc */
+	dc_io_wreg(io_base, pll_reg + 0x0, val);
+
+	/* set set_pll_param */
+	val = dc_io_rreg(io_base, pll_reg + 0x4);
+	val |= (1UL << 11);
+	dc_io_wreg(io_base, pll_reg + 0x4, val);
+	/* clear pll_pd */
+	val = dc_io_rreg(io_base, pll_reg + 0x4);
+	val &= ~(1UL << 13);
+	dc_io_wreg(io_base, pll_reg + 0x4, val);
+
+	while (!(dc_io_rreg(io_base, pll_reg + 0x4) & 0x80)) {
+		cpu_relax();
+		count++;
+		if (count >= 1000) {
+			DRM_ERROR("loongson-7A PLL lock failed\n");
+			return false;
+		}
+		schedule_timeout(usecs_to_jiffies(100));
+	}
+
+	val = dc_io_rreg(io_base, pll_reg + 0x4);
+	val |= (1UL << 8);
+	dc_io_wreg(io_base, pll_reg + 0x4, val);
+
+	return true;
+}
+
+bool dc_crtc_timing_set(struct gsgpu_dc_crtc *crtc, struct dc_timing_info *timing)
+{
+	struct gsgpu_device *adev = crtc->dc->adev;
+	u32 depth;
+	u32 link;
+	u32 value;
+	u32 stride_reg;
+	u32 temp;
+
+	if (IS_ERR_OR_NULL(crtc) || IS_ERR_OR_NULL(timing))
+		return false;
+
+	DRM_DEBUG_DRIVER("crtc %d timing set: clock %d, stride %d\n",
+		crtc->resource->base.link, timing->clock, timing->stride);
+	DRM_DEBUG_DRIVER("hdisplay %d, hsync_start %d, hsync_end %d, htotal %d\n",
+		timing->hdisplay, timing->hsync_start, timing->hsync_end, timing->htotal);
+	DRM_DEBUG_DRIVER("vdisplay %d, vsync_start %d, vsync_end %d, vtotal %d\n",
+		timing->vdisplay, timing->vsync_start, timing->vsync_end, timing->vtotal);
+	DRM_DEBUG_DRIVER("depth %d, use_dma32 %d\n", timing->depth, timing->use_dma32);
+
+	link = crtc->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	if (config_pll(adev, timing->clock, CURRENT_REG(DC_IO_PIX_PLL, link)) == false)
+		return false;
+
+	value = dc_readl(adev, CURRENT_REG(DC_CRTC_CFG_REG, link));
+	value |= CRTC_CFG_RESET;
+
+	switch (crtc->array_mode) {
+	case 0:
+		value &= CRTC_CFG_LINEAR;
+		stride_reg = timing->stride;
+		break;
+	case 2:
+		value |= CRTC_CFG_TILE4x4;
+		stride_reg = timing->stride * 4;
+		break;
+	}
+
+	value &= ~CRTC_CFG_DMA_MASK;
+	if (timing->use_dma32)
+		value |= CRTC_CFG_DMA_32;
+	else if (!(timing->hdisplay % 64))
+		value |= CRTC_CFG_DMA_256;
+	else if (!(timing->hdisplay % 32))
+		value |= CRTC_CFG_DMA_128;
+	else if (!(timing->hdisplay % 16))
+		value |= CRTC_CFG_DMA_64;
+	else if (!(timing->hdisplay % 8))
+		value |= CRTC_CFG_DMA_32;
+
+	value &= ((~CRTC_CFG_FORMAT_MASK) << 0);
+	depth = timing->depth;
+	switch (depth) {
+	case 12:
+		value |= (DC_FB_FORMAT12 & CRTC_CFG_FORMAT_MASK);
+		break;
+	case 15:
+		value |= (DC_FB_FORMAT15 & CRTC_CFG_FORMAT_MASK);
+		break;
+	case 16:
+		value |= (DC_FB_FORMAT16 & CRTC_CFG_FORMAT_MASK);
+		break;
+	case 32:
+	case 24:
+	default:
+		value |= (DC_FB_FORMAT32 & CRTC_CFG_FORMAT_MASK);
+		break;
+	}
+	temp = value | CRTC_CFG_ENABLE;
+
+	dc_writel(adev, CURRENT_REG(DC_CRTC_STRIDE_REG, link), stride_reg);
+
+	value = ((timing->hdisplay & CRTC_HPIXEL_MASK) << CRTC_HPIXEL_SHIFT);
+	value |= ((timing->htotal & CRTC_HTOTAL_MASK) << CRTC_HTOTAL_SHIFT);
+	dc_writel(adev, CURRENT_REG(DC_CRTC_HDISPLAY_REG, link), value);
+
+	value = CRTC_HSYNC_POLSE;
+	value |= ((timing->hsync_start & CRTC_HSYNC_START_MASK) << CRTC_HSYNC_START_SHIFT);
+	value |= ((timing->hsync_end & CRTC_HSYNC_END_MASK) << CRTC_HSYNC_END_SHIFT);
+	dc_writel(adev, CURRENT_REG(DC_CRTC_HSYNC_REG, link), value);
+
+	value = ((timing->vdisplay & CRTC_VPIXEL_MASK) << CRTC_VPIXEL_SHIFT);
+	value |= ((timing->vtotal & CRTC_VTOTAL_MASK) << CRTC_VTOTAL_SHIFT);
+	dc_writel(adev, CURRENT_REG(DC_CRTC_VDISPLAY_REG, link), value);
+
+	value = CRTC_VSYNC_POLSE;
+	value |= ((timing->vsync_start  & CRTC_VSYNC_START_MASK) << CRTC_VSYNC_START_SHIFT);
+	value |= ((timing->vsync_end & CRTC_VSYNC_END_MASK) << CRTC_VSYNC_END_SHIFT);
+	dc_writel(adev, CURRENT_REG(DC_CRTC_VSYNC_REG, link), value);
+
+	value = CRTC_PANCFG_BASE | CRTC_PANCFG_DE | CRTC_PANCFG_CLKEN |
+		CRTC_PANCFG_CLKPOL;
+	dc_writel(adev, CURRENT_REG(DC_CRTC_PANELCFG_REG, link), value);
+	dc_writel(adev, CURRENT_REG(DC_CRTC_PANELTIM_REG, link), 0);
+
+	hdmi_phy_pll_config(adev, link, timing->clock);
+
+	dc_writel(adev, CURRENT_REG(DC_CRTC_CFG_REG, link), temp);
+
+	return true;
+}
+
+bool dc_crtc_enable(struct gsgpu_crtc *acrtc, bool enable)
+{
+	struct gsgpu_device *adev = acrtc->base.dev->dev_private;
+	u32 crtc_cfg, crtc_pan;
+	u32 hsync_val, vsync_val;
+	u32 hdmi_phy;
+	u32 crtc_id;
+
+	if (IS_ERR_OR_NULL(acrtc))
+		return false;
+
+	crtc_id = acrtc->crtc_id;
+	if (crtc_id >= DC_DVO_MAXLINK)
+		return false;
+
+	crtc_cfg = dc_readl(adev, CURRENT_REG(DC_CRTC_CFG_REG, crtc_id));
+	hdmi_phy = dc_readl(adev, CURRENT_REG(DC_HDMI_PHY_CTRL_REG, crtc_id));
+	crtc_pan = dc_readl(adev, CURRENT_REG(DC_CRTC_PANELCFG_REG, crtc_id));
+	hsync_val = dc_readl(adev, CURRENT_REG(DC_CRTC_HSYNC_REG, crtc_id));
+	vsync_val = dc_readl(adev, CURRENT_REG(DC_CRTC_VSYNC_REG, crtc_id));
+
+	if (enable) {
+		crtc_cfg |= CRTC_CFG_ENABLE;
+		hdmi_phy |= HDMI_PHY_CTRL_ENABLE;
+		crtc_pan |= CRTC_PANCFG_DE;
+		crtc_pan |= CRTC_PANCFG_CLKEN;
+		hsync_val |= CRTC_HSYNC_POLSE;
+		vsync_val |= CRTC_VSYNC_POLSE;
+	} else {
+		crtc_cfg &= ~CRTC_CFG_ENABLE;
+		hdmi_phy &= ~HDMI_PHY_CTRL_ENABLE;
+		crtc_pan &= ~CRTC_PANCFG_DE;
+		crtc_pan &= ~CRTC_PANCFG_CLKEN;
+		hsync_val &= ~CRTC_HSYNC_POLSE;
+		vsync_val &= ~CRTC_VSYNC_POLSE;
+	}
+
+	dc_writel(adev, CURRENT_REG(DC_CRTC_CFG_REG, crtc_id), crtc_cfg);
+	dc_writel(adev, CURRENT_REG(DC_HDMI_PHY_CTRL_REG, crtc_id), hdmi_phy);
+	dc_writel(adev, CURRENT_REG(DC_CRTC_PANELCFG_REG, crtc_id), crtc_pan);
+	dc_writel(adev, CURRENT_REG(DC_CRTC_HSYNC_REG, crtc_id), hsync_val);
+	dc_writel(adev, CURRENT_REG(DC_CRTC_VSYNC_REG, crtc_id), vsync_val);
+
+	return true;
+}
+
+bool dc_crtc_vblank_enable(struct gsgpu_dc_crtc *crtc, bool enable)
+{
+	u32 link;
+	u32 value;
+	struct gsgpu_device *adev = crtc->dc->adev;
+
+	if (IS_ERR_OR_NULL(crtc))
+		return false;
+
+	link = crtc->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	value = dc_readl(adev, DC_INT_REG);
+	switch (link) {
+	case 0:
+		if (enable)
+			value |= INT_VSYNC0_ENABLE;
+		else
+			value &= ~INT_VSYNC0_ENABLE;
+		break;
+	case 1:
+		if (enable)
+			value |= INT_VSYNC1_ENABLE;
+		else
+			value &= ~INT_VSYNC1_ENABLE;
+		break;
+	default:
+		return false;
+	}
+
+	dc_writel(adev, DC_INT_REG, value);
+
+	return true;
+}
+
+u32 dc_vblank_get_counter(struct gsgpu_device *adev, int crtc_num)
+{
+	if (crtc_num >= adev->mode_info.num_crtc)
+		return 0;
+	else {
+		return dc_readl(adev, DC_VSYNC_COUNTER_REG + (0x10 * crtc_num));
+	}
+
+	return 0;
+}
+
+int dc_crtc_get_scanoutpos(struct gsgpu_device *adev, int crtc_num,
+				  u32 *vbl, u32 *position)
+{
+	struct gsgpu_dc *dc = adev->dc;
+	uint32_t v_blank_start, v_blank_end, h_position, v_position;
+	int reg_val = 0;
+
+	if (IS_ERR_OR_NULL(dc) || (crtc_num >= dc->links))
+		return false;
+
+	if (IS_ERR_OR_NULL(dc->link_info[crtc_num].crtc))
+		return false;
+
+	if ((crtc_num < 0) || (crtc_num >= adev->mode_info.num_crtc))
+		return -EINVAL;
+	else {
+		reg_val = dc_readl(adev, DC_CRTC_DISPLAY_POS_REG + (0x10 * crtc_num));
+//		DRM_INFO("dc_crtc_get_scanoutpos reg_val 0x%x\n", reg_val);
+
+		v_blank_start = 0;
+		v_blank_end = 0;
+		h_position = (reg_val & 0xffff);
+		v_position = (reg_val >> 16);
+
+//		position = v_position | (h_position << 16);
+//		vbl = v_blank_start | (v_blank_end << 16);
+		position = 0;
+		vbl = 0;
+	}
+
+	return 0;
+}
+
+bool dc_crtc_vblank_ack(struct gsgpu_dc_crtc *crtc)
+{
+	u32 link;
+	u32 value;
+	struct gsgpu_device *adev = crtc->dc->adev;
+
+	if (IS_ERR_OR_NULL(crtc))
+		return false;
+
+	link = crtc->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	value = dc_readl(adev, DC_INT_REG);
+
+	switch (link) {
+	case 0:
+		value |= INT_VSYNC0_ENABLE;
+		break;
+	case 1:
+		value |= INT_VSYNC1_ENABLE;
+		break;
+	}
+
+	dc_writel(adev, DC_INT_REG, value);
+
+	return true;
+}
+
+static bool crtc_update_fb_address(struct gsgpu_dc_crtc *crtc,
+				   union plane_address address)
+{
+	u32 link;
+	struct gsgpu_device *adev;
+
+	if (IS_ERR_OR_NULL(crtc))
+		return false;
+
+	link = crtc->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	adev = crtc->dc->adev;
+
+	dc_writel(adev, CURRENT_REG(DC_CRTC_FBADDR0_LO_REG, link), address.low_part);
+	dc_writel(adev, CURRENT_REG(DC_CRTC_FBADDR1_LO_REG, link), address.low_part);
+
+	dc_writel(adev, CURRENT_REG(DC_CRTC_FBADDR0_HI_REG, link), address.high_part);
+	dc_writel(adev, CURRENT_REG(DC_CRTC_FBADDR1_HI_REG, link), address.high_part);
+
+	return true;
+}
+
+static bool crtc_primary_plane_set(struct gsgpu_dc_crtc *crtc,
+					 struct dc_primary_plane *primary)
+{
+	if (IS_ERR_OR_NULL(crtc) || IS_ERR_OR_NULL(primary))
+		return false;
+
+	return crtc_update_fb_address(crtc, primary->address);
+}
+
+bool dc_crtc_plane_update(struct gsgpu_dc_crtc *crtc, struct dc_plane_update *update)
+{
+	bool ret;
+
+	if (IS_ERR_OR_NULL(crtc) || IS_ERR_OR_NULL(update))
+		return false;
+
+	switch (update->type) {
+	case DC_PLANE_CURSOR:
+		ret = crtc_cursor_set(crtc, &update->cursor);
+		break;
+	case DC_PLANE_PRIMARY:
+		ret = crtc_primary_plane_set(crtc, &update->primary);
+		break;
+	case DC_PLANE_OVERLAY:
+	default:
+		pr_err("%s 7A1000 not support overlay \n", __func__);
+		ret = false;
+		break;
+	}
+
+	return ret;
+}
+
+struct gsgpu_dc_crtc *dc_crtc_construct(struct gsgpu_dc *dc, struct crtc_resource *resource)
+{
+	struct gsgpu_dc_crtc *crtc;
+	u32 link;
+
+	if (IS_ERR_OR_NULL(dc) || IS_ERR_OR_NULL(resource))
+		return NULL;
+
+	crtc = kzalloc(sizeof(*crtc), GFP_KERNEL);
+
+	if (IS_ERR_OR_NULL(crtc))
+		return NULL;
+
+	crtc->dc = dc;
+	crtc->resource = resource;
+
+	link = crtc->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	list_add_tail(&crtc->node, &dc->crtc_list);
+
+	return crtc;
+}
+
+void dc_crtc_destroy(struct gsgpu_dc_crtc *crtc)
+{
+	if (IS_ERR_OR_NULL(crtc))
+		return;
+
+	list_del(&crtc->node);
+	kfree(crtc);
+	crtc = NULL;
+}
+
+static int crtc_helper_atomic_check(struct drm_crtc *crtc,
+				       struct drm_crtc_state *state)
+{
+	return 0;
+}
+
+static enum drm_mode_status gsgpu_dc_mode_valid(struct drm_crtc *crtc,
+				const struct drm_display_mode *mode)
+{
+	struct gsgpu_device *adev = crtc->dev->dev_private;
+
+	switch (adev->chip) {
+	case dev_7a2000:
+		if (mode->hdisplay > 4096)
+			return MODE_BAD;
+		if (mode->vdisplay > 2160)
+			return MODE_BAD;
+		break;
+	case dev_2k2000:
+		if (mode->hdisplay > 1920 && crtc->index == 1)
+			return MODE_BAD;
+		else if (mode->hdisplay > 4096)
+			return MODE_BAD;
+		if (mode->vdisplay > 1200 && crtc->index == 1)
+			return MODE_BAD;
+		else if (mode->vdisplay > 2160)
+			return MODE_BAD;
+		break;
+	}
+
+	if (mode->hdisplay % 8)
+		return MODE_BAD;
+	if (mode->clock > 340000)
+		return MODE_CLOCK_HIGH;
+	if (mode->vdisplay < 480)
+		return MODE_BAD;
+
+	return MODE_OK;
+}
+
+static const struct drm_crtc_helper_funcs gsgpu_dc_crtc_helper_funcs = {
+	.atomic_check = crtc_helper_atomic_check,
+	.mode_valid = gsgpu_dc_mode_valid,
+};
+
+static inline int dc_set_vblank(struct drm_crtc *crtc, bool enable)
+{
+	enum dc_irq_source irq_source;
+	struct gsgpu_crtc *acrtc = to_gsgpu_crtc(crtc);
+	struct gsgpu_device *adev = crtc->dev->dev_private;
+
+	irq_source = DC_IRQ_TYPE_VSYNC + acrtc->crtc_id;
+	return dc_interrupt_enable(adev->dc, irq_source, enable) ? 0 : -EBUSY;
+}
+
+static int dc_enable_vblank(struct drm_crtc *crtc)
+{
+	return dc_set_vblank(crtc, true);
+}
+
+static void dc_disable_vblank(struct drm_crtc *crtc)
+{
+	dc_set_vblank(crtc, false);
+}
+
+static const struct drm_crtc_funcs gsgpu_dc_crtc_funcs = {
+	.set_config = drm_atomic_helper_set_config,
+	.page_flip = drm_atomic_helper_page_flip,
+	.reset = drm_atomic_helper_crtc_reset,
+	.destroy = drm_crtc_cleanup,
+	.atomic_duplicate_state = drm_atomic_helper_crtc_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_crtc_destroy_state,
+	.enable_vblank = dc_enable_vblank,
+	.disable_vblank = dc_disable_vblank,
+};
+
+int gsgpu_dc_crtc_init(struct gsgpu_device *adev,
+		       struct drm_plane *plane, uint32_t crtc_index)
+{
+	struct gsgpu_crtc *acrtc = NULL;
+	struct gsgpu_plane *cursor_plane;
+	int res = -ENOMEM;
+
+	cursor_plane = kzalloc(sizeof(*cursor_plane), GFP_KERNEL);
+	if (!cursor_plane)
+		goto fail;
+
+	cursor_plane->base.type = DRM_PLANE_TYPE_CURSOR;
+	res = gsgpu_dc_plane_init(adev, cursor_plane, 0);
+
+	acrtc = kzalloc(sizeof(struct gsgpu_crtc), GFP_KERNEL);
+	if (!acrtc)
+		goto fail;
+
+	res = drm_crtc_init_with_planes(adev->ddev, &acrtc->base, plane,
+			&cursor_plane->base, &gsgpu_dc_crtc_funcs, NULL);
+	if (!res)
+		acrtc->crtc_id = crtc_index;
+	else {
+		acrtc->crtc_id = -1;
+		goto fail;
+	}
+
+	drm_crtc_helper_add(&acrtc->base, &gsgpu_dc_crtc_helper_funcs);
+
+	mutex_init(&acrtc->cursor_lock);
+	acrtc->max_cursor_width = 64;
+	acrtc->max_cursor_height = 64;
+
+	acrtc->irq_source_vsync = DC_IRQ_TYPE_VSYNC + crtc_index;
+	acrtc->base.enabled = false;
+
+	adev->mode_info.crtcs[crtc_index] = acrtc;
+
+	gsgpu_hdmi_init(adev);
+
+	return 0;
+
+fail:
+	kfree(acrtc);
+	kfree(cursor_plane);
+	return res;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_cursor.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_cursor.c
new file mode 100644
index 000000000000..bb78688506c2
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_cursor.c
@@ -0,0 +1,212 @@
+#include "gsgpu.h"
+#include "gsgpu_dc_crtc.h"
+#include "gsgpu_dc_plane.h"
+#include "gsgpu_dc.h"
+#include "gsgpu_dc_resource.h"
+#include "gsgpu_dc_reg.h"
+
+static int get_cursor_position(struct drm_plane *plane, struct drm_crtc *crtc,
+			       struct dc_cursor_position *position)
+{
+	struct gsgpu_crtc *gsgpu_crtc = to_gsgpu_crtc(crtc);
+	int x, y;
+	int xorigin = 0, yorigin = 0;
+
+	if (!crtc || !plane->state->fb) {
+		position->enable = false;
+		position->x = 0;
+		position->y = 0;
+		return 0;
+	}
+
+	if ((plane->state->crtc_w > gsgpu_crtc->max_cursor_width) ||
+	    (plane->state->crtc_h > gsgpu_crtc->max_cursor_height)) {
+		DRM_ERROR("%s: bad cursor width or height %d x %d\n",
+			  __func__,
+			  plane->state->crtc_w,
+			  plane->state->crtc_h);
+		return -EINVAL;
+	}
+
+	x = plane->state->crtc_x;
+	y = plane->state->crtc_y;
+
+	if (x <= -gsgpu_crtc->max_cursor_width ||
+	    y <= -gsgpu_crtc->max_cursor_height)
+		return 0;
+
+	if (x < 0) {
+		xorigin = min(-x, gsgpu_crtc->max_cursor_width - 1);
+		x = 0;
+	}
+
+	if (y < 0) {
+		yorigin = min(-y, gsgpu_crtc->max_cursor_height - 1);
+		y = 0;
+	}
+
+	position->enable = true;
+	position->x = x;
+	position->y = y;
+	position->x_hotspot = xorigin;
+	position->y_hotspot = yorigin;
+
+	return 0;
+}
+
+static bool dc_crtc_cursor_move(struct gsgpu_device *adev, int link,
+				struct dc_cursor_move *move)
+{
+	struct gsgpu_crtc *acrtc = adev->mode_info.crtcs[link];
+	u32 cfg_val = 0;
+	u32 pos_val = 0;
+
+	if (IS_ERR_OR_NULL(move))
+		return false;
+
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	DRM_DEBUG_DRIVER("crusor link%d cur move x%d y%d hotx%d hoty%d enable%d\n",
+		link, move->x, move->y, move->hot_x, move->hot_y, move->enable);
+
+	mutex_lock(&acrtc->cursor_lock);
+	if (link == 0)
+		cfg_val = dc_readl(adev, DC_CURSOR0_CFG_REG);
+	else
+		cfg_val = dc_readl(adev, DC_CURSOR1_CFG_REG);
+
+	cfg_val &= ~(DC_CURSOR_POS_HOT_X_MASK << DC_CURSOR_POS_HOT_X_SHIFT);
+	cfg_val &= ~(DC_CURSOR_POS_HOT_Y_MASK << DC_CURSOR_POS_HOT_Y_SHIFT);
+	if (move->enable == false) {
+		cfg_val &= ~DC_CURSOR_FORMAT_MASK;
+		pos_val = 0x0;
+	} else {
+		cfg_val |= ((CUR_FORMAT_ARGB8888 & DC_CURSOR_FORMAT_MASK) << DC_CURSOR_FORMAT_SHIFT);
+		cfg_val |= ((link & DC_CURSOR_DISPLAY_MASK) << DC_CURSOR_DISPLAY_SHIFT);
+		cfg_val |= ((move->hot_x & DC_CURSOR_POS_HOT_X_MASK) << DC_CURSOR_POS_HOT_X_SHIFT);
+		cfg_val |= ((move->hot_y & DC_CURSOR_POS_HOT_Y_MASK) << DC_CURSOR_POS_HOT_Y_SHIFT);
+		pos_val = ((move->x & DC_CURSOR_POS_X_MASK) << DC_CURSOR_POS_X_SHIFT);
+		pos_val |= ((move->y & DC_CURSOR_POS_Y_MASK) << DC_CURSOR_POS_Y_SHIFT);
+	}
+
+	if (link == 0) {
+		dc_writel(adev, DC_CURSOR0_CFG_REG, cfg_val);
+		dc_writel(adev, DC_CURSOR0_POSITION_REG, pos_val);
+	} else {
+		dc_writel(adev, DC_CURSOR1_CFG_REG, cfg_val);
+		dc_writel(adev, DC_CURSOR1_POSITION_REG, pos_val);
+	}
+	mutex_unlock(&acrtc->cursor_lock);
+
+	return true;
+}
+
+bool crtc_cursor_set(struct gsgpu_dc_crtc *crtc, struct dc_cursor_info *cursor)
+{
+	struct gsgpu_device *adev = crtc->dc->adev;
+	struct gsgpu_crtc *acrtc;
+	int value = 0;
+	u32 addr_l, addr_h;
+	u32 link;
+
+	if (IS_ERR_OR_NULL(crtc) || IS_ERR_OR_NULL(cursor))
+		return false;
+
+	link = crtc->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	acrtc = adev->mode_info.crtcs[link];
+
+	mutex_lock(&acrtc->cursor_lock);
+	if (link == 0)
+		value = dc_readl(adev, DC_CURSOR0_CFG_REG);
+	else
+		value = dc_readl(adev, DC_CURSOR1_CFG_REG);
+
+	value &= DC_CURSOR_MODE_CLEAN;
+	switch (cursor->x) {
+	case 32:
+		value |= DC_CURSOR_MODE_32x32;
+		break;
+	case 64:
+		value |= DC_CURSOR_MODE_64x64;
+		break;
+	default:
+		DRM_ERROR("cursor x %u y %u not supported\n",
+			  cursor->x, cursor->y);
+		return false;
+	}
+
+	value |= (link & DC_CURSOR_DISPLAY_MASK) << DC_CURSOR_DISPLAY_SHIFT;
+	value |= (CUR_FORMAT_ARGB8888 & DC_CURSOR_FORMAT_MASK) << DC_CURSOR_FORMAT_SHIFT;
+
+	addr_l = cursor->address.low_part;
+	addr_h = cursor->address.high_part;
+
+	if (link == 0) {
+		dc_writel(adev, DC_CURSOR0_CFG_REG, value);
+		dc_writel(adev, DC_CURSOR0_LADDR_REG, addr_l);
+		dc_writel(adev, DC_CURSOR0_HADDR_REG, addr_h);
+	} else {
+		dc_writel(adev, DC_CURSOR1_CFG_REG, value);
+		dc_writel(adev, DC_CURSOR1_LADDR_REG, addr_l);
+		dc_writel(adev, DC_CURSOR1_HADDR_REG, addr_h);
+	}
+	mutex_unlock(&acrtc->cursor_lock);
+
+	return true;
+}
+
+void handle_cursor_update(struct drm_plane *plane,
+			  struct drm_plane_state *old_plane_state)
+{
+	struct gsgpu_device *adev = plane->dev->dev_private;
+	struct gsgpu_framebuffer *afb = to_gsgpu_framebuffer(plane->state->fb);
+	struct drm_crtc *crtc = afb ? plane->state->crtc : old_plane_state->crtc;
+	struct gsgpu_crtc *gsgpu_crtc = to_gsgpu_crtc(crtc);
+	uint64_t address = afb ? afb->address : 0;
+	struct dc_cursor_position position;
+	struct dc_plane_update dc_plane;
+	struct dc_cursor_move move = {0};
+	int ret;
+
+	if (!plane->state->fb && !old_plane_state->fb)
+		return;
+
+	DRM_DEBUG_DRIVER("%s: crtc_id=%d with size %d to %d\n",
+			 __func__,
+			 gsgpu_crtc->crtc_id,
+			 plane->state->crtc_w,
+			 plane->state->crtc_h);
+
+	ret = get_cursor_position(plane, crtc, &position);
+	if (ret)
+		return;
+
+	if (!position.enable) {
+		/* turn off cursor */
+		dc_crtc_cursor_move(adev, gsgpu_crtc->crtc_id, &move);
+		return;
+	}
+
+	dc_plane.type = DC_PLANE_CURSOR;
+	dc_plane.cursor.x = plane->state->crtc_w;
+	dc_plane.cursor.y = plane->state->crtc_h;
+	dc_plane.cursor.address.low_part = lower_32_bits(address);
+	dc_plane.cursor.address.high_part = upper_32_bits(address);
+
+	move.enable = position.enable;
+	move.hot_x = position.x_hotspot;
+	move.hot_y = position.y_hotspot;
+	move.x = position.x;
+	move.y = position.y;
+
+	if (!dc_submit_plane_update(adev->dc, gsgpu_crtc->crtc_id, &dc_plane))
+		DRM_ERROR("DC failed to set cursor attributes\n");
+	if (!dc_crtc_cursor_move(adev, gsgpu_crtc->crtc_id, &move))
+		DRM_ERROR("DC failed to set cursor position\n");
+
+	return;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_drv.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_drv.c
new file mode 100644
index 000000000000..6b5c6d8d9bd4
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_drv.c
@@ -0,0 +1,1038 @@
+#include <linux/pm_runtime.h>
+#include <drm/drm_atomic.h>
+#include <drm/drm_atomic_helper.h>
+
+#include "gsgpu.h"
+#include "gsgpu_dc_plane.h"
+#include "gsgpu_dc_crtc.h"
+#include "gsgpu_dc.h"
+#include "gsgpu_display.h"
+#include "gsgpu_dc_encoder.h"
+#include "gsgpu_dc_connector.h"
+#include "gsgpu_dc_vbios.h"
+#include "gsgpu_dc_reg.h"
+#include "gsgpu_dc_hdmi.h"
+#include "bridge_phy.h"
+
+static const struct gsgpu_display_funcs dc_display_funcs = {
+	.vblank_get_counter = dc_vblank_get_counter,
+	.page_flip_get_scanoutpos = dc_crtc_get_scanoutpos,
+};
+
+u32 dc_readl(struct gsgpu_device *adev, u32 reg)
+{
+	return readl(adev->loongson_dc_rmmio + reg);
+}
+
+void dc_writel(struct gsgpu_device *adev, u32 reg, u32 val)
+{
+	writel(val, adev->loongson_dc_rmmio + reg);
+}
+
+u32 dc_readl_locked(struct gsgpu_device *adev, u32 reg)
+{
+	u32 val;
+	unsigned long flags;
+
+	spin_lock_irqsave(&adev->dc_mmio_lock, flags);
+	val = readl(adev->loongson_dc_rmmio + reg);
+	spin_unlock_irqrestore(&adev->dc_mmio_lock, flags);
+
+	return val;
+}
+
+void dc_writel_locked(struct gsgpu_device *adev, u32 reg, u32 val)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&adev->dc_mmio_lock, flags);
+	writel(val, adev->loongson_dc_rmmio + reg);
+	spin_unlock_irqrestore(&adev->dc_mmio_lock, flags);
+}
+
+static bool dc_links_init(struct gsgpu_dc *dc)
+{
+	struct header_resource *header_res = NULL;
+	struct crtc_resource *crtc_resource;
+	struct encoder_resource *encoder_resource;
+	struct connector_resource *connector_resource;
+	struct gsgpu_link_info *link_info;
+	s32 links, i;
+
+	if (IS_ERR_OR_NULL(dc))
+		return false;
+
+	header_res = dc_get_vbios_resource(dc->vbios, 0, GSGPU_RESOURCE_HEADER);
+	links = header_res->links;
+
+	link_info = kzalloc(sizeof(*link_info) * links, GFP_KERNEL);
+	if (IS_ERR_OR_NULL(link_info))
+		return false;
+
+	dc->link_info = link_info;
+
+	for (i = 0; i < links; i++) {
+		crtc_resource = dc_get_vbios_resource(dc->vbios, i, GSGPU_RESOURCE_CRTC);
+		link_info[i].crtc = dc_crtc_construct(dc, crtc_resource);
+		if (!link_info[i].crtc) {
+			DRM_ERROR("link-%d  crtc construct failed \n", i);
+			continue;
+		}
+
+		encoder_resource = dc_get_vbios_resource(dc->vbios, i, GSGPU_RESOURCE_ENCODER);
+		link_info[i].encoder = dc_encoder_construct(dc, encoder_resource);
+		if (!link_info[i].encoder) {
+			DRM_ERROR("link-%d  encoder construct failed \n", i);
+			continue;
+		}
+
+		connector_resource = dc_get_vbios_resource(dc->vbios,
+						i, GSGPU_RESOURCE_CONNECTOR);
+		link_info[i].bridge = dc_bridge_construct(dc,
+					encoder_resource, connector_resource);
+		if (!link_info[i].bridge) {
+			DRM_ERROR("link-%d bridge construct failed\n", i);
+			continue;
+		}
+		link_info[i].connector = dc_connector_construct(dc, connector_resource);
+		if (!link_info[i].connector) {
+			DRM_ERROR("link-%d  encoder construct failed \n", i);
+			continue;
+		}
+
+		link_info[i].fine = true;
+		dc->links++;
+	}
+
+	return true;
+}
+
+static void dc_link_exit(struct gsgpu_dc *dc)
+{
+	u32 i;
+	struct gsgpu_link_info *link_info;
+
+	if (IS_ERR_OR_NULL(dc))
+		return;
+
+	link_info = dc->link_info;
+
+	for (i = 0; i < dc->links; i++) {
+		dc_crtc_destroy(link_info[i].crtc);
+
+		link_info[i].fine = false;
+	}
+
+	kfree(link_info);
+	dc->link_info = NULL;
+	dc->links = 0;
+}
+
+static struct gsgpu_dc *dc_construct(struct gsgpu_device *adev)
+{
+	struct gsgpu_dc *dc;
+	bool status;
+
+	if (IS_ERR_OR_NULL(adev))
+		return false;
+
+	dc = kzalloc(sizeof(*dc), GFP_KERNEL);
+
+	if (IS_ERR_OR_NULL(dc))
+		return ERR_PTR(-ENOMEM);
+
+	dc->adev = adev;
+
+	INIT_LIST_HEAD(&dc->crtc_list);
+	INIT_LIST_HEAD(&dc->encoder_list);
+	INIT_LIST_HEAD(&dc->connector_list);
+
+	status = dc_vbios_init(dc);
+	if (!status) {
+		kfree(dc);
+		DRM_ERROR("GSGPU dc init vbios failed\n");
+		return NULL;
+	}
+
+	if (dc_links_init(dc) == false) {
+		DRM_ERROR("GSGPU dc init links failed\n");
+		kfree(dc);
+		dc = NULL;
+	}
+
+	return dc;
+}
+
+static void dc_destruct(struct gsgpu_dc *dc)
+{
+	if (IS_ERR_OR_NULL(dc))
+		return;
+
+	dc_link_exit(dc);
+	dc_vbios_exit(dc->vbios);
+
+	kfree(dc);
+	dc = NULL;
+}
+
+bool dc_submit_timing_update(struct gsgpu_dc *dc, u32 link, struct dc_timing_info *timing)
+{
+	if (IS_ERR_OR_NULL(dc) || (link >= dc->links))
+		return false;
+
+	return dc_crtc_timing_set(dc->link_info[link].crtc, timing);
+}
+
+bool dc_submit_plane_update(struct gsgpu_dc *dc, u32 link, struct dc_plane_update *update)
+{
+	if (IS_ERR_OR_NULL(dc) || (link >= dc->links))
+		return false;
+
+	return dc_crtc_plane_update(dc->link_info[link].crtc, update);
+}
+
+static void manage_dc_interrupts(struct gsgpu_device *adev,
+				 struct gsgpu_crtc *acrtc,
+				 bool enable)
+{
+	if (enable) {
+		drm_crtc_vblank_on(&acrtc->base);
+		gsgpu_irq_get(adev, &adev->vsync_irq, acrtc->crtc_id);
+	} else {
+		gsgpu_irq_put(adev, &adev->vsync_irq, acrtc->crtc_id);
+		drm_crtc_vblank_off(&acrtc->base);
+	}
+}
+
+static int gsgpu_dc_meta_enable(struct gsgpu_device *adev, bool clear)
+{
+	uint64_t meta_gpu_addr;
+	int r, i;
+
+	if (adev->dc->meta_bo == NULL) {
+		dev_err(adev->dev, "No VRAM object for PCIE DC_META.\n");
+		return -EINVAL;
+	}
+
+	r = gsgpu_bo_reserve(adev->dc->meta_bo, false);
+	if (unlikely(r != 0))
+		return r;
+
+	r = gsgpu_bo_pin(adev->dc->meta_bo, GSGPU_GEM_DOMAIN_VRAM);
+	if (r) {
+		gsgpu_bo_unreserve(adev->dc->meta_bo);
+		return r;
+	}
+
+	r = gsgpu_bo_kmap(adev->dc->meta_bo, &adev->dc->meta_cpu_addr);
+	if (r)
+		gsgpu_bo_unpin(adev->dc->meta_bo);
+	gsgpu_bo_unreserve(adev->dc->meta_bo);
+
+	adev->dc->meta_gpu_addr = gsgpu_bo_gpu_offset(adev->dc->meta_bo);
+
+	if (clear)
+		memset(adev->dc->meta_cpu_addr, 0x00, 0x200000);
+
+	meta_gpu_addr = adev->dc->meta_gpu_addr;
+	for (i = 0; i < adev->mode_info.num_crtc; i++) {
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META0_REG_L, i), (u32)meta_gpu_addr);
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META0_REG_H, i), (meta_gpu_addr >> 32));
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META1_REG_L, i), (u32)meta_gpu_addr);
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META1_REG_H, i), (meta_gpu_addr >> 32));
+	}
+
+	DRM_INFO("DC META of 2M enabled (table at 0x%016llX).\n",
+		 (unsigned long long)adev->dc->meta_gpu_addr);
+
+	return 0;
+}
+
+static int gsgpu_dc_meta_disable(struct gsgpu_device *adev)
+{
+	int r, i;
+
+	if (adev->dc->meta_bo == NULL) {
+		dev_err(adev->dev, "No VRAM object for PCIE DC_META.\n");
+		return -EINVAL;
+	}
+
+	for (i = 0; i < adev->mode_info.num_crtc; i++) {
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META0_REG_L, i), 0);
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META0_REG_H, i), 0);
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META1_REG_L, i), 0);
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META1_REG_H, i), 0);
+	}
+
+	r = gsgpu_bo_reserve(adev->dc->meta_bo, true);
+	if (likely(r == 0)) {
+		gsgpu_bo_kunmap(adev->dc->meta_bo);
+		gsgpu_bo_unpin(adev->dc->meta_bo);
+		gsgpu_bo_unreserve(adev->dc->meta_bo);
+		adev->dc->meta_cpu_addr = NULL;
+	}
+
+	return 0;
+}
+
+static void gsgpu_dc_meta_set(struct gsgpu_device *adev)
+{
+	uint64_t meta_gpu_addr;
+	int dc_meta_size = 0x200000;
+	int i;
+	int r;
+
+	r = gsgpu_bo_create_kernel(adev, dc_meta_size,
+				   GSGPU_GEM_COMPRESSED_SIZE,
+				   GSGPU_GEM_DOMAIN_VRAM, &adev->dc->meta_bo,
+				   &meta_gpu_addr, &adev->dc->meta_cpu_addr);
+	if (r) {
+		dev_warn(adev->dev, "(%d) create dc meta bo failed\n", r);
+		return;
+	}
+
+	adev->dc->meta_gpu_addr = meta_gpu_addr;
+	memset(adev->dc->meta_cpu_addr, 0x00, dc_meta_size);
+	for (i = 0; i < adev->mode_info.num_crtc; i++) {
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META0_REG_L, i), (u32)meta_gpu_addr);
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META0_REG_H, i), (meta_gpu_addr >> 32));
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META1_REG_L, i), (u32)meta_gpu_addr);
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META1_REG_H, i), (meta_gpu_addr >> 32));
+	}
+
+	DRM_INFO("Config crtc number:%d meta addr 0x%llx\n", i, meta_gpu_addr);
+}
+
+static void gsgpu_dc_meta_free(struct gsgpu_device *adev)
+{
+	int i;
+
+	gsgpu_bo_free_kernel(&adev->dc->meta_bo,
+			     &adev->dc->meta_gpu_addr,
+			     &adev->dc->meta_cpu_addr);
+
+	for (i = 0; i < adev->mode_info.num_crtc; i++) {
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META0_REG_L, i), 0);
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META0_REG_H, i), 0);
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META1_REG_L, i), 0);
+		dc_writel(adev, CURRENT_REG(DC_CRTC_META1_REG_H, i), 0);
+	}
+	adev->dc->meta_bo = NULL;
+
+	DRM_INFO("Free crtc number:%d meta addr\n", i);
+}
+
+static int gsgpu_dc_atomic_commit(struct drm_device *dev,
+				  struct drm_atomic_state *state,
+				  bool nonblock)
+{
+	return drm_atomic_helper_commit(dev, state, nonblock);
+}
+
+static const struct drm_mode_config_funcs gsgpu_dc_mode_funcs = {
+	.fb_create = gsgpu_display_user_framebuffer_create,
+	.output_poll_changed = drm_fb_helper_output_poll_changed,
+	.atomic_check = drm_atomic_helper_check,
+	.atomic_commit = gsgpu_dc_atomic_commit,
+};
+
+static bool modeset_required(struct drm_crtc_state *crtc_state)
+{
+	if (!drm_atomic_crtc_needs_modeset(crtc_state))
+		return false;
+
+	if (!crtc_state->enable)
+		return false;
+
+	return crtc_state->active;
+}
+
+static bool modereset_required(struct drm_crtc_state *crtc_state)
+{
+	if (!drm_atomic_crtc_needs_modeset(crtc_state))
+		return false;
+
+	return !crtc_state->enable || !crtc_state->active;
+}
+
+static void prepare_flip_isr(struct gsgpu_crtc *acrtc)
+{
+	assert_spin_locked(&acrtc->base.dev->event_lock);
+	WARN_ON(acrtc->event);
+
+	acrtc->event = acrtc->base.state->event;
+
+	/* Set the flip status */
+	acrtc->pflip_status = GSGPU_FLIP_SUBMITTED;
+
+	/* Mark this event as consumed */
+	acrtc->base.state->event = NULL;
+
+	DRM_DEBUG_DRIVER("crtc:%d, pflip_stat:GSGPU_FLIP_SUBMITTED\n",
+						 acrtc->crtc_id);
+}
+
+static void gsgpu_dc_do_flip(struct drm_crtc *crtc,
+			      struct drm_framebuffer *fb,
+			      uint32_t target)
+{
+	unsigned long flags;
+	uint32_t target_vblank;
+	int r, vpos, hpos;
+	struct dc_plane_update plane;
+	struct gsgpu_crtc *acrtc = to_gsgpu_crtc(crtc);
+	struct gsgpu_framebuffer *afb = to_gsgpu_framebuffer(fb);
+	struct gsgpu_bo *abo = gem_to_gsgpu_bo(fb->obj[0]);
+	struct gsgpu_device *adev = crtc->dev->dev_private;
+	uint64_t crtc_array_mode, crtc_address;
+
+	/* Prepare wait for target vblank early - before the fence-waits */
+	target_vblank = target - (uint32_t)drm_crtc_vblank_count(crtc) +
+			gsgpu_get_vblank_counter_kms(crtc->dev, acrtc->crtc_id);
+
+	/* TODO This might fail and hence better not used, wait
+	 * explicitly on fences instead
+	 * and in general should be called for
+	 * blocking commit to as per framework helpers
+	 */
+	r = gsgpu_bo_reserve(abo, true);
+	if (unlikely(r != 0)) {
+		DRM_ERROR("failed to reserve buffer before flip\n");
+		WARN_ON(1);
+	}
+
+	/* Wait for all fences on this FB */
+	WARN_ON(reservation_object_wait_timeout_rcu(abo->tbo.resv, true, false,
+								    MAX_SCHEDULE_TIMEOUT) < 0);
+
+	gsgpu_bo_unreserve(abo);
+
+	/* Wait until we're out of the vertical blank period before the one
+	 * targeted by the flip
+	 */
+	while ((acrtc->enabled &&
+		(gsgpu_display_get_crtc_scanoutpos(adev->ddev, acrtc->crtc_id,
+						    0, &vpos, &hpos, NULL,
+						    NULL, &crtc->hwmode)
+		 & (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_IN_VBLANK)) ==
+		(DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_IN_VBLANK) &&
+		(int)(target_vblank -
+		  gsgpu_get_vblank_counter_kms(adev->ddev, acrtc->crtc_id)) > 0)) {
+		usleep_range(1000, 1100);
+	}
+
+	/* Flip */
+	spin_lock_irqsave(&crtc->dev->event_lock, flags);
+
+	WARN_ON(acrtc->pflip_status != GSGPU_FLIP_NONE);
+
+	if (acrtc->base.state->event)
+		prepare_flip_isr(acrtc);
+
+	spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
+
+	crtc_array_mode = GSGPU_TILING_GET(abo->tiling_flags, ARRAY_MODE);
+	switch (crtc_array_mode) {
+	case 0:
+		crtc_address = afb->address + crtc->y * afb->base.pitches[0] + ALIGN(crtc->x, 8) * 4;
+		break;
+	case 2:
+		crtc_address = afb->address + crtc->y * afb->base.pitches[0] + ALIGN(crtc->x, 8) * 4 * 4;
+		break;
+	}
+
+	plane.type = DC_PLANE_PRIMARY;
+	plane.primary.address.low_part = lower_32_bits(crtc_address);
+	plane.primary.address.high_part = upper_32_bits(crtc_address);
+
+	dc_submit_plane_update(adev->dc, acrtc->crtc_id, &plane);
+
+	DRM_DEBUG_DRIVER("%s Flipping to hi: 0x%x, low: 0x%x \n",
+			 __func__,
+			 upper_32_bits(afb->address),
+			 lower_32_bits(afb->address));
+}
+
+static void gsgpu_dc_commit_planes(struct drm_atomic_state *state,
+				   struct drm_device *dev,
+				   struct drm_crtc *pcrtc,
+				   bool *wait_for_vblank)
+{
+	struct drm_plane *plane;
+	struct drm_plane_state *old_plane_state, *new_plane_state;
+	struct gsgpu_crtc *acrtc = to_gsgpu_crtc(pcrtc);
+	struct drm_crtc_state *new_pcrtc_state =
+			drm_atomic_get_new_crtc_state(state, pcrtc);
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_dc_crtc *dc_crtc = adev->dc->link_info[acrtc->crtc_id].crtc;
+	struct drm_framebuffer *modeset_fbs[1];
+	int planes_count = 0;
+	uint64_t tiling_flags = 0;
+	unsigned long flags;
+	uint32_t i;
+	int x, y, cpp, ret;
+
+	/* update planes when needed */
+	for_each_oldnew_plane_in_state(state, plane, old_plane_state, new_plane_state, i) {
+		struct drm_crtc *crtc = new_plane_state->crtc;
+		struct drm_crtc_state *new_crtc_state;
+		struct drm_framebuffer *fb = new_plane_state->fb;
+		bool pflip_needed;
+
+		if (plane->type == DRM_PLANE_TYPE_CURSOR) {
+			handle_cursor_update(plane, old_plane_state);
+			continue;
+		}
+
+		if (!fb || !crtc || pcrtc != crtc)
+			continue;
+
+		new_crtc_state = drm_atomic_get_new_crtc_state(state, crtc);
+		if (!new_crtc_state->active) {
+			dc_crtc_enable(acrtc, false);
+			continue;
+		} else
+			dc_crtc_enable(acrtc, true);
+
+		x = plane->state->crtc->x;
+		y = plane->state->crtc->y;
+		pflip_needed = !state->allow_modeset;
+
+		spin_lock_irqsave(&crtc->dev->event_lock, flags);
+		if (acrtc->pflip_status != GSGPU_FLIP_NONE) {
+			DRM_ERROR("%s: acrtc %d, already busy\n",
+				  __func__, acrtc->crtc_id);
+			/* In commit tail framework this cannot happen */
+			WARN_ON(1);
+		}
+		spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
+
+		if (!pflip_needed) {
+			planes_count++;
+			modeset_fbs[0] = fb;
+		} else if (new_crtc_state->planes_changed) {
+			/* Assume even ONE crtc with immediate flip means
+			 * entire can't wait for VBLANK
+			 * TODO Check if it's correct
+			 */
+			*wait_for_vblank =
+				new_pcrtc_state->pageflip_flags & DRM_MODE_PAGE_FLIP_ASYNC ?
+				false : true;
+
+			/* TODO: Needs rework for multiplane flip */
+			if (plane->type == DRM_PLANE_TYPE_PRIMARY)
+				drm_crtc_vblank_get(crtc);
+
+			gsgpu_dc_do_flip(crtc, fb,
+				(uint32_t)drm_crtc_vblank_count(crtc) + *wait_for_vblank);
+		}
+	}
+
+	if (planes_count) {
+		unsigned long flags;
+		struct dc_timing_info timing;
+		struct dc_plane_update plane;
+		struct drm_display_mode *mode = &pcrtc->mode;
+		uint64_t address;
+
+		struct gsgpu_framebuffer *afb =
+				to_gsgpu_framebuffer(modeset_fbs[0]);
+
+		struct gsgpu_bo *bo = gem_to_gsgpu_bo(afb->base.obj[0]);
+		ret = gsgpu_bo_reserve(bo, false);
+		if (unlikely(ret)) {
+			if (ret != -ERESTARTSYS)
+				DRM_ERROR("Unable to reserve buffer: %d\n", ret);
+			return;
+		}
+		gsgpu_bo_get_tiling_flags(bo, &tiling_flags);
+		gsgpu_bo_unreserve(bo);
+		dc_crtc->array_mode = GSGPU_TILING_GET(tiling_flags, ARRAY_MODE);
+
+		if (new_pcrtc_state->event) {
+			drm_crtc_vblank_get(pcrtc);
+			spin_lock_irqsave(&pcrtc->dev->event_lock, flags);
+			prepare_flip_isr(acrtc);
+			spin_unlock_irqrestore(&pcrtc->dev->event_lock, flags);
+		}
+
+		cpp = afb->base.format->cpp[0];
+		timing.depth = afb->base.format->cpp[0] << 3;
+		timing.stride = afb->base.pitches[0];
+		timing.clock = mode->clock;
+		timing.hdisplay = mode->hdisplay;
+		timing.htotal = mode->htotal;
+		timing.hsync_start = mode->hsync_start;
+		timing.hsync_end  = mode->hsync_end;
+		timing.vdisplay = mode->vdisplay;
+		timing.vtotal = mode->vtotal;
+		timing.vsync_start = mode->vsync_start;
+		timing.vsync_end = mode->vsync_end;
+		timing.use_dma32 = 0;
+
+		if (x != 0 && x % 64 && dc_crtc->array_mode == 0)
+			timing.use_dma32 = 1;
+
+		dc_submit_timing_update(adev->dc, acrtc->crtc_id, &timing);
+
+		DRM_DEBUG_DRIVER("gsgpu crtc-%d hdisplay %d vdisplay %d x %d y %d cpp %d stride %d\n",
+				 acrtc->crtc_id, timing.hdisplay, timing.vdisplay,
+				 x, y, cpp, timing.stride);
+
+		plane.type = DC_PLANE_PRIMARY;
+		switch (dc_crtc->array_mode) {
+		case 0:
+			address = afb->address + y * timing.stride + ALIGN(x, 8) * cpp;
+			break;
+		case 2:
+			y = (y + 3) & ~3;
+			x = ALIGN(x, 8);
+			address = afb->address + y * timing.stride + x * cpp * 4;
+			break;
+		}
+
+		plane.primary.address.low_part = lower_32_bits(address);
+		plane.primary.address.high_part = upper_32_bits(address);
+		dc_submit_plane_update(adev->dc, acrtc->crtc_id, &plane);
+	}
+}
+
+void gsgpu_dc_atomic_commit_tail(struct drm_atomic_state *state)
+{
+	struct drm_device *dev = state->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+	uint32_t i, j;
+	struct drm_crtc *crtc;
+	struct drm_crtc_state *old_crtc_state, *new_crtc_state;
+	unsigned long flags;
+	bool wait_for_vblank = true;
+	int crtc_disable_count = 0;
+
+	/*
+	 * We evade vblanks and pflips on crtc that
+	 * should be changed. We do it here to flush & disable
+	 * interrupts before drm_swap_state is called in drm_atomic_helper_commit
+	 * it will update crtc->dm_crtc_state->stream pointer which is used in
+	 * the ISRs.
+	 */
+	for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state, new_crtc_state, i) {
+		struct gsgpu_crtc *acrtc = to_gsgpu_crtc(crtc);
+
+		if (drm_atomic_crtc_needs_modeset(new_crtc_state)) {
+			manage_dc_interrupts(adev, acrtc, false);
+		}
+	}
+
+	drm_atomic_helper_update_legacy_modeset_state(dev, state);
+
+	/* update changed items */
+	for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state, new_crtc_state, i) {
+		struct gsgpu_crtc *acrtc = to_gsgpu_crtc(crtc);
+
+		DRM_DEBUG_DRIVER(
+			"gsgpu_crtc id:%d crtc_state_flags: enable:%d, active:%d, "
+			"planes_changed:%d, mode_changed:%d,active_changed:%d,"
+			"connectors_changed:%d\n",
+			acrtc->crtc_id,
+			new_crtc_state->enable,
+			new_crtc_state->active,
+			new_crtc_state->planes_changed,
+			new_crtc_state->mode_changed,
+			new_crtc_state->active_changed,
+			new_crtc_state->connectors_changed);
+
+		/* handles headless hotplug case, updating new_state and
+		 * aconnector as needed
+		 */
+		if (modeset_required(new_crtc_state)) {
+			DRM_DEBUG_DRIVER("Atomic commit: SET crtc id %d: [%p]\n", acrtc->crtc_id, acrtc);
+			pm_runtime_get_noresume(dev->dev);
+			acrtc->enabled = true;
+			acrtc->hw_mode = new_crtc_state->mode;
+			crtc->hwmode = new_crtc_state->mode;
+		} else if (modereset_required(new_crtc_state)) {
+			DRM_DEBUG_DRIVER("gsgpu_crtc id:%d enable:%d, active:%d\n", acrtc->crtc_id,
+				new_crtc_state->enable,	new_crtc_state->active);
+			acrtc->enabled = false;
+		}
+	} /* for_each_crtc_in_state() */
+
+	for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state, new_crtc_state, i) {
+		/*
+		 * loop to enable interrupts on newly arrived crtc
+		 */
+		struct gsgpu_crtc *acrtc = to_gsgpu_crtc(crtc);
+		bool modeset_needed;
+
+		if (old_crtc_state->active && !new_crtc_state->active)
+			crtc_disable_count++;
+
+		modeset_needed = modeset_required(new_crtc_state);
+
+		if (!modeset_needed)
+			continue;
+
+		manage_dc_interrupts(adev, acrtc, true);
+	}
+
+	/* update planes when needed per crtc*/
+	for_each_new_crtc_in_state(state, crtc, new_crtc_state, j) {
+		gsgpu_dc_commit_planes(state, dev, crtc, &wait_for_vblank);
+	}
+
+	/*
+	 * send vblank event on all events not handled in flip and
+	 * mark consumed event for drm_atomic_helper_commit_hw_done
+	 */
+	spin_lock_irqsave(&adev->ddev->event_lock, flags);
+	for_each_new_crtc_in_state(state, crtc, new_crtc_state, i) {
+
+		if (new_crtc_state->event)
+			drm_send_event_locked(dev, &new_crtc_state->event->base);
+
+		new_crtc_state->event = NULL;
+	}
+	spin_unlock_irqrestore(&adev->ddev->event_lock, flags);
+
+	drm_atomic_helper_commit_hw_done(state);
+
+	if (wait_for_vblank)
+		drm_atomic_helper_wait_for_flip_done(dev, state);
+
+	drm_atomic_helper_cleanup_planes(dev, state);
+
+	/* Finally, drop a runtime PM reference for each newly disabled CRTC,
+	 * so we can put the GPU into runtime suspend if we're not driving any
+	 * displays anymore
+	 */
+	for (i = 0; i < crtc_disable_count; i++)
+		pm_runtime_put_autosuspend(dev->dev);
+	pm_runtime_mark_last_busy(dev->dev);
+}
+
+static struct drm_mode_config_helper_funcs gsgpu_dc_mode_config_helper = {
+	.atomic_commit_tail = gsgpu_dc_atomic_commit_tail
+};
+
+static int dc_mode_config_init(struct gsgpu_device *adev)
+{
+	int r;
+
+	adev->mode_info.mode_config_initialized = true;
+
+	adev->ddev->mode_config.funcs = (void *)&gsgpu_dc_mode_funcs;
+	adev->ddev->mode_config.helper_private = &gsgpu_dc_mode_config_helper;
+
+	adev->ddev->mode_config.max_width = 16384;
+	adev->ddev->mode_config.max_height = 16384;
+
+	adev->ddev->mode_config.preferred_depth = 24;
+	adev->ddev->mode_config.prefer_shadow = 1;
+	/* indicate support of immediate flip */
+	adev->ddev->mode_config.async_page_flip = true;
+
+	adev->ddev->mode_config.fb_base = adev->gmc.aper_base;
+
+	r = gsgpu_display_modeset_create_props(adev);
+	if (r)
+		return r;
+
+	return 0;
+}
+
+static void gsgpu_attach_encoder_connector(struct gsgpu_device *adev)
+{
+	struct gsgpu_connector *lconnector;
+	struct gsgpu_encoder *lencoder;
+	int i;
+
+	for (i = 0; i < adev->mode_info.num_crtc; i++) {
+		lconnector = adev->mode_info.connectors[i];
+		lencoder = adev->mode_info.encoders[i];
+		drm_connector_attach_encoder(&lconnector->base, &lencoder->base);
+	}
+}
+
+static void gsgpu_display_print_display_setup(struct drm_device *dev)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	struct drm_crtc *crtc;
+	struct gsgpu_crtc *gsgpu_crtc;
+	struct drm_connector *connector;
+	struct gsgpu_connector *gsgpu_connector;
+	struct drm_encoder *encoder;
+	struct gsgpu_encoder *gsgpu_encoder;
+
+	DRM_DEBUG_DRIVER("GSGPU DC revision %d\n", adev->dc_revision);
+	DRM_INFO("GSGPU Display Crtcs\n");
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		gsgpu_crtc = to_gsgpu_crtc(crtc);
+		DRM_INFO("Crtc %d: name:%s\n", crtc->index, crtc->name);
+	}
+
+	DRM_INFO("GSGPU Display Connectors\n");
+	list_for_each_entry(connector, &dev->mode_config.connector_list, head) {
+		gsgpu_connector = to_gsgpu_connector(connector);
+		DRM_INFO("Connector %d: name:%s\n", connector->index, connector->name);
+	}
+
+	DRM_INFO("GSGPU Display Encoders\n");
+	list_for_each_entry(encoder, &dev->mode_config.encoder_list, head) {
+		gsgpu_encoder = to_gsgpu_encoder(encoder);
+		DRM_INFO("Encoder %d: name:%s\n", encoder->index, encoder->name);
+	}
+}
+
+static int dc_initialize_drm_device(struct gsgpu_device *adev)
+{
+	struct gsgpu_mode_info *mode_info = &adev->mode_info;
+	int32_t total_primary_planes;
+	int32_t i;
+
+	if (dc_mode_config_init(adev)) {
+		DRM_ERROR("Failed to initialize mode config\n");
+		return -1;
+	}
+
+	total_primary_planes = 2;
+	for (i = (total_primary_planes - 1); i >= 0; i--) {
+		if (initialize_plane(adev, mode_info, i)) {
+			DRM_ERROR("KMS: Failed to initialize primary plane\n");
+			goto fail;
+		}
+	}
+
+	for (i = 0; i < 2; i++) {
+		if (gsgpu_dc_crtc_init(adev, &mode_info->planes[i]->base, i)) {
+			DRM_ERROR("KMS: Failed to initialize crtc\n");
+			goto fail;
+		}
+	}
+
+	for (i = 0; i < 2; i++) {
+		gsgpu_i2c_init(adev, i);
+		if (gsgpu_dc_encoder_init(adev, i)) {
+			DRM_ERROR("KMS: Failed to initialize encoder\n");
+			goto fail;
+		}
+
+		if (gsgpu_dc_bridge_init(adev, i)) {
+			DRM_ERROR("KMS: Failed to initialize bridge\n");
+			goto fail;
+		}
+
+		if (gsgpu_dc_connector_init(adev, i)) {
+			DRM_ERROR("KMS: Failed to initialize connector\n");
+			goto fail;
+		}
+	}
+
+	gsgpu_attach_encoder_connector(adev);
+	if (dc_register_irq_handlers(adev)) {
+		DRM_ERROR("DC: Failed to initialize IRQ\n");
+		goto fail;
+	}
+
+	return 0;
+
+fail:
+	for (i = 0; i < 2/*max_planes*/; i++)
+		kfree(mode_info->planes[i]);
+	return -1;
+}
+
+static void gsgpu_dc_fini(struct gsgpu_device *adev)
+{
+	drm_mode_config_cleanup(adev->ddev);
+	/*
+	 * TODO: pageflip, vlank interrupt
+	 *
+	 * gsgpu_dc_irq_fini(adev);
+	 */
+
+	return;
+}
+
+#define DC_MAX_PLANES 4
+static const enum drm_plane_type plane_type[DC_MAX_PLANES] = {
+	DRM_PLANE_TYPE_PRIMARY,
+	DRM_PLANE_TYPE_PRIMARY,
+};
+
+static int dc_early_init(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	adev->mode_info.num_crtc = 2;
+	adev->mode_info.num_i2c = 2;
+	adev->mode_info.num_hpd = 3;
+	adev->mode_info.plane_type = plane_type;
+
+	dc_set_irq_funcs(adev);
+
+	if (adev->mode_info.funcs == NULL)
+		adev->mode_info.funcs = &dc_display_funcs;
+
+	return 0;
+}
+
+static int dc_late_init(void *handle)
+{
+	return 0;
+}
+
+static int dc_sw_init(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	adev->dc = dc_construct(adev);
+	if (adev->dc) {
+		DRM_INFO("GSGPU Display Core initialized with v%s!\n", DC_VER);
+	} else {
+		DRM_INFO("GSGPU Display Core failed to init with v%s!\n", DC_VER);
+		goto error;
+	}
+
+	DRM_INFO("GSGPU DC construct links:%d", adev->dc->links);
+	DRM_DEBUG_DRIVER("GSGPU DC sw init success!\n");
+
+	return 0;
+
+error:
+	gsgpu_dc_fini(adev);
+	return -1;
+}
+
+static int dc_sw_fini(void *handle)
+{
+	return 0;
+}
+
+static int dc_hw_init(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	if (gsgpu_dc_irq_init(adev)) {
+		DRM_ERROR("Failed to initialize IRQ support.\n");
+		goto error;
+	}
+
+	if (dc_initialize_drm_device(adev)) {
+		DRM_ERROR("Failed to initialize sw for display support.\n");
+		goto error;
+	}
+	gsgpu_display_print_display_setup(adev->ddev);
+
+	adev->mode_info.num_crtc = adev->dc->links;
+	adev->ddev->mode_config.cursor_width = 32;
+	adev->ddev->mode_config.cursor_height = 32;
+
+	if (drm_vblank_init(adev->ddev, adev->dc->links)) {
+		DRM_ERROR("Failed to initialize vblank.\n");
+		goto error;
+	}
+
+	drm_mode_config_reset(adev->ddev);
+	drm_kms_helper_poll_init(adev->ddev);
+	gsgpu_dc_meta_set(adev);
+	gsgpu_dc_hpd_init(adev);
+
+	DRM_DEBUG_DRIVER("GSGPU DC hw init success!\n");
+
+	return 0;
+error:
+	gsgpu_dc_fini(adev);
+	return -1;
+}
+
+static int dc_hw_fini(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	gsgpu_dc_irq_fini(adev);
+	gsgpu_dc_meta_free(adev);
+	gsgpu_dc_fini(adev);
+	return 0;
+}
+
+static int dc_suspend(void *handle)
+{
+	struct gsgpu_device *adev = handle;
+
+	WARN_ON(adev->dc->cached_state);
+
+	adev->dc->cached_state = drm_atomic_helper_suspend(adev->ddev);
+
+	gsgpu_hdmi_suspend(adev);
+	gsgpu_dc_meta_disable(adev);
+	gsgpu_dc_hpd_disable(adev);
+
+	return 0;
+}
+
+static int dc_resume(void *handle)
+{
+	struct gsgpu_device *adev = handle;
+	struct drm_device *ddev = adev->ddev;
+	struct gsgpu_dc *dc = adev->dc;
+	struct drm_crtc *crtc;
+	struct drm_crtc_state *new_crtc_state;
+	int i;
+
+	for_each_new_crtc_in_state(dc->cached_state, crtc, new_crtc_state, i)
+		new_crtc_state->active_changed = true;
+
+	drm_atomic_helper_resume(ddev, dc->cached_state);
+
+	dc->cached_state = NULL;
+
+	gsgpu_dc_meta_enable(adev, true);
+	gsgpu_dc_hpd_init(adev);
+	gsgpu_hdmi_resume(adev);
+
+	return 0;
+}
+
+static bool dc_is_idle(void *handle)
+{
+	return true;
+}
+
+static int dc_wait_for_idle(void *handle)
+{
+	return 0;
+}
+
+static bool dc_check_soft_reset(void *handle)
+{
+	return false;
+}
+
+static int dc_soft_reset(void *handle)
+{
+	return 0;
+}
+
+static const struct gsgpu_ip_funcs gsgpu_dc_funcs = {
+	.name = "display",
+	.early_init = dc_early_init,
+	.late_init = dc_late_init,
+	.sw_init = dc_sw_init,
+	.sw_fini = dc_sw_fini,
+	.hw_init = dc_hw_init,
+	.hw_fini = dc_hw_fini,
+	.suspend = dc_suspend,
+	.resume = dc_resume,
+	.is_idle = dc_is_idle,
+	.wait_for_idle = dc_wait_for_idle,
+	.check_soft_reset = dc_check_soft_reset,
+	.soft_reset = dc_soft_reset,
+};
+
+const struct gsgpu_ip_block_version dc_ip_block = {
+	.type = GSGPU_IP_BLOCK_TYPE_DCE,
+	.major = 1,
+	.minor = 0,
+	.rev = 0,
+	.funcs = &gsgpu_dc_funcs,
+};
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_encoder.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_encoder.c
new file mode 100644
index 000000000000..0eaaacefc07b
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_encoder.c
@@ -0,0 +1,92 @@
+#include <drm/drm_encoder.h>
+#include <drm/drm_atomic_helper.h>
+#include "gsgpu.h"
+#include "gsgpu_dc.h"
+#include "gsgpu_dc_encoder.h"
+#include "bridge_phy.h"
+
+static void dc_encoder_destroy(struct drm_encoder *encoder)
+{
+	struct gsgpu_encoder *lencoder = to_gsgpu_encoder(encoder);
+
+	drm_encoder_cleanup(encoder);
+	kfree(lencoder);
+}
+
+static const struct drm_encoder_funcs dc_encoder_funcs = {
+	.destroy = dc_encoder_destroy,
+};
+
+static void dc_encoder_helper_disable(struct drm_encoder *encoder)
+{
+}
+
+static int dc_encoder_helper_atomic_check(struct drm_encoder *encoder,
+					  struct drm_crtc_state *crtc_state,
+					  struct drm_connector_state *conn_state)
+{
+	struct gsgpu_encoder *lencoder = to_gsgpu_encoder(encoder);
+
+	bridge_phy_mode_set(lencoder->bridge, &crtc_state->mode, NULL);
+
+	return 0;
+}
+
+const struct drm_encoder_helper_funcs dc_encoder_helper_funcs = {
+	.disable = dc_encoder_helper_disable,
+	.atomic_check = dc_encoder_helper_atomic_check
+};
+
+struct gsgpu_dc_encoder *dc_encoder_construct(struct gsgpu_dc *dc, struct encoder_resource *resource)
+{
+	struct gsgpu_dc_encoder *encoder;
+	u32 link;
+
+	if (IS_ERR_OR_NULL(dc) || IS_ERR_OR_NULL(resource))
+		return NULL;
+
+	encoder = kzalloc(sizeof(*encoder), GFP_KERNEL);
+
+	if (IS_ERR_OR_NULL(encoder))
+		return NULL;
+
+	encoder->dc = dc;
+	encoder->resource = resource;
+	encoder->has_ext_encoder = false;
+
+	link = encoder->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	return encoder;
+}
+
+int gsgpu_dc_encoder_init(struct gsgpu_device *adev, int link_index)
+{
+	struct gsgpu_encoder *lencoder;
+	int res;
+
+	if (link_index >= 2)
+		return -1;
+
+	lencoder = kzalloc(sizeof(*lencoder), GFP_KERNEL);
+	if (!lencoder)
+		return -ENOMEM;
+
+	res = drm_encoder_init(adev->ddev, &lencoder->base,
+			       &dc_encoder_funcs,
+			       DRM_MODE_ENCODER_TMDS, NULL);
+	if (!res)
+		lencoder->encoder_id = link_index;
+	else
+		lencoder->encoder_id = -1;
+
+	lencoder->base.possible_crtcs = 1 << link_index;
+
+	adev->mode_info.encoders[link_index] = lencoder;
+	adev->mode_info.encoders[link_index]->bridge = NULL;
+
+	drm_encoder_helper_add(&lencoder->base, &dc_encoder_helper_funcs);
+
+	return res;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_hdmi.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_hdmi.c
new file mode 100644
index 000000000000..134cee7c416f
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_hdmi.c
@@ -0,0 +1,191 @@
+#include "gsgpu.h"
+#include "gsgpu_dc.h"
+#include "gsgpu_dc_hdmi.h"
+#include "gsgpu_dc_crtc.h"
+#include "gsgpu_dc_reg.h"
+
+void hdmi_phy_pll_config(struct gsgpu_device *adev, int index, int clock)
+{
+	int val;
+	int count = 0;
+
+	if (adev->chip == dev_2k2000 && index == 1)
+		return;
+
+	dc_writel(adev, CURRENT_REG(DC_HDMI_PHY_PLLCFG_REG, index), 0x0);
+	dc_writel(adev, CURRENT_REG(DC_HDMI_PHY_CTRL_REG, index), 0xf02);
+
+	if (clock >= 170000)
+		val = (0x0 << 13) | (0x28 << 6) | (0x10 << 1) | (0 << 0);
+	else if (clock >= 85000  && clock < 170000)
+		val = (0x1 << 13) | (0x28 << 6) | (0x8 << 1) | (0 << 0);
+	else if (clock >= 42500 && clock < 85000)
+		val = (0x2 << 13) | (0x28 << 6) | (0x4 << 1) | (0 << 0);
+	else if (clock >= 21250 && clock < 42500)
+		val = (0x3 << 13) | (0x28 << 6) | (0x2 << 1) | (0 << 0);
+
+	dc_writel(adev, CURRENT_REG(DC_HDMI_PHY_PLLCFG_REG, index), val);
+	val |= (1 << 0);
+	dc_writel(adev, CURRENT_REG(DC_HDMI_PHY_PLLCFG_REG, index), val);
+
+	/* wait pll lock */
+	while (!(dc_readl(adev, CURRENT_REG(DC_HDMI_PHY_PLLCFG_REG, index)) & 0x10000)) {
+		count++;
+		if (count >= 1000) {
+			DRM_ERROR("GSGPU HDMI PHY PLL lock failed\n");
+			return;
+		}
+	}
+
+	dc_writel(adev, CURRENT_REG(DC_HDMI_PHY_CTRL_REG, index), 0xf03);
+}
+
+static int hdmi_set_avi_infoframe(struct drm_encoder *encoder,
+			   struct drm_display_mode *mode)
+{
+	struct gsgpu_device *adev = encoder->dev->dev_private;
+	struct hdmi_avi_infoframe avi_frame;
+	u8 buffer[HDMI_INFOFRAME_HEADER_SIZE + HDMI_AVI_INFOFRAME_SIZE];
+	int err, val;
+	uint8_t *frame;
+	int index = encoder->index;
+
+	err = drm_hdmi_avi_infoframe_from_display_mode(&avi_frame, mode, false);
+	if (err < 0) {
+		DRM_ERROR("failed to setup AVI infoframe: %d\n", err);
+		return err;
+	}
+
+	err = hdmi_avi_infoframe_pack(&avi_frame, buffer, sizeof(buffer));
+	if (err < 0) {
+		DRM_ERROR("failed to pack AVI infoframe: %d\n", err);
+		return err;
+	}
+
+	/* PHY config */
+	hdmi_phy_pll_config(adev, index, mode->clock);
+
+	/* set avi infoframe */
+	frame = buffer + 3;
+	val = (avi_frame.scan_mode << 0) | /*scan_info*/
+		(0 << 2) | /*bar_info*/
+		(1 << 4) | /*active_format_info_present*/
+		(avi_frame.colorspace << 5) | /*RGB_OR_YCBCR*/
+		(avi_frame.active_aspect << 8) | /*active_format_aspect_ratio*/
+		(2 << 12) | /*picture_aspect_ratio*/
+		(avi_frame.colorimetry << 14) | /*colorimetry*/
+		(0 << 16) | /*nonuniform_picture_scaling*/
+		(0 << 18) | /*RGB_quantization_range*/
+		(0 << 20) | /*ectended_colorimetry*/
+		(avi_frame.itc << 23) | /*it_content*/
+		(avi_frame.video_code << 24); /*video_format_id_code*/
+	dc_writel(adev, val, (DC_HDMI_AVI_CONT0_REG + (index*0x10)));
+
+	val = (avi_frame.pixel_repeat << 0) | /*pixel_repetition*/
+		(avi_frame.content_type << 4) | /*content_type*/
+		(avi_frame.ycc_quantization_range << 6); /*YCC_quantization_range*/
+	dc_writel(adev, val, (DC_HDMI_AVI_CONT1_REG + (index*0x10)));
+
+	val = ((avi_frame.top_bar & 0xff) << 0) | /*end_top_bar_l*/
+		(((avi_frame.top_bar >> 8) & 0xff) << 8) | /*end_top_bar_h*/
+		((avi_frame.bottom_bar & 0xff) << 16) | /*start_bottom_bar_l*/
+		(((avi_frame.bottom_bar >> 8) & 0xff) << 24); /*start_bottom_bar_h*/
+	dc_writel(adev, val, (DC_HDMI_AVI_CONT2_REG + (index*0x10)));
+
+	val = ((avi_frame.left_bar & 0xff) << 0) | /*end_left_bar_l*/
+		(((avi_frame.left_bar >> 8) & 0xff) << 8) | /*end_left_bar_h*/
+		((avi_frame.right_bar & 0xff) << 16) | /*start_right_bar_l*/
+		(((avi_frame.right_bar >> 8) & 0xff) << 24); /*start_right_bar_h*/
+	dc_writel(adev, val, (DC_HDMI_AVI_CONT3_REG + (index*0x10)));
+
+	dc_writel(adev, HDMI_AVI_ENABLE_PACKET | HDMI_AVI_FREQ_EACH_FRAME | HDMI_AVI_UPDATE,
+		    (DC_HDMI_AVI_CTRL_REG + (index*0x10)));
+
+	return 0;
+}
+
+void dc_hdmi_encoder_enable(struct drm_encoder *encoder)
+{
+	struct gsgpu_device *adev = encoder->dev->dev_private;
+	struct drm_display_mode *mode = &encoder->crtc->state->adjusted_mode;
+	u32 value;
+	u32 index = encoder->index;
+
+	/* hdmi enable */
+	value = dc_readl(adev, DC_HDMI_CTRL_REG + (index*0x10));
+	value |= (1 << 0);
+	dc_writel(adev, value, DC_HDMI_CTRL_REG + (index*0x10));
+	DRM_INFO("hdmi encoder enable %d CTRL reg 0x%x\n",
+		 encoder->index, value);
+
+	hdmi_set_avi_infoframe(encoder, mode);
+}
+
+void gsgpu_hdmi_suspend(struct gsgpu_device *adev)
+{
+	u32 link = 0;
+
+	adev->dc->hdmi_ctrl_reg = dc_readl(adev, DC_HDMI_CTRL_REG);
+	for (link = 0; link < 2; link++) {
+		dc_crtc_enable(adev->mode_info.crtcs[link], false);
+		dc_writel(adev, DC_HDMI_CTRL_REG + (link * 0x10), 0);
+		dc_writel(adev, DC_HDMI_ZONEIDLE_REG + (link * 0x10), 0);
+	}
+}
+
+int gsgpu_hdmi_resume(struct gsgpu_device *adev)
+{
+	u32 link = 0;
+	u32 reg_val;
+
+	for (link = 0; link < 2; link++) {
+		dc_crtc_enable(adev->mode_info.crtcs[link], true);
+		dc_writel(adev, DC_HDMI_CTRL_REG + (link * 0x10), adev->dc->hdmi_ctrl_reg);
+		dc_writel(adev, DC_HDMI_ZONEIDLE_REG + (link * 0x10), 0x00400040);
+
+		dc_writel(adev, DC_HDMI_AUDIO_NCFG_REG + (link * 0x10), 6272);
+		dc_writel(adev, DC_HDMI_AUDIO_CTSCFG_REG + (link * 0x10), 0x80000000);
+		dc_writel(adev, DC_HDMI_AUDIO_INFOFRAME_REG + (link * 0x10), 0x11);
+		reg_val = dc_readl(adev, DC_HDMI_AUDIO_INFOFRAME_REG + (link * 0x10));
+		reg_val |= 0x4;
+		dc_writel(adev, DC_HDMI_AUDIO_INFOFRAME_REG + (link * 0x10), reg_val);
+		dc_writel(adev, DC_HDMI_AUDIO_SAMPLE_REG + (link * 0x10), 0x1);
+	}
+
+	return 0;
+}
+
+int gsgpu_hdmi_init(struct gsgpu_device *adev)
+{
+	u32 val, val1, i;
+	u32 link = 0;
+
+	for (link = 0; link < 2; link++) {
+		/* enable hdmi */
+		dc_writel(adev, DC_HDMI_CTRL_REG + (link * 0x10), 0x287);
+
+		/* hdmi zone idle */
+		dc_writel(adev, DC_HDMI_ZONEIDLE_REG + (link * 0x10), 0x00400040);
+
+		//Audio N
+		// 44.1KHz * 4, dynamic update N && CTS value
+		dc_writel(adev, DC_HDMI_AUDIO_NCFG_REG + (link * 0x10), 6272);
+
+		//Enable Send CTS
+		dc_writel(adev, DC_HDMI_AUDIO_CTSCFG_REG + (link * 0x10), 0x80000000);
+
+		//Audio AIF
+		//enable AIF,set freq,and set CC = 1, CA = 0
+		dc_writel(adev, DC_HDMI_AUDIO_INFOFRAME_REG + (link * 0x10), 0x11);
+
+		//Update AIF
+		val = dc_readl(adev, DC_HDMI_AUDIO_INFOFRAME_REG + (link * 0x10));
+		val |= 0x4;
+		dc_writel(adev, DC_HDMI_AUDIO_INFOFRAME_REG + (link * 0x10), val);
+
+		//Audio Sample Packet
+		dc_writel(adev, DC_HDMI_AUDIO_SAMPLE_REG + (link * 0x10), 0x1);
+	}
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_i2c.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_i2c.c
new file mode 100644
index 000000000000..697cbb14f66c
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_i2c.c
@@ -0,0 +1,433 @@
+#include <linux/i2c.h>
+#include <linux/i2c-algo-bit.h>
+
+#include "gsgpu.h"
+#include "gsgpu_dc.h"
+#include "gsgpu_dc_connector.h"
+#include "gsgpu_dc_crtc.h"
+#include "gsgpu_dc_encoder.h"
+#include "gsgpu_dc_i2c.h"
+#include "gsgpu_dc_hdmi.h"
+#include "gsgpu_dc_reg.h"
+
+static void dc_gpio_set_dir(struct gsgpu_device *adev,
+			    unsigned int pin, int input)
+{
+	u32 temp;
+
+	temp = dc_readl_locked(adev, DC_GPIO_CFG_OFFSET);
+	if (input)
+		temp |= 1UL << pin;
+	else
+		temp &= ~(1UL << pin);
+	dc_writel_locked(adev, DC_GPIO_CFG_OFFSET, temp);
+}
+
+static void dc_gpio_set_val(struct gsgpu_device *adev,
+			    unsigned int pin, int high)
+{
+	u32 temp;
+
+	temp = dc_readl_locked(adev, DC_GPIO_OUT_OFFSET);
+	if (high)
+		temp |= 1UL << pin;
+	else
+		temp &= ~(1UL << pin);
+	dc_writel_locked(adev, DC_GPIO_OUT_OFFSET, temp);
+}
+
+static void gsgpu_gpio_set_data(void *i2c, int value)
+{
+	struct gsgpu_dc_i2c *li2c = i2c;
+	struct gsgpu_device *adev = li2c->adev;
+	unsigned int pin = li2c->data;
+
+	if (value)
+		dc_gpio_set_dir(adev, pin, 1);
+	else {
+		dc_gpio_set_val(adev, pin, 0);
+		dc_gpio_set_dir(adev, pin, 0);
+	}
+}
+
+static void gsgpu_gpio_set_clock(void *i2c, int value)
+{
+	struct gsgpu_dc_i2c *li2c = i2c;
+	struct gsgpu_device *adev = li2c->adev;
+	unsigned int pin = li2c->clock;
+
+	if (value)
+		dc_gpio_set_dir(adev, pin, 1);
+	else {
+		dc_gpio_set_val(adev, pin, 0);
+		dc_gpio_set_dir(adev, pin, 0);
+	}
+}
+
+static int gsgpu_gpio_get_data(void *i2c)
+{
+	int val;
+	struct gsgpu_dc_i2c *li2c = i2c;
+	struct gsgpu_device *adev = li2c->adev;
+	unsigned int pin = li2c->data;
+
+	val = dc_readl_locked(adev, DC_GPIO_IN_OFFSET);
+
+	return (val >> pin) & 1;
+}
+
+static int gsgpu_gpio_get_clock(void *i2c)
+{
+	int val;
+	struct gsgpu_dc_i2c *li2c = i2c;
+	struct gsgpu_device *adev = li2c->adev;
+	unsigned int pin = li2c->clock;
+
+	val = dc_readl_locked(adev, DC_GPIO_IN_OFFSET);
+
+	return (val >> pin) & 1;
+}
+
+void gsgpu_dc_i2c_irq(struct gsgpu_dc_i2c *i2c)
+{
+	unsigned char int_flag;
+
+	int_flag = dc_readb(DC_I2C_SR_REG);
+
+	if (int_flag & SR_IF) {
+		dc_writeb(CR_IACK, DC_I2C_CR_REG);
+		if (!(int_flag & SR_TIP))
+			complete(&i2c->cmd_complete);
+	}
+}
+
+static int i2c_stop(struct gsgpu_dc_i2c *i2c)
+{
+	unsigned long complete;
+
+again:
+	dc_writeb(CR_STOP, DC_I2C_CR_REG);
+	complete = wait_for_completion_timeout(&i2c->cmd_complete,
+			(&i2c->adapter)->timeout);
+	if (!complete) {
+		DRM_ERROR("Timeout abort message cmd\n");
+		return -1;
+	}
+
+	dc_readb(DC_I2C_SR_REG);
+	while (dc_readb(DC_I2C_SR_REG) & SR_BUSY)
+		goto again;
+
+	return 0;
+}
+
+static int i2c_start(struct gsgpu_dc_i2c *i2c, int dev_addr, int flags)
+{
+	unsigned long complete;
+	unsigned char addr = (dev_addr & 0x7f) << 1;
+	int retry = 5;
+
+	addr |= (flags & I2C_M_RD) ? 1:0;
+start:
+	mdelay(1);
+	dc_writeb(addr, DC_I2C_TXR_REG);
+	dc_writeb((CR_START | CR_WRITE), DC_I2C_CR_REG);
+
+	complete = wait_for_completion_timeout(&i2c->cmd_complete,
+			(&i2c->adapter)->timeout);
+	if (!complete) {
+		DRM_ERROR("Timeout abort message cmd\n");
+		return -1;
+	}
+
+	if (dc_readb(DC_I2C_SR_REG) & SR_NOACK) {
+		if (i2c_stop(i2c) < 0)
+			return -1;
+		while (retry--)
+			goto start;
+		return 0;
+	}
+
+	return 1;
+}
+static int i2c_read(struct gsgpu_dc_i2c *i2c, unsigned char *buf, int count)
+{
+	int i;
+	unsigned long complete;
+
+	for (i = 0; i < count; i++) {
+		dc_writeb((i == count - 1) ? (CR_READ | CR_ACK) : CR_READ,
+			    DC_I2C_CR_REG);
+		complete = wait_for_completion_timeout(&i2c->cmd_complete,
+				(&i2c->adapter)->timeout);
+		if (!complete) {
+			DRM_ERROR("Timeout abort message cmd\n");
+			return -1;
+		}
+
+		buf[i] = dc_readb(DC_I2C_RXR_REG);
+	}
+
+	return i;
+}
+
+static int i2c_write(struct gsgpu_dc_i2c *i2c, unsigned char *buf, int count)
+{
+	int i;
+	unsigned long complete;
+
+	for (i = 0; i < count; i++) {
+		dc_writeb(buf[i], DC_I2C_TXR_REG);
+		dc_writeb(CR_WRITE, DC_I2C_CR_REG);
+
+		complete = wait_for_completion_timeout(&i2c->cmd_complete,
+				(&i2c->adapter)->timeout);
+		if (!complete) {
+			DRM_ERROR("Timeout abort message cmd\n");
+			return -1;
+		}
+
+		if (dc_readb(DC_I2C_SR_REG) & SR_NOACK) {
+			DRM_ERROR("gsgpu dc i2c device no ack\n");
+			if (i2c_stop(i2c) < 0)
+				return -1;
+			return 0;
+		}
+	}
+
+	return i;
+}
+
+static int i2c_doxfer(struct gsgpu_dc_i2c *i2c,	struct i2c_msg *msgs, int num)
+{
+	struct i2c_msg *msg = msgs;
+	int ret;
+	int i;
+
+	for (i = 0; i < num; i++) {
+		reinit_completion(&i2c->cmd_complete);
+		ret = i2c_start(i2c, msg->addr, msg->flags);
+		if (ret <= 0)
+			return ret;
+
+		if (msg->flags & I2C_M_RD) {
+			if (i2c_read(i2c, msg->buf, msg->len) < 0)
+				return -1;
+		} else {
+			if (i2c_write(i2c, msg->buf, msg->len) < 0)
+				return -1;
+		}
+
+		++msg;
+		if (i2c_stop(i2c) < 0)
+			return -1;
+	}
+
+	if (i2c_stop(i2c) < 0)
+		return -1;
+
+	return i;
+}
+
+static int gsgpu_dc_i2c_xfer(struct i2c_adapter *adapter,
+			      struct i2c_msg *msgs, int num)
+{
+	struct gsgpu_dc_i2c *i2c = i2c_get_adapdata(adapter);
+	int retry;
+	int ret;
+
+	for (retry = 0; retry < adapter->retries; retry++) {
+		ret = i2c_doxfer(i2c, msgs, num);
+		if (ret != -EAGAIN)
+			return ret;
+
+		udelay(100);
+	}
+
+	return -EREMOTEIO;
+}
+
+static u32 gsgpu_dc_i2c_func(struct i2c_adapter *adapter)
+{
+	return I2C_FUNC_I2C | I2C_FUNC_SMBUS_EMUL;
+}
+
+static const struct i2c_algorithm gsgpu_dc_i2c_algo = {
+	.master_xfer = gsgpu_dc_i2c_xfer,
+	.functionality = gsgpu_dc_i2c_func,
+};
+
+static int gsgpu_dc_i2c_init(struct gsgpu_device *adev,
+			     int i2c_addr, uint32_t link_index)
+{
+	struct gsgpu_dc_i2c *i2c;
+	struct i2c_client *ddc_client;
+	int ret = 0;
+	int value = 0;
+	const struct i2c_board_info ddc_info = {
+		.type = "ddc-dev",
+		.addr = i2c_addr,
+		.flags = I2C_CLASS_DDC,
+	};
+
+	i2c = kzalloc(sizeof(struct gsgpu_dc_i2c), GFP_KERNEL);
+	if (!i2c)
+		return -ENOMEM;
+
+	i2c->adapter.owner = THIS_MODULE;
+	i2c->adapter.class = I2C_CLASS_DDC;
+	i2c->adapter.algo = &gsgpu_dc_i2c_algo;
+	i2c->adapter.dev.parent = adev->ddev->dev;
+	i2c->adapter.nr = -1;
+	i2c->adapter.retries = 5;
+	i2c->adapter.timeout = msecs_to_jiffies(100);
+	i2c->reg_base = adev->loongson_dc_rmmio + DC_I2C_ADDR + link_index*0x10;
+	snprintf(i2c->adapter.name, sizeof(i2c->adapter.name), "DC-I2C:%d", link_index);
+
+	init_completion(&i2c->cmd_complete);
+
+	/* use dc i2c */
+	value = dc_readl(adev, CURRENT_REG(DC_HDMI_CTRL_REG, link_index));
+	value |= (1 << 8);
+	dc_writel(adev, CURRENT_REG(DC_HDMI_CTRL_REG, link_index), value);
+
+	/* config PRER and interrupt */
+	dc_writeb(dc_readb(DC_I2C_CTR_REG) & ~0x80, DC_I2C_CTR_REG);
+	dc_writeb(0x2c, DC_I2C_PRER_LO_REG);
+	dc_writeb(0x1, DC_I2C_PRER_HI_REG);
+	dc_writeb(dc_readb(DC_I2C_CTR_REG) | 0xc0, DC_I2C_CTR_REG);
+
+	/* enable i2c interrupt */
+	value = dc_readl(adev, DC_INT_REG);
+	value |= (1 << (link_index + 27));
+	dc_writel(adev, DC_INT_REG, value);
+
+	i2c_set_adapdata(&i2c->adapter, i2c);
+	ret = i2c_add_adapter(&i2c->adapter);
+	if (ret) {
+		DRM_ERROR("Failed to register hw i2c %d\n", link_index);
+		goto out_free;
+	}
+
+	ddc_client = i2c_new_device(&i2c->adapter, &ddc_info);
+	if (IS_ERR(ddc_client)) {
+		ret = PTR_ERR(ddc_client);
+		DRM_ERROR("Failed to create standard ddc client\n");
+		goto out_free;
+	}
+
+	i2c->ddc_client = ddc_client;
+	adev->i2c[link_index] = i2c;
+
+out_free:
+	if (ret) {
+		kfree(i2c);
+		adev->i2c[link_index] = NULL;
+	}
+
+	return ret;
+}
+
+static int gsgpu_dc_gpio_init(struct gsgpu_device *adev,
+			      int i2c_addr, uint32_t link_index)
+{
+	struct gsgpu_dc_i2c *i2c;
+	struct i2c_client *ddc_client;
+	struct i2c_algo_bit_data *i2c_algo_data;
+	int ret = 0;
+	int value = 0;
+	const struct i2c_board_info ddc_info = {
+		.type = "ddc-dev",
+		.addr = i2c_addr,
+		.flags = I2C_CLASS_DDC,
+	};
+
+	i2c = kzalloc(sizeof(struct gsgpu_dc_i2c), GFP_KERNEL);
+	if (!i2c)
+		return -ENOMEM;
+
+	i2c_algo_data = kzalloc(sizeof(struct i2c_algo_bit_data), GFP_KERNEL);
+	if (!i2c_algo_data) {
+		ret = -ENOMEM;
+		goto out_free;
+	}
+
+	i2c->adapter.owner = THIS_MODULE;
+	i2c->adapter.class = I2C_CLASS_DDC;
+	i2c->adapter.algo_data = i2c_algo_data;
+	i2c->adapter.dev.parent = adev->ddev->dev;
+	i2c->adapter.nr = -1;
+	snprintf(i2c->adapter.name, sizeof(i2c->adapter.name),
+		 "DC-GPIO-I2C:%d", link_index);
+
+	i2c->data = link_index * 2;
+	i2c->clock = link_index * 2 + 1;
+	i2c_algo_data->setsda = gsgpu_gpio_set_data;
+	i2c_algo_data->setscl = gsgpu_gpio_set_clock;
+	i2c_algo_data->getsda = gsgpu_gpio_get_data;
+	i2c_algo_data->getscl = gsgpu_gpio_get_clock;
+	i2c_algo_data->udelay = DC_I2C_TON;
+	i2c_algo_data->timeout = usecs_to_jiffies(2200);
+
+	/* use dc gpio */
+	value = dc_readl(adev, CURRENT_REG(DC_HDMI_CTRL_REG, link_index));
+	value &= ~(1 << 8);
+	dc_writel(adev, CURRENT_REG(DC_HDMI_CTRL_REG, link_index), value);
+
+	ret = i2c_bit_add_numbered_bus(&i2c->adapter);
+	if (ret)
+		goto free_algo_data;
+
+	i2c_algo_data->data = i2c;
+	i2c_set_adapdata(&i2c->adapter, i2c);
+
+	ddc_client = i2c_new_device(&i2c->adapter, &ddc_info);
+	if (IS_ERR(ddc_client)) {
+		ret = PTR_ERR(ddc_client);
+		DRM_ERROR("Failed to create standard ddc client\n");
+		goto free_algo_data;
+	}
+
+	i2c->ddc_client = ddc_client;
+	i2c->adev = adev;
+	adev->i2c[link_index] = i2c;
+
+	return ret;
+
+free_algo_data:
+	kfree(i2c_algo_data);
+out_free:
+	if (ret) {
+		kfree(i2c);
+		adev->i2c[link_index] = NULL;
+	}
+
+	return ret;
+}
+
+int gsgpu_i2c_init(struct gsgpu_device *adev, uint32_t link_index)
+{
+	int ret;
+	int i2c_addr;
+	struct gsgpu_link_info *link_info = &adev->dc->link_info[link_index];
+
+	i2c_addr = link_info->encoder->resource->chip_addr;
+	if (i2c_addr == 0 || i2c_addr == 0xff)
+		i2c_addr = DDC_ADDR;
+
+	if (adev->dc_revision != 2 && adev->dc_revision != 0x10) {
+		ret = gsgpu_dc_i2c_init(adev, i2c_addr, link_index);
+		if (ret)
+			return ret;
+		DRM_INFO("GSGPU DC init i2c %d addr 0x%x finish\n",
+			 link_index, i2c_addr);
+	} else {
+		ret = gsgpu_dc_gpio_init(adev, i2c_addr, link_index);
+		if (ret)
+			return ret;
+		DRM_INFO("GSGPU DC init gpio %d addr 0x%x finish\n",
+			 link_index, i2c_addr);
+	}
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_irq.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_irq.c
new file mode 100644
index 000000000000..450bcc33e6a6
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_irq.c
@@ -0,0 +1,906 @@
+#include "gsgpu.h"
+#include "gsgpu_dc_connector.h"
+#include "gsgpu_dc_crtc.h"
+#include "gsgpu_dc_resource.h"
+#include "gsgpu_dc_irq.h"
+#include "gsgpu_dc_reg.h"
+#include "gsgpu_dc_i2c.h"
+
+#define DM_IRQ_TABLE_LOCK(adev, flags) \
+	spin_lock_irqsave(&adev->dc->irq_handler_list_table_lock, flags)
+
+#define DM_IRQ_TABLE_UNLOCK(adev, flags) \
+	spin_unlock_irqrestore(&adev->dc->irq_handler_list_table_lock, flags)
+
+static void dc_handle_hpd_irq(void *param)
+{
+	struct gsgpu_connector *aconnector = (struct gsgpu_connector *)param;
+	struct drm_connector *connector = &aconnector->base;
+	struct drm_device *dev = connector->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+	enum drm_connector_status old_status;
+
+	mutex_lock(&aconnector->hpd_lock);
+	old_status = connector->status;
+	connector->status = drm_helper_probe_detect(connector, NULL, false);
+
+	if ((adev->vga_hpd_status == connector_status_connected)
+	    && (connector->index == 0))
+		connector->connector_type = DRM_MODE_CONNECTOR_VGA;
+	else
+		connector->connector_type = DRM_MODE_CONNECTOR_HDMIA;
+
+	if (old_status != connector->status)
+		drm_kms_helper_hotplug_event(dev);
+
+	mutex_unlock(&aconnector->hpd_lock);
+}
+
+static void dc_handle_i2c_irq(void *param)
+{
+	struct gsgpu_connector *aconnector = (struct gsgpu_connector *)param;
+	struct drm_connector *connector = &aconnector->base;
+	struct gsgpu_device *adev = connector->dev->dev_private;
+	struct gsgpu_dc_i2c *i2c = adev->i2c[connector->index];
+
+	gsgpu_dc_i2c_irq(i2c);
+}
+
+static void dc_handle_vsync_irq(void *interrupt_params)
+{
+	struct gsgpu_crtc *gsgpu_crtc = interrupt_params;
+	struct drm_device *dev = gsgpu_crtc->base.dev;
+	unsigned long flags;
+
+	if (gsgpu_crtc == NULL) {
+		DRM_DEBUG_DRIVER("CRTC is null, returning.\n");
+		return;
+	}
+
+	drm_handle_vblank(dev, gsgpu_crtc->crtc_id);
+	spin_lock_irqsave(&dev->event_lock, flags);
+
+	if (gsgpu_crtc->pflip_status != GSGPU_FLIP_SUBMITTED) {
+		DRM_DEBUG_DRIVER("gsgpu_crtc->pflip_status = %d !=GSGPU_FLIP_SUBMITTED(%d) on crtc:%d[%p] \n",
+						 gsgpu_crtc->pflip_status,
+						 GSGPU_FLIP_SUBMITTED,
+						 gsgpu_crtc->crtc_id,
+						 gsgpu_crtc);
+		spin_unlock_irqrestore(&dev->event_lock, flags);
+		return;
+	}
+
+	/* wakeup usersapce */
+	if (gsgpu_crtc->event) {
+		/* Update to correct count/ts if racing with vblank irq */
+		drm_crtc_accurate_vblank_count(&gsgpu_crtc->base);
+		drm_crtc_send_vblank_event(&gsgpu_crtc->base, gsgpu_crtc->event);
+		/* page flip completed. clean up */
+		gsgpu_crtc->event = NULL;
+
+	} else
+		WARN_ON(1);
+
+	gsgpu_crtc->pflip_status = GSGPU_FLIP_NONE;
+	spin_unlock_irqrestore(&dev->event_lock, flags);
+
+	DRM_DEBUG_DRIVER("%s - crtc :%d[%p], pflip_stat:GSGPU_FLIP_NONE\n",
+					__func__, gsgpu_crtc->crtc_id, gsgpu_crtc);
+
+	drm_crtc_vblank_put(&gsgpu_crtc->base);
+}
+
+static bool dc_i2c_int_enable(struct gsgpu_dc_crtc *crtc, bool enable)
+{
+	struct gsgpu_device *adev = crtc->dc->adev;
+	u32 link;
+	u32 value;
+
+	if (IS_ERR_OR_NULL(crtc))
+		return false;
+
+	link = crtc->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	value = dc_readl(adev, DC_INT_REG);
+	switch (link) {
+	case 0:
+		if (enable)
+			value |= DC_INT_I2C0_EN;
+		else {
+			value &= ~DC_INT_I2C0_EN;
+		}
+		break;
+	case 1:
+		if (enable)
+			value |= DC_INT_I2C1_EN;
+		else
+			value &= ~DC_INT_I2C1_EN;
+		break;
+	default:
+		return false;
+	}
+	dc_writel(adev, DC_INT_REG, value);
+
+	return true;
+}
+
+static bool dc_hpd_enable(struct gsgpu_dc_crtc *crtc, bool enable)
+{
+	struct gsgpu_device *adev = crtc->dc->adev;
+	struct gsgpu_connector *lconnector;
+	u32 link;
+	u32 int_reg;
+	u32 vga_cfg;
+
+	if (IS_ERR_OR_NULL(crtc))
+		return false;
+
+	link = crtc->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+	lconnector = adev->mode_info.connectors[link];
+
+	int_reg = dc_readl(adev, DC_INT_REG);
+	vga_cfg = dc_readl(adev, DC_VGA_HOTPULG_CFG);
+
+	switch (link) {
+	case 0:
+		if (enable) {
+			if (lconnector->base.polled == DRM_CONNECTOR_POLL_HPD) {
+				int_reg |= DC_INT_VGA_HOTPLUG_EN;
+				vga_cfg &= ~0x3;
+				vga_cfg |= 0x1;
+			} else {
+				int_reg &= ~DC_INT_VGA_HOTPLUG_EN;
+				vga_cfg = 0;
+			}
+			if (lconnector->base.polled)
+				int_reg |= DC_INT_HDMI0_HOTPLUG_EN;
+			else
+				int_reg &= ~DC_INT_HDMI0_HOTPLUG_EN;
+		} else {
+			vga_cfg &= ~0x3;
+			int_reg &= ~DC_INT_HDMI0_HOTPLUG_EN;
+			int_reg &= ~DC_INT_VGA_HOTPLUG_EN;
+		}
+		break;
+	case 1:
+		if (enable && lconnector->base.polled)
+			int_reg |= DC_INT_HDMI1_HOTPLUG_EN;
+		else
+			int_reg &= ~DC_INT_HDMI1_HOTPLUG_EN;
+		break;
+	default:
+		return false;
+	}
+
+	dc_writel(adev, DC_INT_REG, int_reg);
+	dc_writel(adev, DC_VGA_HOTPULG_CFG, vga_cfg);
+
+	return true;
+}
+
+static bool
+validate_irq_registration_params(struct dc_interrupt_params *int_params,
+				 void (*ih)(void *))
+{
+	if (NULL == int_params || NULL == ih) {
+		DRM_ERROR("DC_IRQ: invalid input!\n");
+		return false;
+	}
+
+	if (int_params->int_context >= INTERRUPT_CONTEXT_NUMBER) {
+		DRM_ERROR("DC_IRQ: invalid context: %d!\n",
+				int_params->int_context);
+		return false;
+	}
+
+	if (!DC_VALID_IRQ_SRC_NUM(int_params->irq_source)) {
+		DRM_ERROR("DC_IRQ: invalid irq_source: %d!\n",
+				int_params->irq_source);
+		return false;
+	}
+
+	return true;
+}
+
+static void *dc_irq_register_interrupt(struct gsgpu_device *adev,
+				       struct dc_interrupt_params *int_params,
+				       void (*ih)(void *),
+				       void *handler_args)
+{
+	struct list_head *hnd_list;
+	struct dc_irq_handler_data *handler_data;
+	unsigned long irq_table_flags;
+	enum dc_irq_source irq_source;
+
+	if (false == validate_irq_registration_params(int_params, ih))
+		return DAL_INVALID_IRQ_HANDLER_IDX;
+
+	handler_data = kzalloc(sizeof(*handler_data), GFP_KERNEL);
+	if (!handler_data) {
+		DRM_ERROR("DC_IRQ: failed to allocate irq handler!\n");
+		return DAL_INVALID_IRQ_HANDLER_IDX;
+	}
+
+	memset(handler_data, 0, sizeof(*handler_data));
+
+	handler_data->handler = ih;
+	handler_data->handler_arg = handler_args;
+
+	irq_source = int_params->irq_source;
+
+	handler_data->irq_source = irq_source;
+
+	DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+
+	switch (int_params->int_context) {
+	case INTERRUPT_HIGH_IRQ_CONTEXT:
+		hnd_list = &adev->dc->irq_handler_list_high_tab[irq_source];
+		break;
+	case INTERRUPT_LOW_IRQ_CONTEXT:
+	default:
+		hnd_list = &adev->dc->irq_handler_list_low_tab[irq_source].head;
+		break;
+	}
+
+	list_add_tail(&handler_data->list, hnd_list);
+
+	DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+
+	DRM_DEBUG_KMS(
+		"DC_IRQ: added irq handler: %p for: irq_src=%d, irq context=%d\n",
+		handler_data,
+		irq_source,
+		int_params->int_context);
+
+	return handler_data;
+}
+
+static void dc_register_vsync_handlers(struct gsgpu_device *adev)
+{
+	struct drm_device *dev = adev->ddev;
+	struct drm_crtc *crtc;
+	struct gsgpu_crtc *acrtc;
+	struct dc_interrupt_params int_params = {0};
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		acrtc = to_gsgpu_crtc(crtc);
+		int_params.int_context = INTERRUPT_HIGH_IRQ_CONTEXT;
+
+		if (DC_IRQ_SOURCE_INVALID != acrtc->irq_source_vsync) {
+			int_params.irq_source = acrtc->irq_source_vsync;
+			dc_irq_register_interrupt(adev, &int_params,
+					dc_handle_vsync_irq, (void *) acrtc);
+		}
+	}
+}
+
+static void dc_register_i2c_handlers(struct gsgpu_device *adev)
+{
+	struct drm_device *dev = adev->ddev;
+	struct drm_connector *connector;
+	struct gsgpu_connector *aconnector;
+	struct dc_interrupt_params int_params = {0};
+
+	list_for_each_entry(connector, &dev->mode_config.connector_list, head)	{
+		aconnector = to_gsgpu_connector(connector);
+		int_params.int_context = INTERRUPT_LOW_IRQ_CONTEXT;
+
+		if (DC_IRQ_SOURCE_INVALID != aconnector->irq_source_i2c) {
+			int_params.irq_source = aconnector->irq_source_i2c;
+			dc_irq_register_interrupt(adev, &int_params,
+					dc_handle_i2c_irq, (void *) aconnector);
+		}
+	}
+}
+
+static void dc_register_hpd_handlers(struct gsgpu_device *adev)
+{
+	struct drm_device *dev = adev->ddev;
+	struct drm_connector *connector;
+	struct gsgpu_connector *aconnector;
+	struct dc_interrupt_params int_params = {0};
+
+	list_for_each_entry(connector, &dev->mode_config.connector_list, head)	{
+		aconnector = to_gsgpu_connector(connector);
+		int_params.int_context = INTERRUPT_LOW_IRQ_CONTEXT;
+
+		if (DC_IRQ_SOURCE_INVALID != aconnector->irq_source_hpd) {
+			int_params.irq_source = aconnector->irq_source_hpd;
+			dc_irq_register_interrupt(adev, &int_params,
+					dc_handle_hpd_irq,
+					(void *) aconnector);
+		}
+
+		if (DC_IRQ_SOURCE_INVALID != aconnector->irq_source_vga_hpd
+		    && connector->index == 0) {
+			int_params.irq_source = aconnector->irq_source_vga_hpd;
+			dc_irq_register_interrupt(adev, &int_params,
+					dc_handle_hpd_irq,
+					(void *) aconnector);
+		}
+	}
+}
+
+static bool validate_irq_unregistration_params(enum dc_irq_source irq_source,
+					       irq_handler_idx handler_idx)
+{
+	if (DAL_INVALID_IRQ_HANDLER_IDX == handler_idx) {
+		DRM_ERROR("DC_IRQ: invalid handler_idx==NULL!\n");
+		return false;
+	}
+
+	if (!DC_VALID_IRQ_SRC_NUM(irq_source)) {
+		DRM_ERROR("DC_IRQ: invalid irq_source:%d!\n", irq_source);
+		return false;
+	}
+
+	return true;
+}
+
+static struct list_head *remove_irq_handler(struct gsgpu_device *adev,
+					    void *ih,
+					    const struct dc_interrupt_params *int_params)
+{
+	struct list_head *hnd_list;
+	struct list_head *entry, *tmp;
+	struct dc_irq_handler_data *handler;
+	unsigned long irq_table_flags;
+	bool handler_removed = false;
+	enum dc_irq_source irq_source;
+
+	DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+
+	irq_source = int_params->irq_source;
+
+	switch (int_params->int_context) {
+	case INTERRUPT_HIGH_IRQ_CONTEXT:
+		hnd_list = &adev->dc->irq_handler_list_high_tab[irq_source];
+		break;
+	case INTERRUPT_LOW_IRQ_CONTEXT:
+	default:
+		hnd_list = &adev->dc->irq_handler_list_low_tab[irq_source].head;
+		break;
+	}
+
+	list_for_each_safe(entry, tmp, hnd_list) {
+
+		handler = list_entry(entry, struct dc_irq_handler_data, list);
+
+		if (ih == handler) {
+			/* Found our handler. Remove it from the list. */
+			list_del(&handler->list);
+			handler_removed = true;
+			break;
+		}
+	}
+
+	DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+
+	if (handler_removed == false) {
+		/* Not necessarily an error - caller may not
+		 * know the context. */
+		return NULL;
+	}
+
+	kfree(handler);
+
+	DRM_DEBUG_KMS(
+	"DC_IRQ: removed irq handler: %p for: irq_src=%d, irq context=%d\n",
+		ih, int_params->irq_source, int_params->int_context);
+
+	return hnd_list;
+}
+
+static void dc_irq_work_func(struct work_struct *work)
+{
+	struct list_head *entry;
+	struct irq_list_head *irq_list_head =
+		container_of(work, struct irq_list_head, work);
+	struct list_head *handler_list = &irq_list_head->head;
+	struct dc_irq_handler_data *handler_data;
+
+	list_for_each(entry, handler_list) {
+		handler_data = list_entry(entry,
+				struct dc_irq_handler_data, list);
+
+		DRM_DEBUG_KMS("DC_IRQ: work_func: for irq_src=%d\n",
+				handler_data->irq_source);
+
+		DRM_DEBUG_KMS("DC_IRQ: schedule_work: for irq_src=%d\n",
+			handler_data->irq_source);
+
+		handler_data->handler(handler_data->handler_arg);
+	}
+}
+
+static void dc_irq_schedule_work(struct gsgpu_device *adev,
+					enum dc_irq_source irq_source)
+{
+	unsigned long irq_table_flags;
+	struct work_struct *work = NULL;
+
+	DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+
+	if (!list_empty(&adev->dc->irq_handler_list_low_tab[irq_source].head))
+		work = &adev->dc->irq_handler_list_low_tab[irq_source].work;
+
+	DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+
+	if (work) {
+		if (!schedule_work(work))
+			DRM_INFO("Irq schedule work FAILED src %d\n", irq_source);
+	}
+
+}
+
+static void dc_irq_immediate_work(struct gsgpu_device *adev,
+					 enum dc_irq_source irq_source)
+{
+	struct dc_irq_handler_data *handler_data;
+	struct list_head *entry;
+	unsigned long irq_table_flags;
+
+	DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+
+	list_for_each(entry, &adev->dc->irq_handler_list_high_tab[irq_source]) {
+		handler_data = list_entry(entry,
+				struct dc_irq_handler_data, list);
+
+		/* Call a subcomponent which registered for immediate
+		 * interrupt notification */
+		handler_data->handler(handler_data->handler_arg);
+	}
+
+	DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+}
+
+static enum dc_irq_source dc_interrupt_to_irq_source(u32 src_id)
+{
+	enum dc_irq_source dc_irq_source;
+
+	switch (src_id) {
+	case DC_INT_ID_VSYNC1:
+		dc_irq_source = DC_IRQ_SOURCE_VSYNC1;
+		break;
+	case DC_INT_ID_VSYNC0:
+		dc_irq_source = DC_IRQ_SOURCE_VSYNC0;
+		break;
+	case DC_INT_ID_I2C0:
+		dc_irq_source = DC_IRQ_SOURCE_I2C0;
+		break;
+	case DC_INT_ID_I2C1:
+		dc_irq_source = DC_IRQ_SOURCE_I2C1;
+		break;
+	case DC_INT_ID_HPD_HDMI0:
+		dc_irq_source = DC_IRQ_SOURCE_HPD_HDMI0;
+		break;
+	case DC_INT_ID_HPD_HDMI1:
+		dc_irq_source = DC_IRQ_SOURCE_HPD_HDMI1;
+		break;
+	case DC_INT_ID_HPD_VGA:
+		dc_irq_source = DC_IRQ_SOURCE_HPD_VGA;
+		break;
+	default:
+		DRM_ERROR("NO support this irq id:%d!\n", src_id);
+		break;
+	}
+
+	return dc_irq_source;
+}
+
+static bool dc_hpd_ack(struct gsgpu_dc_crtc *crtc)
+{
+	u32 link;
+	u32 value;
+	u32 vga_reg;
+	struct gsgpu_device *adev = crtc->dc->adev;
+
+	if (IS_ERR_OR_NULL(crtc))
+		return false;
+
+	link = crtc->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	value = dc_readl(adev, DC_INT_REG);
+
+	if (value & (1 << 15)) {
+		vga_reg = dc_readl(adev, DC_VGA_HOTPULG_CFG);
+		if ((vga_reg & DC_VGA_HPD_STATUS_MASK) == 1) {
+			adev->vga_hpd_status = connector_status_connected;
+			vga_reg &= ~0x3;
+			vga_reg |= 0x2;
+		} else if ((vga_reg & DC_VGA_HPD_STATUS_MASK) == 2) {
+			adev->vga_hpd_status = connector_status_disconnected;
+			vga_reg &= ~0x3;
+			vga_reg |= 0x1;
+		} else {
+			adev->vga_hpd_status = connector_status_disconnected;
+			dc_writel(adev, DC_VGA_HOTPULG_CFG, 0x0);
+			DRM_ERROR("Error VGA HPD status\n");
+			return false;
+		}
+		dc_writel(adev, DC_VGA_HOTPULG_CFG, vga_reg);
+	}
+
+	switch (link) {
+	case 0:
+		value |= (1 << 13);
+		value |= (1 << 15);
+		break;
+	case 1:
+		value |= (1 << 14);
+		break;
+	}
+
+	dc_writel(adev, DC_INT_REG, value);
+
+	return true;
+}
+
+static bool dc_i2c_ack(struct gsgpu_dc_crtc *crtc)
+{
+	u32 link;
+	u32 value;
+	struct gsgpu_device *adev = crtc->dc->adev;
+
+	if (IS_ERR_OR_NULL(crtc))
+		return false;
+
+	link = crtc->resource->base.link;
+	if (link >= DC_DVO_MAXLINK)
+		return false;
+
+	value = dc_readl(adev, DC_INT_REG);
+
+	switch (link) {
+	case 0:
+		value |= (1 << 11);
+		break;
+	case 1:
+		value |= (1 << 12);
+		break;
+	}
+
+	dc_writel(adev, DC_INT_REG, value);
+
+	return true;
+}
+
+static bool dc_submit_interrupt_ack(struct gsgpu_dc *dc, enum dc_irq_source src)
+{
+	bool ret = false;
+
+	if (IS_ERR_OR_NULL(dc))
+		return false;
+
+	switch (src) {
+	case DC_IRQ_SOURCE_VSYNC0:
+		if (dc->link_info)
+			ret = dc_crtc_vblank_ack(dc->link_info[0].crtc);
+		break;
+	case DC_IRQ_SOURCE_VSYNC1:
+		if (dc->link_info)
+			ret = dc_crtc_vblank_ack(dc->link_info[1].crtc);
+		break;
+	case DC_IRQ_SOURCE_I2C0:
+		if (dc->link_info)
+			ret = dc_i2c_ack(dc->link_info[0].crtc);
+		break;
+	case DC_IRQ_SOURCE_I2C1:
+		if (dc->link_info)
+			ret = dc_i2c_ack(dc->link_info[1].crtc);
+		break;
+	case DC_IRQ_SOURCE_HPD_HDMI0:
+	case DC_IRQ_SOURCE_HPD_HDMI1:
+		if (dc->link_info)
+			ret = dc_hpd_ack(dc->link_info[0].crtc);
+		break;
+	case DC_IRQ_SOURCE_HPD_VGA:
+		if (dc->link_info)
+			ret = dc_hpd_ack(dc->link_info[1].crtc);
+		break;
+	default:
+		DRM_ERROR("%s Can not support this irq %d \n", __func__, src);
+		ret = false;
+		break;
+	}
+
+	return ret;
+}
+
+static int dc_irq_handler(struct gsgpu_device *adev,
+				 struct gsgpu_irq_src *source,
+				 struct gsgpu_iv_entry *entry)
+{
+	enum dc_irq_source src =
+		dc_interrupt_to_irq_source(entry->src_id);
+
+	dc_submit_interrupt_ack(adev->dc, src);
+
+	/* Call high irq work immediately */
+	dc_irq_immediate_work(adev, src);
+
+	/*Schedule low_irq work */
+	dc_irq_schedule_work(adev, src);
+
+	return 0;
+}
+
+static inline int dc_irq_state(struct gsgpu_device *adev,
+			       struct gsgpu_irq_src *source,
+			       unsigned crtc_id,
+			       enum gsgpu_interrupt_state state,
+			       const enum irq_type dc_irq_type,
+			       const char *func)
+{
+	struct gsgpu_crtc *acrtc = adev->mode_info.crtcs[crtc_id];
+	enum dc_irq_source irq_source;
+	bool st;
+
+	if (!acrtc) {
+		DRM_ERROR("%s: crtc is NULL at id :%d\n", func, crtc_id);
+		return 0;
+	}
+
+	if (acrtc->crtc_id == -1)
+		return 0;
+
+	irq_source = dc_irq_type + crtc_id;
+	st = (state == GSGPU_IRQ_STATE_ENABLE);
+
+	DRM_DEBUG_DRIVER("dc irq state crtc_id:%d, irq_src:%d, st:%d\n",
+			 crtc_id, irq_source, st);
+	dc_interrupt_enable(adev->dc, irq_source, st);
+
+	return 0;
+}
+
+static int dc_set_pflip_irq_state(struct gsgpu_device *adev,
+				  struct gsgpu_irq_src *source,
+				  unsigned crtc_id,
+				  enum gsgpu_interrupt_state state)
+{
+	return dc_irq_state(adev, source, crtc_id, state, DC_IRQ_TYPE_VSYNC,
+			    __func__);
+}
+
+static int dc_set_i2c_irq_state(struct gsgpu_device *adev,
+				struct gsgpu_irq_src *source,
+				unsigned crtc_id,
+				enum gsgpu_interrupt_state state)
+{
+	return dc_irq_state(adev, source, crtc_id, state, DC_IRQ_TYPE_I2C,
+			    __func__);
+}
+
+static int dc_set_hpd_irq_state(struct gsgpu_device *adev,
+				struct gsgpu_irq_src *source,
+				unsigned crtc_id,
+				enum gsgpu_interrupt_state state)
+{
+	return dc_irq_state(adev, source, crtc_id, state, DC_IRQ_TYPE_HPD,
+			    __func__);
+}
+
+static const struct gsgpu_irq_src_funcs dc_vsync_irq_funcs = {
+	.set = dc_set_pflip_irq_state,
+	.process = dc_irq_handler,
+};
+
+static const struct gsgpu_irq_src_funcs dc_i2c_irq_funcs = {
+	.set = dc_set_i2c_irq_state,
+	.process = dc_irq_handler,
+};
+
+static const struct gsgpu_irq_src_funcs dc_hpd_irq_funcs = {
+	.set = dc_set_hpd_irq_state,
+	.process = dc_irq_handler,
+};
+
+void dc_set_irq_funcs(struct gsgpu_device *adev)
+{
+	adev->vsync_irq.num_types = adev->mode_info.num_crtc;
+	adev->vsync_irq.funcs = &dc_vsync_irq_funcs;
+
+	adev->i2c_irq.num_types = adev->mode_info.num_i2c;
+	adev->i2c_irq.funcs = &dc_i2c_irq_funcs;
+
+	adev->hpd_irq.num_types = adev->mode_info.num_hpd;
+	adev->hpd_irq.funcs = &dc_hpd_irq_funcs;
+}
+
+bool dc_interrupt_enable(struct gsgpu_dc *dc, enum dc_irq_source src, bool enable)
+{
+	bool ret = false;
+
+	if (IS_ERR_OR_NULL(dc))
+		return false;
+
+	switch (src) {
+	case DC_IRQ_SOURCE_VSYNC0:
+		if (dc->link_info)
+			ret = dc_crtc_vblank_enable(dc->link_info[0].crtc, enable);
+		break;
+	case DC_IRQ_SOURCE_VSYNC1:
+		if (dc->link_info)
+			ret = dc_crtc_vblank_enable(dc->link_info[1].crtc, enable);
+		break;
+	case DC_IRQ_SOURCE_I2C0:
+		if (dc->link_info)
+			ret = dc_i2c_int_enable(dc->link_info[0].crtc, enable);
+		break;
+	case DC_IRQ_SOURCE_I2C1:
+		if (dc->link_info)
+			ret = dc_i2c_int_enable(dc->link_info[1].crtc, enable);
+		break;
+	case DC_IRQ_SOURCE_HPD_HDMI0:
+	case DC_IRQ_SOURCE_HPD_VGA:
+		if (dc->link_info)
+			ret = dc_hpd_enable(dc->link_info[0].crtc, enable);
+		break;
+	case DC_IRQ_SOURCE_HPD_HDMI1:
+		if (dc->link_info)
+			ret = dc_hpd_enable(dc->link_info[1].crtc, enable);
+		break;
+	case DC_IRQ_SOURCE_HPD_HDMI0_NULL:
+		if (dc->link_info)
+			ret = dc_hpd_enable(dc->link_info[0].crtc, false);
+		break;
+	case DC_IRQ_SOURCE_HPD_HDMI1_NULL:
+		if (dc->link_info)
+			ret = dc_hpd_enable(dc->link_info[1].crtc, false);
+		break;
+	default:
+		DRM_ERROR("%s Can not support this irq %d \n", __func__, src);
+		break;
+	}
+
+	return ret;
+}
+
+void gsgpu_dc_hpd_init(struct gsgpu_device *adev)
+{
+	struct drm_device *dev = adev->ddev;
+	struct drm_connector *connector;
+	struct gsgpu_connector *aconnector;
+
+	list_for_each_entry(connector, &dev->mode_config.connector_list, head) {
+		aconnector = to_gsgpu_connector(connector);
+		dc_interrupt_enable(adev->dc, aconnector->irq_source_hpd, true);
+	}
+
+	adev->vga_hpd_status = connector_status_unknown;
+
+	return;
+}
+
+void gsgpu_dc_hpd_disable(struct gsgpu_device *adev)
+{
+	struct drm_device *dev = adev->ddev;
+	struct drm_connector *connector;
+	struct gsgpu_connector *aconnector;
+
+	if (adev->chip == dev_7a2000) {
+		list_for_each_entry(connector, &dev->mode_config.connector_list, head) {
+			aconnector = to_gsgpu_connector(connector);
+			dc_interrupt_enable(adev->dc, aconnector->irq_source_hpd, false);
+		}
+		adev->vga_hpd_status = connector_status_unknown;
+	} else if (adev->chip == dev_2k2000)
+		return;
+
+	return;
+}
+
+int dc_register_irq_handlers(struct gsgpu_device *adev)
+{
+	int r;
+	int i;
+	unsigned client_id = SOC15_IH_CLIENTID_DCE;
+
+	/* vsync */
+	for (i = DC_INT_ID_VSYNC1; i <= DC_INT_ID_VSYNC0; i += 2) {
+		r = gsgpu_irq_add_id(adev, client_id, i, &adev->vsync_irq);
+		if (r) {
+			DRM_ERROR("Failed to add page flip irq id!\n");
+			return r;
+		}
+	}
+	dc_register_vsync_handlers(adev);
+
+	/* I2C */
+	for (i = DC_INT_ID_I2C0; i <= DC_INT_ID_I2C1; i++) {
+		r = gsgpu_irq_add_id(adev, client_id, i, &adev->i2c_irq);
+		if (r) {
+			DRM_ERROR("Failed to add page flip irq id!\n");
+			return r;
+		}
+	}
+	dc_register_i2c_handlers(adev);
+
+	/* HPD */
+	for (i = DC_INT_ID_HPD_HDMI0; i <= DC_INT_ID_HPD_VGA; i++) {
+		r = gsgpu_irq_add_id(adev, client_id, i, &adev->hpd_irq);
+		if (r) {
+			DRM_ERROR("Failed to add hpd irq id %d!\n", i);
+			return r;
+		}
+	}
+	dc_register_hpd_handlers(adev);
+
+	return 0;
+}
+
+void gsgpu_dc_irq_unregister_interrupt(struct gsgpu_device *adev,
+					enum dc_irq_source irq_source,
+					void *ih)
+{
+	struct list_head *handler_list;
+	struct dc_interrupt_params int_params;
+	int i;
+
+	if (false == validate_irq_unregistration_params(irq_source, ih))
+		return;
+
+	memset(&int_params, 0, sizeof(int_params));
+
+	int_params.irq_source = irq_source;
+
+	for (i = 0; i < INTERRUPT_CONTEXT_NUMBER; i++) {
+		int_params.int_context = i;
+		handler_list = remove_irq_handler(adev, ih, &int_params);
+		if (handler_list != NULL)
+			break;
+	}
+
+	if (handler_list == NULL) {
+		/* If we got here, it means we searched all irq contexts
+		 * for this irq source, but the handler was not found. */
+		DRM_ERROR(
+		"DM_IRQ: failed to find irq handler:%p for irq_source:%d!\n",
+			ih, irq_source);
+	}
+}
+
+int gsgpu_dc_irq_init(struct gsgpu_device *adev)
+{
+	int src;
+	struct irq_list_head *lh;
+
+	spin_lock_init(&adev->dc->irq_handler_list_table_lock);
+
+	for (src = 0; src < DC_IRQ_SOURCES_NUMBER; src++) {
+		/* low context handler list init */
+		lh = &adev->dc->irq_handler_list_low_tab[src];
+		INIT_LIST_HEAD(&lh->head);
+		INIT_WORK(&lh->work, dc_irq_work_func);
+
+		/* high context handler init */
+		INIT_LIST_HEAD(&adev->dc->irq_handler_list_high_tab[src]);
+	}
+
+	DRM_INFO("GSGPU DC irq init sources number:%d\n", src - 1);
+	return 0;
+}
+
+void gsgpu_dc_irq_fini(struct gsgpu_device *adev)
+{
+	int src;
+	struct irq_list_head *lh;
+	unsigned long irq_table_flags;
+
+	for (src = 0; src < DC_IRQ_SOURCES_NUMBER; src++) {
+		DM_IRQ_TABLE_LOCK(adev, irq_table_flags);
+		/* The handler was removed from the table,
+		 * it means it is safe to flush all the 'work'
+		 * (because no code can schedule a new one). */
+		lh = &adev->dc->irq_handler_list_low_tab[src];
+		DM_IRQ_TABLE_UNLOCK(adev, irq_table_flags);
+		flush_work(&lh->work);
+	}
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_plane.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_plane.c
new file mode 100644
index 000000000000..325daa403ac7
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_plane.c
@@ -0,0 +1,195 @@
+#include <drm/drm_atomic_helper.h>
+#include "gsgpu.h"
+#include "gsgpu_display.h"
+#include "gsgpu_dc_plane.h"
+
+static const uint32_t rgb_formats[] = {
+	DRM_FORMAT_RGB565,
+	DRM_FORMAT_RGB888,
+	DRM_FORMAT_XRGB8888,
+	DRM_FORMAT_ARGB8888,
+};
+
+static const u32 cursor_formats[] = {
+	DRM_FORMAT_ARGB8888
+};
+
+static int dc_plane_helper_prepare_fb(struct drm_plane *plane,
+				      struct drm_plane_state *new_state)
+{
+	struct gsgpu_framebuffer *afb;
+	struct drm_gem_object *obj;
+	struct gsgpu_device *adev;
+	struct gsgpu_bo *rbo;
+	uint32_t domain;
+	u64 fb_addr;
+	void *fb_vaddr;
+	int r;
+
+	if (!new_state->fb) {
+		DRM_DEBUG_DRIVER("No FB bound\n");
+		return 0;
+	}
+
+	afb = to_gsgpu_framebuffer(new_state->fb);
+	obj = new_state->fb->obj[0];
+	rbo = gem_to_gsgpu_bo(obj);
+	adev = gsgpu_ttm_adev(rbo->tbo.bdev);
+	r = gsgpu_bo_reserve(rbo, false);
+	if (unlikely(r != 0))
+		return r;
+
+	if (plane->type != DRM_PLANE_TYPE_CURSOR)
+		domain = gsgpu_display_supported_domains(adev);
+	else
+		domain = GSGPU_GEM_DOMAIN_VRAM;
+
+	r = gsgpu_bo_pin(rbo, domain);
+	if (unlikely(r != 0)) {
+		if (r != -ERESTARTSYS)
+			DRM_ERROR("Failed to pin framebuffer with error %d\n", r);
+		gsgpu_bo_unreserve(rbo);
+		return r;
+	}
+
+	r = gsgpu_bo_kmap(rbo, &fb_vaddr);
+	if (unlikely(r != 0)) {
+		gsgpu_bo_unpin(rbo);
+		gsgpu_bo_unreserve(rbo);
+		DRM_ERROR("%p kmap failed\n", rbo);
+		return r;
+	}
+
+	if (gsgpu_using_ram) {
+		fb_addr = virt_to_phys(fb_vaddr);
+		fb_addr = (fb_addr & 0x1ffffffffffffULL);
+		afb->address = fb_addr;
+	} else {
+		afb->address = gsgpu_bo_gpu_offset(rbo);
+	}
+
+	if (plane->type == DRM_PLANE_TYPE_PRIMARY) {
+		DRM_DEBUG_DRIVER("fb kernel virtual addr: %p\n", fb_vaddr);
+		DRM_DEBUG_DRIVER("fb physical addr: 0x%llx\n", afb->address);
+	}
+
+	gsgpu_bo_unreserve(rbo);
+	gsgpu_bo_ref(rbo);
+
+	return 0;
+}
+
+static void dc_plane_helper_cleanup_fb(struct drm_plane *plane,
+				       struct drm_plane_state *old_state)
+{
+	struct gsgpu_bo *rbo;
+	int r;
+
+	if (!old_state->fb)
+		return;
+
+	rbo = gem_to_gsgpu_bo(old_state->fb->obj[0]);
+	r = gsgpu_bo_reserve(rbo, false);
+	if (unlikely(r)) {
+		DRM_ERROR("failed to reserve rbo before unpin\n");
+		return;
+	}
+
+	gsgpu_bo_unpin(rbo);
+	gsgpu_bo_unreserve(rbo);
+	gsgpu_bo_unref(&rbo);
+}
+
+static int dc_plane_atomic_check(struct drm_plane *plane,
+				 struct drm_plane_state *state)
+{
+	return 0;
+}
+
+static const struct drm_plane_helper_funcs dc_plane_helper_funcs = {
+	.prepare_fb = dc_plane_helper_prepare_fb,
+	.cleanup_fb = dc_plane_helper_cleanup_fb,
+	.atomic_check = dc_plane_atomic_check,
+};
+
+static void dc_plane_destroy(struct drm_plane *plane)
+{
+	struct gsgpu_plane *p = to_gsgpu_plane(plane);
+
+	drm_plane_cleanup(plane);
+	kfree(p);
+}
+
+static const struct drm_plane_funcs dc_plane_funcs = {
+	.update_plane = drm_atomic_helper_update_plane,
+	.disable_plane = drm_atomic_helper_disable_plane,
+	.destroy = dc_plane_destroy,
+	.reset = drm_atomic_helper_plane_reset,
+	.atomic_duplicate_state = drm_atomic_helper_plane_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_plane_destroy_state,
+};
+
+int gsgpu_dc_plane_init(struct gsgpu_device *adev,
+			struct gsgpu_plane *aplane,
+			unsigned long possible_crtcs)
+{
+	int res = -EPERM;
+
+	switch (aplane->base.type) {
+	case DRM_PLANE_TYPE_PRIMARY:
+		res = drm_universal_plane_init(
+				adev->ddev,
+				&aplane->base,
+				possible_crtcs,
+				&dc_plane_funcs,
+				rgb_formats,
+				ARRAY_SIZE(rgb_formats),
+				NULL, aplane->base.type, NULL);
+		break;
+	case DRM_PLANE_TYPE_CURSOR:
+		res = drm_universal_plane_init(
+				adev->ddev,
+				&aplane->base,
+				possible_crtcs,
+				&dc_plane_funcs,
+				cursor_formats,
+				ARRAY_SIZE(cursor_formats),
+				NULL, aplane->base.type, NULL);
+		break;
+	default:
+		break;
+	}
+
+	drm_plane_helper_add(&aplane->base, &dc_plane_helper_funcs);
+
+	return res;
+}
+
+int initialize_plane(struct gsgpu_device *adev,
+		     struct gsgpu_mode_info *mode_info,
+		     int plane_id)
+{
+	struct gsgpu_plane *plane;
+	unsigned long possible_crtcs;
+	int ret = 0;
+
+	plane = kzalloc(sizeof(struct gsgpu_plane), GFP_KERNEL);
+	mode_info->planes[plane_id] = plane;
+
+	if (!plane) {
+		DRM_ERROR("KMS: Failed to allocate plane\n");
+		return -ENOMEM;
+	}
+	plane->base.type = mode_info->plane_type[plane_id];
+
+	possible_crtcs = 1 << plane_id;
+
+	ret = gsgpu_dc_plane_init(adev, mode_info->planes[plane_id], possible_crtcs);
+
+	if (ret) {
+		DRM_ERROR("KMS: Failed to initialize plane\n");
+		return ret;
+	}
+
+	return ret;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_vbios.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_vbios.c
new file mode 100644
index 000000000000..9dde574b9b47
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_dc_vbios.c
@@ -0,0 +1,1147 @@
+#include "gsgpu.h"
+#include "gsgpu_dc_vbios.h"
+#include "gsgpu_dc_resource.h"
+
+#define VBIOS_START 0x1000
+#define VBIOS_SIZE 0x40000
+#define VBIOS_OFFSET 0x100000
+#define VBIOS_DESC_OFFSET 0x6000
+#define VBIOS_DESC_TOTAL 0xA00
+#define LOONGSON_VBIOS_TITLE "Loongson-VBIOS"
+
+static bool is_valid_vbios(void *vbios)
+{
+	struct vbios_info *vb_header = NULL;
+	u8 header[16] = {0};
+
+	vb_header = (struct vbios_info *)vbios;
+	memcpy(&header[0], vb_header->title, sizeof(vb_header->title));
+
+	if (0 != memcmp((char *)&header[0],
+			LOONGSON_VBIOS_TITLE,
+			strlen(LOONGSON_VBIOS_TITLE))) {
+		DRM_WARN("vbios signature is invation!\n");
+		return false;
+	}
+
+	return true;
+}
+
+static bool read_bios_from_vram(struct gsgpu_dc *dc)
+{
+	void *bios;
+	u64 vbios_addr = dc->adev->gmc.aper_base +
+			 dc->adev->gmc.aper_size - VBIOS_OFFSET;
+	bios = ioremap(vbios_addr, VBIOS_SIZE);
+	if (!bios)
+		return false;
+
+	dc->vbios->vbios_ptr = kmalloc(VBIOS_SIZE, GFP_KERNEL);
+	if (!dc->vbios->vbios_ptr)
+		return false;
+
+	memcpy(dc->vbios->vbios_ptr, bios, VBIOS_SIZE);
+	iounmap(bios);
+	if (!is_valid_vbios(dc->vbios->vbios_ptr)) {
+		kfree(dc->vbios->vbios_ptr);
+		return false;
+	}
+
+	DRM_INFO("GSGPU get vbios from vram Success \n");
+	return true;
+}
+
+static bool read_bios_from_sysconf(struct gsgpu_dc *dc)
+{
+	if (!loongson_sysconf.vgabios_addr)
+		return false;
+
+	dc->vbios->vbios_ptr = kmalloc(VBIOS_SIZE, GFP_KERNEL);
+	if (!dc->vbios->vbios_ptr)
+		return false;
+
+	memcpy(dc->vbios->vbios_ptr, (void *)loongson_sysconf.vgabios_addr, VBIOS_SIZE);
+	DRM_INFO("GSGPU get vbios from sysconf Success \n");
+
+	return true;
+}
+
+#ifdef CONFIG_ACPI
+static bool read_bios_from_acpi(struct gsgpu_dc *dc)
+{
+	struct acpi_table_header *hdr;
+	struct acpi_viat_table *viat;
+	void *vaddr;
+	acpi_size tbl_size;
+
+	if (!ACPI_SUCCESS(acpi_get_table("VIAT", 1, &hdr)))
+		return false;
+
+	tbl_size = hdr->length;
+	if (tbl_size != sizeof(struct acpi_viat_table)) {
+		DRM_WARN("ACPI viat table present but broken (length error #1)\n");
+		return false;
+	}
+
+	viat = (struct acpi_viat_table *)hdr;
+	dc->vbios->vbios_ptr = kmalloc(VBIOS_SIZE, GFP_KERNEL);
+	if (!dc->vbios->vbios_ptr)
+		return false;
+
+	vaddr = phys_to_virt(viat->vbios_addr);
+	memcpy(dc->vbios->vbios_ptr, vaddr, VBIOS_SIZE);
+	DRM_INFO("Get vbios from ACPI success!\n");
+	return true;
+}
+#else
+static bool read_bios_from_acpi(struct gsgpu_device *adev)
+{
+	return false;
+}
+#endif
+
+static bool get_vbios_data(struct gsgpu_dc *dc)
+{
+	if (read_bios_from_vram(dc))
+		goto success;
+
+	if (read_bios_from_acpi(dc))
+		goto success;
+
+	if (read_bios_from_sysconf(dc))
+		goto success;
+
+	DRM_ERROR("Unable to locate a BIOS ROM\n");
+	return false;
+
+success:
+	return true;
+}
+
+static bool parse_vbios_header(struct vbios_desc *vb_desc, struct gsgpu_vbios *vbios)
+{
+	bool ret = false;
+	u8 *data;
+
+	if (IS_ERR_OR_NULL(vb_desc) || IS_ERR_OR_NULL(vbios))
+		return ret;
+
+	data = (u8 *)vbios->vbios_ptr + vb_desc->offset;
+
+	if (vbios->funcs && vbios->funcs->create_header_resource)
+		ret = vbios->funcs->create_header_resource(vbios, data, vb_desc->size);
+
+	return ret;
+}
+
+static bool parse_vbios_crtc(struct vbios_desc *vb_desc, struct gsgpu_vbios *vbios)
+{
+	bool ret = false;
+	u8 *data;
+
+	if (IS_ERR_OR_NULL(vb_desc) || IS_ERR_OR_NULL(vbios))
+		return ret;
+
+	data = (u8 *)vbios->vbios_ptr + vb_desc->offset;
+
+	if (vbios->funcs && vbios->funcs->create_crtc_resource)
+		ret = vbios->funcs->create_crtc_resource(vbios, data, vb_desc->link, vb_desc->size);
+
+	return ret;
+}
+
+static bool parse_vbios_connector(struct vbios_desc *vb_desc, struct gsgpu_vbios *vbios)
+{
+	bool ret = false;
+	u8 *data;
+
+	if (IS_ERR_OR_NULL(vb_desc) || IS_ERR_OR_NULL(vbios))
+		return ret;
+
+	data = (u8 *)vbios->vbios_ptr + vb_desc->offset;
+
+	if (vbios->funcs && vbios->funcs->create_connecor_resource)
+		ret = vbios->funcs->create_connecor_resource(vbios, data, vb_desc->link, vb_desc->size);
+
+	return ret;
+}
+
+static bool parse_vbios_encoder(struct vbios_desc *vb_desc, struct gsgpu_vbios *vbios)
+{
+	bool ret = false;
+	u8 *data;
+
+	if (IS_ERR_OR_NULL(vb_desc) || IS_ERR_OR_NULL(vbios))
+		return ret;
+
+	data = (u8 *)vbios->vbios_ptr + vb_desc->offset;
+
+	if (vbios->funcs && vbios->funcs->create_encoder_resource)
+		ret = vbios->funcs->create_encoder_resource(vbios, data, vb_desc->link, vb_desc->size);
+
+	return ret;
+}
+
+static bool parse_vbios_i2c(struct vbios_desc *vb_desc, struct gsgpu_vbios *vbios)
+{
+	bool ret = false;
+	u8 *data;
+
+	if (IS_ERR_OR_NULL(vb_desc) || IS_ERR_OR_NULL(vbios))
+		return ret;
+
+	data = (u8 *)vbios->vbios_ptr + vb_desc->offset;
+
+	if (vbios->funcs && vbios->funcs->create_i2c_resource)
+		ret = vbios->funcs->create_i2c_resource(vbios, data, vb_desc->link, vb_desc->size);
+
+	return ret;
+}
+
+static bool parse_vbios_pwm(struct vbios_desc *vb_desc, struct gsgpu_vbios *vbios)
+{
+	bool ret = false;
+	u8 *data;
+
+	if (IS_ERR_OR_NULL(vb_desc) || IS_ERR_OR_NULL(vbios))
+		return ret;
+
+	data = (u8 *)vbios->vbios_ptr + vb_desc->offset;
+
+	if (vbios->funcs && vbios->funcs->create_pwm_resource)
+		ret = vbios->funcs->create_pwm_resource(vbios, data, vb_desc->link, vb_desc->size);
+
+	return ret;
+}
+
+static bool parse_vbios_gpu(struct vbios_desc *vb_desc, struct gsgpu_vbios *vbios)
+{
+	bool ret = false;
+	u8 *data;
+
+	if (IS_ERR_OR_NULL(vb_desc) || IS_ERR_OR_NULL(vbios))
+		return ret;
+
+	data = (u8 *)vbios->vbios_ptr + vb_desc->offset;
+
+	if (vbios->funcs && vbios->funcs->create_gpu_resource)
+		ret = vbios->funcs->create_gpu_resource(vbios, data, vb_desc->link, vb_desc->size);
+
+	return ret;
+}
+
+static bool parse_vbios_ext_encoder(struct vbios_desc *vb_desc, struct gsgpu_vbios *vbios)
+{
+	bool ret = false;
+	u8 *data;
+
+	if (IS_ERR_OR_NULL(vb_desc) || IS_ERR_OR_NULL(vbios))
+		return ret;
+
+	data = (u8 *)vbios->vbios_ptr + vb_desc->offset;
+	if (vbios->funcs && vbios->funcs->create_ext_encoder_resource)
+		ret = vbios->funcs->create_ext_encoder_resource(vbios, data, vb_desc->link, vb_desc->size);
+
+	return ret;
+}
+
+static bool parse_vbios_default(struct vbios_desc *vb_desc, struct gsgpu_vbios *vbios)
+{
+	DRM_ERROR("Current descriptor[T-%d][V-%d] cannot be interprete.\n",
+		  vb_desc->type, vb_desc->ver);
+	return false;
+}
+
+#define FUNC(t, v, f)                                                         \
+	{                                                                     \
+		.type = t, .ver = v, .func = f,                               \
+	}
+
+static struct desc_func tables[] = {
+	FUNC(desc_header, ver_v1, parse_vbios_header),
+	FUNC(desc_crtc, ver_v1, parse_vbios_crtc),
+	FUNC(desc_encoder, ver_v1, parse_vbios_encoder),
+	FUNC(desc_connector, ver_v1, parse_vbios_connector),
+	FUNC(desc_i2c, ver_v1, parse_vbios_i2c),
+	FUNC(desc_pwm, ver_v1, parse_vbios_pwm),
+	FUNC(desc_gpu, ver_v1, parse_vbios_gpu),
+	FUNC(desc_res_encoder, ver_v1, parse_vbios_ext_encoder),
+};
+
+static inline parse_func *get_parse_func(struct vbios_desc *desc)
+{
+	parse_func *func = parse_vbios_default;
+	u32 tt_num = ARRAY_SIZE(tables);
+	u32 type = desc->type;
+	u32 ver = desc->ver;
+	int i;
+
+	for (i = 0; i < tt_num; i++) {
+		if ((tables[i].ver == ver) && (tables[i].type == type)) {
+			func = tables[i].func;
+			break;
+		}
+	}
+
+	return func;
+}
+
+static inline void parse_vbios_info(struct gsgpu_vbios *vbios)
+{
+	struct vbios_info *vb_info;
+	struct header_resource *header;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return;
+
+	header = vbios->funcs->get_header_resource(vbios);
+	if (IS_ERR_OR_NULL(header))
+		return;
+
+	vb_info = (struct vbios_info *)vbios->vbios_ptr;
+	header->links = vb_info->link_num;
+	header->ver_majro = vb_info->version_major;
+	header->ver_minor = vb_info->version_minor;
+	memcpy(header->name, vb_info->title, 16);
+}
+
+static bool dc_vbios_parse(struct gsgpu_vbios *vbios)
+{
+	struct vbios_header *vb_header;
+	struct vbios_desc *start;
+	struct vbios_desc *desc;
+	enum desc_type type;
+	parse_func *func;
+	u8 *vbios_ptr;
+	bool ret;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return false;
+
+	vbios_ptr = (u8 *)vbios->vbios_ptr;
+	if (IS_ERR_OR_NULL(vbios_ptr))
+		return false;
+
+	/* get header for global information of vbios */
+	desc = (struct vbios_desc *)(vbios_ptr + VBIOS_DESC_OFFSET);
+	if (desc->type != desc_header) {
+		pr_err("vbios first desc not header type\n");
+		return false;
+	}
+
+	func = get_parse_func(desc);
+	if (IS_ERR_OR_NULL(func)) {
+		pr_err("vbios get header parser funcs err %pf \n", func);
+		return false;
+	}
+
+	ret = (*func)(desc, vbios);
+	if (!ret) {
+		pr_err("get vbios header info error \n");
+		return false;
+	}
+
+	vb_header = (struct vbios_header *)(vbios_ptr + desc->offset);
+	DRM_DEBUG("oem-vendor %s oem-product %s\n", vb_header->oem_vendor,
+		 vb_header->oem_product);
+
+	/* start parsing vbios components */
+	start = desc = (struct vbios_desc *)(vbios_ptr + vb_header->desc_offset);
+	while (1) {
+		type = desc->type;
+		if (type == desc_header) {
+			desc++;
+			continue;
+		}
+
+		if (type == desc_max || ((desc - start) > vb_header->desc_size) ||
+		    ((desc - start) > VBIOS_DESC_TOTAL))
+			break;
+
+		func = get_parse_func(desc);
+		if (IS_ERR_OR_NULL(func))
+			continue;
+
+		ret = (*func)(desc, vbios);
+		if (!ret)
+			pr_err("Parse T-%d V-%d failed[%d]\n", desc->ver, desc->type, ret);
+
+		desc++;
+	}
+
+	/* append legacy information to header resource */
+	parse_vbios_info(vbios);
+
+	return true;
+}
+
+static bool vbios_resource_pool_create(struct gsgpu_vbios *vbios)
+{
+	if (IS_ERR_OR_NULL(vbios))
+		return false;
+
+	return dc_vbios_parse(vbios);
+}
+
+static bool vbios_resource_pool_destory(struct gsgpu_vbios *vbios)
+{
+	struct resource_object *entry, *tmp;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return false;
+
+	if (list_empty(&vbios->resource_list))
+		return true;
+
+	list_for_each_entry_safe (entry, tmp, &vbios->resource_list, node) {
+		list_del(&entry->node);
+		kvfree(entry);
+		entry = NULL;
+	}
+
+	return true;
+}
+
+static bool vbios_create_header_resource(struct gsgpu_vbios *vbios, void *data, u32 size)
+{
+	struct vbios_header vb_header;
+	struct header_resource *header;
+	u32 header_size = sizeof(struct vbios_header);
+
+	header = kvmalloc(sizeof(*header), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(header))
+		return false;
+
+	memset(&vb_header, VBIOS_DATA_INVAL, header_size);
+	memcpy(&vb_header, data, min(size, header_size));
+
+	memcpy(header->oem_product, vb_header.oem_product, 32);
+	memcpy(header->oem_vendor, vb_header.oem_vendor, 32);
+	header->base.type = GSGPU_RESOURCE_HEADER;
+
+	list_add_tail(&header->base.node, &vbios->resource_list);
+
+	return true;
+}
+
+static bool vbios_create_crtc_resource(struct gsgpu_vbios *vbios,
+				       void *data, u32 link, u32 size)
+{
+	struct vbios_crtc vb_crtc;
+	struct crtc_resource *crtc;
+	u32 crtc_size = sizeof(struct vbios_crtc);
+
+	if (IS_ERR_OR_NULL(vbios) || IS_ERR_OR_NULL(data))
+		return false;
+
+	crtc = kvmalloc(sizeof(*crtc), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(crtc))
+		return false;
+
+	memset(&vb_crtc, VBIOS_DATA_INVAL, crtc_size);
+	memcpy(&vb_crtc, data, min(size, crtc_size));
+
+	crtc->base.link = link;
+	crtc->base.type = GSGPU_RESOURCE_CRTC;
+	crtc->feature = vb_crtc.feature;
+	crtc->crtc_id = vb_crtc.crtc_id;
+	crtc->encoder_id = vb_crtc.encoder_id;
+	crtc->max_freq = vb_crtc.max_freq;
+	crtc->max_width = vb_crtc.max_width;
+	crtc->max_height = vb_crtc.max_height;
+	crtc->is_vb_timing = vb_crtc.is_vb_timing;
+
+	list_add_tail(&crtc->base.node, &vbios->resource_list);
+
+	return true;
+}
+
+static bool vbios_create_encoder_resource(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size)
+{
+	struct vbios_encoder vb_encoder;
+	struct encoder_resource *encoder;
+	u32 encoder_size;
+
+	if (IS_ERR_OR_NULL(vbios) || IS_ERR_OR_NULL(data))
+		return false;
+
+	encoder = kvmalloc(sizeof(*encoder), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(encoder))
+		return false;
+
+	encoder_size = sizeof(struct vbios_encoder);
+	memset(&vb_encoder, VBIOS_DATA_INVAL, encoder_size);
+	memcpy(&vb_encoder, data, min(size, encoder_size));
+
+	encoder->base.link = link;
+	encoder->base.type = GSGPU_RESOURCE_ENCODER;
+	encoder->feature = vb_encoder.feature;
+	encoder->i2c_id = vb_encoder.i2c_id;
+	encoder->connector_id = vb_encoder.connector_id;
+	encoder->type = vb_encoder.type;
+	encoder->config_type = vb_encoder.config_type;
+	encoder->chip_addr = vb_encoder.chip_addr;
+	encoder->chip = vb_encoder.chip;
+
+	list_add_tail(&encoder->base.node, &vbios->resource_list);
+
+	return true;
+}
+
+static bool vbios_create_connector_resource(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size)
+{
+	struct vbios_connector vb_connector;
+	struct connector_resource *connector;
+	u32 connector_size;
+
+	if (IS_ERR_OR_NULL(vbios) || IS_ERR_OR_NULL(data))
+		return false;
+
+	connector = kvmalloc(sizeof(*connector), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(connector))
+		return false;
+
+	connector_size = sizeof(struct vbios_connector);
+	memset(&vb_connector, VBIOS_DATA_INVAL, connector_size);
+	memcpy(&vb_connector, data, min(size, connector_size));
+
+	connector->base.link = link;
+	connector->base.type = GSGPU_RESOURCE_CONNECTOR;
+
+	connector->feature = vb_connector.feature;
+	connector->i2c_id = vb_connector.i2c_id;
+
+	connector->type = vb_connector.type;
+	connector->hotplug = vb_connector.hotplug;
+	connector->edid_method = vb_connector.edid_method;
+	connector->irq_gpio = vb_connector.irq_gpio;
+	connector->gpio_placement = vb_connector.gpio_placement;
+
+	list_add_tail(&connector->base.node, &vbios->resource_list);
+
+	return true;
+}
+
+static bool vbios_create_i2c_resource(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size)
+{
+	struct i2c_resource *i2c_resource;
+	struct vbios_i2c vb_i2c;
+	u32 i2c_size;
+
+	if (IS_ERR_OR_NULL(vbios) || IS_ERR_OR_NULL(data))
+		return false;
+
+	i2c_resource = kvmalloc(sizeof(*i2c_resource), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(i2c_resource))
+		return false;
+
+	i2c_size = sizeof(struct vbios_i2c);
+	memset(&vb_i2c, VBIOS_DATA_INVAL, i2c_size);
+	memcpy(&vb_i2c, data, min(size, i2c_size));
+
+	i2c_resource->speed = vb_i2c.speed == VBIOS_DATA_INVAL ? 10 : vb_i2c.speed;
+	i2c_resource->base.link = link;
+	i2c_resource->base.type = GSGPU_RESOURCE_I2C;
+
+	i2c_resource->feature = vb_i2c.feature;
+	i2c_resource->id = vb_i2c.id;
+
+	list_add_tail(&i2c_resource->base.node, &vbios->resource_list);
+
+	return true;
+}
+
+static bool vbios_create_gpio_resource(struct gsgpu_vbios *vbios,
+				       void *data, u32 link, u32 size)
+{
+	return false;
+}
+
+static bool vbios_create_pwm_resource(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size)
+{
+	struct pwm_resource *pwm_resource;
+	struct vbios_pwm vb_pwm;
+	u32 pwm_size;
+
+	if (IS_ERR_OR_NULL(vbios) || IS_ERR_OR_NULL(data))
+		return false;
+
+	pwm_resource = kvmalloc(sizeof(*pwm_resource), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(pwm_resource))
+		return false;
+
+	pwm_size = sizeof(struct vbios_pwm);
+	memset(&vb_pwm, VBIOS_DATA_INVAL, pwm_size);
+	memcpy(&vb_pwm, data, min(size, pwm_size));
+
+	pwm_resource->base.link = link;
+	pwm_resource->base.type = GSGPU_RESOURCE_PWM;
+
+	pwm_resource->feature = vb_pwm.feature;
+	pwm_resource->pwm = vb_pwm.pwm;
+	pwm_resource->polarity = vb_pwm.polarity;
+	pwm_resource->peroid = vb_pwm.peroid;
+
+	list_add_tail(&pwm_resource->base.node, &vbios->resource_list);
+
+	return true;
+}
+
+static bool vbios_create_gpu_resource(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size)
+{
+	struct gpu_resource *gpu_resource;
+	struct vbios_gpu vb_gpu;
+	u32 gpu_size;
+
+	if (IS_ERR_OR_NULL(vbios) || IS_ERR_OR_NULL(data))
+		return false;
+
+	gpu_resource = kvmalloc(sizeof(*gpu_resource), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(gpu_resource))
+		return false;
+
+	gpu_size = sizeof(struct vbios_gpu);
+	memset(&vb_gpu, VBIOS_DATA_INVAL, gpu_size);
+	memcpy(&vb_gpu, data, min(size, gpu_size));
+
+	gpu_resource->base.link = 0;
+	gpu_resource->base.type = GSGPU_RESOURCE_GPU;
+
+	gpu_resource->vram_type = vb_gpu.type;
+	gpu_resource->bit_width = vb_gpu.bit_width;
+	gpu_resource->cap = vb_gpu.cap;
+	gpu_resource->count_freq = vb_gpu.count_freq;
+	gpu_resource->freq = vb_gpu.freq;
+	gpu_resource->shaders_num = vb_gpu.shaders_num;
+	gpu_resource->shaders_freq = vb_gpu.shaders_freq;
+
+	list_add_tail(&gpu_resource->base.node, &vbios->resource_list);
+
+	return true;
+}
+
+static bool vbios_create_ext_encoder_resource(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size)
+{
+	struct ext_encoder_resources *ext_encoder_resource;
+	struct vbios_ext_encoder vb_ext_encoder;
+	u32 ext_encoder_size;
+
+	if (IS_ERR_OR_NULL(vbios) || IS_ERR_OR_NULL(data))
+		return false;
+
+	ext_encoder_resource = kvmalloc(sizeof(*ext_encoder_resource), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(ext_encoder_resource))
+		return false;
+
+	ext_encoder_size = sizeof(struct vbios_ext_encoder);
+	memset(&vb_ext_encoder, VBIOS_DATA_INVAL, ext_encoder_size);
+	memcpy(&vb_ext_encoder, data, min(size, ext_encoder_size));
+
+	ext_encoder_resource->base.link = link;
+	ext_encoder_resource->base.type = GSGPU_RESOURCE_EXT_ENCODER;
+
+	ext_encoder_resource->data_checksum = vb_ext_encoder.data_checksum;
+	ext_encoder_resource->data_size = vb_ext_encoder.data_size;
+	memcpy(ext_encoder_resource->data, vb_ext_encoder.data, vb_ext_encoder.data_size);
+
+	list_add_tail(&ext_encoder_resource->base.node, &vbios->resource_list);
+	return true;
+}
+
+static struct header_resource *vbios_get_header_resource(struct gsgpu_vbios *vbios)
+{
+	struct resource_object *entry;
+	struct header_resource *header;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return NULL;
+
+	if (list_empty(&vbios->resource_list))
+		return NULL;
+
+	list_for_each_entry (entry, &vbios->resource_list, node) {
+		if (entry->type == GSGPU_RESOURCE_HEADER) {
+			header = container_of(entry, struct header_resource, base);
+			return header;
+		}
+	}
+
+	return NULL;
+}
+
+static struct crtc_resource *vbios_get_crtc_resource(struct gsgpu_vbios *vbios, u32 link)
+{
+	struct resource_object *entry;
+	struct crtc_resource *crtc;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return NULL;
+
+	if (list_empty(&vbios->resource_list))
+		return NULL;
+
+	list_for_each_entry (entry, &vbios->resource_list, node) {
+		if ((entry->link == link) && entry->type == GSGPU_RESOURCE_CRTC) {
+			crtc = container_of(entry, struct crtc_resource, base);
+			return crtc;
+		}
+	}
+
+	return NULL;
+}
+
+static struct encoder_resource *vbios_get_encoder_resource(struct gsgpu_vbios *vbios, u32 link)
+{
+	struct resource_object *entry;
+	struct encoder_resource *encoder;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return NULL;
+
+	if (list_empty(&vbios->resource_list))
+		return NULL;
+
+	list_for_each_entry (entry, &vbios->resource_list, node) {
+		if ((entry->link == link) && entry->type == GSGPU_RESOURCE_ENCODER) {
+			encoder = container_of(entry, struct encoder_resource, base);
+			return encoder;
+		}
+	}
+
+	return NULL;
+}
+
+static struct connector_resource *vbios_get_connector_resource(struct gsgpu_vbios *vbios, u32 link)
+{
+	struct resource_object *entry;
+	struct connector_resource *connector;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return NULL;
+
+	if (list_empty(&vbios->resource_list))
+		return NULL;
+
+	list_for_each_entry (entry, &vbios->resource_list, node) {
+		if ((entry->link == link) && entry->type == GSGPU_RESOURCE_CONNECTOR) {
+			connector = container_of(entry, struct connector_resource, base);
+			return connector;
+		}
+	}
+
+	return NULL;
+}
+
+static struct i2c_resource *vbios_get_i2c_resource(struct gsgpu_vbios *vbios, u32 link)
+{
+	struct resource_object *entry;
+	struct i2c_resource *i2c_resource;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return NULL;
+
+	if (list_empty(&vbios->resource_list))
+		return NULL;
+
+	list_for_each_entry (entry, &vbios->resource_list, node) {
+		if ((entry->link == link) && (entry->type == GSGPU_RESOURCE_I2C)) {
+			i2c_resource = container_of(entry, struct i2c_resource, base);
+			return i2c_resource;
+		}
+	}
+
+	return NULL;
+}
+
+static struct gpio_resource
+*vbios_get_gpio_resource(struct gsgpu_vbios *vbios, u32 link)
+{
+	return NULL;
+}
+
+static struct pwm_resource *vbios_get_pwm_resource(struct gsgpu_vbios *vbios, u32 link)
+{
+	struct resource_object *entry;
+	struct pwm_resource *pwm_resource;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return NULL;
+
+	if (list_empty(&vbios->resource_list))
+		return NULL;
+
+	list_for_each_entry (entry, &vbios->resource_list, node) {
+		if ((entry->link == link) && (entry->type == GSGPU_RESOURCE_PWM)) {
+			pwm_resource = container_of(entry, struct pwm_resource, base);
+			return pwm_resource;
+		}
+	}
+
+	return NULL;
+}
+
+static struct gpu_resource *vbios_get_gpu_resource(struct gsgpu_vbios *vbios, u32 link)
+{
+	struct resource_object *entry;
+	struct gpu_resource *gpu_resource;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return NULL;
+
+	if (list_empty(&vbios->resource_list))
+		return NULL;
+
+	list_for_each_entry (entry, &vbios->resource_list, node) {
+		if ((entry->link == link) && (entry->type == GSGPU_RESOURCE_GPU)) {
+			gpu_resource = container_of(entry, struct gpu_resource, base);
+			return gpu_resource;
+		}
+	}
+
+	return NULL;
+}
+
+static struct ext_encoder_resources *vbios_get_ext_encoder_resource(struct gsgpu_vbios *vbios, u32 link)
+{
+	struct resource_object *entry;
+	struct ext_encoder_resources *ext_encoder_resources;
+
+	if (IS_ERR_OR_NULL(vbios))
+		return NULL;
+
+	if (list_empty(&vbios->resource_list))
+		return NULL;
+
+	list_for_each_entry (entry, &vbios->resource_list, node) {
+		if ((entry->link == link) && (entry->type == GSGPU_RESOURCE_EXT_ENCODER)) {
+			ext_encoder_resources = container_of(entry, struct ext_encoder_resources, base);
+			return ext_encoder_resources;
+		}
+	}
+
+	return NULL;
+}
+
+static struct vbios_funcs vbios_funcs = {
+	.resource_pool_create = vbios_resource_pool_create,
+	.resource_pool_destory = vbios_resource_pool_destory,
+
+	.create_header_resource = vbios_create_header_resource,
+	.create_crtc_resource = vbios_create_crtc_resource,
+	.create_encoder_resource = vbios_create_encoder_resource,
+	.create_connecor_resource = vbios_create_connector_resource,
+	.create_i2c_resource = vbios_create_i2c_resource,
+	.create_gpio_resource = vbios_create_gpio_resource,
+	.create_pwm_resource = vbios_create_pwm_resource,
+	.create_gpu_resource = vbios_create_gpu_resource,
+	.create_ext_encoder_resource = vbios_create_ext_encoder_resource,
+
+	.get_header_resource = vbios_get_header_resource,
+	.get_crtc_resource = vbios_get_crtc_resource,
+	.get_encoder_resource = vbios_get_encoder_resource,
+	.get_connector_resource = vbios_get_connector_resource,
+	.get_i2c_resource = vbios_get_i2c_resource,
+	.get_gpio_resource = vbios_get_gpio_resource,
+	.get_pwm_resource = vbios_get_pwm_resource,
+	.get_gpu_resource = vbios_get_gpu_resource,
+	.get_ext_encoder_resource = vbios_get_ext_encoder_resource,
+};
+
+u8 gsgpu_vbios_checksum(const u8 *data, int size)
+{
+	u8 sum = 0;
+
+	while (size--)
+		sum += *data++;
+	return sum;
+}
+
+u32 gsgpu_vbios_version(struct gsgpu_vbios *vbios)
+{
+	struct vbios_info *vb_info = vbios->vbios_ptr;
+	u32 minor, major, version;
+
+	major = vb_info->version_major;
+	minor = vb_info->version_minor;
+	version = major * 10 + minor;
+
+	return version;
+}
+
+static bool dc_vbios_create(struct gsgpu_vbios *vbios)
+{
+	if (IS_ERR_OR_NULL(vbios) || IS_ERR_OR_NULL(vbios->funcs))
+		return false;
+
+	if (vbios->funcs->resource_pool_create)
+		return vbios->funcs->resource_pool_create(vbios);
+
+	return false;
+}
+
+void *dc_get_vbios_resource(struct gsgpu_vbios *vbios, u32 link,
+			    enum resource_type type)
+{
+	if (IS_ERR_OR_NULL(vbios) || IS_ERR_OR_NULL(vbios->funcs)) {
+		DRM_ERROR("GSGPU get vbios resource%d failed\n", type);
+		return NULL;
+	}
+
+	switch (type) {
+	case GSGPU_RESOURCE_HEADER:
+		if (vbios->funcs->get_header_resource)
+			return (void *)vbios->funcs->get_header_resource(vbios);
+		break;
+	case GSGPU_RESOURCE_CRTC:
+		if (vbios->funcs->get_crtc_resource)
+			return (void *)vbios->funcs->get_crtc_resource(vbios, link);
+		break;
+	case GSGPU_RESOURCE_ENCODER:
+		if (vbios->funcs->get_encoder_resource)
+			return (void *)vbios->funcs->get_encoder_resource(vbios, link);
+		break;
+	case GSGPU_RESOURCE_CONNECTOR:
+		if (vbios->funcs->get_connector_resource)
+			return (void *)vbios->funcs->get_connector_resource(vbios, link);
+		break;
+	case GSGPU_RESOURCE_GPIO:
+		if (vbios->funcs->get_gpio_resource)
+			return (void *)vbios->funcs->get_gpio_resource(vbios, link);
+		break;
+	case GSGPU_RESOURCE_I2C:
+		if (vbios->funcs->get_i2c_resource)
+			return (void *)vbios->funcs->get_i2c_resource(vbios, link);
+		break;
+	case GSGPU_RESOURCE_PWM:
+		if (vbios->funcs->get_pwm_resource)
+			return (void *)vbios->funcs->get_pwm_resource(vbios, link);
+		break;
+	case GSGPU_RESOURCE_GPU:
+		if (vbios->funcs->get_gpu_resource)
+			return (void *)vbios->funcs->get_gpu_resource(vbios, 0);
+		break;
+	case GSGPU_RESOURCE_EXT_ENCODER:
+		if (vbios->funcs->get_ext_encoder_resource)
+			return (void *)vbios->funcs->get_ext_encoder_resource(vbios, link);
+		break;
+	default:
+		return NULL;
+		break;
+	}
+
+	return NULL;
+}
+
+void dc_vbios_show(struct gsgpu_vbios *vbios)
+{
+	struct header_resource *header_res = NULL;
+	struct crtc_resource *crtc_res;
+	struct gpu_resource *gpu_res;
+	char *vram_type[] = {"DDR3", "DDR4", "DDR5"};
+	int i;
+
+	header_res = dc_get_vbios_resource(vbios, 0, GSGPU_RESOURCE_HEADER);
+	if (header_res == NULL)
+		return;
+
+	DRM_INFO("GSGPU vbios header info:\n");
+	DRM_INFO("ver:%d.%d links%d max_planes%d name %s\n",
+		header_res->ver_majro, header_res->ver_minor,
+		header_res->links, header_res->max_planes, header_res->name);
+	DRM_INFO("oem-vendor %s oem-product %s\n", header_res->oem_vendor,
+		 header_res->oem_product);
+
+	for (i = 0; i < header_res->links; i++) {
+		crtc_res = dc_get_vbios_resource(vbios, i, GSGPU_RESOURCE_CRTC);
+		DRM_INFO("GSGPU vbios crtc%d max frep%d width%d height%d\n",
+			 i, crtc_res->max_freq,
+			 crtc_res->max_width, crtc_res->max_height);
+	}
+
+	gpu_res = dc_get_vbios_resource(vbios, 0, GSGPU_RESOURCE_GPU);
+	if (!gpu_res)
+		DRM_WARN("The video memory and gpu information is not obtained from the vbios! \n");
+	else {
+		dev_info(vbios->dc->adev->dev, "VRAM: %dM %s %dbit %dMhz.\n",
+			gpu_res->cap, vram_type[gpu_res->vram_type], gpu_res->bit_width, gpu_res->freq);
+		dev_info(vbios->dc->adev->dev, "GSGPU: shaders_num: %d, shaders_freq: %d, freq_count: %d.\n",
+			gpu_res->shaders_num, gpu_res->shaders_freq, gpu_res->count_freq);
+	}
+}
+
+bool dc_vbios_init(struct gsgpu_dc *dc)
+{
+	struct vbios_info *header;
+	bool status;
+	bool ret;
+
+	if (IS_ERR_OR_NULL(dc))
+		return false;
+
+	dc->vbios = kmalloc(sizeof(*dc->vbios), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(dc->vbios))
+		return false;
+
+	dc->vbios->funcs = &vbios_funcs;
+	dc->vbios->dc = dc;
+	INIT_LIST_HEAD(&dc->vbios->resource_list);
+
+	status = get_vbios_data(dc);
+	if (!status) {
+		DRM_ERROR("GSGPU Can not get vbios from sysconf!!!\n");
+	} else {
+		header = dc->vbios->vbios_ptr;
+	}
+
+//TODO: Add vbios default when read bios failed.
+	ret = dc_vbios_create(dc->vbios);
+	if (ret == false) {
+		pr_err("%s %d failed \n", __func__, __LINE__);
+		kvfree(dc->vbios);
+		dc->vbios = NULL;
+	}
+
+	dc_vbios_show(dc->vbios);
+
+	return true;
+}
+
+void dc_vbios_exit(struct gsgpu_vbios *vbios)
+{
+	if (IS_ERR_OR_NULL(vbios))
+		return;
+
+	if (!IS_ERR_OR_NULL(vbios->vbios_ptr)) {
+		kvfree(vbios->vbios_ptr);
+		vbios->vbios_ptr = NULL;
+	}
+
+	if (!IS_ERR_OR_NULL(vbios->funcs) &&
+	    (vbios->funcs->resource_pool_destory))
+		vbios->funcs->resource_pool_destory(vbios);
+
+	kvfree(vbios);
+	vbios = NULL;
+}
+
+bool check_vbios_info(void)
+{
+	struct vbios_desc *start;
+	struct vbios_desc *desc;
+	struct vbios_header *vb_header;
+	struct vbios_encoder vb_encoder;
+	struct acpi_table_header *hdr;
+	struct acpi_viat_table *viat;
+	acpi_size tbl_size;
+	enum desc_type desc_type;
+	u32 encoder_size;
+	void *data;
+	u8 *vbios_ptr;
+	void *vaddr;
+	bool support = false;
+	bool get_vbios = false;
+
+	struct pci_dev *pdev = pci_get_device(0x0014, 0x7A25, NULL);
+	pci_enable_device(pdev);
+	resource_size_t vram_base = pci_resource_start(pdev, 2);
+	resource_size_t vram_size = pci_resource_len(pdev, 2);
+	u64 vbios_addr = vram_base + vram_size - VBIOS_OFFSET;
+	vaddr = ioremap(vbios_addr, VBIOS_SIZE);
+	if (!vaddr)
+		goto acpi;
+
+	vbios_ptr = kmalloc(VBIOS_SIZE, GFP_KERNEL);
+	if (!vbios_ptr) {
+		iounmap(vaddr);
+		goto acpi;
+	}
+
+	memcpy(vbios_ptr, vaddr, VBIOS_SIZE);
+	iounmap(vaddr);
+	if (!is_valid_vbios((void *)vbios_ptr)) {
+		kfree(vbios_ptr);
+		get_vbios = false;
+	} else
+		get_vbios = true;
+
+acpi:
+	if (!get_vbios) {
+#ifdef CONFIG_ACPI
+		if (!ACPI_SUCCESS(acpi_get_table("VIAT", 1, &hdr)))
+			goto sysconf;
+
+		tbl_size = hdr->length;
+		if (tbl_size != sizeof(struct acpi_viat_table))
+			goto sysconf;
+
+		viat = (struct acpi_viat_table *)hdr;
+		vbios_ptr = kmalloc(VBIOS_SIZE, GFP_KERNEL);
+		if (!vbios_ptr)
+			goto sysconf;
+
+		vaddr = phys_to_virt(viat->vbios_addr);
+		memcpy(vbios_ptr, vaddr, VBIOS_SIZE);
+
+		DRM_DEBUG_DRIVER("Get vbios from ACPI success!\n");
+		get_vbios = true;
+#else
+		get_vbios = false;
+#endif
+	}
+
+sysconf:
+	if (!get_vbios) {
+		if (!loongson_sysconf.vgabios_addr)
+			return false;
+
+		vbios_ptr = kmalloc(VBIOS_SIZE, GFP_KERNEL);
+		if (!vbios_ptr)
+			return false;
+
+		memcpy(vbios_ptr, (void *)loongson_sysconf.vgabios_addr,
+		       VBIOS_SIZE);
+	}
+
+	desc = (struct vbios_desc *)(vbios_ptr + 0x6000);
+	vb_header = (struct vbios_header *)(vbios_ptr + desc->offset);
+	start = (struct vbios_desc *)(vbios_ptr + vb_header->desc_offset);
+	desc = (struct vbios_desc *)(vbios_ptr + vb_header->desc_offset);
+	while (1) {
+		desc_type = desc->type;
+		if (desc_type != desc_encoder) {
+			desc++;
+			continue;
+		}
+
+		if (desc_type == desc_max ||
+		    ((desc - start) > vb_header->desc_size) ||
+		    ((desc - start) > VBIOS_DESC_TOTAL))
+			break;
+
+		data = (u8 *)vbios_ptr + desc->offset;
+		encoder_size = sizeof(struct vbios_encoder);
+		memset(&vb_encoder, 0xff, min(desc->size, encoder_size));
+		memcpy(&vb_encoder, data, min(desc->size, encoder_size));
+		DRM_DEBUG_DRIVER("vbios desc type:%d encoder_chip:0x%x\n",
+				 desc->type, vb_encoder.chip);
+
+		switch (vb_encoder.chip) {
+		case INTERNAL_DVO:
+		case INTERNAL_HDMI:
+		case EDP_LT9721:
+		case EDP_LT6711:
+		case LVDS_LT8619:
+		case EDP_NCS8805:
+			support = true;
+			break;
+		default:
+			kfree(vbios_ptr);
+			return false;
+		}
+
+		desc++;
+	}
+
+	kfree(vbios_ptr);
+	return support;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_debugfs.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_debugfs.c
new file mode 100644
index 000000000000..45ebbc495fb3
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_debugfs.c
@@ -0,0 +1,588 @@
+#include <linux/kthread.h>
+#include <drm/drmP.h>
+#include <linux/debugfs.h>
+#include "gsgpu.h"
+
+/**
+ * gsgpu_debugfs_add_files - Add simple debugfs entries
+ *
+ * @adev:  Device to attach debugfs entries to
+ * @files:  Array of function callbacks that respond to reads
+ * @nfiles: Number of callbacks to register
+ *
+ */
+int gsgpu_debugfs_add_files(struct gsgpu_device *adev,
+			     const struct drm_info_list *files,
+			     unsigned nfiles)
+{
+	unsigned i;
+
+	for (i = 0; i < adev->debugfs_count; i++) {
+		if (adev->debugfs[i].files == files) {
+			/* Already registered */
+			return 0;
+		}
+	}
+
+	i = adev->debugfs_count + 1;
+	if (i > GSGPU_DEBUGFS_MAX_COMPONENTS) {
+		DRM_ERROR("Reached maximum number of debugfs components.\n");
+		DRM_ERROR("Report so we increase "
+			  "GSGPU_DEBUGFS_MAX_COMPONENTS.\n");
+		return -EINVAL;
+	}
+	adev->debugfs[adev->debugfs_count].files = files;
+	adev->debugfs[adev->debugfs_count].num_files = nfiles;
+	adev->debugfs_count = i;
+#if defined(CONFIG_DEBUG_FS)
+	drm_debugfs_create_files(files, nfiles,
+				 adev->ddev->primary->debugfs_root,
+				 adev->ddev->primary);
+#endif
+	return 0;
+}
+
+#if defined(CONFIG_DEBUG_FS)
+
+/**
+ * gsgpu_debugfs_regs_read - Callback for reading MMIO registers
+ */
+static ssize_t gsgpu_debugfs_regs_read(struct file *f, char __user *buf,
+					size_t size, loff_t *pos)
+{
+	struct gsgpu_device *adev = file_inode(f)->i_private;
+
+	dev_err(adev->dev, "%s Not Implement\n", __func__);
+
+	return -EINVAL;
+}
+
+/**
+ * gsgpu_debugfs_regs_write - Callback for writing MMIO registers
+ */
+static ssize_t gsgpu_debugfs_regs_write(struct file *f, const char __user *buf,
+					 size_t size, loff_t *pos)
+{
+	struct gsgpu_device *adev = file_inode(f)->i_private;
+
+	dev_err(adev->dev, "%s Not Implement\n", __func__);
+
+	return -EINVAL;
+}
+
+/**
+ * gsgpu_debugfs_gca_config_read - Read from gfx config data
+ *
+ * @f: open file handle
+ * @buf: User buffer to store read data in
+ * @size: Number of bytes to read
+ * @pos:  Offset to seek to
+ *
+ * This file is used to access configuration data in a somewhat
+ * stable fashion.  The format is a series of DWORDs with the first
+ * indicating which revision it is.  New content is appended to the
+ * end so that older software can still read the data.
+ */
+
+static ssize_t gsgpu_debugfs_gca_config_read(struct file *f, char __user *buf,
+					size_t size, loff_t *pos)
+{
+	struct gsgpu_device *adev = file_inode(f)->i_private;
+	ssize_t result = 0;
+	int r;
+	uint32_t *config, no_regs = 0;
+
+	if (size & 0x3 || *pos & 0x3)
+		return -EINVAL;
+
+	config = kmalloc_array(256, sizeof(*config), GFP_KERNEL);
+	if (!config)
+		return -ENOMEM;
+
+	/* version, increment each time something is added */
+	config[no_regs++] = 3;
+	config[no_regs++] = adev->gfx.config.max_shader_engines;
+	config[no_regs++] = adev->gfx.config.max_tile_pipes;
+	config[no_regs++] = adev->gfx.config.max_cu_per_sh;
+	config[no_regs++] = adev->gfx.config.max_sh_per_se;
+	config[no_regs++] = adev->gfx.config.max_backends_per_se;
+	config[no_regs++] = adev->gfx.config.max_texture_channel_caches;
+	config[no_regs++] = adev->gfx.config.max_gprs;
+	config[no_regs++] = adev->gfx.config.max_gs_threads;
+	config[no_regs++] = adev->gfx.config.max_hw_contexts;
+	config[no_regs++] = adev->gfx.config.sc_prim_fifo_size_frontend;
+	config[no_regs++] = adev->gfx.config.sc_prim_fifo_size_backend;
+	config[no_regs++] = adev->gfx.config.sc_hiz_tile_fifo_size;
+	config[no_regs++] = adev->gfx.config.sc_earlyz_tile_fifo_size;
+	config[no_regs++] = adev->gfx.config.num_tile_pipes;
+	config[no_regs++] = adev->gfx.config.backend_enable_mask;
+	config[no_regs++] = adev->gfx.config.mem_max_burst_length_bytes;
+	config[no_regs++] = adev->gfx.config.mem_row_size_in_kb;
+	config[no_regs++] = adev->gfx.config.shader_engine_tile_size;
+	config[no_regs++] = adev->gfx.config.num_gpus;
+	config[no_regs++] = adev->gfx.config.multi_gpu_tile_size;
+	config[no_regs++] = adev->gfx.config.mc_arb_ramcfg;
+	config[no_regs++] = adev->gfx.config.gb_addr_config;
+	config[no_regs++] = adev->gfx.config.num_rbs;
+
+	/* rev==2 */
+	config[no_regs++] = adev->family;
+
+	/* rev==3 */
+	config[no_regs++] = adev->pdev->device;
+	config[no_regs++] = adev->pdev->revision;
+	config[no_regs++] = adev->pdev->subsystem_device;
+	config[no_regs++] = adev->pdev->subsystem_vendor;
+
+	while (size && (*pos < no_regs * 4)) {
+		uint32_t value;
+
+		value = config[*pos >> 2];
+		r = put_user(value, (uint32_t *)buf);
+		if (r) {
+			kfree(config);
+			return r;
+		}
+
+		result += 4;
+		buf += 4;
+		*pos += 4;
+		size -= 4;
+	}
+
+	kfree(config);
+	return result;
+}
+
+/**
+ * gsgpu_debugfs_sensor_read - Read from the powerplay sensors
+ *
+ * @f: open file handle
+ * @buf: User buffer to store read data in
+ * @size: Number of bytes to read
+ * @pos:  Offset to seek to
+ */
+static ssize_t gsgpu_debugfs_sensor_read(struct file *f, char __user *buf,
+					size_t size, loff_t *pos)
+{
+	return -EINVAL;
+}
+
+/** gsgpu_debugfs_wave_read - Read WAVE STATUS data
+ *
+ * @f: open file handle
+ * @buf: User buffer to store read data in
+ * @size: Number of bytes to read
+ * @pos:  Offset to seek to
+ *
+ * The offset being sought changes which wave that the status data
+ * will be returned for.  The bits are used as follows:
+ *
+ * Bits 0..6: 	Byte offset into data
+ * Bits 7..14:	SE selector
+ * Bits 15..22:	SH/SA selector
+ * Bits 23..30: CU/{WGP+SIMD} selector
+ * Bits 31..36: WAVE ID selector
+ * Bits 37..44: SIMD ID selector
+ *
+ * The returned data begins with one DWORD of version information
+ * Followed by WAVE STATUS registers relevant to the GFX IP version
+ * being used.  See gfx_read_wave_data() for an example output.
+ */
+static ssize_t gsgpu_debugfs_wave_read(struct file *f, char __user *buf,
+					size_t size, loff_t *pos)
+{
+	struct gsgpu_device *adev = f->f_inode->i_private;
+	int r, x;
+	ssize_t result = 0;
+	uint32_t offset, se, sh, cu, wave, simd, data[32];
+
+	if (size & 3 || *pos & 3)
+		return -EINVAL;
+
+	/* decode offset */
+	offset = (*pos & GENMASK_ULL(6, 0));
+	se = (*pos & GENMASK_ULL(14, 7)) >> 7;
+	sh = (*pos & GENMASK_ULL(22, 15)) >> 15;
+	cu = (*pos & GENMASK_ULL(30, 23)) >> 23;
+	wave = (*pos & GENMASK_ULL(36, 31)) >> 31;
+	simd = (*pos & GENMASK_ULL(44, 37)) >> 37;
+
+	/* switch to the specific se/sh/cu */
+	mutex_lock(&adev->grbm_idx_mutex);
+
+	x = 0;
+	if (adev->gfx.funcs->read_wave_data)
+		adev->gfx.funcs->read_wave_data(adev, simd, wave, data, &x);
+
+	mutex_unlock(&adev->grbm_idx_mutex);
+
+	if (!x)
+		return -EINVAL;
+
+	while (size && (offset < x * 4)) {
+		uint32_t value;
+
+		value = data[offset >> 2];
+		r = put_user(value, (uint32_t *)buf);
+		if (r)
+			return r;
+
+		result += 4;
+		buf += 4;
+		offset += 4;
+		size -= 4;
+	}
+
+	return result;
+}
+
+/** gsgpu_debugfs_gpr_read - Read wave gprs
+ *
+ * @f: open file handle
+ * @buf: User buffer to store read data in
+ * @size: Number of bytes to read
+ * @pos:  Offset to seek to
+ *
+ * The offset being sought changes which wave that the status data
+ * will be returned for.  The bits are used as follows:
+ *
+ * Bits 0..11:	Byte offset into data
+ * Bits 12..19:	SE selector
+ * Bits 20..27:	SH/SA selector
+ * Bits 28..35: CU/{WGP+SIMD} selector
+ * Bits 36..43: WAVE ID selector
+ * Bits 37..44: SIMD ID selector
+ * Bits 52..59: Thread selector
+ * Bits 60..61: Bank selector (VGPR=0,SGPR=1)
+ *
+ * The return data comes from the SGPR or VGPR register bank for
+ * the selected operational unit.
+ */
+static ssize_t gsgpu_debugfs_gpr_read(struct file *f, char __user *buf,
+					size_t size, loff_t *pos)
+{
+	struct gsgpu_device *adev = f->f_inode->i_private;
+	int r;
+	ssize_t result = 0;
+	uint32_t offset, se, sh, cu, wave, simd, thread, bank, *data;
+
+	if (size & 3 || *pos & 3)
+		return -EINVAL;
+
+	/* decode offset */
+	offset = *pos & GENMASK_ULL(11, 0);
+	se = (*pos & GENMASK_ULL(19, 12)) >> 12;
+	sh = (*pos & GENMASK_ULL(27, 20)) >> 20;
+	cu = (*pos & GENMASK_ULL(35, 28)) >> 28;
+	wave = (*pos & GENMASK_ULL(43, 36)) >> 36;
+	simd = (*pos & GENMASK_ULL(51, 44)) >> 44;
+	thread = (*pos & GENMASK_ULL(59, 52)) >> 52;
+	bank = (*pos & GENMASK_ULL(61, 60)) >> 60;
+
+	data = kcalloc(1024, sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	/* switch to the specific se/sh/cu */
+	mutex_lock(&adev->grbm_idx_mutex);
+
+	if (bank == 0) {
+		if (adev->gfx.funcs->read_wave_vgprs)
+			adev->gfx.funcs->read_wave_vgprs(adev, simd, wave, thread, offset, size>>2, data);
+	} else {
+		if (adev->gfx.funcs->read_wave_sgprs)
+			adev->gfx.funcs->read_wave_sgprs(adev, simd, wave, offset, size>>2, data);
+	}
+
+	mutex_unlock(&adev->grbm_idx_mutex);
+
+	while (size) {
+		uint32_t value;
+
+		value = data[offset++];
+		r = put_user(value, (uint32_t *)buf);
+		if (r) {
+			result = r;
+			goto err;
+		}
+
+		result += 4;
+		buf += 4;
+		size -= 4;
+	}
+
+err:
+	kfree(data);
+	return result;
+}
+
+static const struct file_operations gsgpu_debugfs_regs_fops = {
+	.owner = THIS_MODULE,
+	.read = gsgpu_debugfs_regs_read,
+	.write = gsgpu_debugfs_regs_write,
+	.llseek = default_llseek
+};
+
+static const struct file_operations gsgpu_debugfs_gca_config_fops = {
+	.owner = THIS_MODULE,
+	.read = gsgpu_debugfs_gca_config_read,
+	.llseek = default_llseek
+};
+
+static const struct file_operations gsgpu_debugfs_sensors_fops = {
+	.owner = THIS_MODULE,
+	.read = gsgpu_debugfs_sensor_read,
+	.llseek = default_llseek
+};
+
+static const struct file_operations gsgpu_debugfs_wave_fops = {
+	.owner = THIS_MODULE,
+	.read = gsgpu_debugfs_wave_read,
+	.llseek = default_llseek
+};
+static const struct file_operations gsgpu_debugfs_gpr_fops = {
+	.owner = THIS_MODULE,
+	.read = gsgpu_debugfs_gpr_read,
+	.llseek = default_llseek
+};
+
+static const struct file_operations *debugfs_regs[] = {
+	&gsgpu_debugfs_regs_fops,
+	&gsgpu_debugfs_gca_config_fops,
+	&gsgpu_debugfs_sensors_fops,
+	&gsgpu_debugfs_wave_fops,
+	&gsgpu_debugfs_gpr_fops,
+};
+
+static const char *debugfs_regs_names[] = {
+	"gsgpu_regs",
+	"gsgpu_gca_config",
+	"gsgpu_sensors",
+	"gsgpu_wave",
+	"gsgpu_gpr",
+};
+
+/**
+ * gsgpu_debugfs_regs_init -	Initialize debugfs entries that provide
+ * 								register access.
+ *
+ * @adev: The device to attach the debugfs entries to
+ */
+int gsgpu_debugfs_regs_init(struct gsgpu_device *adev)
+{
+	struct drm_minor *minor = adev->ddev->primary;
+	struct dentry *ent, *root = minor->debugfs_root;
+	unsigned i, j;
+
+	for (i = 0; i < ARRAY_SIZE(debugfs_regs); i++) {
+		ent = debugfs_create_file(debugfs_regs_names[i],
+					  S_IFREG | S_IRUGO, root,
+					  adev, debugfs_regs[i]);
+		if (IS_ERR(ent)) {
+			for (j = 0; j < i; j++) {
+				debugfs_remove(adev->debugfs_regs[i]);
+				adev->debugfs_regs[i] = NULL;
+			}
+			return PTR_ERR(ent);
+		}
+
+		if (!i)
+			i_size_write(ent->d_inode, adev->rmmio_size);
+		adev->debugfs_regs[i] = ent;
+	}
+
+	return 0;
+}
+
+void gsgpu_debugfs_regs_cleanup(struct gsgpu_device *adev)
+{
+	unsigned i;
+
+	for (i = 0; i < ARRAY_SIZE(debugfs_regs); i++) {
+		if (adev->debugfs_regs[i]) {
+			debugfs_remove(adev->debugfs_regs[i]);
+			adev->debugfs_regs[i] = NULL;
+		}
+	}
+}
+
+static int gsgpu_debugfs_test_ib(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+	int r = 0, i;
+
+	/* hold on the scheduler */
+	for (i = 0; i < GSGPU_MAX_RINGS; i++) {
+		struct gsgpu_ring *ring = adev->rings[i];
+
+		if (!ring || !ring->sched.thread)
+			continue;
+		kthread_park(ring->sched.thread);
+	}
+
+	seq_printf(m, "run ib test:\n");
+	r = gsgpu_ib_ring_tests(adev);
+	if (r)
+		seq_printf(m, "ib ring tests failed (%d).\n", r);
+	else
+		seq_printf(m, "ib ring tests passed.\n");
+
+	/* go on the scheduler */
+	for (i = 0; i < GSGPU_MAX_RINGS; i++) {
+		struct gsgpu_ring *ring = adev->rings[i];
+
+		if (!ring || !ring->sched.thread)
+			continue;
+		kthread_unpark(ring->sched.thread);
+	}
+
+	return 0;
+}
+
+static int gsgpu_debugfs_test_ring(struct seq_file *m, void *data)
+{
+
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_ring *ring;
+	int r = 0, i;
+
+	/* hold on the scheduler */
+	for (i = 0; i < GSGPU_MAX_RINGS; i++) {
+		ring = adev->rings[i];
+
+		if (!ring || !ring->sched.thread)
+			continue;
+		kthread_park(ring->sched.thread);
+	}
+
+	seq_printf(m, "run ring test:\n");
+	ring = &adev->gfx.gfx_ring[0];
+	r = gsgpu_ring_test_ring(ring);
+	if (r)
+		seq_printf(m, "%s ring tests failed (%d).\n", ring->name, r);
+	else
+		seq_printf(m, "%s ring tests passed.\n", ring->name);
+
+
+	for (i = 0; i < adev->xdma.num_instances; i++) {
+		ring = &adev->xdma.instance[i].ring;
+		r = gsgpu_ring_test_ring(ring);
+		if (r)
+			seq_printf(m, "%s ring tests failed (%d).\n", ring->name, r);
+		else
+			seq_printf(m, "%s ring tests passed.\n", ring->name);
+	}
+
+	/* go on the scheduler */
+	for (i = 0; i < GSGPU_MAX_RINGS; i++) {
+		struct gsgpu_ring *ring = adev->rings[i];
+
+		if (!ring || !ring->sched.thread)
+			continue;
+		kthread_unpark(ring->sched.thread);
+	}
+
+	return 0;
+}
+
+static int gsgpu_debugfs_test_xdma(struct seq_file *m, void *data)
+{
+
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_ring *ring;
+	int r = 0, i;
+
+	/* hold on the scheduler */
+	for (i = 0; i < GSGPU_MAX_RINGS; i++) {
+		ring = adev->rings[i];
+
+		if (!ring || !ring->sched.thread)
+			continue;
+		kthread_park(ring->sched.thread);
+	}
+
+	seq_printf(m, "run xdma test:\n");
+	for (i = 0; i < adev->xdma.num_instances; i++) {
+		ring = &adev->xdma.instance[i].ring;
+		r = gsgpu_ring_test_xdma(ring, msecs_to_jiffies(5000));
+		if (r)
+			seq_printf(m, "%s ring tests failed (%d).\n", ring->name, r);
+		else
+			seq_printf(m, "%s ring tests passed.\n", ring->name);
+	}
+
+	/* go on the scheduler */
+	for (i = 0; i < GSGPU_MAX_RINGS; i++) {
+		struct gsgpu_ring *ring = adev->rings[i];
+
+		if (!ring || !ring->sched.thread)
+			continue;
+		kthread_unpark(ring->sched.thread);
+	}
+
+	return 0;
+}
+
+static int gsgpu_debugfs_get_vbios_dump(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+
+	seq_write(m, adev->bios, adev->bios_size);
+	return 0;
+}
+
+static int gsgpu_debugfs_evict_vram(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *)m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+
+	seq_printf(m, "(%d)\n", gsgpu_bo_evict_vram(adev));
+	return 0;
+}
+
+static int gsgpu_debugfs_evict_gtt(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *)m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+
+	seq_printf(m, "(%d)\n", ttm_bo_evict_mm(&adev->mman.bdev, TTM_PL_TT));
+	return 0;
+}
+
+static const struct drm_info_list gsgpu_debugfs_list[] = {
+	{"gsgpu_vbios", gsgpu_debugfs_get_vbios_dump},
+	{"gsgpu_test_ib", &gsgpu_debugfs_test_ib},
+	{"gsgpu_test_ring", &gsgpu_debugfs_test_ring},
+	{"gsgpu_test_xdma", &gsgpu_debugfs_test_xdma},
+	{"gsgpu_evict_vram", &gsgpu_debugfs_evict_vram},
+	{"gsgpu_evict_gtt", &gsgpu_debugfs_evict_gtt},
+};
+
+int gsgpu_debugfs_init(struct gsgpu_device *adev)
+{
+	return gsgpu_debugfs_add_files(adev, gsgpu_debugfs_list,
+					ARRAY_SIZE(gsgpu_debugfs_list));
+}
+
+#else
+int gsgpu_debugfs_init(struct gsgpu_device *adev)
+{
+	return 0;
+}
+int gsgpu_debugfs_regs_init(struct gsgpu_device *adev)
+{
+	return 0;
+}
+void gsgpu_debugfs_regs_cleanup(struct gsgpu_device *adev) { }
+#endif
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c
new file mode 100644
index 000000000000..241d1bb8efa6
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c
@@ -0,0 +1,2172 @@
+#include <linux/power_supply.h>
+#include <linux/kthread.h>
+#include <linux/console.h>
+#include <linux/slab.h>
+#include <drm/drmP.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/drm_atomic_helper.h>
+#include <drm/gsgpu_drm.h>
+#include <linux/vgaarb.h>
+#include <linux/vga_switcheroo.h>
+#include <linux/efi.h>
+#include <loongson-pch.h>
+#include "gsgpu.h"
+#include "gsgpu_trace.h"
+#include "gsgpu_cp.h"
+#include "gsgpu_common.h"
+#include "gsgpu_xdma.h"
+#include <linux/pci.h>
+#include <linux/firmware.h>
+#include "gsgpu_pm.h"
+
+#define GSGPU_RESUME_MS		2000
+
+static const char *gsgpu_family_name[] = {
+	"LG100",
+};
+
+/**
+ * gsgpu_cmd_exec
+ * XXX while block will taking kernel hang!
+ * @adev
+ * @cmd
+ * @arg0
+ * @arg1
+ *
+ * Return:
+
+ */
+uint64_t gsgpu_cmd_exec(struct gsgpu_device *adev, uint32_t cmd, uint32_t arg0, uint32_t arg1)
+{
+	uint64_t ret;
+
+	if (gsgpu_cp_wait_done(adev) == false)
+		return  ~0ULL;
+
+	writel(GSCMD_STS_NULL, ((void __iomem *)adev->rmmio) + GSGPU_STATUS);
+
+	writel(cmd, ((void __iomem *)adev->rmmio) + GSGPU_COMMAND);
+	writel(arg0, ((void __iomem *)adev->rmmio) + GSGPU_ARGUMENT0);
+	writel(arg1, ((void __iomem *)adev->rmmio) + GSGPU_ARGUMENT1);
+
+	writel(1, ((void __iomem *)adev->rmmio) + GSGPU_EC_INT);
+
+	if (gsgpu_cp_wait_done(adev) == false)
+		return  ~0ULL;
+
+	ret = readl(((void __iomem *)adev->rmmio) + GSGPU_RETURN0);
+	ret |= (uint64_t)readl(((void __iomem *)adev->rmmio) + GSGPU_RETURN1)<<32;
+
+	return ret;
+}
+
+/*
+ * MMIO register access helper functions.
+ */
+/**
+ * gsgpu_mm_rreg - read a memory mapped IO register
+ *
+ * @adev: gsgpu_device pointer
+ * @reg: dword aligned register offset
+ * @acc_flags: access flags which require special behavior
+ *
+ * Returns the 32 bit value from the offset specified.
+ */
+uint32_t gsgpu_mm_rreg(struct gsgpu_device *adev, uint32_t reg,
+			uint32_t acc_flags)
+{
+	uint32_t ret;
+
+	if (reg < adev->rmmio_size && !(acc_flags & GSGPU_REGS_IDX))
+		ret = readl(((void __iomem *)adev->rmmio) + reg);
+	else {
+		unsigned long flags;
+
+		spin_lock_irqsave(&adev->mmio_idx_lock, flags);
+		ret = 0;
+		DRM_DEBUG_DRIVER("%s Not implemented\n", __func__);
+		spin_unlock_irqrestore(&adev->mmio_idx_lock, flags);
+	}
+	trace_gsgpu_mm_rreg(adev->pdev->device, reg, ret);
+	return ret;
+}
+
+/*
+ * MMIO register read with bytes helper functions
+ * @offset:bytes offset from MMIO start
+ *
+*/
+
+/**
+ * gsgpu_mm_rreg8 - read a memory mapped IO register
+ *
+ * @adev: gsgpu_device pointer
+ * @offset: byte aligned register offset
+ *
+ * Returns the 8 bit value from the offset specified.
+ */
+uint8_t gsgpu_mm_rreg8(struct gsgpu_device *adev, uint32_t offset)
+{
+	if (offset < adev->rmmio_size)
+		return (readb(adev->rmmio + offset));
+	BUG();
+}
+
+/*
+ * MMIO register write with bytes helper functions
+ * @offset:bytes offset from MMIO start
+ * @value: the value want to be written to the register
+ *
+*/
+/**
+ * gsgpu_mm_wreg8 - read a memory mapped IO register
+ *
+ * @adev: gsgpu_device pointer
+ * @offset: byte aligned register offset
+ * @value: 8 bit value to write
+ *
+ * Writes the value specified to the offset specified.
+ */
+void gsgpu_mm_wreg8(struct gsgpu_device *adev, uint32_t offset, uint8_t value)
+{
+	if (offset < adev->rmmio_size)
+		writeb(value, adev->rmmio + offset);
+	else
+		BUG();
+}
+
+/**
+ * gsgpu_mm_wreg - write to a memory mapped IO register
+ *
+ * @adev: gsgpu_device pointer
+ * @reg: dword aligned register offset
+ * @v: 32 bit value to write to the register
+ * @acc_flags: access flags which require special behavior
+ *
+ * Writes the value specified to the offset specified.
+ */
+void gsgpu_mm_wreg(struct gsgpu_device *adev, uint32_t reg, uint32_t v)
+{
+	trace_gsgpu_mm_wreg(adev->pdev->device, reg, v);
+
+	//if(reg!=0xf0) DRM_ERROR("write mmio [%x] %x\n", reg, v);
+
+	if ((reg) < adev->rmmio_size) {
+		writel(v, ((void __iomem *)adev->rmmio) + reg);
+	} else
+		BUG();
+}
+
+/**
+ * gsgpu_device_vram_scratch_init - allocate the VRAM scratch page
+ *
+ * @adev: gsgpu device pointer
+ *
+ * Allocates a scratch page of VRAM for use by various things in the
+ * driver.
+ */
+static int gsgpu_device_vram_scratch_init(struct gsgpu_device *adev)
+{
+	return gsgpu_bo_create_kernel(adev, GSGPU_GPU_PAGE_SIZE,
+				       PAGE_SIZE, GSGPU_GEM_DOMAIN_VRAM,
+				       &adev->vram_scratch.robj,
+				       &adev->vram_scratch.gpu_addr,
+				       (void **)&adev->vram_scratch.ptr);
+}
+
+/**
+ * gsgpu_device_vram_scratch_fini - Free the VRAM scratch page
+ *
+ * @adev: gsgpu device pointer
+ *
+ * Frees the VRAM scratch page.
+ */
+static void gsgpu_device_vram_scratch_fini(struct gsgpu_device *adev)
+{
+	gsgpu_bo_free_kernel(&adev->vram_scratch.robj, NULL, NULL);
+}
+
+/**
+ * gsgpu_device_program_register_sequence - program an array of registers.
+ *
+ * @adev: gsgpu_device pointer
+ * @registers: pointer to the register array
+ * @array_size: size of the register array
+ *
+ * Programs an array or registers with and and or masks.
+ * This is a helper for setting golden registers.
+ */
+void gsgpu_device_program_register_sequence(struct gsgpu_device *adev,
+					     const u32 *registers,
+					     const u32 array_size)
+{
+	u32 tmp, reg, and_mask, or_mask;
+	int i;
+
+	if (array_size % 3)
+		return;
+
+	for (i = 0; i < array_size; i += 3) {
+		reg = registers[i + 0];
+		and_mask = registers[i + 1];
+		or_mask = registers[i + 2];
+
+		if (and_mask == 0xffffffff) {
+			tmp = or_mask;
+		} else {
+			tmp = RREG32(reg);
+			tmp &= ~and_mask;
+			tmp |= or_mask;
+		}
+		WREG32(reg, tmp);
+	}
+}
+
+/**
+ * gsgpu_device_pci_config_reset - reset the GPU
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Resets the GPU using the pci config reset sequence.
+ * Only applicable to asics prior to vega10.
+ */
+void gsgpu_device_pci_config_reset(struct gsgpu_device *adev)
+{
+	pci_write_config_dword(adev->pdev, 0x7c, GSGPU_ASIC_RESET_DATA);
+}
+
+/*
+ * gsgpu_device_wb_*()
+ * Writeback is the method by which the GPU updates special pages in memory
+ * with the status of certain GPU events (fences, ring pointers,etc.).
+ */
+
+/**
+ * gsgpu_device_wb_fini - Disable Writeback and free memory
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Disables Writeback and frees the Writeback memory (all asics).
+ * Used at driver shutdown.
+ */
+static void gsgpu_device_wb_fini(struct gsgpu_device *adev)
+{
+	if (adev->wb.wb_obj) {
+		gsgpu_bo_free_kernel(&adev->wb.wb_obj,
+				      &adev->wb.gpu_addr,
+				      (void **)&adev->wb.wb);
+		adev->wb.wb_obj = NULL;
+	}
+}
+
+/**
+ * gsgpu_device_wb_init- Init Writeback driver info and allocate memory
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Initializes writeback and allocates writeback memory (all asics).
+ * Used at driver startup.
+ * Returns 0 on success or an -error on failure.
+ */
+static int gsgpu_device_wb_init(struct gsgpu_device *adev)
+{
+	int r;
+
+	if (adev->wb.wb_obj == NULL) {
+		/* GSGPU_MAX_WB * sizeof(uint32_t) * 8 = GSGPU_MAX_WB 256bit slots */
+		r = gsgpu_bo_create_kernel(adev, GSGPU_MAX_WB * sizeof(uint32_t) * 8,
+					    PAGE_SIZE, GSGPU_GEM_DOMAIN_GTT,
+					    &adev->wb.wb_obj, &adev->wb.gpu_addr,
+					    (void **)&adev->wb.wb);
+		if (r) {
+			dev_warn(adev->dev, "(%d) create WB bo failed\n", r);
+			return r;
+		}
+
+		adev->wb.num_wb = GSGPU_MAX_WB;
+		memset(&adev->wb.used, 0, sizeof(adev->wb.used));
+
+		/* clear wb memory */
+		memset((char *)adev->wb.wb, 0, GSGPU_MAX_WB * sizeof(uint32_t) * 8);
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_wb_get - Allocate a wb entry
+ *
+ * @adev: gsgpu_device pointer
+ * @wb: wb index
+ *
+ * Allocate a wb slot for use by the driver (all asics).
+ * Returns 0 on success or -EINVAL on failure.
+ */
+int gsgpu_device_wb_get(struct gsgpu_device *adev, u32 *wb)
+{
+	unsigned long offset = find_first_zero_bit(adev->wb.used, adev->wb.num_wb);
+
+	if (offset < adev->wb.num_wb) {
+		__set_bit(offset, adev->wb.used);
+		*wb = offset << 3; /* convert to dw offset */
+		return 0;
+	} else {
+		return -EINVAL;
+	}
+}
+
+/**
+ * gsgpu_device_wb_free - Free a wb entry
+ *
+ * @adev: gsgpu_device pointer
+ * @wb: wb index
+ *
+ * Free a wb slot allocated for use by the driver (all asics)
+ */
+void gsgpu_device_wb_free(struct gsgpu_device *adev, u32 wb)
+{
+	wb >>= 3;
+	if (wb < adev->wb.num_wb)
+		__clear_bit(wb, adev->wb.used);
+}
+
+/**
+ * gsgpu_device_vram_location - try to find VRAM location
+ *
+ * @adev: gsgpu device structure holding all necessary informations
+ * @mc: memory controller structure holding memory informations
+ * @base: base address at which to put VRAM
+ *
+ * Function will try to place VRAM at base address provided
+ * as parameter.
+ */
+void gsgpu_device_vram_location(struct gsgpu_device *adev,
+				 struct gsgpu_gmc *mc, u64 base)
+{
+	uint64_t limit = (uint64_t)gsgpu_vram_limit << 20;
+
+	mc->vram_start = base;
+	mc->vram_end = mc->vram_start + mc->mc_vram_size - 1;
+	if (limit && limit < mc->real_vram_size)
+		mc->real_vram_size = limit;
+	dev_info(adev->dev, "Video RAM: %lluM 0x%016llX - 0x%016llX (%lluM used)\n",
+			mc->mc_vram_size >> 20, mc->vram_start,
+			mc->vram_end, mc->real_vram_size >> 20);
+}
+
+/**
+ * gsgpu_device_gart_location - try to find GART location
+ *
+ * @adev: gsgpu device structure holding all necessary informations
+ * @mc: memory controller structure holding memory informations
+ *
+ * Function will place try to place GART before or after VRAM.
+ *
+ * If GART size is bigger than space left then we ajust GART size.
+ * Thus function will never fails.
+ */
+void gsgpu_device_gart_location(struct gsgpu_device *adev,
+				 struct gsgpu_gmc *mc)
+{
+	u64 size_af, size_bf;
+
+	size_af = adev->gmc.mc_mask - mc->vram_end;
+	size_bf = mc->vram_start;
+	if (size_bf > size_af) {
+		if (mc->gart_size > size_bf) {
+			dev_warn(adev->dev, "limiting GART\n");
+			mc->gart_size = size_bf;
+		}
+		mc->gart_start = 0;
+		mc->gart_size = 256 << 20;
+	} else {
+		if (mc->gart_size > size_af) {
+			dev_warn(adev->dev, "limiting GART\n");
+			mc->gart_size = size_af;
+		}
+		/* VCE doesn't like it when BOs cross a 4GB segment, so align */
+		/* the GART base on a 4GB boundary as well. */
+		mc->gart_start = ALIGN(mc->vram_end + 1, 0x100000000ULL);
+	}
+
+	mc->gart_start = 0;
+	mc->gart_size = 256 << 20;
+
+	mc->gart_end = mc->gart_start + mc->gart_size - 1;
+	dev_info(adev->dev, "GART: %lluM 0x%016llX - 0x%016llX\n",
+			mc->gart_size >> 20, mc->gart_start, mc->gart_end);
+}
+
+/**
+ * gsgpu_device_resize_fb_bar - try to resize FB BAR
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Try to resize FB BAR to make all VRAM CPU accessible. We try very hard not
+ * to fail, but if any of the BARs is not accessible after the size we abort
+ * driver loading by returning -ENODEV.
+ */
+int gsgpu_device_resize_fb_bar(struct gsgpu_device *adev)
+{
+	u64 space_needed = roundup_pow_of_two(adev->gmc.real_vram_size);
+	u32 rbar_size = order_base_2(((space_needed >> 20) | 1)) - 1;
+	struct pci_bus *root;
+	struct resource *res;
+	unsigned i;
+	u16 cmd;
+	int r;
+
+	/* Check if the root BUS has 64bit memory resources */
+	root = adev->pdev->bus;
+	while (root->parent)
+		root = root->parent;
+
+	pci_bus_for_each_resource(root, res, i) {
+		if (res && res->flags & (IORESOURCE_MEM | IORESOURCE_MEM_64) &&
+		    res->start > 0x100000000ull)
+			break;
+	}
+
+	/* Trying to resize is pointless without a root hub window above 4GB */
+	if (!res)
+		return 0;
+
+	/* Disable memory decoding while we change the BAR addresses and size */
+	pci_read_config_word(adev->pdev, PCI_COMMAND, &cmd);
+	pci_write_config_word(adev->pdev, PCI_COMMAND,
+			      cmd & ~PCI_COMMAND_MEMORY);
+
+	pci_release_resource(adev->pdev, 3);
+
+	pci_release_resource(adev->pdev, 0);
+
+	r = pci_resize_resource(adev->pdev, 0, rbar_size);
+	if (r == -ENOSPC)
+		DRM_INFO("Not enough PCI address space for a large BAR.");
+	else if (r && r != -ENOTSUPP)
+		DRM_ERROR("Problem resizing BAR0 (%d).", r);
+
+	pci_assign_unassigned_bus_resources(adev->pdev->bus);
+
+	/* When the fb BAR isn't available we have no chance of
+	 * using the device.
+	 */
+	if ((pci_resource_flags(adev->pdev, 0) & IORESOURCE_UNSET))
+		return -ENODEV;
+
+	pci_write_config_word(adev->pdev, PCI_COMMAND, cmd);
+
+	return 0;
+}
+
+/*
+ * GPU helpers function.
+ */
+/**
+ * gsgpu_device_need_post - check if the hw need post or not
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Check if the asic has been initialized (all asics) at driver startup
+ * or post is needed if  hw reset is performed.
+ * Returns true if need or false if not.
+ */
+bool gsgpu_device_need_post(struct gsgpu_device *adev)
+{
+	if (adev->has_hw_reset) {
+		adev->has_hw_reset = false;
+	}
+
+	return true;
+}
+
+/* if we get transitioned to only one device, take VGA back */
+/**
+ * gsgpu_device_vga_set_decode - enable/disable vga decode
+ *
+ * @cookie: gsgpu_device pointer
+ * @state: enable/disable vga decode
+ *
+ * Enable/disable vga decode (all asics).
+ * Returns VGA resource flags.
+ */
+static unsigned int gsgpu_device_vga_set_decode(void *cookie, bool state)
+{
+	struct gsgpu_device *adev = cookie;
+	gsgpu_asic_set_vga_state(adev, state);
+	if (state)
+		return VGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM |
+		       VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM;
+	else
+		return VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM;
+}
+
+/**
+ * gsgpu_device_check_block_size - validate the vm block size
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Validates the vm block size specified via module parameter.
+ * The vm block size defines number of bits in page table versus page directory,
+ * a page is 4KB so we have 12 bits offset, minimum 9 bits in the
+ * page table and the remaining bits are in the page directory.
+ */
+static void gsgpu_device_check_block_size(struct gsgpu_device *adev)
+{
+	/* defines number of bits in page table versus page directory,
+	 * a page is 4KB so we have 12 bits offset, minimum 9 bits in the
+	 * page table and the remaining bits are in the page directory */
+	if (gsgpu_vm_block_size == -1)
+		return;
+
+	if (gsgpu_vm_block_size < GSGPU_PAGE_PTE_SHIFT) {
+		dev_warn(adev->dev, "VM page table size (%d) too small\n",
+			 gsgpu_vm_block_size);
+		gsgpu_vm_block_size = -1;
+	}
+}
+
+/**
+ * gsgpu_device_check_vm_size - validate the vm size
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Validates the vm size in GB specified via module parameter.
+ * The VM size is the size of the GPU virtual memory space in GB.
+ */
+static void gsgpu_device_check_vm_size(struct gsgpu_device *adev)
+{
+	/* no need to check the default value */
+	if (gsgpu_vm_size == -1)
+		return;
+
+	if (gsgpu_vm_size < 1) {
+		dev_warn(adev->dev, "VM size (%d) too small, min is 1GB\n",
+			 gsgpu_vm_size);
+		gsgpu_vm_size = -1;
+	}
+}
+
+/**
+ * gsgpu_device_check_arguments - validate module params
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Validates certain module parameters and updates
+ * the associated values used by the driver (all asics).
+ */
+static void gsgpu_device_check_arguments(struct gsgpu_device *adev)
+{
+	if (gsgpu_sched_jobs < 4) {
+		dev_warn(adev->dev, "sched jobs (%d) must be at least 4\n",
+			gsgpu_sched_jobs);
+		gsgpu_sched_jobs = 4;
+	} else if (!is_power_of_2(gsgpu_sched_jobs)) {
+		dev_warn(adev->dev, "sched jobs (%d) must be a power of 2\n",
+			 gsgpu_sched_jobs);
+		gsgpu_sched_jobs = roundup_pow_of_two(gsgpu_sched_jobs);
+	}
+
+	if (gsgpu_gart_size != -1 && gsgpu_gart_size < 32) {
+		/* gart size must be greater or equal to 32M */
+		dev_warn(adev->dev, "gart size (%d) too small\n",
+			 gsgpu_gart_size);
+		gsgpu_gart_size = -1;
+	}
+
+	if (gsgpu_gtt_size != -1 && gsgpu_gtt_size < 32) {
+		/* gtt size must be greater or equal to 32M */
+		dev_warn(adev->dev, "gtt size (%d) too small\n",
+				 gsgpu_gtt_size);
+		gsgpu_gtt_size = -1;
+	}
+
+	gsgpu_device_check_vm_size(adev);
+
+	gsgpu_device_check_block_size(adev);
+
+	if (gsgpu_vram_page_split != -1 && (gsgpu_vram_page_split < 16 ||
+	    !is_power_of_2(gsgpu_vram_page_split))) {
+		dev_warn(adev->dev, "invalid VRAM page split (%d)\n",
+			 gsgpu_vram_page_split);
+		gsgpu_vram_page_split = 2048;
+	}
+
+	if (gsgpu_lockup_timeout == 0) {
+		dev_warn(adev->dev, "lockup_timeout msut be > 0, adjusting to 10000\n");
+		gsgpu_lockup_timeout = 10000;
+	}
+}
+
+/**
+ * gsgpu_switcheroo_set_state - set switcheroo state
+ *
+ * @pdev: pci dev pointer
+ * @state: vga_switcheroo state
+ *
+ * Callback for the switcheroo driver.  Suspends or resumes the
+ * the asics before or after it is powered up using ACPI methods.
+ */
+static void gsgpu_switcheroo_set_state(struct pci_dev *pdev, enum vga_switcheroo_state state)
+{
+	struct drm_device *dev = pci_get_drvdata(pdev);
+
+	if (state == VGA_SWITCHEROO_OFF)
+		return;
+
+	if (state == VGA_SWITCHEROO_ON) {
+		pr_info("gsgpu: switched on\n");
+		/* don't suspend or resume card normally */
+		dev->switch_power_state = DRM_SWITCH_POWER_CHANGING;
+
+		gsgpu_device_resume(dev, true, true);
+
+		dev->switch_power_state = DRM_SWITCH_POWER_ON;
+		drm_kms_helper_poll_enable(dev);
+	} else {
+		pr_info("gsgpu: switched off\n");
+		drm_kms_helper_poll_disable(dev);
+		dev->switch_power_state = DRM_SWITCH_POWER_CHANGING;
+		gsgpu_device_suspend(dev, true, true);
+		dev->switch_power_state = DRM_SWITCH_POWER_OFF;
+	}
+}
+
+/**
+ * gsgpu_switcheroo_can_switch - see if switcheroo state can change
+ *
+ * @pdev: pci dev pointer
+ *
+ * Callback for the switcheroo driver.  Check of the switcheroo
+ * state can be changed.
+ * Returns true if the state can be changed, false if not.
+ */
+static bool gsgpu_switcheroo_can_switch(struct pci_dev *pdev)
+{
+	struct drm_device *dev = pci_get_drvdata(pdev);
+
+	/*
+	* FIXME: open_count is protected by drm_global_mutex but that would lead to
+	* locking inversion with the driver load path. And the access here is
+	* completely racy anyway. So don't bother with locking for now.
+	*/
+	return dev->open_count == 0;
+}
+
+static const struct vga_switcheroo_client_ops gsgpu_switcheroo_ops = {
+	.set_gpu_state = gsgpu_switcheroo_set_state,
+	.reprobe = NULL,
+	.can_switch = gsgpu_switcheroo_can_switch,
+};
+
+/**
+ * gsgpu_device_ip_wait_for_idle - wait for idle
+ *
+ * @adev: gsgpu_device pointer
+ * @block_type: Type of hardware IP (SMU, GFX, UVD, etc.)
+ *
+ * Waits for the request hardware IP to be idle.
+ * Returns 0 for success or a negative error code on failure.
+ */
+int gsgpu_device_ip_wait_for_idle(struct gsgpu_device *adev,
+				   enum gsgpu_ip_block_type block_type)
+{
+	int i, r;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		if (adev->ip_blocks[i].version->type == block_type) {
+			r = adev->ip_blocks[i].version->funcs->wait_for_idle((void *)adev);
+			if (r)
+				return r;
+			break;
+		}
+	}
+	return 0;
+
+}
+
+/**
+ * gsgpu_device_ip_is_idle - is the hardware IP idle
+ *
+ * @adev: gsgpu_device pointer
+ * @block_type: Type of hardware IP (SMU, GFX, UVD, etc.)
+ *
+ * Check if the hardware IP is idle or not.
+ * Returns true if it the IP is idle, false if not.
+ */
+bool gsgpu_device_ip_is_idle(struct gsgpu_device *adev,
+			      enum gsgpu_ip_block_type block_type)
+{
+	int i;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		if (adev->ip_blocks[i].version->type == block_type)
+			return adev->ip_blocks[i].version->funcs->is_idle((void *)adev);
+	}
+	return true;
+
+}
+
+/**
+ * gsgpu_device_ip_get_ip_block - get a hw IP pointer
+ *
+ * @adev: gsgpu_device pointer
+ * @type: Type of hardware IP (SMU, GFX, UVD, etc.)
+ *
+ * Returns a pointer to the hardware IP block structure
+ * if it exists for the asic, otherwise NULL.
+ */
+struct gsgpu_ip_block *
+gsgpu_device_ip_get_ip_block(struct gsgpu_device *adev,
+			      enum gsgpu_ip_block_type type)
+{
+	int i;
+
+	for (i = 0; i < adev->num_ip_blocks; i++)
+		if (adev->ip_blocks[i].version->type == type)
+			return &adev->ip_blocks[i];
+
+	return NULL;
+}
+
+/**
+ * gsgpu_device_ip_block_version_cmp
+ *
+ * @adev: gsgpu_device pointer
+ * @type: enum gsgpu_ip_block_type
+ * @major: major version
+ * @minor: minor version
+ *
+ * return 0 if equal or greater
+ * return 1 if smaller or the ip_block doesn't exist
+ */
+int gsgpu_device_ip_block_version_cmp(struct gsgpu_device *adev,
+				       enum gsgpu_ip_block_type type,
+				       u32 major, u32 minor)
+{
+	struct gsgpu_ip_block *ip_block = gsgpu_device_ip_get_ip_block(adev, type);
+
+	if (ip_block && ((ip_block->version->major > major) ||
+			((ip_block->version->major == major) &&
+			(ip_block->version->minor >= minor))))
+		return 0;
+
+	return 1;
+}
+
+/**
+ * gsgpu_device_ip_block_add
+ *
+ * @adev: gsgpu_device pointer
+ * @ip_block_version: pointer to the IP to add
+ *
+ * Adds the IP block driver information to the collection of IPs
+ * on the asic.
+ */
+int gsgpu_device_ip_block_add(struct gsgpu_device *adev,
+			       const struct gsgpu_ip_block_version *ip_block_version)
+{
+	if (!ip_block_version)
+		return -EINVAL;
+
+	DRM_DEBUG_DRIVER("add ip block number %d <%s>\n", adev->num_ip_blocks,
+			 ip_block_version->funcs->name);
+
+	adev->ip_blocks[adev->num_ip_blocks++].version = ip_block_version;
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_early_init - run early init for hardware IPs
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Early initialization pass for hardware IPs.  The hardware IPs that make
+ * up each asic are discovered each IP's early_init callback is run.  This
+ * is the first stage in initializing the asic.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_early_init(struct gsgpu_device *adev)
+{
+	int i, r;
+
+	adev->family = GSGPU_FAMILY_VI;
+	r = gsgpu_set_ip_blocks(adev);
+	if (r)
+		return r;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (adev->ip_blocks[i].version->funcs->early_init) {
+			r = adev->ip_blocks[i].version->funcs->early_init((void *)adev);
+			if (r == -ENOENT) {
+				adev->ip_blocks[i].status.valid = false;
+			} else if (r) {
+				DRM_ERROR("early_init of IP block <%s> failed %d\n",
+					  adev->ip_blocks[i].version->funcs->name, r);
+				return r;
+			} else {
+				adev->ip_blocks[i].status.valid = true;
+			}
+		} else {
+			adev->ip_blocks[i].status.valid = true;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_init - run init for hardware IPs
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Main initialization pass for hardware IPs.  The list of all the hardware
+ * IPs that make up the asic is walked and the sw_init and hw_init callbacks
+ * are run.  sw_init initializes the software state associated with each IP
+ * and hw_init initializes the hardware associated with each IP.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_init(struct gsgpu_device *adev)
+{
+	int i, r;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		r = adev->ip_blocks[i].version->funcs->sw_init((void *)adev);
+		if (r) {
+			DRM_ERROR("sw_init of IP block <%s> failed %d\n",
+				  adev->ip_blocks[i].version->funcs->name, r);
+			return r;
+		}
+		adev->ip_blocks[i].status.sw = true;
+
+		/* need to do gmc hw init early so we can allocate gpu mem */
+		if (adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_GMC) {
+			r = gsgpu_device_vram_scratch_init(adev);
+			if (r) {
+				DRM_ERROR("gsgpu_vram_scratch_init failed %d\n", r);
+				return r;
+			}
+			r = adev->ip_blocks[i].version->funcs->hw_init((void *)adev);
+			if (r) {
+				DRM_ERROR("hw_init %d failed %d\n", i, r);
+				return r;
+			}
+			r = gsgpu_device_wb_init(adev);
+			if (r) {
+				DRM_ERROR("gsgpu_device_wb_init failed %d\n", r);
+				return r;
+			}
+			adev->ip_blocks[i].status.hw = true;
+		}
+	}
+
+	r = gsgpu_ib_pool_init(adev);
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.sw)
+			continue;
+		if (adev->ip_blocks[i].status.hw)
+			continue;
+		r = adev->ip_blocks[i].version->funcs->hw_init((void *)adev);
+		if (r) {
+			DRM_ERROR("hw_init of IP block <%s> failed %d\n",
+				  adev->ip_blocks[i].version->funcs->name, r);
+			return r;
+		}
+		adev->ip_blocks[i].status.hw = true;
+	}
+
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_fill_reset_magic - writes reset magic to gart pointer
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Writes a reset magic value to the gart pointer in VRAM.  The driver calls
+ * this function before a GPU reset.  If the value is retained after a
+ * GPU reset, VRAM has not been lost.  Some GPU resets may destry VRAM contents.
+ */
+static void gsgpu_device_fill_reset_magic(struct gsgpu_device *adev)
+{
+	memcpy(adev->reset_magic, adev->gart.ptr, GSGPU_RESET_MAGIC_NUM);
+}
+
+/**
+ * gsgpu_device_check_vram_lost - check if vram is valid
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Checks the reset magic value written to the gart pointer in VRAM.
+ * The driver calls this after a GPU reset to see if the contents of
+ * VRAM is lost or now.
+ * returns true if vram is lost, false if not.
+ */
+static bool gsgpu_device_check_vram_lost(struct gsgpu_device *adev)
+{
+	return !!memcmp(adev->gart.ptr, adev->reset_magic,
+			GSGPU_RESET_MAGIC_NUM);
+}
+
+/**
+ * gsgpu_device_ip_late_init - run late init for hardware IPs
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Late initialization pass for hardware IPs.  The list of all the hardware
+ * IPs that make up the asic is walked and the late_init callbacks are run.
+ * late_init covers any special initialization that an IP requires
+ * after all of the have been initialized or something that needs to happen
+ * late in the init process.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_late_init(struct gsgpu_device *adev)
+{
+	int i = 0, r;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		if (adev->ip_blocks[i].version->funcs->late_init) {
+			r = adev->ip_blocks[i].version->funcs->late_init((void *)adev);
+			if (r) {
+				DRM_ERROR("late_init of IP block <%s> failed %d\n",
+					  adev->ip_blocks[i].version->funcs->name, r);
+				return r;
+			}
+			adev->ip_blocks[i].status.late_initialized = true;
+		}
+	}
+
+	queue_delayed_work(system_wq, &adev->late_init_work,
+			   msecs_to_jiffies(GSGPU_RESUME_MS));
+
+	gsgpu_device_fill_reset_magic(adev);
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_fini - run fini for hardware IPs
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Main teardown pass for hardware IPs.  The list of all the hardware
+ * IPs that make up the asic is walked and the hw_fini and sw_fini callbacks
+ * are run.  hw_fini tears down the hardware associated with each IP
+ * and sw_fini tears down any software state associated with each IP.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_fini(struct gsgpu_device *adev)
+{
+	int i, r;
+
+	for (i = adev->num_ip_blocks - 1; i >= 0; i--) {
+		if (!adev->ip_blocks[i].status.hw)
+			continue;
+
+		r = adev->ip_blocks[i].version->funcs->hw_fini((void *)adev);
+		/* XXX handle errors */
+		if (r) {
+			DRM_DEBUG("hw_fini of IP block <%s> failed %d\n",
+				  adev->ip_blocks[i].version->funcs->name, r);
+		}
+
+		adev->ip_blocks[i].status.hw = false;
+	}
+
+
+	for (i = adev->num_ip_blocks - 1; i >= 0; i--) {
+		if (!adev->ip_blocks[i].status.sw)
+			continue;
+
+		if (adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_GMC) {
+			gsgpu_device_wb_fini(adev);
+			gsgpu_device_vram_scratch_fini(adev);
+		}
+
+		r = adev->ip_blocks[i].version->funcs->sw_fini((void *)adev);
+		/* XXX handle errors */
+		if (r) {
+			DRM_DEBUG("sw_fini of IP block <%s> failed %d\n",
+				  adev->ip_blocks[i].version->funcs->name, r);
+		}
+		adev->ip_blocks[i].status.sw = false;
+		adev->ip_blocks[i].status.valid = false;
+	}
+
+	for (i = adev->num_ip_blocks - 1; i >= 0; i--) {
+		if (!adev->ip_blocks[i].status.late_initialized)
+			continue;
+		if (adev->ip_blocks[i].version->funcs->late_fini)
+			adev->ip_blocks[i].version->funcs->late_fini((void *)adev);
+		adev->ip_blocks[i].status.late_initialized = false;
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_late_init_func_handler - work handler for clockgating
+ *
+ * @work: work_struct
+ *
+ * Work handler for gsgpu_device_ip_late_set_cg_state.  We put the
+ * clockgating setup into a worker thread to speed up driver init and
+ * resume from suspend.
+ */
+static void gsgpu_device_ip_late_init_func_handler(struct work_struct *work)
+{
+#if 1
+	return;
+#else
+	struct gsgpu_device *adev =
+		container_of(work, struct gsgpu_device, late_init_work.work);
+	int r;
+
+	r = gsgpu_ib_ring_tests(adev);
+	if (r)
+		DRM_ERROR("ib ring test failed (%d).\n", r);
+
+	r = gsgpu_ring_test_xdma(&adev->xdma.instance[0].ring, msecs_to_jiffies(5000));
+	if (r)
+		DRM_ERROR("xdma test failed (%d).\n", r);
+#endif
+}
+
+/**
+ * gsgpu_device_ip_suspend_phase1 - run suspend for hardware IPs (phase 1)
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Main suspend function for hardware IPs.  The list of all the hardware
+ * IPs that make up the asic is walked, clockgating is disabled and the
+ * suspend callbacks are run.  suspend puts the hardware and software state
+ * in each IP into a state suitable for suspend.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_suspend_phase1(struct gsgpu_device *adev)
+{
+	int i, r;
+
+	for (i = adev->num_ip_blocks - 1; i >= 0; i--) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		/* displays are handled separately */
+		if (adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_DCE) {
+			/* XXX handle errors */
+			r = adev->ip_blocks[i].version->funcs->suspend(adev);
+			/* XXX handle errors */
+			if (r) {
+				DRM_ERROR("suspend of IP block <%s> failed %d\n",
+					  adev->ip_blocks[i].version->funcs->name, r);
+			}
+		}
+	}
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_suspend_phase2 - run suspend for hardware IPs (phase 2)
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Main suspend function for hardware IPs.  The list of all the hardware
+ * IPs that make up the asic is walked, clockgating is disabled and the
+ * suspend callbacks are run.  suspend puts the hardware and software state
+ * in each IP into a state suitable for suspend.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_suspend_phase2(struct gsgpu_device *adev)
+{
+	int i, r;
+
+	/* call smu to disable gfx off feature first when suspend */
+
+	for (i = adev->num_ip_blocks - 1; i >= 0; i--) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		/* displays are handled in phase1 */
+		if (adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_DCE)
+			continue;
+		/* XXX handle errors */
+		r = adev->ip_blocks[i].version->funcs->suspend(adev);
+		/* XXX handle errors */
+		if (r) {
+			DRM_ERROR("suspend of IP block <%s> failed %d\n",
+				  adev->ip_blocks[i].version->funcs->name, r);
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_suspend - run suspend for hardware IPs
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Main suspend function for hardware IPs.  The list of all the hardware
+ * IPs that make up the asic is walked, clockgating is disabled and the
+ * suspend callbacks are run.  suspend puts the hardware and software state
+ * in each IP into a state suitable for suspend.
+ * Returns 0 on success, negative error code on failure.
+ */
+int gsgpu_device_ip_suspend(struct gsgpu_device *adev)
+{
+	int r;
+
+	r = gsgpu_device_ip_suspend_phase1(adev);
+	if (r)
+		return r;
+	r = gsgpu_device_ip_suspend_phase2(adev);
+
+	return r;
+}
+
+
+/**
+ * gsgpu_device_ip_resume_phase1 - run resume for hardware IPs
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * First resume function for hardware IPs.  The list of all the hardware
+ * IPs that make up the asic is walked and the resume callbacks are run for
+ * COMMON, GMC, and IH.  resume puts the hardware into a functional state
+ * after a suspend and updates the software state as necessary.  This
+ * function is also used for restoring the GPU after a GPU reset.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_resume_phase1(struct gsgpu_device *adev)
+{
+	int i, r;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		if (adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_COMMON ||
+		    adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_GMC ||
+		    adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_IH) {
+			r = adev->ip_blocks[i].version->funcs->resume(adev);
+			if (r) {
+				DRM_ERROR("resume of IP block <%s> failed %d\n",
+					  adev->ip_blocks[i].version->funcs->name, r);
+				return r;
+			}
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_resume_phase2 - run resume for hardware IPs
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * First resume function for hardware IPs.  The list of all the hardware
+ * IPs that make up the asic is walked and the resume callbacks are run for
+ * all blocks except COMMON, GMC, and IH.  resume puts the hardware into a
+ * functional state after a suspend and updates the software state as
+ * necessary.  This function is also used for restoring the GPU after a GPU
+ * reset.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_resume_phase2(struct gsgpu_device *adev)
+{
+	int i, r;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		if (adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_COMMON ||
+		    adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_GMC ||
+		    adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_IH)
+			continue;
+		r = adev->ip_blocks[i].version->funcs->resume(adev);
+		if (r) {
+			DRM_ERROR("resume of IP block <%s> failed %d\n",
+				  adev->ip_blocks[i].version->funcs->name, r);
+			return r;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_resume - run resume for hardware IPs
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Main resume function for hardware IPs.  The hardware IPs
+ * are split into two resume functions because they are
+ * are also used in in recovering from a GPU reset and some additional
+ * steps need to be take between them.  In this case (S3/S4) they are
+ * run sequentially.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_resume(struct gsgpu_device *adev)
+{
+	int r;
+
+	r = gsgpu_device_ip_resume_phase1(adev);
+	if (r)
+		return r;
+	r = gsgpu_device_ip_resume_phase2(adev);
+
+	return r;
+}
+
+extern struct pci_dev *loongson_dc_pdev;
+
+/**
+ * gsgpu_device_init - initialize the driver
+ *
+ * @adev: gsgpu_device pointer
+ * @ddev: drm dev pointer
+ * @pdev: pci dev pointer
+ * @flags: driver flags
+ *
+ * Initializes the driver info and hw (all asics).
+ * Returns 0 for success or an error on failure.
+ * Called at driver startup.
+ */
+int gsgpu_device_init(struct gsgpu_device *adev,
+		       struct drm_device *ddev,
+		       struct pci_dev *pdev,
+		       uint32_t flags)
+{
+	int r;
+	bool runtime = false;
+	u32 max_MBps;
+
+	adev->shutdown = false;
+	adev->dev = &pdev->dev;
+	adev->ddev = ddev;
+	adev->pdev = pdev;
+	adev->loongson_dc = loongson_dc_pdev;
+	adev->flags = flags;
+	adev->family_type = flags & GAGPU_ASIC_MASK;
+	adev->usec_timeout = GSGPU_MAX_USEC_TIMEOUT;
+	adev->gmc.gart_size = 512 * 1024 * 1024;
+	adev->accel_working = false;
+	adev->num_rings = 0;
+	adev->mman.buffer_funcs = NULL;
+	adev->mman.buffer_funcs_ring = NULL;
+	adev->vm_manager.vm_pte_funcs = NULL;
+	adev->vm_manager.vm_pte_num_rings = 0;
+	adev->gmc.gmc_funcs = NULL;
+	adev->fence_context = dma_fence_context_alloc(GSGPU_MAX_RINGS);
+
+	DRM_INFO("initializing kernel modesetting (%s 0x%04X:0x%04X 0x%04X:0x%04X 0x%02X).\n",
+		 gsgpu_family_name[adev->family_type], pdev->vendor, pdev->device,
+		 pdev->subsystem_vendor, pdev->subsystem_device, pdev->revision);
+
+	/* mutex initialization are all done here so we
+	 * can recall function without having locking issues */
+	atomic_set(&adev->irq.ih.lock, 0);
+	mutex_init(&adev->firmware.mutex);
+	mutex_init(&adev->gfx.gpu_clock_mutex);
+	mutex_init(&adev->srbm_mutex);
+	mutex_init(&adev->grbm_idx_mutex);
+	mutex_init(&adev->mn_lock);
+	hash_init(adev->mn_hash);
+	mutex_init(&adev->lock_reset);
+	spin_lock_init(&adev->dc_mmio_lock);
+
+	gsgpu_device_check_arguments(adev);
+
+	spin_lock_init(&adev->mmio_idx_lock);
+	spin_lock_init(&adev->pcie_idx_lock);
+	spin_lock_init(&adev->se_cac_idx_lock);
+	spin_lock_init(&adev->mm_stats.lock);
+
+	INIT_LIST_HEAD(&adev->shadow_list);
+	mutex_init(&adev->shadow_list_lock);
+
+	INIT_LIST_HEAD(&adev->ring_lru_list);
+	spin_lock_init(&adev->ring_lru_list_lock);
+
+	INIT_DELAYED_WORK(&adev->late_init_work,
+			  gsgpu_device_ip_late_init_func_handler);
+
+	/* pci get dc revision */
+	pci_read_config_byte(adev->loongson_dc, 0x8, &adev->dc_revision);
+	DRM_DEBUG_DRIVER("GSGPU dc revision id 0x%x\n", adev->dc_revision);
+	if (adev->dc_revision == 0x10) {
+		adev->chip = dev_2k2000;
+		DRM_INFO("Set 2K2000 device in gsgpu driver\n");
+	} else if (adev->dc_revision < 0x10) {
+		adev->chip = dev_7a2000;
+		DRM_INFO("Set 7A2000 device in gsgpu driver\n");
+	}
+
+	adev->rmmio_base = pci_resource_start(adev->pdev, 0);
+	adev->rmmio_size = pci_resource_len(adev->pdev, 0);
+
+	adev->rmmio = ioremap(adev->rmmio_base, adev->rmmio_size);
+	if (adev->rmmio == NULL) {
+		return -ENOMEM;
+	}
+	DRM_INFO("register mmio base: 0x%08X\n", (uint32_t)adev->rmmio_base);
+	DRM_INFO("register mmio size: %u\n", (unsigned)adev->rmmio_size);
+
+	/* loongson dc */
+	adev->loongson_dc_rmmio_base = pci_resource_start(adev->loongson_dc, 0);
+	adev->loongson_dc_rmmio_size = pci_resource_len(adev->loongson_dc, 0);
+
+	adev->loongson_dc_rmmio = pci_iomap(adev->loongson_dc, 0, adev->loongson_dc_rmmio_size);
+	if (adev->loongson_dc_rmmio == NULL) {
+		return -ENOMEM;
+	}
+
+	DRM_INFO("gsgpu dc register mmio base: 0x%08X\n", (uint32_t)adev->loongson_dc_rmmio_base);
+	DRM_INFO("gsgpu dc register mmio size: %u\n", (unsigned)adev->loongson_dc_rmmio_size);
+
+	adev->io_base = ioremap(LS7A_CHIPCFG_REG_BASE, 0xf);
+	if (adev->io_base == NULL)
+		return -ENOMEM;
+	DRM_INFO("gsgpu dc io base: 0x%lx\n", (unsigned long)adev->io_base);
+
+	/* early init functions */
+	r = gsgpu_device_ip_early_init(adev);
+	if (r)
+		return r;
+
+	/* if we have > 1 VGA cards, then disable the gsgpu VGA resources */
+	/* this will fail for cards that aren't VGA class devices, just
+	 * ignore it */
+	vga_client_register(adev->loongson_dc, adev, NULL, gsgpu_device_vga_set_decode);
+	vga_switcheroo_register_client(adev->pdev,
+				       &gsgpu_switcheroo_ops, runtime);
+
+	/* Fence driver */
+	r = gsgpu_fence_driver_init(adev);
+	if (r) {
+		dev_err(adev->dev, "gsgpu_fence_driver_init failed\n");
+		goto failed;
+	}
+
+	/* init the mode config */
+	drm_mode_config_init(adev->ddev);
+
+	r = gsgpu_cp_init(adev);
+	if (r) {
+		/* failed in exclusive mode due to timeout */
+		dev_err(adev->dev, "gsgpu_cp_init failed\n");
+		goto failed;
+	}
+
+	r = gsgpu_device_ip_init(adev);
+	if (r) {
+		/* failed in exclusive mode due to timeout */
+		dev_err(adev->dev, "gsgpu_device_ip_init failed\n");
+		goto failed;
+	}
+
+	adev->accel_working = true;
+
+	/* Initialize the buffer migration limit. */
+	if (gsgpu_moverate >= 0)
+		max_MBps = gsgpu_moverate;
+	else
+		max_MBps = 8; /* Allow 8 MB/s. */
+	/* Get a log2 for easy divisions. */
+	adev->mm_stats.log2_max_MBps = ilog2(max(1u, max_MBps));
+
+	r = gsgpu_ib_pool_init(adev);
+	if (r) {
+		dev_err(adev->dev, "IB initialization failed (%d).\n", r);
+		goto failed;
+	}
+
+	gsgpu_fbdev_init(adev);
+
+	r = gsgpu_pm_sysfs_init(adev);
+	if (r)
+		DRM_ERROR("registering pm debugfs failed (%d).\n", r);
+
+	r = gsgpu_debugfs_gem_init(adev);
+	if (r)
+		DRM_ERROR("registering gem debugfs failed (%d).\n", r);
+
+	r = gsgpu_debugfs_sema_init(adev);
+	if (r)
+		DRM_ERROR("registering sema debugfs failed (%d).\n", r);
+
+	r = gsgpu_debugfs_regs_init(adev);
+	if (r)
+		DRM_ERROR("registering register debugfs failed (%d).\n", r);
+
+	r = gsgpu_debugfs_firmware_init(adev);
+	if (r)
+		DRM_ERROR("registering firmware debugfs failed (%d).\n", r);
+
+	r = gsgpu_debugfs_init(adev);
+	if (r)
+		DRM_ERROR("Creating debugfs files failed (%d).\n", r);
+
+	if ((gsgpu_testing & 1)) {
+		if (adev->accel_working)
+			gsgpu_test_moves(adev);
+		else
+			DRM_INFO("gsgpu: acceleration disabled, skipping move tests\n");
+	}
+	if (gsgpu_benchmarking) {
+		if (adev->accel_working)
+			gsgpu_benchmark(adev, gsgpu_benchmarking);
+		else
+			DRM_INFO("gsgpu: acceleration disabled, skipping benchmarks\n");
+	}
+
+	/* enable clockgating, etc. after ib tests, etc. since some blocks require
+	 * explicit gating rather than handling it automatically.
+	 */
+	r = gsgpu_device_ip_late_init(adev);
+	if (r) {
+		dev_err(adev->dev, "gsgpu_device_ip_late_init failed\n");
+		goto failed;
+	}
+
+	xdma_ring_test_xdma_loop(&adev->xdma.instance[0].ring, msecs_to_jiffies(5000));
+
+	return 0;
+
+failed:
+	if (runtime)
+		vga_switcheroo_fini_domain_pm_ops(adev->dev);
+
+	return r;
+}
+
+/**
+ * gsgpu_device_fini - tear down the driver
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Tear down the driver info (all asics).
+ * Called at driver shutdown.
+ */
+void gsgpu_device_fini(struct gsgpu_device *adev)
+{
+	int r;
+
+	adev->shutdown = true;
+	/* disable all interrupts */
+	gsgpu_irq_disable_all(adev);
+	if (adev->mode_info.mode_config_initialized)
+		drm_atomic_helper_shutdown(adev->ddev);
+
+	gsgpu_ib_pool_fini(adev);
+	gsgpu_fence_driver_fini(adev);
+	gsgpu_fbdev_fini(adev);
+	r = gsgpu_device_ip_fini(adev);
+	if (adev->firmware.gpu_info_fw) {
+		release_firmware(adev->firmware.gpu_info_fw);
+		adev->firmware.gpu_info_fw = NULL;
+	}
+	adev->accel_working = false;
+	cancel_delayed_work_sync(&adev->late_init_work);
+
+	gsgpu_cp_fini(adev);
+
+	kfree(adev->bios);
+	adev->bios = NULL;
+
+	vga_switcheroo_unregister_client(adev->pdev);
+
+	if (adev->flags & GSGPU_IS_PX)
+		vga_switcheroo_fini_domain_pm_ops(adev->dev);
+	vga_client_register(adev->loongson_dc, NULL, NULL, NULL);
+
+	iounmap(adev->rmmio);
+	adev->rmmio = NULL;
+
+	gsgpu_debugfs_regs_cleanup(adev);
+}
+
+static int gsgpu_zip_gem_bo_validate(int id, void *ptr, void *data)
+{
+	struct drm_gem_object *gobj = ptr;
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(gobj);
+	int r, i;
+	struct ttm_operation_ctx ctx = { false, false };
+	struct drm_mm_node *node = NULL;
+	unsigned domain;
+
+	domain = gsgpu_mem_type_to_domain(bo->tbo.mem.mem_type);
+
+	if (bo->flags & GSGPU_GEM_CREATE_COMPRESSED_MASK) {
+
+		gsgpu_bo_reserve(bo, false);
+
+		domain = bo->preferred_domains;
+
+		node = bo->tbo.mem.mm_node;
+
+		bo->flags |= GSGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+		/* force to pin into visible video ram */
+		if (!(bo->flags & GSGPU_GEM_CREATE_NO_CPU_ACCESS))
+			bo->flags |= GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+		gsgpu_bo_placement_from_domain(bo, domain);
+		for (i = 0; i < bo->placement.num_placement; i++) {
+			unsigned fpfn, lpfn;
+
+			fpfn = bo->node_offset;
+			lpfn = bo->node_offset + bo->tbo.num_pages;
+
+			if (fpfn > bo->placements[i].fpfn)
+				bo->placements[i].fpfn = fpfn;
+			if (!bo->placements[i].lpfn ||
+				(lpfn && lpfn < bo->placements[i].lpfn))
+				bo->placements[i].lpfn = lpfn;
+			bo->placements[i].flags |= TTM_PL_FLAG_NO_EVICT;
+		}
+
+		r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+		if (unlikely(r)) {
+			DRM_ERROR("gsgpu zip bo validate failed %d\n", r);
+		}
+
+		gsgpu_bo_unreserve(bo);
+
+	}
+
+	return 0;
+}
+
+static int gsgpu_zip_gem_bo_evict(int id, void *ptr, void *data)
+{
+	struct drm_gem_object *gobj = ptr;
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(gobj);
+	int r, i;
+	struct ttm_operation_ctx ctx = { false, false };
+	struct drm_mm_node *node = NULL;
+	unsigned domain;
+
+	domain = gsgpu_mem_type_to_domain(bo->tbo.mem.mem_type);
+
+	if (bo->flags & GSGPU_GEM_CREATE_COMPRESSED_MASK && domain == GSGPU_GEM_DOMAIN_VRAM) {
+
+		gsgpu_bo_reserve(bo, false);
+
+		node = bo->tbo.mem.mm_node;
+
+		bo->node_offset = node->start;
+
+		for (i = 0; i < bo->placement.num_placement; i++) {
+			bo->placements[i].lpfn = 0;
+			bo->placements[i].flags &= ~TTM_PL_FLAG_NO_EVICT;
+		}
+
+		r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+		if (unlikely(r))
+			DRM_ERROR("gsgpu zip bo evict failed %d\n", r);
+
+		gsgpu_bo_unreserve(bo);
+	}
+
+	return 0;
+}
+
+/*
+ * Suspend & resume.
+ */
+/**
+ * gsgpu_device_suspend - initiate device suspend
+ *
+ * @dev: drm dev pointer
+ * @suspend: suspend state
+ * @fbcon : notify the fbdev of suspend
+ *
+ * Puts the hw in the suspend state (all asics).
+ * Returns 0 for success or an error on failure.
+ * Called at driver suspend.
+ */
+int gsgpu_device_suspend(struct drm_device *dev, bool suspend, bool fbcon)
+{
+	struct gsgpu_device *adev;
+	int r;
+	struct drm_file *file;
+
+	if (dev == NULL || dev->dev_private == NULL) {
+		return -ENODEV;
+	}
+
+	adev = dev->dev_private;
+
+	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
+		return 0;
+
+	drm_kms_helper_poll_disable(dev);
+
+	if (fbcon)
+		gsgpu_fbdev_set_suspend(adev, 1);
+
+	r = gsgpu_device_ip_suspend_phase1(adev);
+
+	r = mutex_lock_interruptible(&dev->filelist_mutex);
+	if (r)
+		return r;
+
+	list_for_each_entry(file, &dev->filelist, lhead) {
+		spin_lock(&file->table_lock);
+		idr_for_each(&file->object_idr, gsgpu_zip_gem_bo_evict, NULL);
+		spin_unlock(&file->table_lock);
+	}
+
+	mutex_unlock(&dev->filelist_mutex);
+
+	/* evict vram memory */
+	gsgpu_bo_evict_vram(adev);
+
+	gsgpu_fence_driver_suspend(adev);
+
+	r = gsgpu_device_ip_suspend_phase2(adev);
+
+	/* evict remaining vram memory
+	 * This second call to evict vram is to evict the gart page table
+	 * using the CPU.
+	 */
+	gsgpu_bo_evict_vram(adev);
+
+	pci_save_state(dev->pdev);
+	if (suspend) {
+		/* Shut down the device */
+		pci_disable_device(dev->pdev);
+		pci_set_power_state(dev->pdev, PCI_D3hot);
+	} else {
+		r = gsgpu_asic_reset(adev);
+		if (r)
+			DRM_ERROR("gsgpu asic reset failed\n");
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_resume - initiate device resume
+ *
+ * @dev: drm dev pointer
+ * @resume: resume state
+ * @fbcon : notify the fbdev of resume
+ *
+ * Bring the hw back to operating state (all asics).
+ * Returns 0 for success or an error on failure.
+ * Called at driver resume.
+ */
+int gsgpu_device_resume(struct drm_device *dev, bool resume, bool fbcon)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	int r = 0;
+	struct drm_file *file;
+
+	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
+		return 0;
+
+	if (resume) {
+		pci_set_power_state(dev->pdev, PCI_D0);
+		pci_restore_state(dev->pdev);
+		r = pci_enable_device(dev->pdev);
+		if (r)
+			return r;
+	}
+
+	*(int *)(0x80000e0010010444) |= 0x10;
+
+	r = gsgpu_cp_gfx_load_microcode(adev);
+	if (r) {
+		DRM_ERROR(" gsgpu_cp_gfx_load_microcode fail\n");
+		return r;
+	}
+
+	r = gsgpu_cp_enable(adev, true);
+	if (r) {
+		DRM_ERROR(" gsgpu_cp_enable fail\n");
+		return r;
+	}
+
+	r = mutex_lock_interruptible(&dev->filelist_mutex);
+	if (r)
+		return r;
+
+	list_for_each_entry(file, &dev->filelist, lhead) {
+		spin_lock(&file->table_lock);
+		idr_for_each(&file->object_idr, gsgpu_zip_gem_bo_validate, NULL);
+		spin_unlock(&file->table_lock);
+	}
+
+	mutex_unlock(&dev->filelist_mutex);
+
+	r = gsgpu_device_ip_resume(adev);
+	if (r) {
+		DRM_ERROR("gsgpu_device_ip_resume failed (%d).\n", r);
+		return r;
+	}
+	gsgpu_fence_driver_resume(adev);
+
+
+	r = gsgpu_device_ip_late_init(adev);
+	if (r)
+		return r;
+
+	/* Make sure IB tests flushed */
+	flush_delayed_work(&adev->late_init_work);
+
+	/* blat the mode back in */
+	if (fbcon)
+		gsgpu_fbdev_set_suspend(adev, 0);
+
+	drm_kms_helper_poll_enable(dev);
+
+	/*
+	 * Most of the connector probing functions try to acquire runtime pm
+	 * refs to ensure that the GPU is powered on when connector polling is
+	 * performed. Since we're calling this from a runtime PM callback,
+	 * trying to acquire rpm refs will cause us to deadlock.
+	 *
+	 * Since we're guaranteed to be holding the rpm lock, it's safe to
+	 * temporarily disable the rpm helpers so this doesn't deadlock us.
+	 */
+#ifdef CONFIG_PM
+	dev->dev->power.disable_depth++;
+#endif
+	drm_kms_helper_hotplug_event(dev);
+#ifdef CONFIG_PM
+	dev->dev->power.disable_depth--;
+#endif
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_check_soft_reset - did soft reset succeed
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * The list of all the hardware IPs that make up the asic is walked and
+ * the check_soft_reset callbacks are run.  check_soft_reset determines
+ * if the asic is still hung or not.
+ * Returns true if any of the IPs are still in a hung state, false if not.
+ */
+static bool gsgpu_device_ip_check_soft_reset(struct gsgpu_device *adev)
+{
+	int i;
+	bool asic_hang = false;
+
+	if (gsgpu_asic_need_full_reset(adev))
+		return true;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		if (adev->ip_blocks[i].version->funcs->check_soft_reset)
+			adev->ip_blocks[i].status.hang =
+				adev->ip_blocks[i].version->funcs->check_soft_reset(adev);
+		if (adev->ip_blocks[i].status.hang) {
+			DRM_INFO("IP block:%s is hung!\n", adev->ip_blocks[i].version->funcs->name);
+			asic_hang = true;
+		}
+	}
+	return asic_hang;
+}
+
+/**
+ * gsgpu_device_ip_pre_soft_reset - prepare for soft reset
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * The list of all the hardware IPs that make up the asic is walked and the
+ * pre_soft_reset callbacks are run if the block is hung.  pre_soft_reset
+ * handles any IP specific hardware or software state changes that are
+ * necessary for a soft reset to succeed.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_pre_soft_reset(struct gsgpu_device *adev)
+{
+	int i, r = 0;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		if (adev->ip_blocks[i].status.hang &&
+		    adev->ip_blocks[i].version->funcs->pre_soft_reset) {
+			r = adev->ip_blocks[i].version->funcs->pre_soft_reset(adev);
+			if (r)
+				return r;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_need_full_reset - check if a full asic reset is needed
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Some hardware IPs cannot be soft reset.  If they are hung, a full gpu
+ * reset is necessary to recover.
+ * Returns true if a full asic reset is required, false if not.
+ */
+static bool gsgpu_device_ip_need_full_reset(struct gsgpu_device *adev)
+{
+	int i;
+
+	if (gsgpu_asic_need_full_reset(adev))
+		return true;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		if ((adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_GMC) ||
+		    (adev->ip_blocks[i].version->type == GSGPU_IP_BLOCK_TYPE_DCE)) {
+			if (adev->ip_blocks[i].status.hang) {
+				DRM_INFO("Some block need full reset!\n");
+				return true;
+			}
+		}
+	}
+	return false;
+}
+
+/**
+ * gsgpu_device_ip_soft_reset - do a soft reset
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * The list of all the hardware IPs that make up the asic is walked and the
+ * soft_reset callbacks are run if the block is hung.  soft_reset handles any
+ * IP specific hardware or software state changes that are necessary to soft
+ * reset the IP.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_soft_reset(struct gsgpu_device *adev)
+{
+	int i, r = 0;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		if (adev->ip_blocks[i].status.hang &&
+		    adev->ip_blocks[i].version->funcs->soft_reset) {
+			r = adev->ip_blocks[i].version->funcs->soft_reset(adev);
+			if (r)
+				return r;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_ip_post_soft_reset - clean up from soft reset
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * The list of all the hardware IPs that make up the asic is walked and the
+ * post_soft_reset callbacks are run if the asic was hung.  post_soft_reset
+ * handles any IP specific hardware or software state changes that are
+ * necessary after the IP has been soft reset.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_ip_post_soft_reset(struct gsgpu_device *adev)
+{
+	int i, r = 0;
+
+	for (i = 0; i < adev->num_ip_blocks; i++) {
+		if (!adev->ip_blocks[i].status.valid)
+			continue;
+		if (adev->ip_blocks[i].status.hang &&
+		    adev->ip_blocks[i].version->funcs->post_soft_reset)
+			r = adev->ip_blocks[i].version->funcs->post_soft_reset(adev);
+		if (r)
+			return r;
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_device_recover_vram_from_shadow - restore shadowed VRAM buffers
+ *
+ * @adev: gsgpu_device pointer
+ * @ring: gsgpu_ring for the engine handling the buffer operations
+ * @bo: gsgpu_bo buffer whose shadow is being restored
+ * @fence: dma_fence associated with the operation
+ *
+ * Restores the VRAM buffer contents from the shadow in GTT.  Used to
+ * restore things like GPUVM page tables after a GPU reset where
+ * the contents of VRAM might be lost.
+ * Returns 0 on success, negative error code on failure.
+ */
+static int gsgpu_device_recover_vram_from_shadow(struct gsgpu_device *adev,
+						  struct gsgpu_ring *ring,
+						  struct gsgpu_bo *bo,
+						  struct dma_fence **fence)
+{
+	uint32_t domain;
+	int r;
+
+	if (!bo->shadow)
+		return 0;
+
+	r = gsgpu_bo_reserve(bo, true);
+	if (r)
+		return r;
+	domain = gsgpu_mem_type_to_domain(bo->tbo.mem.mem_type);
+	/* if bo has been evicted, then no need to recover */
+	if (domain == GSGPU_GEM_DOMAIN_VRAM) {
+		r = gsgpu_bo_validate(bo->shadow);
+		if (r) {
+			DRM_ERROR("bo validate failed!\n");
+			goto err;
+		}
+
+		r = gsgpu_bo_restore_from_shadow(adev, ring, bo,
+						 NULL, fence, true);
+		if (r) {
+			DRM_ERROR("recover page table failed!\n");
+			goto err;
+		}
+	}
+err:
+	gsgpu_bo_unreserve(bo);
+	return r;
+}
+
+/**
+ * gsgpu_device_handle_vram_lost - Handle the loss of VRAM contents
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Restores the contents of VRAM buffers from the shadows in GTT.  Used to
+ * restore things like GPUVM page tables after a GPU reset where
+ * the contents of VRAM might be lost.
+ * Returns 0 on success, 1 on failure.
+ */
+static int gsgpu_device_handle_vram_lost(struct gsgpu_device *adev)
+{
+	struct gsgpu_ring *ring = adev->mman.buffer_funcs_ring;
+	struct gsgpu_bo *bo, *tmp;
+	struct dma_fence *fence = NULL, *next = NULL;
+	long r = 1;
+	int i = 0;
+	long tmo;
+
+	tmo = msecs_to_jiffies(100);
+
+	DRM_INFO("recover vram bo from shadow start\n");
+	mutex_lock(&adev->shadow_list_lock);
+	list_for_each_entry_safe(bo, tmp, &adev->shadow_list, shadow_list) {
+		next = NULL;
+		gsgpu_device_recover_vram_from_shadow(adev, ring, bo, &next);
+		if (fence) {
+			r = dma_fence_wait_timeout(fence, false, tmo);
+			if (r == 0)
+				pr_err("wait fence %p[%d] timeout\n", fence, i);
+			else if (r < 0)
+				pr_err("wait fence %p[%d] interrupted\n", fence, i);
+			if (r < 1) {
+				dma_fence_put(fence);
+				fence = next;
+				break;
+			}
+			i++;
+		}
+
+		dma_fence_put(fence);
+		fence = next;
+	}
+	mutex_unlock(&adev->shadow_list_lock);
+
+	if (fence) {
+		r = dma_fence_wait_timeout(fence, false, tmo);
+		if (r == 0)
+			pr_err("wait fence %p[%d] timeout\n", fence, i);
+		else if (r < 0)
+			pr_err("wait fence %p[%d] interrupted\n", fence, i);
+
+	}
+	dma_fence_put(fence);
+
+	if (r > 0)
+		DRM_INFO("recover vram bo from shadow done\n");
+	else
+		DRM_ERROR("recover vram bo from shadow failed\n");
+
+	return (r > 0) ? 0 : 1;
+}
+
+/**
+ * gsgpu_device_reset - reset ASIC/GPU for bare-metal or passthrough
+ *
+ * @adev: gsgpu device pointer
+ *
+ * attempt to do soft-reset or full-reset and reinitialize Asic
+ * return 0 means succeeded otherwise failed
+ */
+static int gsgpu_device_reset(struct gsgpu_device *adev)
+{
+	bool need_full_reset, vram_lost = 0;
+	int r;
+
+	need_full_reset = gsgpu_device_ip_need_full_reset(adev);
+
+	if (!need_full_reset) {
+		gsgpu_device_ip_pre_soft_reset(adev);
+		r = gsgpu_device_ip_soft_reset(adev);
+		gsgpu_device_ip_post_soft_reset(adev);
+		if (r || gsgpu_device_ip_check_soft_reset(adev)) {
+			DRM_INFO("soft reset failed, will fallback to full reset!\n");
+			need_full_reset = true;
+		}
+	}
+
+	if (need_full_reset) {
+		r = gsgpu_device_ip_suspend(adev);
+
+retry:
+		r = gsgpu_asic_reset(adev);
+
+		if (!r) {
+			dev_info(adev->dev, "GPU reset succeeded, trying to resume\n");
+			r = gsgpu_device_ip_resume_phase1(adev);
+			if (r)
+				goto out;
+
+			vram_lost = gsgpu_device_check_vram_lost(adev);
+			if (vram_lost) {
+				DRM_ERROR("VRAM is lost!\n");
+				atomic_inc(&adev->vram_lost_counter);
+			}
+
+			r = gsgpu_gtt_mgr_recover(
+				&adev->mman.bdev.man[TTM_PL_TT]);
+			if (r)
+				goto out;
+
+			r = gsgpu_device_ip_resume_phase2(adev);
+			if (r)
+				goto out;
+
+			if (vram_lost)
+				gsgpu_device_fill_reset_magic(adev);
+		}
+	}
+
+out:
+	if (!r) {
+		gsgpu_irq_gpu_reset_resume_helper(adev);
+		r = gsgpu_ib_ring_tests(adev);
+		if (r) {
+			dev_err(adev->dev, "ib ring test failed (%d).\n", r);
+			r = gsgpu_device_ip_suspend(adev);
+			need_full_reset = true;
+			goto retry;
+		}
+	}
+
+	if (!r && ((need_full_reset && !(adev->flags & GSGPU_IS_APU)) || vram_lost))
+		r = gsgpu_device_handle_vram_lost(adev);
+
+	return r;
+}
+
+/**
+ * gsgpu_device_gpu_recover - reset the asic and recover scheduler
+ *
+ * @adev: gsgpu device pointer
+ * @job: which job trigger hang
+ * @force: forces reset regardless of gsgpu_gpu_recovery
+ *
+ * Attempt to reset the GPU if it has hung (all asics).
+ * Returns 0 for success or an error on failure.
+ */
+int gsgpu_device_gpu_recover(struct gsgpu_device *adev,
+			      struct gsgpu_job *job, bool force)
+{
+	int i, r, resched;
+
+	if (!force && !gsgpu_device_ip_check_soft_reset(adev)) {
+		DRM_INFO("No hardware hang detected. Did some blocks stall?\n");
+		return 0;
+	}
+
+	if (!force && (gsgpu_gpu_recovery == 0 || gsgpu_gpu_recovery == -1)) {
+		DRM_INFO("GPU recovery disabled.\n");
+		return 0;
+	}
+
+	dev_info(adev->dev, "GPU reset begin!\n");
+
+	mutex_lock(&adev->lock_reset);
+	atomic_inc(&adev->gpu_reset_counter);
+	adev->in_gpu_reset = 1;
+
+	/* block TTM */
+	resched = ttm_bo_lock_delayed_workqueue(&adev->mman.bdev);
+
+	/* block all schedulers and reset given job's ring */
+	for (i = 0; i < GSGPU_MAX_RINGS; ++i) {
+		struct gsgpu_ring *ring = adev->rings[i];
+
+		if (!ring || !ring->sched.thread)
+			continue;
+
+		kthread_park(ring->sched.thread);
+
+		if (job && job->base.sched == &ring->sched)
+			continue;
+
+		drm_sched_hw_job_reset(&ring->sched, job ? &job->base : NULL);
+
+		/* after all hw jobs are reset, hw fence is meaningless, so force_completion */
+		gsgpu_fence_driver_force_completion(ring);
+	}
+
+	r = gsgpu_device_reset(adev);
+
+	for (i = 0; i < GSGPU_MAX_RINGS; ++i) {
+		struct gsgpu_ring *ring = adev->rings[i];
+
+		if (!ring || !ring->sched.thread)
+			continue;
+
+		/* only need recovery sched of the given job's ring
+		 * or all rings (in the case @job is NULL)
+		 * after above gsgpu_reset accomplished
+		 */
+#if 0
+		/* XXX
+		 * GSGPU: Oops!! We Can't Find A nice method to Rework This Hanged Job
+		 * So We Ignore this*/
+		if ((!job || job->base.sched == &ring->sched) && !r)
+			drm_sched_job_recovery(&ring->sched);
+#endif
+
+		kthread_unpark(ring->sched.thread);
+	}
+
+	ttm_bo_unlock_delayed_workqueue(&adev->mman.bdev, resched);
+
+	if (r) {
+		/* bad news, how to tell it to userspace ? */
+		dev_info(adev->dev, "GPU reset(%d) failed\n", atomic_read(&adev->gpu_reset_counter));
+	} else {
+		dev_info(adev->dev, "GPU reset(%d) succeeded!\n", atomic_read(&adev->gpu_reset_counter));
+	}
+
+	adev->in_gpu_reset = 0;
+	mutex_unlock(&adev->lock_reset);
+	return r;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_display.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_display.c
new file mode 100644
index 000000000000..f5505faae264
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_display.c
@@ -0,0 +1,434 @@
+#include <linux/pm_runtime.h>
+
+#include <drm/drmP.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/drm_edid.h>
+#include <drm/drm_gem_framebuffer_helper.h>
+#include <drm/drm_fb_helper.h>
+#include <drm/gsgpu_drm.h>
+
+#include "gsgpu.h"
+#include "gsgpu_display.h"
+
+static void gsgpu_display_flip_callback(struct dma_fence *f,
+					 struct dma_fence_cb *cb)
+{
+	struct gsgpu_flip_work *work =
+		container_of(cb, struct gsgpu_flip_work, cb);
+
+	dma_fence_put(f);
+	schedule_work(&work->flip_work.work);
+}
+
+static bool gsgpu_display_flip_handle_fence(struct gsgpu_flip_work *work,
+					     struct dma_fence **f)
+{
+	struct dma_fence *fence = *f;
+
+	if (fence == NULL)
+		return false;
+
+	*f = NULL;
+
+	if (!dma_fence_add_callback(fence, &work->cb,
+				    gsgpu_display_flip_callback))
+		return true;
+
+	dma_fence_put(fence);
+	return false;
+}
+
+static void gsgpu_display_flip_work_func(struct work_struct *__work)
+{
+	struct delayed_work *delayed_work =
+		container_of(__work, struct delayed_work, work);
+	struct gsgpu_flip_work *work =
+		container_of(delayed_work, struct gsgpu_flip_work, flip_work);
+	struct gsgpu_device *adev = work->adev;
+	struct gsgpu_crtc *gsgpu_crtc = adev->mode_info.crtcs[work->crtc_id];
+
+	struct drm_crtc *crtc = &gsgpu_crtc->base;
+	unsigned long flags;
+	unsigned i;
+	int vpos, hpos;
+
+	if (gsgpu_display_flip_handle_fence(work, &work->excl))
+		return;
+
+	for (i = 0; i < work->shared_count; ++i)
+		if (gsgpu_display_flip_handle_fence(work, &work->shared[i]))
+			return;
+
+	/* Wait until we're out of the vertical blank period before the one
+	 * targeted by the flip
+	 */
+	if (gsgpu_crtc->enabled &&
+	    (gsgpu_display_get_crtc_scanoutpos(adev->ddev, work->crtc_id, 0,
+						&vpos, &hpos, NULL, NULL,
+						&crtc->hwmode)
+	     & (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_IN_VBLANK)) ==
+	    (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_IN_VBLANK) &&
+	    (int)(work->target_vblank -
+		  gsgpu_get_vblank_counter_kms(adev->ddev, gsgpu_crtc->crtc_id)) > 0) {
+		schedule_delayed_work(&work->flip_work, usecs_to_jiffies(1000));
+		return;
+	}
+
+	/* We borrow the event spin lock for protecting flip_status */
+	spin_lock_irqsave(&crtc->dev->event_lock, flags);
+
+	/* Do the flip (mmio) */
+	adev->mode_info.funcs->page_flip(adev, work->crtc_id, work->base, work->async);
+
+	/* Set the flip status */
+	gsgpu_crtc->pflip_status = GSGPU_FLIP_SUBMITTED;
+	spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
+
+
+	DRM_DEBUG_DRIVER("crtc:%d[%p], pflip_stat:GSGPU_FLIP_SUBMITTED, work: %p,\n",
+					 gsgpu_crtc->crtc_id, gsgpu_crtc, work);
+
+}
+
+/*
+ * Handle unpin events outside the interrupt handler proper.
+ */
+static void gsgpu_display_unpin_work_func(struct work_struct *__work)
+{
+	struct gsgpu_flip_work *work =
+		container_of(__work, struct gsgpu_flip_work, unpin_work);
+	int r;
+
+	/* unpin of the old buffer */
+	r = gsgpu_bo_reserve(work->old_abo, true);
+	if (likely(r == 0)) {
+		r = gsgpu_bo_unpin(work->old_abo);
+		if (unlikely(r != 0)) {
+			DRM_ERROR("failed to unpin buffer after flip\n");
+		}
+		gsgpu_bo_unreserve(work->old_abo);
+	} else
+		DRM_ERROR("failed to reserve buffer after flip\n");
+
+	gsgpu_bo_unref(&work->old_abo);
+	kfree(work->shared);
+	kfree(work);
+}
+
+int gsgpu_display_crtc_page_flip_target(struct drm_crtc *crtc,
+				struct drm_framebuffer *fb,
+				struct drm_pending_vblank_event *event,
+				uint32_t page_flip_flags, uint32_t target,
+				struct drm_modeset_acquire_ctx *ctx)
+{
+	struct drm_device *dev = crtc->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_crtc *gsgpu_crtc = to_gsgpu_crtc(crtc);
+	struct drm_gem_object *obj;
+	struct gsgpu_flip_work *work;
+	struct gsgpu_bo *new_abo;
+	unsigned long flags;
+	u64 tiling_flags;
+	int i, r;
+	void *fb_vaddr = NULL;
+
+	work = kzalloc(sizeof *work, GFP_KERNEL);
+	if (work == NULL)
+		return -ENOMEM;
+
+	INIT_DELAYED_WORK(&work->flip_work, gsgpu_display_flip_work_func);
+	INIT_WORK(&work->unpin_work, gsgpu_display_unpin_work_func);
+
+	work->event = event;
+	work->adev = adev;
+	work->crtc_id = gsgpu_crtc->crtc_id;
+	work->async = (page_flip_flags & DRM_MODE_PAGE_FLIP_ASYNC) != 0;
+
+	/* schedule unpin of the old buffer */
+	obj = crtc->primary->fb->obj[0];
+
+	/* take a reference to the old object */
+	work->old_abo = gem_to_gsgpu_bo(obj);
+	gsgpu_bo_ref(work->old_abo);
+
+	obj = fb->obj[0];
+	new_abo = gem_to_gsgpu_bo(obj);
+
+	/* pin the new buffer */
+	r = gsgpu_bo_reserve(new_abo, false);
+	if (unlikely(r != 0)) {
+		DRM_ERROR("failed to reserve new abo buffer before flip\n");
+		goto cleanup;
+	}
+
+	r = gsgpu_bo_pin(new_abo, gsgpu_display_supported_domains(adev));
+	if (unlikely(r != 0)) {
+		DRM_ERROR("failed to pin new abo buffer before flip\n");
+		goto unreserve;
+	}
+
+	r = gsgpu_ttm_alloc_gart(&new_abo->tbo);
+	if (unlikely(r != 0)) {
+		DRM_ERROR("%p bind failed\n", new_abo);
+		goto unpin;
+	}
+
+	r = reservation_object_get_fences_rcu(new_abo->tbo.resv, &work->excl,
+					      &work->shared_count,
+					      &work->shared);
+	if (unlikely(r != 0)) {
+		DRM_ERROR("failed to get fences for buffer\n");
+		goto unpin;
+	}
+
+	gsgpu_bo_get_tiling_flags(new_abo, &tiling_flags);
+
+	gsgpu_bo_kmap(new_abo, &fb_vaddr);
+
+	if (gsgpu_using_ram) {
+		work->base = virt_to_phys(fb_vaddr);
+		/* 0x460000000 - 0x46fffffff to 0x20000000 - 0x2fffffff */
+		work->base = work->base & 0x3fffffff;
+	} else {
+		work->base = gsgpu_bo_gpu_offset(new_abo);
+	}
+
+	gsgpu_bo_unreserve(new_abo);
+
+	work->target_vblank = target - (uint32_t)drm_crtc_vblank_count(crtc) +
+		gsgpu_get_vblank_counter_kms(dev, work->crtc_id);
+
+	/* we borrow the event spin lock for protecting flip_wrok */
+	spin_lock_irqsave(&crtc->dev->event_lock, flags);
+	if (gsgpu_crtc->pflip_status != GSGPU_FLIP_NONE) {
+		DRM_DEBUG_DRIVER("flip queue: crtc already busy\n");
+		spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
+		r = -EBUSY;
+		goto pflip_cleanup;
+	}
+
+	gsgpu_crtc->pflip_status = GSGPU_FLIP_PENDING;
+	gsgpu_crtc->pflip_works = work;
+
+
+	DRM_DEBUG_DRIVER("crtc:%d[%p], pflip_stat:GSGPU_FLIP_PENDING, work: %p,\n",
+					 gsgpu_crtc->crtc_id, gsgpu_crtc, work);
+	/* update crtc fb */
+	crtc->primary->fb = fb;
+	spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
+	gsgpu_display_flip_work_func(&work->flip_work.work);
+	return 0;
+
+pflip_cleanup:
+	if (unlikely(gsgpu_bo_reserve(new_abo, false) != 0)) {
+		DRM_ERROR("failed to reserve new abo in error path\n");
+		goto cleanup;
+	}
+unpin:
+	if (unlikely(gsgpu_bo_unpin(new_abo) != 0)) {
+		DRM_ERROR("failed to unpin new abo in error path\n");
+	}
+unreserve:
+	gsgpu_bo_unreserve(new_abo);
+
+cleanup:
+	gsgpu_bo_unref(&work->old_abo);
+	dma_fence_put(work->excl);
+	for (i = 0; i < work->shared_count; ++i)
+		dma_fence_put(work->shared[i]);
+	kfree(work->shared);
+	kfree(work);
+
+	return r;
+}
+
+static const struct drm_framebuffer_funcs gsgpu_fb_funcs = {
+	.destroy = drm_gem_fb_destroy,
+	.create_handle = drm_gem_fb_create_handle,
+};
+
+uint32_t gsgpu_display_supported_domains(struct gsgpu_device *adev)
+{
+	uint32_t domain = GSGPU_GEM_DOMAIN_VRAM;
+
+	return domain;
+}
+
+int gsgpu_display_framebuffer_init(struct drm_device *dev,
+				    struct gsgpu_framebuffer *rfb,
+				    const struct drm_mode_fb_cmd2 *mode_cmd,
+				    struct drm_gem_object *obj)
+{
+	int ret;
+	rfb->base.obj[0] = obj;
+	drm_helper_mode_fill_fb_struct(dev, &rfb->base, mode_cmd);
+	ret = drm_framebuffer_init(dev, &rfb->base, &gsgpu_fb_funcs);
+	if (ret) {
+		rfb->base.obj[0] = NULL;
+		return ret;
+	}
+	return 0;
+}
+
+struct drm_framebuffer *
+gsgpu_display_user_framebuffer_create(struct drm_device *dev,
+				       struct drm_file *file_priv,
+				       const struct drm_mode_fb_cmd2 *mode_cmd)
+{
+	struct drm_gem_object *obj;
+	struct gsgpu_framebuffer *gsgpu_fb;
+	int ret;
+
+	obj = drm_gem_object_lookup(file_priv, mode_cmd->handles[0]);
+	if (obj ==  NULL) {
+		dev_err(&dev->pdev->dev, "No GEM object associated to handle 0x%08X, "
+			"can't create framebuffer\n", mode_cmd->handles[0]);
+		return ERR_PTR(-ENOENT);
+	}
+
+	/* Handle is imported dma-buf, so cannot be migrated to VRAM for scanout */
+	if (obj->import_attach) {
+		DRM_DEBUG_KMS("Cannot create framebuffer from imported dma_buf\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	gsgpu_fb = kzalloc(sizeof(*gsgpu_fb), GFP_KERNEL);
+	if (gsgpu_fb == NULL) {
+		drm_gem_object_put_unlocked(obj);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	ret = gsgpu_display_framebuffer_init(dev, gsgpu_fb, mode_cmd, obj);
+	if (ret) {
+		kfree(gsgpu_fb);
+		drm_gem_object_put_unlocked(obj);
+		return ERR_PTR(ret);
+	}
+
+	return &gsgpu_fb->base;
+}
+
+static const struct drm_prop_enum_list gsgpu_audio_enum_list[] = {
+	{ GSGPU_AUDIO_DISABLE, "off" },
+	{ GSGPU_AUDIO_ENABLE, "on" },
+	{ GSGPU_AUDIO_AUTO, "auto" },
+};
+
+int gsgpu_display_modeset_create_props(struct gsgpu_device *adev)
+{
+	int sz;
+
+	drm_mode_create_scaling_mode_property(adev->ddev);
+
+	sz = ARRAY_SIZE(gsgpu_audio_enum_list);
+	adev->mode_info.audio_property =
+		drm_property_create_enum(adev->ddev, 0,
+					 "audio",
+					 gsgpu_audio_enum_list, sz);
+
+	return 0;
+}
+
+void gsgpu_display_update_priority(struct gsgpu_device *adev)
+{
+	/* adjustment options for the display watermarks */
+	if ((gsgpu_disp_priority == 0) || (gsgpu_disp_priority > 2))
+		adev->mode_info.disp_priority = 0;
+	else
+		adev->mode_info.disp_priority = gsgpu_disp_priority;
+
+}
+
+int gsgpu_display_get_crtc_scanoutpos(struct drm_device *dev,
+			unsigned int pipe, unsigned int flags, int *vpos,
+			int *hpos, ktime_t *stime, ktime_t *etime,
+			const struct drm_display_mode *mode)
+{
+	u32 vbl = 0, position = 0;
+	int vbl_start, vbl_end, vtotal, ret = 0;
+	bool in_vbl = true;
+
+	struct gsgpu_device *adev = dev->dev_private;
+
+	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
+
+	/* Get optional system timestamp before query. */
+	if (stime)
+		*stime = ktime_get();
+
+	if (gsgpu_display_page_flip_get_scanoutpos(adev, pipe, &vbl, &position) == 0)
+		ret |= DRM_SCANOUTPOS_VALID;
+
+	/* Get optional system timestamp after query. */
+	if (etime)
+		*etime = ktime_get();
+
+	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
+
+	/* Decode into vertical and horizontal scanout position. */
+	*vpos = position & 0x1fff;
+	*hpos = (position >> 16) & 0x1fff;
+
+	/* Valid vblank area boundaries from gpu retrieved? */
+	if (vbl > 0) {
+		/* Yes: Decode. */
+		ret |= DRM_SCANOUTPOS_ACCURATE;
+		vbl_start = vbl & 0x1fff;
+		vbl_end = (vbl >> 16) & 0x1fff;
+	} else {
+		/* No: Fake something reasonable which gives at least ok results. */
+		vbl_start = mode->crtc_vdisplay;
+		vbl_end = 0;
+	}
+
+	/* Called from driver internal vblank counter query code? */
+	if (flags & GET_DISTANCE_TO_VBLANKSTART) {
+	    /* Caller wants distance from real vbl_start in *hpos */
+	    *hpos = *vpos - vbl_start;
+	}
+
+	/* Fudge vblank to start a few scanlines earlier to handle the
+	 * problem that vblank irqs fire a few scanlines before start
+	 * of vblank. Some driver internal callers need the true vblank
+	 * start to be used and signal this via the USE_REAL_VBLANKSTART flag.
+	 *
+	 * The cause of the "early" vblank irq is that the irq is triggered
+	 * by the line buffer logic when the line buffer read position enters
+	 * the vblank, whereas our crtc scanout position naturally lags the
+	 * line buffer read position.
+	 */
+	if (!(flags & USE_REAL_VBLANKSTART))
+		vbl_start -= adev->mode_info.crtcs[pipe]->lb_vblank_lead_lines;
+
+	/* Test scanout position against vblank region. */
+	if ((*vpos < vbl_start) && (*vpos >= vbl_end))
+		in_vbl = false;
+
+	/* In vblank? */
+	if (in_vbl)
+	    ret |= DRM_SCANOUTPOS_IN_VBLANK;
+
+	/* Called from driver internal vblank counter query code? */
+	if (flags & GET_DISTANCE_TO_VBLANKSTART) {
+		/* Caller wants distance from fudged earlier vbl_start */
+		*vpos -= vbl_start;
+		return ret;
+	}
+
+	/* Check if inside vblank area and apply corrective offsets:
+	 * vpos will then be >=0 in video scanout area, but negative
+	 * within vblank area, counting down the number of lines until
+	 * start of scanout.
+	 */
+
+	/* Inside "upper part" of vblank area? Apply corrective offset if so: */
+	if (in_vbl && (*vpos >= vbl_start)) {
+		vtotal = mode->crtc_vtotal;
+		*vpos = *vpos - vtotal;
+	}
+
+	/* Correct for shifted end of vbl at vbl_end. */
+	*vpos = *vpos - vbl_end;
+
+	return ret;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_drv.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_drv.c
new file mode 100644
index 000000000000..48f50e562eee
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_drv.c
@@ -0,0 +1,648 @@
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include <drm/drm_gem.h>
+#include "gsgpu_drv.h"
+
+#include <drm/drm_pciids.h>
+#include <linux/console.h>
+#include <linux/module.h>
+#include <linux/pm_runtime.h>
+#include <linux/vga_switcheroo.h>
+#include <drm/drm_crtc_helper.h>
+
+#include "gsgpu.h"
+#include "gsgpu_irq.h"
+#include "gsgpu_dc_vbios.h"
+#include "gsgpu_dc_reg.h"
+
+#define KMS_DRIVER_MAJOR	0
+#define KMS_DRIVER_MINOR	1
+#define KMS_DRIVER_PATCHLEVEL	0
+
+int gsgpu_vram_limit;
+int gsgpu_vis_vram_limit;
+int gsgpu_gart_size = -1; /* auto */
+int gsgpu_gtt_size = -1; /* auto */
+int gsgpu_moverate = -1; /* auto */
+int gsgpu_benchmarking;
+int gsgpu_testing;
+int gsgpu_disp_priority;
+int gsgpu_msi = -1;
+int gsgpu_lockup_timeout = 10000;
+int gsgpu_runtime_pm = -1;
+int gsgpu_vm_size = -1;
+int gsgpu_vm_block_size = -1;
+int gsgpu_vm_fault_stop;
+int gsgpu_vm_debug;
+int gsgpu_vram_page_split = 2048;
+int gsgpu_vm_update_mode = -1;
+int gsgpu_exp_hw_support;
+int gsgpu_sched_jobs = 32;
+int gsgpu_sched_hw_submission = 2;
+int gsgpu_job_hang_limit;
+int gsgpu_gpu_recovery = 1; /* auto */
+int gsgpu_using_ram; /* using system memory for gpu*/
+
+int gsgpu_lg100_support = 1;
+MODULE_PARM_DESC(LG100_support, "LG100 support (1 = enabled (default), 0 = disabled");
+module_param_named(LG100_support, gsgpu_lg100_support, int, 0444);
+
+/**
+ * DOC: vramlimit (int)
+ * Restrict the total amount of VRAM in MiB for testing.  The default is 0 (Use full VRAM).
+ */
+MODULE_PARM_DESC(vramlimit, "Restrict VRAM for testing, in megabytes");
+module_param_named(vramlimit, gsgpu_vram_limit, int, 0600);
+
+/**
+ * DOC: vis_vramlimit (int)
+ * Restrict the amount of CPU visible VRAM in MiB for testing.  The default is 0 (Use full CPU visible VRAM).
+ */
+MODULE_PARM_DESC(vis_vramlimit, "Restrict visible VRAM for testing, in megabytes");
+module_param_named(vis_vramlimit, gsgpu_vis_vram_limit, int, 0444);
+
+/**
+ * DOC: gartsize (uint)
+ * Restrict the size of GART in Mib (32, 64, etc.) for testing. The default is -1 (The size depends on asic).
+ */
+MODULE_PARM_DESC(gartsize, "Size of GART to setup in megabytes (32, 64, etc., -1=auto)");
+module_param_named(gartsize, gsgpu_gart_size, uint, 0600);
+
+/**
+ * DOC: gttsize (int)
+ * Restrict the size of GTT domain in MiB for testing. The default is -1 (It's VRAM size if 3GB < VRAM < 3/4 RAM,
+ * otherwise 3/4 RAM size).
+ */
+MODULE_PARM_DESC(gttsize, "Size of the GTT domain in megabytes (-1 = auto)");
+module_param_named(gttsize, gsgpu_gtt_size, int, 0600);
+
+/**
+ * DOC: moverate (int)
+ * Set maximum buffer migration rate in MB/s. The default is -1 (8 MB/s).
+ */
+MODULE_PARM_DESC(moverate, "Maximum buffer migration rate in MB/s. (32, 64, etc., -1=auto, 0=1=disabled)");
+module_param_named(moverate, gsgpu_moverate, int, 0600);
+
+/**
+ * DOC: benchmark (int)
+ * Run benchmarks. The default is 0 (Skip benchmarks).
+ */
+MODULE_PARM_DESC(benchmark, "Run benchmark");
+module_param_named(benchmark, gsgpu_benchmarking, int, 0444);
+
+/**
+ * DOC: test (int)
+ * Test BO GTT->VRAM and VRAM->GTT GPU copies. The default is 0 (Skip test, only set 1 to run test).
+ */
+MODULE_PARM_DESC(test, "Run tests");
+module_param_named(test, gsgpu_testing, int, 0444);
+
+/**
+ * DOC: disp_priority (int)
+ * Set display Priority (1 = normal, 2 = high). Only affects non-DC display handling. The default is 0 (auto).
+ */
+MODULE_PARM_DESC(disp_priority, "Display Priority (0 = auto, 1 = normal, 2 = high)");
+module_param_named(disp_priority, gsgpu_disp_priority, int, 0444);
+
+/**
+ * DOC: msi (int)
+ * To disable Message Signaled Interrupts (MSI) functionality (1 = enable, 0 = disable). The default is -1 (auto, enabled).
+ */
+MODULE_PARM_DESC(msi, "MSI support (1 = enable, 0 = disable, -1 = auto)");
+module_param_named(msi, gsgpu_msi, int, 0444);
+
+/**
+ * DOC: lockup_timeout (int)
+ * Set GPU scheduler timeout value in ms. Value 0 is invalidated, will be adjusted to 10000.
+ * Negative values mean 'infinite timeout' (MAX_JIFFY_OFFSET). The default is 10000.
+ */
+MODULE_PARM_DESC(lockup_timeout, "GPU lockup timeout in ms > 0 (default 10000)");
+module_param_named(lockup_timeout, gsgpu_lockup_timeout, int, 0444);
+
+/**
+ * DOC: runpm (int)
+ * Override for runtime power management control for dGPUs in PX/HG laptops. The gsgpu driver can dynamically power down
+ * the dGPU on PX/HG laptops when it is idle. The default is -1 (auto enable). Setting the value to 0 disables this functionality.
+ */
+MODULE_PARM_DESC(runpm, "PX runtime pm (1 = force enable, 0 = disable, -1 = PX only default)");
+module_param_named(runpm, gsgpu_runtime_pm, int, 0444);
+
+/**
+ * DOC: vm_size (int)
+ * Override the size of the GPU's per client virtual address space in GiB.  The default is -1 (automatic for each asic).
+ */
+MODULE_PARM_DESC(vm_size, "VM address space size in gigabytes (default 64GB)");
+module_param_named(vm_size, gsgpu_vm_size, int, 0444);
+
+/**
+ * DOC: vm_block_size (int)
+ * Override VM page table size in bits (default depending on vm_size and hw setup). The default is -1 (automatic for each asic).
+ */
+MODULE_PARM_DESC(vm_block_size, "VM page table size in bits (default depending on vm_size)");
+module_param_named(vm_block_size, gsgpu_vm_block_size, int, 0444);
+
+/**
+ * DOC: vm_fault_stop (int)
+ * Stop on VM fault for debugging (0 = never, 1 = print first, 2 = always). The default is 0 (No stop).
+ */
+MODULE_PARM_DESC(vm_fault_stop, "Stop on VM fault (0 = never (default), 1 = print first, 2 = always)");
+module_param_named(vm_fault_stop, gsgpu_vm_fault_stop, int, 0444);
+
+/**
+ * DOC: vm_debug (int)
+ * Debug VM handling (0 = disabled, 1 = enabled). The default is 0 (Disabled).
+ */
+MODULE_PARM_DESC(vm_debug, "Debug VM handling (0 = disabled (default), 1 = enabled)");
+module_param_named(vm_debug, gsgpu_vm_debug, int, 0644);
+
+/**
+ * DOC: vm_update_mode (int)
+ * Override VM update mode. VM updated by using CPU (0 = never, 1 = Graphics only, 2 = Compute only, 3 = Both). The default
+ * is -1 (Only in large BAR(LB) systems Compute VM tables will be updated by CPU, otherwise 0, never).
+ */
+MODULE_PARM_DESC(vm_update_mode, "VM update using CPU (0 = never (default except for large BAR(LB)), 1 = Graphics only, 2 = Compute only (default for LB), 3 = Both");
+module_param_named(vm_update_mode, gsgpu_vm_update_mode, int, 0444);
+
+/**
+ * DOC: vram_page_split (int)
+ * Override the number of pages after we split VRAM allocations (default 1024, -1 = disable). The default is 1024.
+ */
+MODULE_PARM_DESC(vram_page_split, "Number of pages after we split VRAM allocations (default 1024, -1 = disable)");
+module_param_named(vram_page_split, gsgpu_vram_page_split, int, 0444);
+
+/**
+ * DOC: exp_hw_support (int)
+ * Enable experimental hw support (1 = enable). The default is 0 (disabled).
+ */
+MODULE_PARM_DESC(exp_hw_support, "experimental hw support (1 = enable, 0 = disable (default))");
+module_param_named(exp_hw_support, gsgpu_exp_hw_support, int, 0444);
+
+/**
+ * DOC: sched_jobs (int)
+ * Override the max number of jobs supported in the sw queue. The default is 32.
+ */
+MODULE_PARM_DESC(sched_jobs, "the max number of jobs supported in the sw queue (default 32)");
+module_param_named(sched_jobs, gsgpu_sched_jobs, int, 0444);
+
+/**
+ * DOC: sched_hw_submission (int)
+ * Override the max number of HW submissions. The default is 2.
+ */
+MODULE_PARM_DESC(sched_hw_submission, "the max number of HW submissions (default 2)");
+module_param_named(sched_hw_submission, gsgpu_sched_hw_submission, int, 0444);
+
+/**
+ * DOC: job_hang_limit (int)
+ * Set how much time allow a job hang and not drop it. The default is 0.
+ */
+MODULE_PARM_DESC(job_hang_limit, "how much time allow a job hang and not drop it (default 0)");
+module_param_named(job_hang_limit, gsgpu_job_hang_limit, int, 0444);
+
+/**
+ * DOC: gpu_recovery (int)
+ * Set to enable GPU recovery mechanism (1 = enable, 0 = disable). The default is -1 (auto, disabled except SRIOV).
+ */
+MODULE_PARM_DESC(gpu_recovery, "Enable GPU recovery mechanism, (1 = enable, 0 = disable, -1 = auto)");
+module_param_named(gpu_recovery, gsgpu_gpu_recovery, int, 0444);
+
+MODULE_PARM_DESC(gsgpu_using_ram, "Gpu uses memory instead vram"
+		 "0: using vram for gpu 1:use system for gpu");
+module_param_named(gsgpu_using_ram, gsgpu_using_ram, uint, 0444);
+
+
+static const struct pci_device_id pciidlist[] = {
+	{0x0014, 0x7A25, PCI_ANY_ID, PCI_ANY_ID, 0, 0, CHIP_LG100}, //GSGPU
+	{0, 0, 0}
+};
+
+MODULE_DEVICE_TABLE(pci, pciidlist);
+
+static struct drm_driver kms_driver;
+
+static int gsgpu_kick_out_firmware_fb(struct pci_dev *pdev)
+{
+	struct apertures_struct *ap;
+	bool primary = false;
+
+	ap = alloc_apertures(1);
+	if (!ap)
+		return -ENOMEM;
+
+	ap->ranges[0].base = 0;
+	ap->ranges[0].size = ~0;
+
+	drm_fb_helper_remove_conflicting_framebuffers(ap, "gsgpudrmfb", primary);
+	kfree(ap);
+
+	return 0;
+}
+
+static int gsgpu_pci_probe(struct pci_dev *pdev,
+			    const struct pci_device_id *ent)
+{
+	struct drm_device *dev;
+	unsigned long flags = ent->driver_data;
+	int ret, retry = 0;
+
+	if ((flags & GSGPU_EXP_HW_SUPPORT) && !gsgpu_exp_hw_support) {
+		DRM_INFO("This hardware requires experimental hardware support.\n"
+			 "See modparam exp_hw_support\n");
+		return -ENODEV;
+	}
+
+	/* Get rid of things like offb */
+	ret = gsgpu_kick_out_firmware_fb(pdev);
+	if (ret)
+		return ret;
+
+	dev = drm_dev_alloc(&kms_driver, &pdev->dev);
+	if (IS_ERR(dev))
+		return PTR_ERR(dev);
+
+	ret = pci_enable_device(pdev);
+	if (ret)
+		goto err_free;
+
+	dev->pdev = pdev;
+
+	pci_set_drvdata(pdev, dev);
+
+retry_init:
+	ret = drm_dev_register(dev, ent->driver_data);
+	if (ret == -EAGAIN && ++retry <= 3) {
+		DRM_INFO("retry init %d\n", retry);
+		/* Don't request EX mode too frequently which is attacking */
+		msleep(5000);
+		goto retry_init;
+	} else if (ret)
+		goto err_pci;
+
+	return 0;
+
+err_pci:
+	pci_disable_device(pdev);
+err_free:
+	drm_dev_put(dev);
+	return ret;
+}
+
+static void
+gsgpu_pci_remove(struct pci_dev *pdev)
+{
+	struct drm_device *dev = pci_get_drvdata(pdev);
+
+	drm_dev_unregister(dev);
+	drm_dev_put(dev);
+	pci_disable_device(pdev);
+	pci_set_drvdata(pdev, NULL);
+}
+
+static void
+gsgpu_pci_shutdown(struct pci_dev *pdev)
+{
+	struct drm_device *dev = pci_get_drvdata(pdev);
+	struct gsgpu_device *adev = dev->dev_private;
+
+	/* if we are running in a VM, make sure the device
+	 * torn down properly on reboot/shutdown.
+	 * unfortunately we can't detect certain
+	 * hypervisors so just do this all the time.
+	 */
+	gsgpu_device_ip_suspend(adev);
+}
+
+static int gsgpu_pmops_suspend(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+
+	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+	return gsgpu_device_suspend(drm_dev, true, true);
+}
+
+static int gsgpu_pmops_resume(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+
+	return gsgpu_device_resume(drm_dev, true, true);
+}
+
+static int gsgpu_pmops_freeze(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+
+	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+	return gsgpu_device_suspend(drm_dev, false, true);
+}
+
+static int gsgpu_pmops_thaw(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+
+	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+	return gsgpu_device_resume(drm_dev, false, true);
+}
+
+static int gsgpu_pmops_poweroff(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+
+	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+	return gsgpu_device_suspend(drm_dev, true, true);
+}
+
+static int gsgpu_pmops_restore(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+
+	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+	return gsgpu_device_resume(drm_dev, false, true);
+}
+
+static int gsgpu_pmops_runtime_suspend(struct device *dev)
+{
+	pm_runtime_forbid(dev);
+	return -EBUSY;
+}
+
+static int gsgpu_pmops_runtime_resume(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static int gsgpu_pmops_runtime_idle(struct device *dev)
+{
+	pm_runtime_forbid(dev);
+	return -EBUSY;
+}
+
+long gsgpu_drm_ioctl(struct file *filp,
+		      unsigned int cmd, unsigned long arg)
+{
+	struct drm_file *file_priv = filp->private_data;
+	struct drm_device *dev;
+	long ret;
+	dev = file_priv->minor->dev;
+	ret = pm_runtime_get_sync(dev->dev);
+	if (ret < 0)
+		return ret;
+
+	ret = drm_ioctl(filp, cmd, arg);
+
+	pm_runtime_mark_last_busy(dev->dev);
+	pm_runtime_put_autosuspend(dev->dev);
+	return ret;
+}
+
+/**
+ * loongson_vga_pci_devices  -- pci device id info
+ *
+ * __u32 vendor, device            Vendor and device ID or PCI_ANY_ID
+ * __u32 subvendor, subdevice     Subsystem ID's or PCI_ANY_ID
+ * __u32 class, class_mask        (class,subclass,prog-if) triplet
+ * kernel_ulong_t driver_data     Data private to the driver
+ */
+static struct pci_device_id loongson_vga_pci_devices[] = {
+	{PCI_DEVICE(PCI_VENDOR_ID_LOONGSON, 0x7a36)},
+	{0, 0, 0, 0, 0, 0, 0},
+};
+
+struct pci_dev *loongson_dc_pdev;
+EXPORT_SYMBOL(loongson_dc_pdev);
+
+/**
+ * loongson_vga_pci_register -- add pci device
+ *
+ * @pdev PCI device
+ * @ent pci device id
+ */
+static int loongson_vga_pci_register(struct pci_dev *pdev,
+				 const struct pci_device_id *ent)
+
+{
+	int ret;
+	u32 crtc_cfg;
+	u32 i;
+	resource_size_t dc_rmmio_base;
+	resource_size_t dc_rmmio_size;
+	void __iomem *dc_rmmio;
+
+	ret = pci_enable_device(pdev);
+	loongson_dc_pdev = pdev;
+	dc_rmmio_base = pci_resource_start(pdev, 0);
+	dc_rmmio_size = pci_resource_len(pdev, 0);
+	dc_rmmio = pci_iomap(pdev, 0, dc_rmmio_size);
+
+	for (i = 0; i < 2; i++) {
+		crtc_cfg = readl(dc_rmmio + DC_CRTC_CFG_REG + (i * 0x10));
+		crtc_cfg &= ~CRTC_CFG_ENABLE;
+		writel(crtc_cfg, dc_rmmio + DC_CRTC_CFG_REG + (i * 0x10));
+	}
+
+	return ret;
+}
+
+/**
+ * loongson_vga_pci_unregister -- release drm device
+ *
+ * @pdev PCI device
+ */
+static void loongson_vga_pci_unregister(struct pci_dev *pdev)
+{
+	pci_disable_device(pdev);
+}
+
+static const struct dev_pm_ops gsgpu_pm_ops = {
+	.suspend = gsgpu_pmops_suspend,
+	.resume = gsgpu_pmops_resume,
+	.freeze = gsgpu_pmops_freeze,
+	.thaw = gsgpu_pmops_thaw,
+	.poweroff = gsgpu_pmops_poweroff,
+	.restore = gsgpu_pmops_restore,
+	.runtime_suspend = gsgpu_pmops_runtime_suspend,
+	.runtime_resume = gsgpu_pmops_runtime_resume,
+	.runtime_idle = gsgpu_pmops_runtime_idle,
+};
+
+static int gsgpu_flush(struct file *f, fl_owner_t id)
+{
+	struct drm_file *file_priv = f->private_data;
+	struct gsgpu_fpriv *fpriv = file_priv->driver_priv;
+
+	gsgpu_ctx_mgr_entity_flush(&fpriv->ctx_mgr);
+
+	return 0;
+}
+
+static const struct file_operations gsgpu_driver_kms_fops = {
+	.owner = THIS_MODULE,
+	.open = drm_open,
+	.flush = gsgpu_flush,
+	.release = drm_release,
+	.unlocked_ioctl = gsgpu_drm_ioctl,
+	.mmap = gsgpu_mmap,
+	.poll = drm_poll,
+	.read = drm_read,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = gsgpu_kms_compat_ioctl,
+#endif
+};
+
+static bool
+gsgpu_get_crtc_scanout_position(struct drm_device *dev, unsigned int pipe,
+				 bool in_vblank_irq, int *vpos, int *hpos,
+				 ktime_t *stime, ktime_t *etime,
+				 const struct drm_display_mode *mode)
+{
+	return gsgpu_display_get_crtc_scanoutpos(dev, pipe, 0, vpos, hpos,
+						  stime, etime, mode);
+}
+
+static struct drm_driver kms_driver = {
+	.driver_features = DRIVER_HAVE_IRQ | DRIVER_IRQ_SHARED | DRIVER_GEM |
+		DRIVER_PRIME |  DRIVER_MODESET | DRIVER_SYNCOBJ
+		|DRIVER_RENDER | DRIVER_ATOMIC,
+	.load = gsgpu_driver_load_kms,
+	.open = gsgpu_driver_open_kms,
+	.postclose = gsgpu_driver_postclose_kms,
+	.lastclose = gsgpu_driver_lastclose_kms,
+	.unload = gsgpu_driver_unload_kms,
+	.get_vblank_counter = gsgpu_get_vblank_counter_kms,
+	.get_vblank_timestamp = drm_calc_vbltimestamp_from_scanoutpos,
+	.get_scanout_position = gsgpu_get_crtc_scanout_position,
+	.irq_handler = gsgpu_irq_handler,
+	.ioctls = gsgpu_ioctls_kms,
+	.gem_free_object_unlocked = gsgpu_gem_object_free,
+	.gem_open_object = gsgpu_gem_object_open,
+	.gem_close_object = gsgpu_gem_object_close,
+	.dumb_create = gsgpu_mode_dumb_create,
+	.dumb_map_offset = gsgpu_mode_dumb_mmap,
+	.fops = &gsgpu_driver_kms_fops,
+
+	.prime_handle_to_fd = drm_gem_prime_handle_to_fd,
+	.prime_fd_to_handle = drm_gem_prime_fd_to_handle,
+	.gem_prime_export = gsgpu_gem_prime_export,
+	.gem_prime_import = gsgpu_gem_prime_import,
+	.gem_prime_res_obj = gsgpu_gem_prime_res_obj,
+	.gem_prime_get_sg_table = gsgpu_gem_prime_get_sg_table,
+	.gem_prime_import_sg_table = gsgpu_gem_prime_import_sg_table,
+	.gem_prime_vmap = gsgpu_gem_prime_vmap,
+	.gem_prime_vunmap = gsgpu_gem_prime_vunmap,
+	.gem_prime_mmap = gsgpu_gem_prime_mmap,
+
+	.name = DRIVER_NAME,
+	.desc = DRIVER_DESC,
+	.date = DRIVER_DATE,
+	.major = KMS_DRIVER_MAJOR,
+	.minor = KMS_DRIVER_MINOR,
+	.patchlevel = KMS_DRIVER_PATCHLEVEL,
+};
+
+static struct drm_driver *driver;
+static struct pci_driver *pdriver;
+static struct pci_driver *loongson_dc_pdriver;
+
+static struct pci_driver gsgpu_kms_pci_driver = {
+	.name = DRIVER_NAME,
+	.id_table = pciidlist,
+	.probe = gsgpu_pci_probe,
+	.remove = gsgpu_pci_remove,
+	.shutdown = gsgpu_pci_shutdown,
+	.driver.pm = &gsgpu_pm_ops,
+};
+
+/**
+ * loongson_vga_pci_driver -- pci driver structure
+ *
+ * .id_table : must be non-NULL for probe to be called
+ * .probe: New device inserted
+ * .remove: Device removed
+ * .resume: Device suspended
+ * .suspend: Device woken up
+ */
+static struct pci_driver loongson_vga_pci_driver = {
+	.name = "gsgpu-dc",
+	.id_table = loongson_vga_pci_devices,
+	.probe = loongson_vga_pci_register,
+	.remove = loongson_vga_pci_unregister,
+};
+
+static int __init gsgpu_init(void)
+{
+	struct pci_dev *pdev = NULL;
+	struct file *fw_file = NULL;
+	int r;
+
+	if (vgacon_text_force()) {
+		DRM_ERROR("VGACON disables gsgpu kernel modesetting.\n");
+		return -EINVAL;
+	}
+
+	/* Prefer discrete card if present */
+	while ((pdev = pci_get_class(PCI_CLASS_DISPLAY_VGA << 8, pdev))) {
+		if (pdev->vendor != PCI_VENDOR_ID_LOONGSON)
+			return 0;
+
+		if (!gsgpu_lg100_support || (pdev->device != 0x7a36))
+			return -EINVAL;
+
+		fw_file = filp_open("/lib/firmware/loongson/lg100_cp.bin",
+				    O_RDONLY, 0600);
+		if (IS_ERR(fw_file))
+			return -EINVAL;
+
+		filp_close(fw_file, NULL);
+	}
+
+	if (!check_vbios_info()) {
+		DRM_INFO("gsgpu can not support this board!!!\n");
+		return -EINVAL;
+	}
+
+	r = gsgpu_sync_init();
+	if (r)
+		goto error_sync;
+
+	r = gsgpu_fence_slab_init();
+	if (r)
+		goto error_fence;
+
+	DRM_INFO("gsgpu kernel modesetting enabled.\n");
+	driver = &kms_driver;
+	pdriver = &gsgpu_kms_pci_driver;
+	loongson_dc_pdriver = &loongson_vga_pci_driver;
+	driver->num_ioctls = gsgpu_max_kms_ioctl;
+
+	r = pci_register_driver(loongson_dc_pdriver);
+	if (r) {
+		goto error_sync;
+	}
+
+	r = pci_register_driver(pdriver);
+	if (r) {
+		goto error_sync;
+	}
+
+	return 0;
+
+error_fence:
+	gsgpu_sync_fini();
+
+error_sync:
+	return r;
+}
+
+static void __exit gsgpu_exit(void)
+{
+	pci_unregister_driver(pdriver);
+	pci_unregister_driver(loongson_dc_pdriver);
+	gsgpu_sync_fini();
+	gsgpu_fence_slab_fini();
+}
+
+module_init(gsgpu_init);
+module_exit(gsgpu_exit);
+
+MODULE_AUTHOR(DRIVER_AUTHOR);
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_LICENSE("GPL and additional rights");
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_fb.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_fb.c
new file mode 100644
index 000000000000..3abc22a15bf3
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_fb.c
@@ -0,0 +1,363 @@
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/pm_runtime.h>
+
+#include <drm/drmP.h>
+#include <drm/drm_crtc.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+
+#include <drm/drm_fb_helper.h>
+
+#include <linux/vga_switcheroo.h>
+
+#include "gsgpu_display.h"
+
+/* object hierarchy -
+   this contains a helper + a gsgpu fb
+   the helper contains a pointer to gsgpu framebuffer baseclass.
+*/
+
+static int
+gsgpufb_open(struct fb_info *info, int user)
+{
+	struct gsgpu_fbdev *rfbdev = info->par;
+	struct gsgpu_device *adev = rfbdev->adev;
+	int ret = pm_runtime_get_sync(adev->ddev->dev);
+	if (ret < 0 && ret != -EACCES) {
+		pm_runtime_mark_last_busy(adev->ddev->dev);
+		pm_runtime_put_autosuspend(adev->ddev->dev);
+		return ret;
+	}
+	return 0;
+}
+
+static int
+gsgpufb_release(struct fb_info *info, int user)
+{
+	struct gsgpu_fbdev *rfbdev = info->par;
+	struct gsgpu_device *adev = rfbdev->adev;
+
+	pm_runtime_mark_last_busy(adev->ddev->dev);
+	pm_runtime_put_autosuspend(adev->ddev->dev);
+	return 0;
+}
+
+static struct fb_ops gsgpufb_ops = {
+	.owner = THIS_MODULE,
+	DRM_FB_HELPER_DEFAULT_OPS,
+	.fb_open = gsgpufb_open,
+	.fb_release = gsgpufb_release,
+	.fb_fillrect = drm_fb_helper_cfb_fillrect,
+	.fb_copyarea = drm_fb_helper_cfb_copyarea,
+	.fb_imageblit = drm_fb_helper_cfb_imageblit,
+};
+
+
+int gsgpu_align_pitch(struct gsgpu_device *adev, int width, int cpp, bool tiled)
+{
+	int aligned = width;
+
+	aligned *= cpp;
+	aligned += 255;
+	aligned &= ~255;
+
+	return aligned;
+}
+
+static void gsgpufb_destroy_pinned_object(struct drm_gem_object *gobj)
+{
+	struct gsgpu_bo *abo = gem_to_gsgpu_bo(gobj);
+	int ret;
+
+	ret = gsgpu_bo_reserve(abo, true);
+	if (likely(ret == 0)) {
+		gsgpu_bo_kunmap(abo);
+		gsgpu_bo_unpin(abo);
+		gsgpu_bo_unreserve(abo);
+	}
+	drm_gem_object_put_unlocked(gobj);
+}
+
+static int gsgpufb_create_pinned_object(struct gsgpu_fbdev *rfbdev,
+					 struct drm_mode_fb_cmd2 *mode_cmd,
+					 struct drm_gem_object **gobj_p)
+{
+	struct gsgpu_device *adev = rfbdev->adev;
+	struct drm_gem_object *gobj = NULL;
+	struct gsgpu_bo *abo = NULL;
+	bool fb_tiled = false; /* useful for testing */
+	u32 tiling_flags = 0, domain;
+	int ret;
+	int aligned_size, size;
+	int height = mode_cmd->height;
+	u32 cpp;
+
+	cpp = drm_format_plane_cpp(mode_cmd->pixel_format, 0);
+
+	/* need to align pitch with crtc limits */
+	mode_cmd->pitches[0] = gsgpu_align_pitch(adev, mode_cmd->width, cpp,
+						  fb_tiled);
+	domain = gsgpu_display_supported_domains(adev);
+
+	height = ALIGN(mode_cmd->height, 8);
+	size = mode_cmd->pitches[0] * height;
+	aligned_size = ALIGN(size, PAGE_SIZE);
+
+	ret = gsgpu_gem_object_create(adev, aligned_size, 0, domain,
+				       GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
+				       GSGPU_GEM_CREATE_VRAM_CONTIGUOUS |
+				       GSGPU_GEM_CREATE_VRAM_CLEARED,
+				       ttm_bo_type_device, NULL, &gobj);
+
+	if (ret) {
+		pr_err("failed to allocate framebuffer (%d)\n", aligned_size);
+		return -ENOMEM;
+	}
+	abo = gem_to_gsgpu_bo(gobj);
+
+	ret = gsgpu_bo_reserve(abo, false);
+	if (unlikely(ret != 0))
+		goto out_unref;
+
+	if (tiling_flags) {
+		ret = gsgpu_bo_set_tiling_flags(abo,
+						 tiling_flags);
+		if (ret)
+			dev_err(adev->dev, "FB failed to set tiling flags\n");
+	}
+
+	ret = gsgpu_bo_pin(abo, domain);
+	if (ret) {
+		gsgpu_bo_unreserve(abo);
+		goto out_unref;
+	}
+
+	ret = gsgpu_ttm_alloc_gart(&abo->tbo);
+	if (ret) {
+		gsgpu_bo_unreserve(abo);
+		dev_err(adev->dev, "%p bind failed\n", abo);
+		goto out_unref;
+	}
+
+	ret = gsgpu_bo_kmap(abo, NULL);
+	gsgpu_bo_unreserve(abo);
+	if (ret) {
+		goto out_unref;
+	}
+
+	//for (i = 0; i < abo->tbo.ttm->num_pages; i++) {
+	//	pr_err("page[%d], 0x%lX\n", i, page_to_pfn(abo->tbo.ttm->pages[i]));
+	//}
+
+	*gobj_p = gobj;
+	return 0;
+out_unref:
+	gsgpufb_destroy_pinned_object(gobj);
+	*gobj_p = NULL;
+	return ret;
+}
+
+static int gsgpufb_create(struct drm_fb_helper *helper,
+			   struct drm_fb_helper_surface_size *sizes)
+{
+	struct gsgpu_fbdev *rfbdev = (struct gsgpu_fbdev *)helper;
+	struct gsgpu_device *adev = rfbdev->adev;
+	struct fb_info *info;
+	struct drm_framebuffer *fb = NULL;
+	struct drm_mode_fb_cmd2 mode_cmd;
+	struct drm_gem_object *gobj = NULL;
+	struct gsgpu_bo *abo = NULL;
+	unsigned long tmp;
+	int ret;
+
+	mode_cmd.width = sizes->surface_width;
+	mode_cmd.height = sizes->surface_height;
+
+	if (sizes->surface_bpp == 24)
+		sizes->surface_bpp = 32;
+
+	mode_cmd.pixel_format = drm_mode_legacy_fb_format(sizes->surface_bpp,
+							  sizes->surface_depth);
+
+	ret = gsgpufb_create_pinned_object(rfbdev, &mode_cmd, &gobj);
+	if (ret) {
+		DRM_ERROR("failed to create fbcon object %d\n", ret);
+		return ret;
+	}
+
+	abo = gem_to_gsgpu_bo(gobj);
+
+	/* okay we have an object now allocate the framebuffer */
+	info = drm_fb_helper_alloc_fbi(helper);
+	if (IS_ERR(info)) {
+		ret = PTR_ERR(info);
+		goto out;
+	}
+
+	info->par = rfbdev;
+	info->skip_vt_switch = true;
+
+	ret = gsgpu_display_framebuffer_init(adev->ddev, &rfbdev->rfb,
+					      &mode_cmd, gobj);
+	if (ret) {
+		DRM_ERROR("failed to initialize framebuffer %d\n", ret);
+		goto out;
+	}
+
+	fb = &rfbdev->rfb.base;
+
+	/* setup helper */
+	rfbdev->helper.fb = fb;
+
+	strcpy(info->fix.id, "gsgpudrmfb");
+
+	drm_fb_helper_fill_fix(info, fb->pitches[0], fb->format->depth);
+
+	info->fbops = &gsgpufb_ops;
+
+	/* not alloc from VRAM but GTT */
+	tmp = gsgpu_bo_gpu_offset(abo) - adev->gmc.vram_start;
+	info->fix.smem_start = adev->gmc.aper_base + tmp;
+	info->fix.smem_len = gsgpu_bo_size(abo);
+	info->screen_base = gsgpu_bo_kptr(abo);
+	info->screen_size = gsgpu_bo_size(abo);
+
+	drm_fb_helper_fill_var(info, &rfbdev->helper, sizes->fb_width, sizes->fb_height);
+
+	/* setup aperture base/size for vesafb takeover */
+	info->apertures->ranges[0].base = adev->ddev->mode_config.fb_base;
+	info->apertures->ranges[0].size = adev->gmc.aper_size;
+
+	/* Use default scratch pixmap (info->pixmap.flags = FB_PIXMAP_SYSTEM) */
+
+	if (info->screen_base == NULL) {
+		ret = -ENOSPC;
+		goto out;
+	}
+
+	DRM_INFO("fb mappable at 0x%lX\n",  info->fix.smem_start);
+	DRM_INFO("vram apper at 0x%lX\n",  (unsigned long)adev->gmc.aper_base);
+	DRM_INFO("size %lu\n", (unsigned long)gsgpu_bo_size(abo));
+	DRM_INFO("fb depth is %d\n", fb->format->depth);
+	DRM_INFO("   pitch is %d\n", fb->pitches[0]);
+	DRM_INFO("screen_base 0x%llX\n", (u64)info->screen_base);
+	DRM_INFO("screen_size 0x%lX\n", info->screen_size);
+
+	vga_switcheroo_client_fb_set(adev->ddev->pdev, info);
+	return 0;
+
+out:
+	if (abo) {
+
+	}
+	if (fb && ret) {
+		drm_gem_object_put_unlocked(gobj);
+		drm_framebuffer_unregister_private(fb);
+		drm_framebuffer_cleanup(fb);
+		kfree(fb);
+	}
+	return ret;
+}
+
+static int gsgpu_fbdev_destroy(struct drm_device *dev, struct gsgpu_fbdev *rfbdev)
+{
+	struct gsgpu_framebuffer *rfb = &rfbdev->rfb;
+
+	drm_fb_helper_unregister_fbi(&rfbdev->helper);
+
+	if (rfb->base.obj[0]) {
+		gsgpufb_destroy_pinned_object(rfb->base.obj[0]);
+		rfb->base.obj[0] = NULL;
+		drm_framebuffer_unregister_private(&rfb->base);
+		drm_framebuffer_cleanup(&rfb->base);
+	}
+	drm_fb_helper_fini(&rfbdev->helper);
+
+	return 0;
+}
+
+static const struct drm_fb_helper_funcs gsgpu_fb_helper_funcs = {
+	.fb_probe = gsgpufb_create,
+};
+
+int gsgpu_fbdev_init(struct gsgpu_device *adev)
+{
+	struct gsgpu_fbdev *rfbdev;
+	int bpp_sel = 32;
+	int ret;
+
+	/* don't init fbdev on hw without DCE */
+	if (!adev->mode_info.mode_config_initialized)
+		return 0;
+
+	/* don't init fbdev if there are no connectors */
+	if (list_empty(&adev->ddev->mode_config.connector_list))
+		return 0;
+
+	/* select 8 bpp console on low vram cards */
+	if (adev->gmc.real_vram_size <= (32*1024*1024))
+		bpp_sel = 8;
+
+	rfbdev = kzalloc(sizeof(struct gsgpu_fbdev), GFP_KERNEL);
+	if (!rfbdev)
+		return -ENOMEM;
+
+	rfbdev->adev = adev;
+	adev->mode_info.rfbdev = rfbdev;
+
+	drm_fb_helper_prepare(adev->ddev, &rfbdev->helper,
+			&gsgpu_fb_helper_funcs);
+
+	ret = drm_fb_helper_init(adev->ddev, &rfbdev->helper,
+				 GSGPUFB_CONN_LIMIT);
+	if (ret) {
+		kfree(rfbdev);
+		return ret;
+	}
+
+	drm_fb_helper_single_add_all_connectors(&rfbdev->helper);
+	drm_fb_helper_initial_config(&rfbdev->helper, bpp_sel);
+
+	return 0;
+}
+
+void gsgpu_fbdev_fini(struct gsgpu_device *adev)
+{
+	if (!adev->mode_info.rfbdev)
+		return;
+
+	gsgpu_fbdev_destroy(adev->ddev, adev->mode_info.rfbdev);
+	kfree(adev->mode_info.rfbdev);
+	adev->mode_info.rfbdev = NULL;
+}
+
+void gsgpu_fbdev_set_suspend(struct gsgpu_device *adev, int state)
+{
+	if (adev->mode_info.rfbdev)
+		drm_fb_helper_set_suspend_unlocked(&adev->mode_info.rfbdev->helper,
+						   state);
+}
+
+int gsgpu_fbdev_total_size(struct gsgpu_device *adev)
+{
+	struct gsgpu_bo *robj;
+	int size = 0;
+
+	if (!adev->mode_info.rfbdev)
+		return 0;
+
+	robj = gem_to_gsgpu_bo(adev->mode_info.rfbdev->rfb.base.obj[0]);
+	size += gsgpu_bo_size(robj);
+	return size;
+}
+
+bool gsgpu_fbdev_robj_is_fb(struct gsgpu_device *adev, struct gsgpu_bo *robj)
+{
+	if (!adev->mode_info.rfbdev)
+		return false;
+	if (robj == gem_to_gsgpu_bo(adev->mode_info.rfbdev->rfb.base.obj[0]))
+		return true;
+	return false;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_fence.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_fence.c
new file mode 100644
index 000000000000..708404d56e81
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_fence.c
@@ -0,0 +1,688 @@
+#include <linux/seq_file.h>
+#include <linux/atomic.h>
+#include <linux/wait.h>
+#include <linux/kref.h>
+#include <linux/slab.h>
+#include <linux/firmware.h>
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include "gsgpu_trace.h"
+
+/*
+ * Fences
+ * Fences mark an event in the GPUs pipeline and are used
+ * for GPU/CPU synchronization.  When the fence is written,
+ * it is expected that all buffers associated with that fence
+ * are no longer in use by the associated ring on the GPU and
+ * that the the relevant GPU caches have been flushed.
+ */
+
+struct gsgpu_fence {
+	struct dma_fence base;
+
+	/* RB, DMA, etc. */
+	struct gsgpu_ring		*ring;
+};
+
+static struct kmem_cache *gsgpu_fence_slab;
+
+int gsgpu_fence_slab_init(void)
+{
+	gsgpu_fence_slab = kmem_cache_create(
+		"gsgpu_fence", sizeof(struct gsgpu_fence), 0,
+		SLAB_HWCACHE_ALIGN, NULL);
+	if (!gsgpu_fence_slab)
+		return -ENOMEM;
+	return 0;
+}
+
+void gsgpu_fence_slab_fini(void)
+{
+	rcu_barrier();
+	kmem_cache_destroy(gsgpu_fence_slab);
+}
+/*
+ * Cast helper
+ */
+static const struct dma_fence_ops gsgpu_fence_ops;
+static inline struct gsgpu_fence *to_gsgpu_fence(struct dma_fence *f)
+{
+	struct gsgpu_fence *__f = container_of(f, struct gsgpu_fence, base);
+
+	if (__f->base.ops == &gsgpu_fence_ops)
+		return __f;
+
+	return NULL;
+}
+
+/**
+ * gsgpu_fence_write - write a fence value
+ *
+ * @ring: ring the fence is associated with
+ * @seq: sequence number to write
+ *
+ * Writes a fence value to memory (all asics).
+ */
+static void gsgpu_fence_write(struct gsgpu_ring *ring, u32 seq)
+{
+	struct gsgpu_fence_driver *drv = &ring->fence_drv;
+
+	if (drv->cpu_addr)
+		*drv->cpu_addr = cpu_to_le32(seq);
+}
+
+/**
+ * gsgpu_fence_read - read a fence value
+ *
+ * @ring: ring the fence is associated with
+ *
+ * Reads a fence value from memory (all asics).
+ * Returns the value of the fence read from memory.
+ */
+static u32 gsgpu_fence_read(struct gsgpu_ring *ring)
+{
+	struct gsgpu_fence_driver *drv = &ring->fence_drv;
+	u32 seq = 0;
+
+	if (drv->cpu_addr)
+		seq = le32_to_cpu(*drv->cpu_addr);
+	else
+		seq = atomic_read(&drv->last_seq);
+
+	return seq;
+}
+
+/**
+ * gsgpu_fence_emit - emit a fence on the requested ring
+ *
+ * @ring: ring the fence is associated with
+ * @f: resulting fence object
+ *
+ * Emits a fence command on the requested ring (all asics).
+ * Returns 0 on success, -ENOMEM on failure.
+ */
+int gsgpu_fence_emit(struct gsgpu_ring *ring, struct dma_fence **f,
+		      unsigned flags)
+{
+	struct gsgpu_device *adev = ring->adev;
+	struct gsgpu_fence *fence;
+	struct dma_fence __rcu **ptr;
+	uint32_t seq;
+	int r;
+
+	fence = kmem_cache_alloc(gsgpu_fence_slab, GFP_KERNEL);
+	if (fence == NULL)
+		return -ENOMEM;
+
+	seq = ++ring->fence_drv.sync_seq;
+	fence->ring = ring;
+	dma_fence_init(&fence->base, &gsgpu_fence_ops,
+		       &ring->fence_drv.lock,
+		       adev->fence_context + ring->idx,
+		       seq);
+	gsgpu_ring_emit_fence(ring, ring->fence_drv.gpu_addr,
+			       seq, flags | GSGPU_FENCE_FLAG_INT);
+
+	ptr = &ring->fence_drv.fences[seq & ring->fence_drv.num_fences_mask];
+	if (unlikely(rcu_dereference_protected(*ptr, 1))) {
+		struct dma_fence *old;
+
+		rcu_read_lock();
+		old = dma_fence_get_rcu_safe(ptr);
+		rcu_read_unlock();
+
+		if (old) {
+			r = dma_fence_wait(old, false);
+			dma_fence_put(old);
+			if (r)
+				return r;
+		}
+	}
+
+	/* This function can't be called concurrently anyway, otherwise
+	 * emitting the fence would mess up the hardware ring buffer.
+	 */
+	rcu_assign_pointer(*ptr, dma_fence_get(&fence->base));
+
+	*f = &fence->base;
+
+	return 0;
+}
+
+/**
+ * gsgpu_fence_emit_polling - emit a fence on the requeste ring
+ *
+ * @ring: ring the fence is associated with
+ * @s: resulting sequence number
+ *
+ * Emits a fence command on the requested ring (all asics).
+ * Used For polling fence.
+ * Returns 0 on success, -ENOMEM on failure.
+ */
+int gsgpu_fence_emit_polling(struct gsgpu_ring *ring, uint32_t *s)
+{
+	uint32_t seq;
+
+	if (!s)
+		return -EINVAL;
+
+	seq = ++ring->fence_drv.sync_seq;
+	gsgpu_ring_emit_fence(ring, ring->fence_drv.gpu_addr,
+			       seq, 0);
+
+	*s = seq;
+
+	return 0;
+}
+
+/**
+ * gsgpu_fence_schedule_fallback - schedule fallback check
+ *
+ * @ring: pointer to struct gsgpu_ring
+ *
+ * Start a timer as fallback to our interrupts.
+ */
+static void gsgpu_fence_schedule_fallback(struct gsgpu_ring *ring)
+{
+	mod_timer(&ring->fence_drv.fallback_timer,
+		  jiffies + GSGPU_FENCE_JIFFIES_TIMEOUT);
+}
+
+/**
+ * gsgpu_fence_process - check for fence activity
+ *
+ * @ring: pointer to struct gsgpu_ring
+ *
+ * Checks the current fence value and calculates the last
+ * signalled fence value. Wakes the fence queue if the
+ * sequence number has increased.
+ */
+void gsgpu_fence_process(struct gsgpu_ring *ring)
+{
+	struct gsgpu_fence_driver *drv = &ring->fence_drv;
+	uint32_t seq, last_seq;
+	int r;
+
+	do {
+		last_seq = atomic_read(&ring->fence_drv.last_seq);
+		seq = gsgpu_fence_read(ring);
+
+	} while (atomic_cmpxchg(&drv->last_seq, last_seq, seq) != last_seq);
+
+	if (seq != ring->fence_drv.sync_seq)
+		gsgpu_fence_schedule_fallback(ring);
+
+	if (unlikely(seq == last_seq))
+		return;
+
+	last_seq &= drv->num_fences_mask;
+	seq &= drv->num_fences_mask;
+
+	do {
+		struct dma_fence *fence, **ptr;
+
+		++last_seq;
+		last_seq &= drv->num_fences_mask;
+		ptr = &drv->fences[last_seq];
+
+		/* There is always exactly one thread signaling this fence slot */
+		fence = rcu_dereference_protected(*ptr, 1);
+		RCU_INIT_POINTER(*ptr, NULL);
+
+		if (!fence)
+			continue;
+
+		r = dma_fence_signal(fence);
+		if (!r)
+			DMA_FENCE_TRACE(fence, "signaled from irq context\n");
+		else
+			BUG();
+
+		dma_fence_put(fence);
+	} while (last_seq != seq);
+}
+
+/**
+ * gsgpu_fence_fallback - fallback for hardware interrupts
+ *
+ * @work: delayed work item
+ *
+ * Checks for fence activity.
+ */
+static void gsgpu_fence_fallback(struct timer_list *t)
+{
+	struct gsgpu_ring *ring = from_timer(ring, t,
+					      fence_drv.fallback_timer);
+
+	gsgpu_fence_process(ring);
+}
+
+/**
+ * gsgpu_fence_wait_empty - wait for all fences to signal
+ *
+ * @adev: gsgpu device pointer
+ * @ring: ring index the fence is associated with
+ *
+ * Wait for all fences on the requested ring to signal (all asics).
+ * Returns 0 if the fences have passed, error for all other cases.
+ */
+int gsgpu_fence_wait_empty(struct gsgpu_ring *ring)
+{
+	uint64_t seq = READ_ONCE(ring->fence_drv.sync_seq);
+	struct dma_fence *fence, **ptr;
+	int r;
+
+	if (!seq)
+		return 0;
+
+	ptr = &ring->fence_drv.fences[seq & ring->fence_drv.num_fences_mask];
+	rcu_read_lock();
+	fence = rcu_dereference(*ptr);
+	if (!fence || !dma_fence_get_rcu(fence)) {
+		rcu_read_unlock();
+		return 0;
+	}
+	rcu_read_unlock();
+
+	r = dma_fence_wait(fence, false);
+	dma_fence_put(fence);
+	return r;
+}
+
+/**
+ * gsgpu_fence_wait_polling - busy wait for givn sequence number
+ *
+ * @ring: ring index the fence is associated with
+ * @wait_seq: sequence number to wait
+ * @timeout: the timeout for waiting in usecs
+ *
+ * Wait for all fences on the requested ring to signal (all asics).
+ * Returns left time if no timeout, 0 or minus if timeout.
+ */
+signed long gsgpu_fence_wait_polling(struct gsgpu_ring *ring,
+				      uint32_t wait_seq,
+				      signed long timeout)
+{
+	uint32_t seq;
+
+	do {
+		seq = gsgpu_fence_read(ring);
+		udelay(5);
+		timeout -= 5;
+	} while ((int32_t)(wait_seq - seq) > 0 && timeout > 0);
+
+	return timeout > 0 ? timeout : 0;
+}
+/**
+ * gsgpu_fence_count_emitted - get the count of emitted fences
+ *
+ * @ring: ring the fence is associated with
+ *
+ * Get the number of fences emitted on the requested ring (all asics).
+ * Returns the number of emitted fences on the ring.  Used by the
+ * dynpm code to ring track activity.
+ */
+unsigned gsgpu_fence_count_emitted(struct gsgpu_ring *ring)
+{
+	uint64_t emitted;
+
+	/* We are not protected by ring lock when reading the last sequence
+	 * but it's ok to report slightly wrong fence count here.
+	 */
+	gsgpu_fence_process(ring);
+	emitted = 0x100000000ull;
+	emitted -= atomic_read(&ring->fence_drv.last_seq);
+	emitted += READ_ONCE(ring->fence_drv.sync_seq);
+	return lower_32_bits(emitted);
+}
+
+/**
+ * gsgpu_fence_driver_start_ring - make the fence driver
+ * ready for use on the requested ring.
+ *
+ * @ring: ring to start the fence driver on
+ * @irq_src: interrupt source to use for this ring
+ * @irq_type: interrupt type to use for this ring
+ *
+ * Make the fence driver ready for processing (all asics).
+ * Not all asics have all rings, so each asic will only
+ * start the fence driver on the rings it has.
+ * Returns 0 for success, errors for failure.
+ */
+int gsgpu_fence_driver_start_ring(struct gsgpu_ring *ring,
+				   struct gsgpu_irq_src *irq_src,
+				   unsigned irq_type)
+{
+	struct gsgpu_device *adev = ring->adev;
+
+	ring->fence_drv.cpu_addr = &adev->wb.wb[ring->fence_offs];
+	ring->fence_drv.gpu_addr = adev->wb.gpu_addr + (ring->fence_offs * 4);
+	gsgpu_fence_write(ring, atomic_read(&ring->fence_drv.last_seq));
+	gsgpu_irq_get(adev, irq_src, irq_type);
+
+	ring->fence_drv.irq_src = irq_src;
+	ring->fence_drv.irq_type = irq_type;
+	ring->fence_drv.initialized = true;
+
+	dev_dbg(adev->dev, "fence driver on ring %d use gpu addr 0x%016llx, "
+		"cpu addr 0x%p\n", ring->idx,
+		ring->fence_drv.gpu_addr, ring->fence_drv.cpu_addr);
+	return 0;
+}
+
+/**
+ * gsgpu_fence_driver_init_ring - init the fence driver
+ * for the requested ring.
+ *
+ * @ring: ring to init the fence driver on
+ * @num_hw_submission: number of entries on the hardware queue
+ *
+ * Init the fence driver for the requested ring (all asics).
+ * Helper function for gsgpu_fence_driver_init().
+ */
+int gsgpu_fence_driver_init_ring(struct gsgpu_ring *ring,
+				  unsigned num_hw_submission)
+{
+	long timeout;
+	int r;
+
+	/* Check that num_hw_submission is a power of two */
+	if ((num_hw_submission & (num_hw_submission - 1)) != 0)
+		return -EINVAL;
+
+	ring->fence_drv.cpu_addr = NULL;
+	ring->fence_drv.gpu_addr = 0;
+	ring->fence_drv.sync_seq = 0;
+	atomic_set(&ring->fence_drv.last_seq, 0);
+	ring->fence_drv.initialized = false;
+
+	timer_setup(&ring->fence_drv.fallback_timer, gsgpu_fence_fallback, 0);
+
+	ring->fence_drv.num_fences_mask = num_hw_submission * 2 - 1;
+	spin_lock_init(&ring->fence_drv.lock);
+	ring->fence_drv.fences = kcalloc(num_hw_submission * 2, sizeof(void *),
+					 GFP_KERNEL);
+	if (!ring->fence_drv.fences)
+		return -ENOMEM;
+
+	timeout = msecs_to_jiffies(gsgpu_lockup_timeout);
+
+	r = drm_sched_init(&ring->sched, &gsgpu_sched_ops,
+			   num_hw_submission, gsgpu_job_hang_limit,
+			   timeout, ring->name);
+	if (r) {
+		DRM_ERROR("Failed to create scheduler on ring %s.\n",
+			  ring->name);
+		return r;
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_fence_driver_init - init the fence driver
+ * for all possible rings.
+ *
+ * @adev: gsgpu device pointer
+ *
+ * Init the fence driver for all possible rings (all asics).
+ * Not all asics have all rings, so each asic will only
+ * start the fence driver on the rings it has using
+ * gsgpu_fence_driver_start_ring().
+ * Returns 0 for success.
+ */
+int gsgpu_fence_driver_init(struct gsgpu_device *adev)
+{
+	if (gsgpu_debugfs_fence_init(adev))
+		dev_err(adev->dev, "fence debugfs file creation failed\n");
+
+	return 0;
+}
+
+/**
+ * gsgpu_fence_driver_fini - tear down the fence driver
+ * for all possible rings.
+ *
+ * @adev: gsgpu device pointer
+ *
+ * Tear down the fence driver for all possible rings (all asics).
+ */
+void gsgpu_fence_driver_fini(struct gsgpu_device *adev)
+{
+	unsigned i, j;
+	int r;
+
+	for (i = 0; i < GSGPU_MAX_RINGS; i++) {
+		struct gsgpu_ring *ring = adev->rings[i];
+
+		if (!ring || !ring->fence_drv.initialized)
+			continue;
+		r = gsgpu_fence_wait_empty(ring);
+		if (r) {
+			/* no need to trigger GPU reset as we are unloading */
+			gsgpu_fence_driver_force_completion(ring);
+		}
+		gsgpu_irq_put(adev, ring->fence_drv.irq_src,
+			       ring->fence_drv.irq_type);
+		drm_sched_fini(&ring->sched);
+		del_timer_sync(&ring->fence_drv.fallback_timer);
+		for (j = 0; j <= ring->fence_drv.num_fences_mask; ++j)
+			dma_fence_put(ring->fence_drv.fences[j]);
+		kfree(ring->fence_drv.fences);
+		ring->fence_drv.fences = NULL;
+		ring->fence_drv.initialized = false;
+	}
+}
+
+/**
+ * gsgpu_fence_driver_suspend - suspend the fence driver
+ * for all possible rings.
+ *
+ * @adev: gsgpu device pointer
+ *
+ * Suspend the fence driver for all possible rings (all asics).
+ */
+void gsgpu_fence_driver_suspend(struct gsgpu_device *adev)
+{
+	int i, r;
+
+	for (i = 0; i < GSGPU_MAX_RINGS; i++) {
+		struct gsgpu_ring *ring = adev->rings[i];
+		if (!ring || !ring->fence_drv.initialized)
+			continue;
+
+		/* wait for gpu to finish processing current batch */
+		r = gsgpu_fence_wait_empty(ring);
+		if (r) {
+			/* delay GPU reset to resume */
+			gsgpu_fence_driver_force_completion(ring);
+		}
+
+		/* disable the interrupt */
+		gsgpu_irq_put(adev, ring->fence_drv.irq_src,
+			       ring->fence_drv.irq_type);
+	}
+}
+
+/**
+ * gsgpu_fence_driver_resume - resume the fence driver
+ * for all possible rings.
+ *
+ * @adev: gsgpu device pointer
+ *
+ * Resume the fence driver for all possible rings (all asics).
+ * Not all asics have all rings, so each asic will only
+ * start the fence driver on the rings it has using
+ * gsgpu_fence_driver_start_ring().
+ * Returns 0 for success.
+ */
+void gsgpu_fence_driver_resume(struct gsgpu_device *adev)
+{
+	int i;
+
+	for (i = 0; i < GSGPU_MAX_RINGS; i++) {
+		struct gsgpu_ring *ring = adev->rings[i];
+		if (!ring || !ring->fence_drv.initialized)
+			continue;
+
+		/* enable the interrupt */
+		gsgpu_irq_get(adev, ring->fence_drv.irq_src,
+			       ring->fence_drv.irq_type);
+	}
+}
+
+/**
+ * gsgpu_fence_driver_force_completion - force signal latest fence of ring
+ *
+ * @ring: fence of the ring to signal
+ *
+ */
+void gsgpu_fence_driver_force_completion(struct gsgpu_ring *ring)
+{
+	gsgpu_fence_write(ring, ring->fence_drv.sync_seq);
+	gsgpu_fence_process(ring);
+}
+
+/*
+ * Common fence implementation
+ */
+
+static const char *gsgpu_fence_get_driver_name(struct dma_fence *fence)
+{
+	return "gsgpu";
+}
+
+static const char *gsgpu_fence_get_timeline_name(struct dma_fence *f)
+{
+	struct gsgpu_fence *fence = to_gsgpu_fence(f);
+	return (const char *)fence->ring->name;
+}
+
+/**
+ * gsgpu_fence_enable_signaling - enable signalling on fence
+ * @fence: fence
+ *
+ * This function is called with fence_queue lock held, and adds a callback
+ * to fence_queue that checks if this fence is signaled, and if so it
+ * signals the fence and removes itself.
+ */
+static bool gsgpu_fence_enable_signaling(struct dma_fence *f)
+{
+	struct gsgpu_fence *fence = to_gsgpu_fence(f);
+	struct gsgpu_ring *ring = fence->ring;
+
+	if (!timer_pending(&ring->fence_drv.fallback_timer))
+		gsgpu_fence_schedule_fallback(ring);
+
+	DMA_FENCE_TRACE(&fence->base, "armed on ring %i!\n", ring->idx);
+
+	return true;
+}
+
+/**
+ * gsgpu_fence_free - free up the fence memory
+ *
+ * @rcu: RCU callback head
+ *
+ * Free up the fence memory after the RCU grace period.
+ */
+static void gsgpu_fence_free(struct rcu_head *rcu)
+{
+	struct dma_fence *f = container_of(rcu, struct dma_fence, rcu);
+	struct gsgpu_fence *fence = to_gsgpu_fence(f);
+	kmem_cache_free(gsgpu_fence_slab, fence);
+}
+
+/**
+ * gsgpu_fence_release - callback that fence can be freed
+ *
+ * @fence: fence
+ *
+ * This function is called when the reference count becomes zero.
+ * It just RCU schedules freeing up the fence.
+ */
+static void gsgpu_fence_release(struct dma_fence *f)
+{
+	call_rcu(&f->rcu, gsgpu_fence_free);
+}
+
+static const struct dma_fence_ops gsgpu_fence_ops = {
+	.get_driver_name = gsgpu_fence_get_driver_name,
+	.get_timeline_name = gsgpu_fence_get_timeline_name,
+	.enable_signaling = gsgpu_fence_enable_signaling,
+	.release = gsgpu_fence_release,
+};
+
+/*
+ * Fence debugfs
+ */
+#if defined(CONFIG_DEBUG_FS)
+static int gsgpu_debugfs_fence_info(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *)m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+	int i;
+
+	for (i = 0; i < GSGPU_MAX_RINGS; ++i) {
+		struct gsgpu_ring *ring = adev->rings[i];
+		if (!ring || !ring->fence_drv.initialized)
+			continue;
+
+		gsgpu_fence_process(ring);
+
+		seq_printf(m, "--- ring %d (%s) ---\n", i, ring->name);
+		seq_printf(m, "Last signaled fence 0x%08x\n",
+			   atomic_read(&ring->fence_drv.last_seq));
+		seq_printf(m, "Last emitted        0x%08x\n",
+			   ring->fence_drv.sync_seq);
+
+		if (ring->funcs->type != GSGPU_RING_TYPE_GFX)
+			continue;
+
+		/* set in CP_VMID_PREEMPT and preemption occurred */
+		seq_printf(m, "Last preempted      0x%08x\n",
+			   le32_to_cpu(*(ring->fence_drv.cpu_addr + 2)));
+		/* set in CP_VMID_RESET and reset occurred */
+		seq_printf(m, "Last reset          0x%08x\n",
+			   le32_to_cpu(*(ring->fence_drv.cpu_addr + 4)));
+		/* Both preemption and reset occurred */
+		seq_printf(m, "Last both           0x%08x\n",
+			   le32_to_cpu(*(ring->fence_drv.cpu_addr + 6)));
+	}
+	return 0;
+}
+
+/**
+ * gsgpu_debugfs_gpu_recover - manually trigger a gpu reset & recover
+ *
+ * Manually trigger a gpu reset at the next fence wait.
+ */
+static int gsgpu_debugfs_gpu_recover(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+
+	seq_printf(m, "gpu recover\n");
+	gsgpu_device_gpu_recover(adev, NULL, true);
+
+	return 0;
+}
+
+static const struct drm_info_list gsgpu_debugfs_fence_list[] = {
+	{"gsgpu_fence_info", &gsgpu_debugfs_fence_info, 0, NULL},
+	{"gsgpu_gpu_recover", &gsgpu_debugfs_gpu_recover, 0, NULL}
+};
+
+#endif
+
+int gsgpu_debugfs_fence_init(struct gsgpu_device *adev)
+{
+#if defined(CONFIG_DEBUG_FS)
+	return gsgpu_debugfs_add_files(adev, gsgpu_debugfs_fence_list, 2);
+#else
+	return 0;
+#endif
+}
+
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_gart.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gart.c
new file mode 100644
index 000000000000..ccf7fd567e35
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gart.c
@@ -0,0 +1,367 @@
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+
+#include "gsgpu.h"
+
+/*
+ * GART
+ * The GART (Graphics Aperture Remapping Table) is an aperture
+ * in the GPU's address space.  System pages can be mapped into
+ * the aperture and look like contiguous pages from the GPU's
+ * perspective.  A page table maps the pages in the aperture
+ * to the actual backing pages in system memory.
+ *
+ * GSGPU GPUs support both an internal GART, as described above,
+ * and AGP.  AGP works similarly, but the GART table is configured
+ * and maintained by the northbridge rather than the driver.
+ * GSGPU hw has a separate AGP aperture that is programmed to
+ * point to the AGP aperture provided by the northbridge and the
+ * requests are passed through to the northbridge aperture.
+ * Both AGP and internal GART can be used at the same time, however
+ * that is not currently supported by the driver.
+ *
+ * This file handles the common internal GART management.
+ */
+
+/*
+ * Common GART table functions.
+ */
+
+/**
+ * gsgpu_dummy_page_init - init dummy page used by the driver
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Allocate the dummy page used by the driver (all asics).
+ * This dummy page is used by the driver as a filler for gart entries
+ * when pages are taken out of the GART
+ * Returns 0 on sucess, -ENOMEM on failure.
+ */
+static int gsgpu_gart_dummy_page_init(struct gsgpu_device *adev)
+{
+	struct page *dummy_page = adev->mman.bdev.glob->dummy_read_page;
+	void *dummy_addr;
+
+	if (adev->dummy_page_addr)
+		return 0;
+
+	dummy_addr = page_address(dummy_page);
+	memset(dummy_addr, 0xdd, PAGE_SIZE);
+
+	adev->dummy_page_addr = pci_map_page(adev->pdev, dummy_page, 0,
+					     PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+	if (pci_dma_mapping_error(adev->pdev, adev->dummy_page_addr)) {
+		dev_err(&adev->pdev->dev, "Failed to DMA MAP the dummy page\n");
+		adev->dummy_page_addr = 0;
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+/**
+ * gsgpu_dummy_page_fini - free dummy page used by the driver
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Frees the dummy page used by the driver (all asics).
+ */
+static void gsgpu_gart_dummy_page_fini(struct gsgpu_device *adev)
+{
+	if (!adev->dummy_page_addr)
+		return;
+	pci_unmap_page(adev->pdev, adev->dummy_page_addr,
+		       PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+	adev->dummy_page_addr = 0;
+}
+
+/**
+ * gsgpu_gart_table_vram_alloc - allocate vram for gart page table
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Allocate video memory for GART page table
+ * (pcie r4xx, r5xx+).  These asics require the
+ * gart table to be in video memory.
+ * Returns 0 for success, error for failure.
+ */
+int gsgpu_gart_table_vram_alloc(struct gsgpu_device *adev)
+{
+	int r;
+
+	if (adev->gart.robj == NULL) {
+		struct gsgpu_bo_param bp;
+
+		memset(&bp, 0, sizeof(bp));
+		bp.size = adev->gart.table_size;
+		bp.byte_align = PAGE_SIZE;
+		bp.domain = GSGPU_GEM_DOMAIN_VRAM;
+		bp.flags = GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
+			GSGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+		bp.type = ttm_bo_type_kernel;
+		bp.resv = NULL;
+		r = gsgpu_bo_create(adev, &bp, &adev->gart.robj);
+		if (r) {
+			return r;
+		}
+	}
+	return 0;
+}
+
+/**
+ * gsgpu_gart_table_vram_pin - pin gart page table in vram
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Pin the GART page table in vram so it will not be moved
+ * by the memory manager (pcie r4xx, r5xx+).  These asics require the
+ * gart table to be in video memory.
+ * Returns 0 for success, error for failure.
+ */
+int gsgpu_gart_table_vram_pin(struct gsgpu_device *adev)
+{
+	int r;
+
+	r = gsgpu_bo_reserve(adev->gart.robj, false);
+	if (unlikely(r != 0))
+		return r;
+	r = gsgpu_bo_pin(adev->gart.robj, GSGPU_GEM_DOMAIN_VRAM);
+	if (r) {
+		gsgpu_bo_unreserve(adev->gart.robj);
+		return r;
+	}
+	r = gsgpu_bo_kmap(adev->gart.robj, &adev->gart.ptr);
+	if (r)
+		gsgpu_bo_unpin(adev->gart.robj);
+	gsgpu_bo_unreserve(adev->gart.robj);
+	adev->gart.table_addr = gsgpu_bo_gpu_offset(adev->gart.robj);
+	return r;
+}
+
+/**
+ * gsgpu_gart_table_vram_unpin - unpin gart page table in vram
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Unpin the GART page table in vram (pcie r4xx, r5xx+).
+ * These asics require the gart table to be in video memory.
+ */
+void gsgpu_gart_table_vram_unpin(struct gsgpu_device *adev)
+{
+	int r;
+
+	if (adev->gart.robj == NULL) {
+		return;
+	}
+	r = gsgpu_bo_reserve(adev->gart.robj, true);
+	if (likely(r == 0)) {
+		gsgpu_bo_kunmap(adev->gart.robj);
+		gsgpu_bo_unpin(adev->gart.robj);
+		gsgpu_bo_unreserve(adev->gart.robj);
+		adev->gart.ptr = NULL;
+	}
+}
+
+/**
+ * gsgpu_gart_table_vram_free - free gart page table vram
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Free the video memory used for the GART page table
+ * (pcie r4xx, r5xx+).  These asics require the gart table to
+ * be in video memory.
+ */
+void gsgpu_gart_table_vram_free(struct gsgpu_device *adev)
+{
+	if (adev->gart.robj == NULL) {
+		return;
+	}
+	gsgpu_bo_unref(&adev->gart.robj);
+}
+
+/*
+ * Common gart functions.
+ */
+/**
+ * gsgpu_gart_unbind - unbind pages from the gart page table
+ *
+ * @adev: gsgpu_device pointer
+ * @offset: offset into the GPU's gart aperture
+ * @pages: number of pages to unbind
+ *
+ * Unbinds the requested pages from the gart page table and
+ * replaces them with the dummy page (all asics).
+ * Returns 0 for success, -EINVAL for failure.
+ */
+int gsgpu_gart_unbind(struct gsgpu_device *adev, uint64_t offset,
+			int pages)
+{
+	unsigned t;
+	unsigned p;
+	int i, j;
+	u64 page_base;
+	/* Starting from VEGA10, system bit must be 0 to mean invalid. */
+	uint64_t flags = 0;
+
+	if (!adev->gart.ready) {
+		WARN(1, "trying to unbind memory from uninitialized GART !\n");
+		return -EINVAL;
+	}
+
+	t = offset / GSGPU_GPU_PAGE_SIZE;
+	p = t / GSGPU_GPU_PAGES_IN_CPU_PAGE;
+	for (i = 0; i < pages; i++, p++) {
+#ifdef CONFIG_DRM_GSGPU_GART_DEBUGFS
+		adev->gart.pages[p] = NULL;
+#endif
+		page_base = adev->dummy_page_addr;
+		if (!adev->gart.ptr)
+			continue;
+
+		for (j = 0; j < GSGPU_GPU_PAGES_IN_CPU_PAGE; j++, t++) {
+			gsgpu_gmc_set_pte_pde(adev, adev->gart.ptr,
+					       t, page_base, flags);
+			page_base += GSGPU_GPU_PAGE_SIZE;
+		}
+	}
+	mb();
+	gsgpu_gmc_flush_gpu_tlb(adev, 0);
+	return 0;
+}
+
+/**
+ * gsgpu_gart_map - map dma_addresses into GART entries
+ *
+ * @adev: gsgpu_device pointer
+ * @offset: offset into the GPU's gart aperture
+ * @pages: number of pages to bind
+ * @dma_addr: DMA addresses of pages
+ *
+ * Map the dma_addresses into GART entries (all asics).
+ * Returns 0 for success, -EINVAL for failure.
+ */
+int gsgpu_gart_map(struct gsgpu_device *adev, uint64_t offset,
+		    int pages, dma_addr_t *dma_addr, uint64_t flags,
+		    void *dst)
+{
+	uint64_t page_base;
+	unsigned i, j, t;
+
+	if (!adev->gart.ready) {
+		WARN(1, "trying to bind memory to uninitialized GART !\n");
+		return -EINVAL;
+	}
+
+	t = offset / GSGPU_GPU_PAGE_SIZE;
+
+	for (i = 0; i < pages; i++) {
+		page_base = dma_addr[i];
+		for (j = 0; j < GSGPU_GPU_PAGES_IN_CPU_PAGE; j++, t++) {
+			gsgpu_gmc_set_pte_pde(adev, dst, t, page_base, flags);
+			page_base += GSGPU_GPU_PAGE_SIZE;
+		}
+	}
+	return 0;
+}
+
+/**
+ * gsgpu_gart_bind - bind pages into the gart page table
+ *
+ * @adev: gsgpu_device pointer
+ * @offset: offset into the GPU's gart aperture
+ * @pages: number of pages to bind
+ * @pagelist: pages to bind
+ * @dma_addr: DMA addresses of pages
+ *
+ * Binds the requested pages to the gart page table
+ * (all asics).
+ * Returns 0 for success, -EINVAL for failure.
+ */
+int gsgpu_gart_bind(struct gsgpu_device *adev, uint64_t offset,
+		     int pages, struct page **pagelist, dma_addr_t *dma_addr,
+		     uint64_t flags)
+{
+#ifdef CONFIG_DRM_GSGPU_GART_DEBUGFS
+	unsigned i, t, p;
+#endif
+	int r;
+
+	if (!adev->gart.ready) {
+		WARN(1, "trying to bind memory to uninitialized GART !\n");
+		return -EINVAL;
+	}
+
+#ifdef CONFIG_DRM_GSGPU_GART_DEBUGFS
+	t = offset / GSGPU_GPU_PAGE_SIZE;
+	p = t / GSGPU_GPU_PAGES_IN_CPU_PAGE;
+	for (i = 0; i < pages; i++, p++)
+		adev->gart.pages[p] = pagelist ? pagelist[i] : NULL;
+#endif
+
+	if (!adev->gart.ptr)
+		return 0;
+
+	r = gsgpu_gart_map(adev, offset, pages, dma_addr, flags,
+		    adev->gart.ptr);
+	if (r)
+		return r;
+
+	mb();
+	gsgpu_gmc_flush_gpu_tlb(adev, 0);
+	return 0;
+}
+
+/**
+ * gsgpu_gart_init - init the driver info for managing the gart
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Allocate the dummy page and init the gart driver info (all asics).
+ * Returns 0 for success, error for failure.
+ */
+int gsgpu_gart_init(struct gsgpu_device *adev)
+{
+	int r;
+
+	if (adev->dummy_page_addr)
+		return 0;
+
+	/* We need PAGE_SIZE >= GSGPU_GPU_PAGE_SIZE */
+	if (PAGE_SIZE < GSGPU_GPU_PAGE_SIZE) {
+		DRM_ERROR("Page size is smaller than GPU page size!\n");
+		return -EINVAL;
+	}
+	r = gsgpu_gart_dummy_page_init(adev);
+	if (r)
+		return r;
+	/* Compute table size */
+	adev->gart.num_cpu_pages = adev->gmc.gart_size / PAGE_SIZE;
+	adev->gart.num_gpu_pages = adev->gmc.gart_size / GSGPU_GPU_PAGE_SIZE;
+	DRM_INFO("GART: num cpu pages %u, num gpu pages %u\n",
+		 adev->gart.num_cpu_pages, adev->gart.num_gpu_pages);
+
+#ifdef CONFIG_DRM_GSGPU_GART_DEBUGFS
+	/* Allocate pages table */
+	adev->gart.pages = vzalloc(array_size(sizeof(void *),
+					      adev->gart.num_cpu_pages));
+	if (adev->gart.pages == NULL)
+		return -ENOMEM;
+#endif
+
+	return 0;
+}
+
+/**
+ * gsgpu_gart_fini - tear down the driver info for managing the gart
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Tear down the gart driver info and free the dummy page (all asics).
+ */
+void gsgpu_gart_fini(struct gsgpu_device *adev)
+{
+#ifdef CONFIG_DRM_GSGPU_GART_DEBUGFS
+	vfree(adev->gart.pages);
+	adev->gart.pages = NULL;
+#endif
+	gsgpu_gart_dummy_page_fini(adev);
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c
new file mode 100644
index 000000000000..4e8137116858
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gem.c
@@ -0,0 +1,854 @@
+#include <linux/ktime.h>
+#include <linux/pagemap.h>
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+#include "gsgpu_display.h"
+
+void gsgpu_gem_object_free(struct drm_gem_object *gobj)
+{
+	struct gsgpu_bo *robj = gem_to_gsgpu_bo(gobj);
+
+	if (robj) {
+		gsgpu_mn_unregister(robj);
+		gsgpu_bo_unref(&robj);
+	}
+}
+
+int gsgpu_gem_object_create(struct gsgpu_device *adev, unsigned long size,
+			     int alignment, u32 initial_domain,
+			     u64 flags, enum ttm_bo_type type,
+			     struct reservation_object *resv,
+			     struct drm_gem_object **obj)
+{
+	struct gsgpu_bo *bo;
+	struct gsgpu_bo_param bp;
+	int r;
+
+	memset(&bp, 0, sizeof(bp));
+	*obj = NULL;
+	/* At least align on page size */
+	if (alignment < PAGE_SIZE) {
+		alignment = PAGE_SIZE;
+	}
+
+	bp.size = size;
+	bp.byte_align = alignment;
+	bp.type = type;
+	bp.resv = resv;
+	bp.preferred_domain = initial_domain;
+retry:
+	bp.flags = flags;
+	bp.domain = initial_domain;
+	r = gsgpu_bo_create(adev, &bp, &bo);
+	if (r) {
+		if (r != -ERESTARTSYS) {
+			if (flags & GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
+				flags &= ~GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+				goto retry;
+			}
+
+			if (initial_domain == GSGPU_GEM_DOMAIN_VRAM &&
+				!(flags & GSGPU_GEM_CREATE_COMPRESSED_MASK)) {
+				initial_domain |= GSGPU_GEM_DOMAIN_GTT;
+				goto retry;
+			}
+			DRM_DEBUG("Failed to allocate GEM object (%ld, %d, %u, %d)\n",
+				  size, initial_domain, alignment, r);
+		}
+		return r;
+	}
+	*obj = &bo->gem_base;
+
+	return 0;
+}
+
+void gsgpu_gem_force_release(struct gsgpu_device *adev)
+{
+	struct drm_device *ddev = adev->ddev;
+	struct drm_file *file;
+
+	mutex_lock(&ddev->filelist_mutex);
+
+	list_for_each_entry(file, &ddev->filelist, lhead) {
+		struct drm_gem_object *gobj;
+		int handle;
+
+		WARN_ONCE(1, "Still active user space clients!\n");
+		spin_lock(&file->table_lock);
+		idr_for_each_entry(&file->object_idr, gobj, handle) {
+			WARN_ONCE(1, "And also active allocations!\n");
+			drm_gem_object_put_unlocked(gobj);
+		}
+		idr_destroy(&file->object_idr);
+		spin_unlock(&file->table_lock);
+	}
+
+	mutex_unlock(&ddev->filelist_mutex);
+}
+
+/*
+ * Call from drm_gem_handle_create which appear in both new and open ioctl
+ * case.
+ */
+int gsgpu_gem_object_open(struct drm_gem_object *obj,
+			   struct drm_file *file_priv)
+{
+	struct gsgpu_bo *abo = gem_to_gsgpu_bo(obj);
+	struct gsgpu_device *adev = gsgpu_ttm_adev(abo->tbo.bdev);
+	struct gsgpu_fpriv *fpriv = file_priv->driver_priv;
+	struct gsgpu_vm *vm = &fpriv->vm;
+	struct gsgpu_bo_va *bo_va;
+	struct mm_struct *mm;
+	int r;
+
+	mm = gsgpu_ttm_tt_get_usermm(abo->tbo.ttm);
+	if (mm && mm != current->mm)
+		return -EPERM;
+
+	if (abo->flags & GSGPU_GEM_CREATE_VM_ALWAYS_VALID &&
+	    abo->tbo.resv != vm->root.base.bo->tbo.resv)
+		return -EPERM;
+
+	r = gsgpu_bo_reserve(abo, false);
+	if (r)
+		return r;
+
+	bo_va = gsgpu_vm_bo_find(vm, abo);
+	if (!bo_va) {
+		bo_va = gsgpu_vm_bo_add(adev, vm, abo);
+	} else {
+		++bo_va->ref_count;
+	}
+	gsgpu_bo_unreserve(abo);
+	return 0;
+}
+
+void gsgpu_gem_object_close(struct drm_gem_object *obj,
+			     struct drm_file *file_priv)
+{
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(obj);
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	struct gsgpu_fpriv *fpriv = file_priv->driver_priv;
+	struct gsgpu_vm *vm = &fpriv->vm;
+
+	struct gsgpu_bo_list_entry vm_pd;
+	struct list_head list, duplicates;
+	struct ttm_validate_buffer tv;
+	struct ww_acquire_ctx ticket;
+	struct gsgpu_bo_va *bo_va;
+	int r;
+
+	INIT_LIST_HEAD(&list);
+	INIT_LIST_HEAD(&duplicates);
+
+	tv.bo = &bo->tbo;
+	tv.shared = true;
+	list_add(&tv.head, &list);
+
+	gsgpu_vm_get_pd_bo(vm, &list, &vm_pd);
+
+	r = ttm_eu_reserve_buffers(&ticket, &list, false, &duplicates);
+	if (r) {
+		dev_err(adev->dev, "leaking bo va because "
+			"we fail to reserve bo (%d)\n", r);
+		return;
+	}
+	bo_va = gsgpu_vm_bo_find(vm, bo);
+	if (bo_va && --bo_va->ref_count == 0) {
+		gsgpu_vm_bo_rmv(adev, bo_va);
+
+		if (gsgpu_vm_ready(vm)) {
+			struct dma_fence *fence = NULL;
+
+			r = gsgpu_vm_clear_freed(adev, vm, &fence);
+			if (unlikely(r)) {
+				dev_err(adev->dev, "failed to clear page "
+					"tables on GEM object close (%d)\n", r);
+			}
+
+			if (fence) {
+				gsgpu_bo_fence(bo, fence, true);
+				dma_fence_put(fence);
+			}
+		}
+	}
+	ttm_eu_backoff_reservation(&ticket, &list);
+}
+
+/*
+ * GEM ioctls.
+ */
+int gsgpu_gem_create_ioctl(struct drm_device *dev, void *data,
+			    struct drm_file *filp)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_fpriv *fpriv = filp->driver_priv;
+	struct gsgpu_vm *vm = &fpriv->vm;
+	union drm_gsgpu_gem_create *args = data;
+	uint64_t flags = args->in.domain_flags;
+	uint64_t size = args->in.bo_size;
+	struct reservation_object *resv = NULL;
+	struct drm_gem_object *gobj;
+	uint32_t handle;
+	int r;
+
+	/* reject invalid gem flags */
+	if (flags & ~(GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
+		      GSGPU_GEM_CREATE_NO_CPU_ACCESS |
+		      GSGPU_GEM_CREATE_CPU_GTT_USWC |
+		      GSGPU_GEM_CREATE_VRAM_CLEARED |
+		      GSGPU_GEM_CREATE_VM_ALWAYS_VALID |
+		      GSGPU_GEM_CREATE_EXPLICIT_SYNC |
+		      GSGPU_GEM_CREATE_COMPRESSED_MASK))
+
+		return -EINVAL;
+
+	/* reject invalid gem domains */
+	if (args->in.domains & ~GSGPU_GEM_DOMAIN_MASK)
+		return -EINVAL;
+
+	/* reject invalid gem compressed_mode */
+	if (flags & GSGPU_GEM_CREATE_COMPRESSED_MASK && (
+			args->in.domains != GSGPU_GEM_DOMAIN_VRAM
+			|| !adev->zip_meta.ready))
+		return -EINVAL;
+
+	size = roundup(size, PAGE_SIZE);
+
+	if (flags & GSGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+		r = gsgpu_bo_reserve(vm->root.base.bo, false);
+		if (r)
+			return r;
+
+		resv = vm->root.base.bo->tbo.resv;
+	}
+
+	r = gsgpu_gem_object_create(adev, size, args->in.alignment,
+				     (u32)(0xffffffff & args->in.domains),
+				     flags, ttm_bo_type_device, resv, &gobj);
+	if (flags & GSGPU_GEM_CREATE_VM_ALWAYS_VALID) {
+		if (!r) {
+			struct gsgpu_bo *abo = gem_to_gsgpu_bo(gobj);
+
+			abo->parent = gsgpu_bo_ref(vm->root.base.bo);
+		}
+		gsgpu_bo_unreserve(vm->root.base.bo);
+	}
+	if (r)
+		return r;
+
+	r = drm_gem_handle_create(filp, gobj, &handle);
+	/* drop reference from allocate - handle holds it now */
+	drm_gem_object_put_unlocked(gobj);
+	if (r)
+		return r;
+
+	memset(args, 0, sizeof(*args));
+	args->out.handle = handle;
+	return 0;
+}
+
+int gsgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
+			     struct drm_file *filp)
+{
+	struct ttm_operation_ctx ctx = { true, false };
+	struct gsgpu_device *adev = dev->dev_private;
+	struct drm_gsgpu_gem_userptr *args = data;
+	struct drm_gem_object *gobj;
+	struct gsgpu_bo *bo;
+	uint32_t handle;
+	int r;
+
+	if (offset_in_page(args->addr | args->size))
+		return -EINVAL;
+
+	/* reject unknown flag values */
+	if (args->flags & ~(GSGPU_GEM_USERPTR_READONLY |
+	    GSGPU_GEM_USERPTR_ANONONLY | GSGPU_GEM_USERPTR_VALIDATE |
+	    GSGPU_GEM_USERPTR_REGISTER))
+		return -EINVAL;
+
+	if (!(args->flags & GSGPU_GEM_USERPTR_READONLY) &&
+	     !(args->flags & GSGPU_GEM_USERPTR_REGISTER)) {
+
+		/* if we want to write to it we must install a MMU notifier */
+		return -EACCES;
+	}
+
+	/* create a gem object to contain this object in */
+	r = gsgpu_gem_object_create(adev, args->size, 0, GSGPU_GEM_DOMAIN_CPU,
+				     0, ttm_bo_type_device, NULL, &gobj);
+	if (r)
+		return r;
+
+	bo = gem_to_gsgpu_bo(gobj);
+	bo->preferred_domains = GSGPU_GEM_DOMAIN_GTT;
+	bo->allowed_domains = GSGPU_GEM_DOMAIN_GTT;
+	r = gsgpu_ttm_tt_set_userptr(bo->tbo.ttm, args->addr, args->flags);
+	if (r)
+		goto release_object;
+
+	if (args->flags & GSGPU_GEM_USERPTR_REGISTER) {
+		r = gsgpu_mn_register(bo, args->addr);
+		if (r)
+			goto release_object;
+	}
+
+	if (args->flags & GSGPU_GEM_USERPTR_VALIDATE) {
+		r = gsgpu_ttm_tt_get_user_pages(bo->tbo.ttm,
+						 bo->tbo.ttm->pages);
+		if (r)
+			goto release_object;
+
+		r = gsgpu_bo_reserve(bo, true);
+		if (r)
+			goto free_pages;
+
+		gsgpu_bo_placement_from_domain(bo, GSGPU_GEM_DOMAIN_GTT);
+		r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+		gsgpu_bo_unreserve(bo);
+		if (r)
+			goto free_pages;
+	}
+
+	r = drm_gem_handle_create(filp, gobj, &handle);
+	/* drop reference from allocate - handle holds it now */
+	drm_gem_object_put_unlocked(gobj);
+	if (r)
+		return r;
+
+	args->handle = handle;
+	return 0;
+
+free_pages:
+	release_pages(bo->tbo.ttm->pages, bo->tbo.ttm->num_pages);
+
+release_object:
+	drm_gem_object_put_unlocked(gobj);
+
+	return r;
+}
+
+int gsgpu_mode_dumb_mmap(struct drm_file *filp,
+			  struct drm_device *dev,
+			  uint32_t handle, uint64_t *offset_p)
+{
+	struct drm_gem_object *gobj;
+	struct gsgpu_bo *robj;
+
+	gobj = drm_gem_object_lookup(filp, handle);
+	if (gobj == NULL) {
+		return -ENOENT;
+	}
+	robj = gem_to_gsgpu_bo(gobj);
+	if (gsgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||
+	    (robj->flags & GSGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		drm_gem_object_put_unlocked(gobj);
+		return -EPERM;
+	}
+	*offset_p = gsgpu_bo_mmap_offset(robj);
+	drm_gem_object_put_unlocked(gobj);
+	return 0;
+}
+
+int gsgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,
+			  struct drm_file *filp)
+{
+	union drm_gsgpu_gem_mmap *args = data;
+	uint32_t handle = args->in.handle;
+	memset(args, 0, sizeof(*args));
+	return gsgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);
+}
+
+/**
+ * gsgpu_gem_timeout - calculate jiffies timeout from absolute value
+ *
+ * @timeout_ns: timeout in ns
+ *
+ * Calculate the timeout in jiffies from an absolute timeout in ns.
+ */
+unsigned long gsgpu_gem_timeout(uint64_t timeout_ns)
+{
+	unsigned long timeout_jiffies;
+	ktime_t timeout;
+
+	/* clamp timeout if it's to large */
+	if (((int64_t)timeout_ns) < 0)
+		return MAX_SCHEDULE_TIMEOUT;
+
+	timeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());
+	if (ktime_to_ns(timeout) < 0)
+		return 0;
+
+	timeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));
+	/*  clamp timeout to avoid unsigned-> signed overflow */
+	if (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)
+		return MAX_SCHEDULE_TIMEOUT - 1;
+
+	return timeout_jiffies;
+}
+
+int gsgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
+			      struct drm_file *filp)
+{
+	union drm_gsgpu_gem_wait_idle *args = data;
+	struct drm_gem_object *gobj;
+	struct gsgpu_bo *robj;
+	uint32_t handle = args->in.handle;
+	unsigned long timeout = gsgpu_gem_timeout(args->in.timeout);
+	int r = 0;
+	long ret;
+
+	gobj = drm_gem_object_lookup(filp, handle);
+	if (gobj == NULL) {
+		return -ENOENT;
+	}
+	robj = gem_to_gsgpu_bo(gobj);
+	ret = reservation_object_wait_timeout_rcu(robj->tbo.resv, true, true,
+						  timeout);
+
+	/* ret == 0 means not signaled,
+	 * ret > 0 means signaled
+	 * ret < 0 means interrupted before timeout
+	 */
+	if (ret >= 0) {
+		memset(args, 0, sizeof(*args));
+		args->out.status = (ret == 0);
+	} else
+		r = ret;
+
+	drm_gem_object_put_unlocked(gobj);
+	return r;
+}
+
+int gsgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *filp)
+{
+	struct drm_gsgpu_gem_metadata *args = data;
+	struct drm_gem_object *gobj;
+	struct gsgpu_bo *robj;
+	int r = -1;
+
+	DRM_DEBUG("%d \n", args->handle);
+	gobj = drm_gem_object_lookup(filp, args->handle);
+	if (gobj == NULL)
+		return -ENOENT;
+	robj = gem_to_gsgpu_bo(gobj);
+
+	r = gsgpu_bo_reserve(robj, false);
+	if (unlikely(r != 0))
+		goto out;
+
+	if (args->op == GSGPU_GEM_METADATA_OP_GET_METADATA) {
+		gsgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);
+		r = gsgpu_bo_get_metadata(robj, args->data.data,
+					   sizeof(args->data.data),
+					   &args->data.data_size_bytes,
+					   &args->data.flags);
+	} else if (args->op == GSGPU_GEM_METADATA_OP_SET_METADATA) {
+		if (args->data.data_size_bytes > sizeof(args->data.data)) {
+			r = -EINVAL;
+			goto unreserve;
+		}
+		r = gsgpu_bo_set_tiling_flags(robj, args->data.tiling_info);
+		if (!r)
+			r = gsgpu_bo_set_metadata(robj, args->data.data,
+						   args->data.data_size_bytes,
+						   args->data.flags);
+	}
+
+unreserve:
+	gsgpu_bo_unreserve(robj);
+out:
+	drm_gem_object_put_unlocked(gobj);
+	return r;
+}
+
+/**
+ * gsgpu_gem_va_update_vm -update the bo_va in its VM
+ *
+ * @adev: gsgpu_device pointer
+ * @vm: vm to update
+ * @bo_va: bo_va to update
+ * @operation: map, unmap or clear
+ *
+ * Update the bo_va directly after setting its address. Errors are not
+ * vital here, so they are not reported back to userspace.
+ */
+static void gsgpu_gem_va_update_vm(struct gsgpu_device *adev,
+				    struct gsgpu_vm *vm,
+				    struct gsgpu_bo_va *bo_va,
+				    uint32_t operation)
+{
+	int r;
+
+	if (!gsgpu_vm_ready(vm))
+		return;
+
+	r = gsgpu_vm_clear_freed(adev, vm, NULL);
+	if (r)
+		goto error;
+
+	if (operation == GSGPU_VA_OP_MAP ||
+	    operation == GSGPU_VA_OP_REPLACE) {
+		r = gsgpu_vm_bo_update(adev, bo_va, false);
+		if (r)
+			goto error;
+	}
+
+	r = gsgpu_vm_update_directories(adev, vm);
+
+error:
+	if (r && r != -ERESTARTSYS)
+		DRM_ERROR("Couldn't update BO_VA (%d)\n", r);
+}
+
+int gsgpu_gem_va_ioctl(struct drm_device *dev, void *data,
+			  struct drm_file *filp)
+{
+	const uint32_t valid_flags = GSGPU_VM_DELAY_UPDATE |
+		GSGPU_VM_PAGE_READABLE | GSGPU_VM_PAGE_WRITEABLE |
+		GSGPU_VM_PAGE_EXECUTABLE | GSGPU_VM_MTYPE_MASK;
+	const uint32_t prt_flags = GSGPU_VM_DELAY_UPDATE |
+		GSGPU_VM_PAGE_PRT;
+
+	struct drm_gsgpu_gem_va *args = data;
+	struct drm_gem_object *gobj;
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_fpriv *fpriv = filp->driver_priv;
+	struct gsgpu_bo *abo;
+	struct gsgpu_bo_va *bo_va;
+	struct gsgpu_bo_list_entry vm_pd;
+	struct ttm_validate_buffer tv;
+	struct ww_acquire_ctx ticket;
+	struct list_head list, duplicates;
+	uint64_t va_flags;
+	int r = 0;
+
+	if (args->va_address < GSGPU_VA_RESERVED_SIZE) {
+		dev_dbg(&dev->pdev->dev,
+			"va_address 0x%LX is in reserved area 0x%LX\n",
+			args->va_address, GSGPU_VA_RESERVED_SIZE);
+		return -EINVAL;
+	}
+
+	if (args->va_address >= GSGPU_VA_HOLE_START &&
+	    args->va_address < GSGPU_VA_HOLE_END) {
+		dev_dbg(&dev->pdev->dev,
+			"va_address 0x%LX is in VA hole 0x%LX-0x%LX\n",
+			args->va_address, GSGPU_VA_HOLE_START,
+			GSGPU_VA_HOLE_END);
+		return -EINVAL;
+	}
+
+	args->va_address &= GSGPU_VA_HOLE_MASK;
+
+	if ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {
+		dev_dbg(&dev->pdev->dev, "invalid flags combination 0x%08X\n",
+			args->flags);
+		return -EINVAL;
+	}
+
+	switch (args->operation) {
+	case GSGPU_VA_OP_MAP:
+	case GSGPU_VA_OP_UNMAP:
+	case GSGPU_VA_OP_CLEAR:
+	case GSGPU_VA_OP_REPLACE:
+		break;
+	default:
+		dev_dbg(&dev->pdev->dev, "unsupported operation %d\n",
+			args->operation);
+		return -EINVAL;
+	}
+
+	INIT_LIST_HEAD(&list);
+	INIT_LIST_HEAD(&duplicates);
+	if ((args->operation != GSGPU_VA_OP_CLEAR) &&
+	    !(args->flags & GSGPU_VM_PAGE_PRT)) {
+		gobj = drm_gem_object_lookup(filp, args->handle);
+		if (gobj == NULL)
+			return -ENOENT;
+		abo = gem_to_gsgpu_bo(gobj);
+		tv.bo = &abo->tbo;
+		tv.shared = !!(abo->flags & GSGPU_GEM_CREATE_VM_ALWAYS_VALID);
+		list_add(&tv.head, &list);
+	} else {
+		gobj = NULL;
+		abo = NULL;
+	}
+
+	gsgpu_vm_get_pd_bo(&fpriv->vm, &list, &vm_pd);
+
+	r = ttm_eu_reserve_buffers(&ticket, &list, true, &duplicates);
+	if (r)
+		goto error_unref;
+
+	if (abo) {
+		bo_va = gsgpu_vm_bo_find(&fpriv->vm, abo);
+		if (!bo_va) {
+			r = -ENOENT;
+			goto error_backoff;
+		}
+	} else if (args->operation != GSGPU_VA_OP_CLEAR) {
+		bo_va = fpriv->prt_va;
+	} else {
+		bo_va = NULL;
+	}
+
+	switch (args->operation) {
+	case GSGPU_VA_OP_MAP:
+		r = gsgpu_vm_alloc_pts(adev, bo_va->base.vm, args->va_address,
+					args->map_size);
+		if (r)
+			goto error_backoff;
+
+		va_flags = gsgpu_gmc_get_pte_flags(adev, args->flags);
+		r = gsgpu_vm_bo_map(adev, bo_va, args->va_address,
+				     args->offset_in_bo, args->map_size,
+				     va_flags);
+		break;
+	case GSGPU_VA_OP_UNMAP:
+		r = gsgpu_vm_bo_unmap(adev, bo_va, args->va_address);
+		break;
+
+	case GSGPU_VA_OP_CLEAR:
+		r = gsgpu_vm_bo_clear_mappings(adev, &fpriv->vm,
+						args->va_address,
+						args->map_size);
+		break;
+	case GSGPU_VA_OP_REPLACE:
+		r = gsgpu_vm_alloc_pts(adev, bo_va->base.vm, args->va_address,
+					args->map_size);
+		if (r)
+			goto error_backoff;
+
+		va_flags = gsgpu_gmc_get_pte_flags(adev, args->flags);
+		r = gsgpu_vm_bo_replace_map(adev, bo_va, args->va_address,
+					     args->offset_in_bo, args->map_size,
+					     va_flags);
+		break;
+	default:
+		break;
+	}
+	if (!r && !(args->flags & GSGPU_VM_DELAY_UPDATE) && !gsgpu_vm_debug)
+		gsgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,
+					args->operation);
+
+error_backoff:
+	ttm_eu_backoff_reservation(&ticket, &list);
+
+error_unref:
+	drm_gem_object_put_unlocked(gobj);
+	return r;
+}
+
+int gsgpu_gem_op_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *filp)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	struct drm_gsgpu_gem_op *args = data;
+	struct drm_gem_object *gobj;
+	struct gsgpu_bo *robj;
+	int r;
+
+	gobj = drm_gem_object_lookup(filp, args->handle);
+	if (gobj == NULL) {
+		return -ENOENT;
+	}
+	robj = gem_to_gsgpu_bo(gobj);
+
+	r = gsgpu_bo_reserve(robj, false);
+	if (unlikely(r))
+		goto out;
+
+	switch (args->op) {
+	case GSGPU_GEM_OP_GET_GEM_CREATE_INFO: {
+		struct drm_gsgpu_gem_create_in info;
+		void __user *out = u64_to_user_ptr(args->value);
+
+		info.bo_size = robj->gem_base.size;
+		info.alignment = robj->tbo.mem.page_alignment << PAGE_SHIFT;
+		info.domains = robj->preferred_domains;
+		info.domain_flags = robj->flags;
+		gsgpu_bo_unreserve(robj);
+		if (copy_to_user(out, &info, sizeof(info)))
+			r = -EFAULT;
+		break;
+	}
+	case GSGPU_GEM_OP_SET_PLACEMENT:
+		if (robj->prime_shared_count && (args->value & GSGPU_GEM_DOMAIN_VRAM)) {
+			r = -EINVAL;
+			gsgpu_bo_unreserve(robj);
+			break;
+		}
+		if (gsgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {
+			r = -EPERM;
+			gsgpu_bo_unreserve(robj);
+			break;
+		}
+		robj->preferred_domains = args->value & (GSGPU_GEM_DOMAIN_VRAM |
+							GSGPU_GEM_DOMAIN_GTT |
+							GSGPU_GEM_DOMAIN_CPU);
+		robj->allowed_domains = robj->preferred_domains;
+		if (robj->allowed_domains == GSGPU_GEM_DOMAIN_VRAM)
+			robj->allowed_domains |= GSGPU_GEM_DOMAIN_GTT;
+
+		if (robj->flags & GSGPU_GEM_CREATE_VM_ALWAYS_VALID)
+			gsgpu_vm_bo_invalidate(adev, robj, true);
+
+		gsgpu_bo_unreserve(robj);
+		break;
+	default:
+		gsgpu_bo_unreserve(robj);
+		r = -EINVAL;
+	}
+
+out:
+	drm_gem_object_put_unlocked(gobj);
+	return r;
+}
+
+int gsgpu_mode_dumb_create(struct drm_file *file_priv,
+			    struct drm_device *dev,
+			    struct drm_mode_create_dumb *args)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	struct drm_gem_object *gobj;
+	uint32_t handle;
+	u32 domain;
+	int r;
+
+	args->pitch = gsgpu_align_pitch(adev, args->width,
+					 DIV_ROUND_UP(args->bpp, 8), 0);
+	args->size = (u64)args->pitch * args->height;
+	args->size = ALIGN(args->size, PAGE_SIZE);
+	domain = gsgpu_bo_get_preferred_pin_domain(adev,
+				gsgpu_display_supported_domains(adev));
+	r = gsgpu_gem_object_create(adev, args->size, 0, domain,
+				     GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
+				     GSGPU_GEM_CREATE_VRAM_CONTIGUOUS |
+				     GSGPU_GEM_CREATE_VRAM_CLEARED,
+				     ttm_bo_type_device, NULL, &gobj);
+	if (r)
+		return -ENOMEM;
+
+	r = drm_gem_handle_create(file_priv, gobj, &handle);
+	/* drop reference from allocate - handle holds it now */
+	drm_gem_object_put_unlocked(gobj);
+	if (r) {
+		return r;
+	}
+	args->handle = handle;
+	return 0;
+}
+
+#if defined(CONFIG_DEBUG_FS)
+
+#define gsgpu_debugfs_gem_bo_print_flag(m, bo, flag) do {	\
+	if (bo->flags & (GSGPU_GEM_CREATE_ ## flag)) {	\
+		seq_printf((m), " " #flag);		\
+	}	\
+} while (0)
+
+static int gsgpu_debugfs_gem_bo_info(int id, void *ptr, void *data)
+{
+	struct drm_gem_object *gobj = ptr;
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(gobj);
+	struct seq_file *m = data;
+
+	struct dma_buf_attachment *attachment;
+	struct dma_buf *dma_buf;
+	unsigned domain;
+	const char *placement;
+	unsigned pin_count;
+
+	domain = gsgpu_mem_type_to_domain(bo->tbo.mem.mem_type);
+	switch (domain) {
+	case GSGPU_GEM_DOMAIN_VRAM:
+		placement = "VRAM";
+		break;
+	case GSGPU_GEM_DOMAIN_GTT:
+		placement = " GTT";
+		break;
+	case GSGPU_GEM_DOMAIN_CPU:
+	default:
+		placement = " CPU";
+		break;
+	}
+	seq_printf(m, "\t0x%08x: %12ld byte %s",
+		   id, gsgpu_bo_size(bo), placement);
+
+	pin_count = READ_ONCE(bo->pin_count);
+	if (pin_count)
+		seq_printf(m, " pin count %d", pin_count);
+
+	dma_buf = READ_ONCE(bo->gem_base.dma_buf);
+	attachment = READ_ONCE(bo->gem_base.import_attach);
+
+	if (attachment)
+		seq_printf(m, " imported from %p", dma_buf);
+	else if (dma_buf)
+		seq_printf(m, " exported as %p", dma_buf);
+
+	gsgpu_debugfs_gem_bo_print_flag(m, bo, CPU_ACCESS_REQUIRED);
+	gsgpu_debugfs_gem_bo_print_flag(m, bo, NO_CPU_ACCESS);
+	gsgpu_debugfs_gem_bo_print_flag(m, bo, CPU_GTT_USWC);
+	gsgpu_debugfs_gem_bo_print_flag(m, bo, VRAM_CLEARED);
+	gsgpu_debugfs_gem_bo_print_flag(m, bo, SHADOW);
+	gsgpu_debugfs_gem_bo_print_flag(m, bo, VRAM_CONTIGUOUS);
+	gsgpu_debugfs_gem_bo_print_flag(m, bo, VM_ALWAYS_VALID);
+	gsgpu_debugfs_gem_bo_print_flag(m, bo, EXPLICIT_SYNC);
+
+	seq_printf(m, "\n");
+
+	return 0;
+}
+
+static int gsgpu_debugfs_gem_info(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *)m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct drm_file *file;
+	int r;
+
+	r = mutex_lock_interruptible(&dev->filelist_mutex);
+	if (r)
+		return r;
+
+	list_for_each_entry(file, &dev->filelist, lhead) {
+		struct task_struct *task;
+
+		/*
+		 * Although we have a valid reference on file->pid, that does
+		 * not guarantee that the task_struct who called get_pid() is
+		 * still alive (e.g. get_pid(current) => fork() => exit()).
+		 * Therefore, we need to protect this ->comm access using RCU.
+		 */
+		rcu_read_lock();
+		task = pid_task(file->pid, PIDTYPE_PID);
+		seq_printf(m, "pid %8d command %s:\n", pid_nr(file->pid),
+			   task ? task->comm : "<unknown>");
+		rcu_read_unlock();
+
+		spin_lock(&file->table_lock);
+		idr_for_each(&file->object_idr, gsgpu_debugfs_gem_bo_info, m);
+		spin_unlock(&file->table_lock);
+	}
+
+	mutex_unlock(&dev->filelist_mutex);
+	return 0;
+}
+
+static const struct drm_info_list gsgpu_debugfs_gem_list[] = {
+	{"gsgpu_gem_info", &gsgpu_debugfs_gem_info, 0, NULL},
+};
+#endif
+
+int gsgpu_debugfs_gem_init(struct gsgpu_device *adev)
+{
+#if defined(CONFIG_DEBUG_FS)
+	return gsgpu_debugfs_add_files(adev, gsgpu_debugfs_gem_list, 1);
+#endif
+	return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_gfx.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gfx.c
new file mode 100644
index 000000000000..b2802c28de04
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gfx.c
@@ -0,0 +1,695 @@
+#include <linux/kernel.h>
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include "gsgpu_common.h"
+#include "gsgpu_cp.h"
+#include "gsgpu_irq.h"
+
+#define GFX8_NUM_GFX_RINGS     1
+
+MODULE_FIRMWARE("loongson/lg100_cp.bin");
+
+static void gfx_set_ring_funcs(struct gsgpu_device *adev);
+static void gfx_set_irq_funcs(struct gsgpu_device *adev);
+
+static int gfx_ring_test_ring(struct gsgpu_ring *ring)
+{
+	struct gsgpu_device *adev = ring->adev;
+	unsigned i;
+	unsigned index;
+	int r;
+	u32 tmp;
+	u64 gpu_addr;
+
+	r = gsgpu_device_wb_get(adev, &index);
+	if (r) {
+		dev_err(adev->dev, "(%d) failed to allocate wb slot\n", r);
+		return r;
+	}
+
+	gpu_addr = adev->wb.gpu_addr + (index * 4);
+	tmp = 0xCAFEDEAD;
+	adev->wb.wb[index] = cpu_to_le32(tmp);
+
+	r = gsgpu_ring_alloc(ring, 4);
+	if (r) {
+		DRM_ERROR("gsgpu: dma failed to lock ring %d (%d).\n", ring->idx, r);
+		gsgpu_device_wb_free(adev, index);
+		return r;
+	}
+
+	gsgpu_ring_write(ring, GSPKT(GSPKT_WRITE, 3) | WRITE_DST_SEL(1) | WRITE_WAIT);
+	gsgpu_ring_write(ring, lower_32_bits(gpu_addr));
+	gsgpu_ring_write(ring, upper_32_bits(gpu_addr));
+	gsgpu_ring_write(ring, 0xDEADBEEF);
+	gsgpu_ring_commit(ring);
+
+	for (i = 0; i < adev->usec_timeout; i++) {
+		tmp = le32_to_cpu(adev->wb.wb[index]);
+		if (tmp == 0xDEADBEEF)
+			break;
+		DRM_UDELAY(1);
+	}
+
+	if (i < adev->usec_timeout) {
+		//DRM_DEBUG("ring test on %d succeeded in %d usecs\n", ring->idx, i);
+		DRM_INFO("ring test on %d succeeded in %d usecs\n", ring->idx, i);
+	} else {
+		DRM_ERROR("gsgpu: ring %d test failed (0x%08X)\n",
+			  ring->idx, tmp);
+		r = -EINVAL;
+	}
+	gsgpu_device_wb_free(adev, index);
+
+	return r;
+}
+
+static int gfx_ring_test_ib(struct gsgpu_ring *ring, long timeout)
+{
+	struct gsgpu_device *adev = ring->adev;
+	struct gsgpu_ib ib;
+	struct dma_fence *f = NULL;
+
+	unsigned int index;
+	uint64_t gpu_addr;
+	uint32_t tmp;
+	long r;
+
+	r = gsgpu_device_wb_get(adev, &index);
+	if (r) {
+		dev_err(adev->dev, "(%ld) failed to allocate wb slot\n", r);
+		return r;
+	}
+
+	gpu_addr = adev->wb.gpu_addr + (index * 4);
+	adev->wb.wb[index] = cpu_to_le32(0xCAFEDEAD);
+	memset(&ib, 0, sizeof(ib));
+	r = gsgpu_ib_get(adev, NULL, 16, &ib);
+	if (r) {
+		DRM_ERROR("gsgpu: failed to get ib (%ld).\n", r);
+		goto err1;
+	}
+	ib.ptr[0] = GSPKT(GSPKT_WRITE, 3) | WRITE_DST_SEL(1) | WRITE_WAIT;
+	ib.ptr[1] = lower_32_bits(gpu_addr);
+	ib.ptr[2] = upper_32_bits(gpu_addr);
+	ib.ptr[3] = 0xDEADBEEF;
+	ib.length_dw = 4;
+
+	r = gsgpu_ib_schedule(ring, 1, &ib, NULL, &f);
+	if (r)
+		goto err2;
+
+	r = dma_fence_wait_timeout(f, false, timeout);
+	if (r == 0) {
+		DRM_ERROR("gsgpu: IB test timed out.\n");
+		r = -ETIMEDOUT;
+		goto err2;
+	} else if (r < 0) {
+		DRM_ERROR("gsgpu: fence wait failed (%ld).\n", r);
+		goto err2;
+	}
+
+	tmp = adev->wb.wb[index];
+	if (tmp == 0xDEADBEEF) {
+		DRM_DEBUG("ib test on ring %d succeeded\n", ring->idx);
+		r = 0;
+	} else {
+		DRM_ERROR("ib test on ring %d failed\n", ring->idx);
+		r = -EINVAL;
+	}
+
+err2:
+	gsgpu_ib_free(adev, &ib, NULL);
+	dma_fence_put(f);
+err1:
+	gsgpu_device_wb_free(adev, index);
+	return r;
+}
+
+static int gfx_gpu_early_init(struct gsgpu_device *adev)
+{
+	u32 gb_addr_config;
+	u32 tmp;
+
+	adev->gfx.config.max_shader_engines = 2;
+	adev->gfx.config.max_tile_pipes = 4;
+	adev->gfx.config.max_cu_per_sh = 2;
+	adev->gfx.config.max_sh_per_se = 1;
+	adev->gfx.config.max_backends_per_se = 2;
+	adev->gfx.config.max_texture_channel_caches = 4;
+	adev->gfx.config.max_gprs = 256;
+	adev->gfx.config.max_gs_threads = 32;
+	adev->gfx.config.max_hw_contexts = 8;
+
+	adev->gfx.config.sc_prim_fifo_size_frontend = 0x20;
+	adev->gfx.config.sc_prim_fifo_size_backend = 0x100;
+	adev->gfx.config.sc_hiz_tile_fifo_size = 0x30;
+	adev->gfx.config.sc_earlyz_tile_fifo_size = 0x130;
+
+	adev->gfx.config.mc_arb_ramcfg = 0;//RREG32(mmMC_ARB_RAMCFG);
+
+	adev->gfx.config.num_tile_pipes = adev->gfx.config.max_tile_pipes;
+	adev->gfx.config.mem_max_burst_length_bytes = 256;
+
+	tmp = 0; //REG_GET_FIELD(mc_arb_ramcfg, MC_ARB_RAMCFG, NOOFCOLS);
+	adev->gfx.config.mem_row_size_in_kb = (4 * (1 << (8 + tmp))) / 1024;
+	if (adev->gfx.config.mem_row_size_in_kb > 4)
+		adev->gfx.config.mem_row_size_in_kb = 4;
+
+	adev->gfx.config.shader_engine_tile_size = 32;
+	adev->gfx.config.num_gpus = 1;
+	adev->gfx.config.multi_gpu_tile_size = 64;
+
+	/* fix up row size */
+	switch (adev->gfx.config.mem_row_size_in_kb) {
+	case 1:
+	default:
+		gb_addr_config = 0;//REG_SET_FIELD(gb_addr_config, GB_ADDR_CONFIG, ROW_SIZE, 0);
+		break;
+	case 2:
+		gb_addr_config = 0;//REG_SET_FIELD(gb_addr_config, GB_ADDR_CONFIG, ROW_SIZE, 1);
+		break;
+	case 4:
+		gb_addr_config = 0;//REG_SET_FIELD(gb_addr_config, GB_ADDR_CONFIG, ROW_SIZE, 2);
+		break;
+	}
+	adev->gfx.config.gb_addr_config = gb_addr_config;
+
+	return 0;
+}
+
+static int gfx_sw_init(void *handle)
+{
+	int i, r;
+	struct gsgpu_ring *ring;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	/* EOP Event */
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_CP_END_OF_PIPE, &adev->gfx.eop_irq);
+	if (r)
+		return r;
+
+	/* Privileged reg */
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_CP_PRIV_REG_FAULT,
+			      &adev->gfx.priv_reg_irq);
+	if (r)
+		return r;
+
+	/* Privileged inst */
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_CP_PRIV_INSTR_FAULT,
+			      &adev->gfx.priv_inst_irq);
+	if (r)
+		return r;
+
+	adev->gfx.gfx_current_status = GSGPU_GFX_NORMAL_MODE;
+
+	//gfx_scratch_init(adev);
+
+	/* set up the gfx ring */
+	for (i = 0; i < adev->gfx.num_gfx_rings; i++) {
+		ring = &adev->gfx.gfx_ring[i];
+		ring->ring_obj = NULL;
+		sprintf(ring->name, "gfx");
+
+		r = gsgpu_ring_init(adev, ring, 256, &adev->gfx.eop_irq,
+				     GSGPU_CP_IRQ_GFX_EOP);
+		if (r)
+			return r;
+	}
+
+	adev->gfx.ce_ram_size = 0x8000;
+
+	r = gfx_gpu_early_init(adev);
+	if (r)
+		return r;
+
+	return 0;
+}
+
+static int gfx_sw_fini(void *handle)
+{
+	int i;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	for (i = 0; i < adev->gfx.num_gfx_rings; i++)
+		gsgpu_ring_fini(&adev->gfx.gfx_ring[i]);
+
+	return 0;
+}
+
+static void gfx_parse_ind_reg_list(int *register_list_format,
+				int ind_offset,
+				int list_size,
+				int *unique_indices,
+				int *indices_count,
+				int max_indices,
+				int *ind_start_offsets,
+				int *offset_count,
+				int max_offset)
+{
+	int indices;
+	bool new_entry = true;
+
+	for (; ind_offset < list_size; ind_offset++) {
+
+		if (new_entry) {
+			new_entry = false;
+			ind_start_offsets[*offset_count] = ind_offset;
+			*offset_count = *offset_count + 1;
+			BUG_ON(*offset_count >= max_offset);
+		}
+
+		if (register_list_format[ind_offset] == 0xFFFFFFFF) {
+			new_entry = true;
+			continue;
+		}
+
+		ind_offset += 2;
+
+		/* look for the matching indice */
+		for (indices = 0;
+			indices < *indices_count;
+			indices++) {
+			if (unique_indices[indices] ==
+				register_list_format[ind_offset])
+				break;
+		}
+
+		if (indices >= *indices_count) {
+			unique_indices[*indices_count] =
+				register_list_format[ind_offset];
+			indices = *indices_count;
+			*indices_count = *indices_count + 1;
+			BUG_ON(*indices_count >= max_indices);
+		}
+
+		register_list_format[ind_offset] = indices;
+	}
+}
+
+static int gfx_cp_gfx_resume(struct gsgpu_device *adev)
+{
+	struct gsgpu_ring *ring;
+	u64 cb_addr;//, rptr_addr, wptr_gpu_addr;
+	int r = 0;
+
+	/*Flush pipeline*/
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_PIPE, GSCMD_PIPE_FLUSH), 1, ~1);
+	/* Wait a little for things to flush pipeline */
+	mdelay(1000);
+
+	/* Set ring buffer size */
+	ring = &adev->gfx.gfx_ring[0];
+
+	/* Initialize the ring buffer's read and write pointers */
+	ring->wptr = 0;
+	WREG32(GSGPU_GFX_CB_WPTR_OFFSET, lower_32_bits(ring->wptr));
+
+	/* set the RPTR */
+	WREG32(GSGPU_GFX_CB_RPTR_OFFSET, 0);
+
+	mdelay(1);
+
+	cb_addr = ring->gpu_addr;
+	WREG32(GSGPU_GFX_CB_BASE_LO_OFFSET, cb_addr);
+	WREG32(GSGPU_GFX_CB_BASE_HI_OFFSET, upper_32_bits(cb_addr));
+
+	/* start the ring */
+	gsgpu_ring_clear_ring(ring);
+
+	ring->ready = true;
+
+	return r;
+}
+
+static int gfx_cp_resume(struct gsgpu_device *adev)
+{
+	int r;
+
+	r = gfx_cp_gfx_resume(adev);
+	if (r)
+		return r;
+
+	return 0;
+}
+
+static int gfx_hw_init(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = gfx_cp_resume(adev);
+
+	return r;
+}
+
+static int gfx_hw_fini(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	gsgpu_irq_put(adev, &adev->gfx.priv_reg_irq, 0);
+	gsgpu_irq_put(adev, &adev->gfx.priv_inst_irq, 0);
+
+	return 0;
+}
+
+static int gfx_suspend(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+	adev->gfx.in_suspend = true;
+	return gfx_hw_fini(adev);
+}
+
+static int gfx_resume(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = gfx_hw_init(adev);
+	adev->gfx.in_suspend = false;
+	return r;
+}
+
+static bool gfx_is_idle(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	return (RREG32(GSGPU_STATUS) == GSCMD_STS_DONE);
+}
+
+static int gfx_wait_for_idle(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	if (gsgpu_cp_wait_done(adev) == true)
+			return 0;
+
+	return -ETIMEDOUT;
+}
+
+/**
+ * gfx_get_gpu_clock_counter - return GPU clock counter snapshot
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Fetches a GPU clock counter snapshot.
+ * Returns the 64 bit clock counter snapshot.
+ */
+static uint64_t gfx_get_gpu_clock_counter(struct gsgpu_device *adev)
+{
+	uint64_t clock = 0;
+
+	//TODO
+	mutex_lock(&adev->gfx.gpu_clock_mutex);
+
+	DRM_DEBUG("%s Not impelet\n", __func__);
+
+	mutex_unlock(&adev->gfx.gpu_clock_mutex);
+	return clock;
+}
+
+static const struct gsgpu_gfx_funcs gfx_gfx_funcs = {
+	.get_gpu_clock_counter = &gfx_get_gpu_clock_counter,
+	.read_wave_data = NULL,//&gfx_read_wave_data,
+	.read_wave_sgprs = NULL,//&gfx_read_wave_sgprs,
+	.select_me_pipe_q = NULL//&gfx_select_me_pipe_q
+};
+
+static int gfx_early_init(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	adev->gfx.num_gfx_rings = GFX8_NUM_GFX_RINGS;
+	adev->gfx.funcs = &gfx_gfx_funcs;
+	gfx_set_ring_funcs(adev);
+	gfx_set_irq_funcs(adev);
+
+	return 0;
+}
+
+static int gfx_late_init(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+	int r;
+
+	r = gsgpu_irq_get(adev, &adev->gfx.priv_reg_irq, 0);
+	if (r)
+		return r;
+
+	r = gsgpu_irq_get(adev, &adev->gfx.priv_inst_irq, 0);
+	if (r)
+		return r;
+
+	return 0;
+}
+
+static u64 gfx_ring_get_rptr(struct gsgpu_ring *ring)
+{
+	return ring->adev->wb.wb[ring->rptr_offs];
+}
+
+static u64 gfx_ring_get_wptr_gfx(struct gsgpu_ring *ring)
+{
+	struct gsgpu_device *adev = ring->adev;
+
+	return RREG32(GSGPU_GFX_CB_WPTR_OFFSET);
+}
+
+static void gfx_ring_set_wptr_gfx(struct gsgpu_ring *ring)
+{
+	struct gsgpu_device *adev = ring->adev;
+
+	WREG32(GSGPU_GFX_CB_WPTR_OFFSET, lower_32_bits(ring->wptr));
+}
+
+static void gfx_ring_emit_ib_gfx(struct gsgpu_ring *ring,
+				      struct gsgpu_ib *ib,
+				      unsigned vmid, bool ctx_switch)
+{
+	u32 header, control = 0;
+
+	header = GSPKT(GSPKT_INDIRECT, 3);
+
+	control |= ib->length_dw | (vmid << 24);
+
+	gsgpu_ring_write(ring, header);
+	gsgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));
+	gsgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));
+	gsgpu_ring_write(ring, control);
+}
+
+static void gfx_ring_emit_fence_gfx(struct gsgpu_ring *ring, u64 addr,
+					 u64 seq, unsigned flags)
+{
+	bool write64bit = flags & GSGPU_FENCE_FLAG_64BIT;
+	bool int_sel = flags & GSGPU_FENCE_FLAG_INT;
+	u32 body_size = write64bit ? 4 : 3;
+
+	/* EVENT_WRITE_EOP - flush caches, send int */
+	gsgpu_ring_write(ring, GSPKT(GSPKT_FENCE, body_size)
+			| (write64bit ? 1 << 9 : 0) | (int_sel ? 1 << 8 : 0));
+	gsgpu_ring_write(ring, lower_32_bits(addr));
+	gsgpu_ring_write(ring, upper_32_bits(addr));
+	gsgpu_ring_write(ring, lower_32_bits(seq));
+	if (write64bit)
+		gsgpu_ring_write(ring, upper_32_bits(seq));
+
+}
+
+static void gfx_ring_emit_pipeline_sync(struct gsgpu_ring *ring)
+{
+	uint32_t seq = ring->fence_drv.sync_seq;
+	uint64_t addr = ring->fence_drv.gpu_addr;
+
+	gsgpu_ring_write(ring, GSPKT(GSPKT_POLL, 5) |
+				POLL_CONDITION(3) | /* equal */
+				POLL_REG_MEM(1)); /* reg/mem */
+	gsgpu_ring_write(ring, lower_32_bits(addr));
+	gsgpu_ring_write(ring, upper_32_bits(addr));
+	gsgpu_ring_write(ring, seq); /* reference */
+	gsgpu_ring_write(ring, 0xffffffff); /* mask */
+	gsgpu_ring_write(ring, POLL_TIMES_INTERVAL(0xfff, 1)); /* retry count, poll interval */
+}
+
+static void gfx_ring_emit_vm_flush(struct gsgpu_ring *ring,
+					unsigned vmid, uint64_t pd_addr)
+{
+	gsgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);
+}
+
+static void gfx_ring_emit_rreg(struct gsgpu_ring *ring, uint32_t reg)
+{
+	struct gsgpu_device *adev = ring->adev;
+
+	gsgpu_ring_write(ring, GSPKT(GSPKT_READ, 3) | READ_SRC_SEL(0) | WRITE_DST_SEL(1) | WRITE_WAIT);
+	gsgpu_ring_write(ring, reg);
+	gsgpu_ring_write(ring, lower_32_bits(adev->wb.gpu_addr +
+				adev->reg_val_offs * 4));
+	gsgpu_ring_write(ring, upper_32_bits(adev->wb.gpu_addr +
+				adev->reg_val_offs * 4));
+}
+
+static void gfx_ring_emit_wreg(struct gsgpu_ring *ring, uint32_t reg,
+				  uint32_t val)
+{
+	gsgpu_ring_write(ring, GSPKT(GSPKT_WRITE, 2) | WRITE_DST_SEL(0) | WRITE_WAIT);
+	gsgpu_ring_write(ring, reg);
+	gsgpu_ring_write(ring, val);
+}
+
+static void gfx_set_gfx_eop_interrupt_state(struct gsgpu_device *adev,
+						 enum gsgpu_interrupt_state state)
+{
+}
+
+static int gfx_set_priv_reg_fault_state(struct gsgpu_device *adev,
+					     struct gsgpu_irq_src *source,
+					     unsigned type,
+					     enum gsgpu_interrupt_state state)
+{
+
+	return 0;
+}
+
+static int gfx_set_priv_inst_fault_state(struct gsgpu_device *adev,
+					      struct gsgpu_irq_src *source,
+					      unsigned type,
+					      enum gsgpu_interrupt_state state)
+{
+
+	return 0;
+}
+
+static int gfx_set_eop_interrupt_state(struct gsgpu_device *adev,
+					    struct gsgpu_irq_src *src,
+					    unsigned type,
+					    enum gsgpu_interrupt_state state)
+{
+	gfx_set_gfx_eop_interrupt_state(adev, state);
+
+	return 0;
+}
+
+static int gfx_eop_irq(struct gsgpu_device *adev,
+			    struct gsgpu_irq_src *source,
+			    struct gsgpu_iv_entry *entry)
+{
+	u8 me_id, pipe_id, queue_id;
+
+	DRM_DEBUG("IH: CP EOP\n");
+	me_id = (entry->ring_id & 0x0c) >> 2;
+	pipe_id = (entry->ring_id & 0x03) >> 0;
+	queue_id = (entry->ring_id & 0x70) >> 4;
+
+	switch (me_id) {
+	case 0:
+		gsgpu_fence_process(&adev->gfx.gfx_ring[0]);
+		break;
+	case 1:
+	case 2:
+		break;
+	}
+	return 0;
+}
+
+static int gfx_priv_reg_irq(struct gsgpu_device *adev,
+				 struct gsgpu_irq_src *source,
+				 struct gsgpu_iv_entry *entry)
+{
+	DRM_ERROR("Illegal register access in command stream\n");
+	schedule_work(&adev->reset_work);
+	return 0;
+}
+
+static int gfx_priv_inst_irq(struct gsgpu_device *adev,
+				  struct gsgpu_irq_src *source,
+				  struct gsgpu_iv_entry *entry)
+{
+	DRM_ERROR("Illegal instruction in command stream\n");
+	schedule_work(&adev->reset_work);
+	return 0;
+}
+
+static const struct gsgpu_ip_funcs gfx_ip_funcs = {
+	.name = "gfx",
+	.early_init = gfx_early_init,
+	.late_init = gfx_late_init,
+	.sw_init = gfx_sw_init,
+	.sw_fini = gfx_sw_fini,
+	.hw_init = gfx_hw_init,
+	.hw_fini = gfx_hw_fini,
+	.suspend = gfx_suspend,
+	.resume = gfx_resume,
+	.is_idle = gfx_is_idle,
+	.wait_for_idle = gfx_wait_for_idle,
+};
+
+static const struct gsgpu_ring_funcs gfx_ring_funcs_gfx = {
+	.type = GSGPU_RING_TYPE_GFX,
+	.align_mask = 0xf,
+	.nop = GSPKT(GSPKT_NOP, 0),
+	.support_64bit_ptrs = false,
+	.get_rptr = gfx_ring_get_rptr,
+	.get_wptr = gfx_ring_get_wptr_gfx,
+	.set_wptr = gfx_ring_set_wptr_gfx,
+	.emit_frame_size = /* maximum 215dw if count 16 IBs in */
+		7 +  /* COND_EXEC */
+		1 +  /* PIPELINE_SYNC */
+		VI_FLUSH_GPU_TLB_NUM_WREG * 5 + 9 + /* VM_FLUSH */
+		5 +  /* FENCE for VM_FLUSH */
+		3 + /* CNTX_CTRL */
+		5 + 5,/* FENCE x2 */
+	.emit_ib_size =	4, /* gfx_ring_emit_ib_gfx */
+	.emit_ib = gfx_ring_emit_ib_gfx,
+	.emit_fence = gfx_ring_emit_fence_gfx,
+	.emit_pipeline_sync = gfx_ring_emit_pipeline_sync,
+	.emit_vm_flush = gfx_ring_emit_vm_flush,
+	.test_ring = gfx_ring_test_ring,
+	.test_ib = gfx_ring_test_ib,
+	.insert_nop = gsgpu_ring_insert_nop,
+	.pad_ib = gsgpu_ring_generic_pad_ib,
+	.emit_wreg = gfx_ring_emit_wreg,
+};
+
+static void gfx_set_ring_funcs(struct gsgpu_device *adev)
+{
+	int i;
+
+	for (i = 0; i < adev->gfx.num_gfx_rings; i++)
+		adev->gfx.gfx_ring[i].funcs = &gfx_ring_funcs_gfx;
+}
+
+static const struct gsgpu_irq_src_funcs gfx_eop_irq_funcs = {
+	.set = gfx_set_eop_interrupt_state,
+	.process = gfx_eop_irq,
+};
+
+static const struct gsgpu_irq_src_funcs gfx_priv_reg_irq_funcs = {
+	.set = gfx_set_priv_reg_fault_state,
+	.process = gfx_priv_reg_irq,
+};
+
+static const struct gsgpu_irq_src_funcs gfx_priv_inst_irq_funcs = {
+	.set = gfx_set_priv_inst_fault_state,
+	.process = gfx_priv_inst_irq,
+};
+
+static void gfx_set_irq_funcs(struct gsgpu_device *adev)
+{
+	adev->gfx.eop_irq.num_types = GSGPU_CP_IRQ_LAST;
+	adev->gfx.eop_irq.funcs = &gfx_eop_irq_funcs;
+
+	adev->gfx.priv_reg_irq.num_types = 1;
+	adev->gfx.priv_reg_irq.funcs = &gfx_priv_reg_irq_funcs;
+
+	adev->gfx.priv_inst_irq.num_types = 1;
+	adev->gfx.priv_inst_irq.funcs = &gfx_priv_inst_irq_funcs;
+}
+
+const struct gsgpu_ip_block_version gfx_ip_block = {
+	.type = GSGPU_IP_BLOCK_TYPE_GFX,
+	.major = 8,
+	.minor = 0,
+	.rev = 0,
+	.funcs = &gfx_ip_funcs,
+};
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_gtt_mgr.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gtt_mgr.c
new file mode 100644
index 000000000000..7fa4d9653575
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gtt_mgr.c
@@ -0,0 +1,276 @@
+#include <drm/drmP.h>
+#include "gsgpu.h"
+
+struct gsgpu_gtt_mgr {
+	struct drm_mm mm;
+	spinlock_t lock;
+	atomic64_t available;
+};
+
+struct gsgpu_gtt_node {
+	struct drm_mm_node node;
+	struct ttm_buffer_object *tbo;
+};
+
+/**
+ * gsgpu_gtt_mgr_init - init GTT manager and DRM MM
+ *
+ * @man: TTM memory type manager
+ * @p_size: maximum size of GTT
+ *
+ * Allocate and initialize the GTT manager.
+ */
+static int gsgpu_gtt_mgr_init(struct ttm_mem_type_manager *man,
+			       unsigned long p_size)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(man->bdev);
+	struct gsgpu_gtt_mgr *mgr;
+	uint64_t start, size;
+
+	mgr = kzalloc(sizeof(*mgr), GFP_KERNEL);
+	if (!mgr)
+		return -ENOMEM;
+
+	start = GSGPU_GTT_MAX_TRANSFER_SIZE * GSGPU_GTT_NUM_TRANSFER_WINDOWS;
+	size = (adev->gmc.gart_size >> PAGE_SHIFT) - start;
+	drm_mm_init(&mgr->mm, start, size);
+	spin_lock_init(&mgr->lock);
+	atomic64_set(&mgr->available, p_size);
+	man->priv = mgr;
+	return 0;
+}
+
+/**
+ * gsgpu_gtt_mgr_fini - free and destroy GTT manager
+ *
+ * @man: TTM memory type manager
+ *
+ * Destroy and free the GTT manager, returns -EBUSY if ranges are still
+ * allocated inside it.
+ */
+static int gsgpu_gtt_mgr_fini(struct ttm_mem_type_manager *man)
+{
+	struct gsgpu_gtt_mgr *mgr = man->priv;
+	spin_lock(&mgr->lock);
+	drm_mm_takedown(&mgr->mm);
+	spin_unlock(&mgr->lock);
+	kfree(mgr);
+	man->priv = NULL;
+	return 0;
+}
+
+/**
+ * gsgpu_gtt_mgr_has_gart_addr - Check if mem has address space
+ *
+ * @mem: the mem object to check
+ *
+ * Check if a mem object has already address space allocated.
+ */
+bool gsgpu_gtt_mgr_has_gart_addr(struct ttm_mem_reg *mem)
+{
+	struct gsgpu_gtt_node *node = mem->mm_node;
+
+	return (node->node.start != GSGPU_BO_INVALID_OFFSET);
+}
+
+/**
+ * gsgpu_gtt_mgr_alloc - allocate new ranges
+ *
+ * @man: TTM memory type manager
+ * @tbo: TTM BO we need this range for
+ * @place: placement flags and restrictions
+ * @mem: the resulting mem object
+ *
+ * Allocate the address space for a node.
+ */
+static int gsgpu_gtt_mgr_alloc(struct ttm_mem_type_manager *man,
+				struct ttm_buffer_object *tbo,
+				const struct ttm_place *place,
+				struct ttm_mem_reg *mem)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(man->bdev);
+	struct gsgpu_gtt_mgr *mgr = man->priv;
+	struct gsgpu_gtt_node *node = mem->mm_node;
+	enum drm_mm_insert_mode mode;
+	unsigned long fpfn, lpfn;
+	int r;
+
+	if (gsgpu_gtt_mgr_has_gart_addr(mem))
+		return 0;
+
+	if (place)
+		fpfn = place->fpfn;
+	else
+		fpfn = 0;
+
+	if (place && place->lpfn)
+		lpfn = place->lpfn;
+	else
+		lpfn = adev->gart.num_cpu_pages;
+
+	mode = DRM_MM_INSERT_BEST;
+	if (place && place->flags & TTM_PL_FLAG_TOPDOWN)
+		mode = DRM_MM_INSERT_HIGH;
+
+	spin_lock(&mgr->lock);
+	r = drm_mm_insert_node_in_range(&mgr->mm, &node->node, mem->num_pages,
+					mem->page_alignment, 0, fpfn, lpfn,
+					mode);
+	spin_unlock(&mgr->lock);
+
+	if (!r)
+		mem->start = node->node.start;
+
+	return r;
+}
+
+/**
+ * gsgpu_gtt_mgr_new - allocate a new node
+ *
+ * @man: TTM memory type manager
+ * @tbo: TTM BO we need this range for
+ * @place: placement flags and restrictions
+ * @mem: the resulting mem object
+ *
+ * Dummy, allocate the node but no space for it yet.
+ */
+static int gsgpu_gtt_mgr_new(struct ttm_mem_type_manager *man,
+			      struct ttm_buffer_object *tbo,
+			      const struct ttm_place *place,
+			      struct ttm_mem_reg *mem)
+{
+	struct gsgpu_gtt_mgr *mgr = man->priv;
+	struct gsgpu_gtt_node *node;
+	int r;
+
+	spin_lock(&mgr->lock);
+	if ((&tbo->mem == mem || tbo->mem.mem_type != TTM_PL_TT) &&
+	    atomic64_read(&mgr->available) < mem->num_pages) {
+		spin_unlock(&mgr->lock);
+		return 0;
+	}
+	atomic64_sub(mem->num_pages, &mgr->available);
+	spin_unlock(&mgr->lock);
+
+	node = kzalloc(sizeof(*node), GFP_KERNEL);
+	if (!node) {
+		r = -ENOMEM;
+		goto err_out;
+	}
+
+	node->node.start = GSGPU_BO_INVALID_OFFSET;
+	node->node.size = mem->num_pages;
+	node->tbo = tbo;
+	mem->mm_node = node;
+
+	if (place->fpfn || place->lpfn || place->flags & TTM_PL_FLAG_TOPDOWN) {
+		r = gsgpu_gtt_mgr_alloc(man, tbo, place, mem);
+		if (unlikely(r)) {
+			kfree(node);
+			mem->mm_node = NULL;
+			r = 0;
+			goto err_out;
+		}
+	} else {
+		mem->start = node->node.start;
+	}
+
+	return 0;
+err_out:
+	atomic64_add(mem->num_pages, &mgr->available);
+
+	return r;
+}
+
+/**
+ * gsgpu_gtt_mgr_del - free ranges
+ *
+ * @man: TTM memory type manager
+ * @tbo: TTM BO we need this range for
+ * @place: placement flags and restrictions
+ * @mem: TTM memory object
+ *
+ * Free the allocated GTT again.
+ */
+static void gsgpu_gtt_mgr_del(struct ttm_mem_type_manager *man,
+			       struct ttm_mem_reg *mem)
+{
+	struct gsgpu_gtt_mgr *mgr = man->priv;
+	struct gsgpu_gtt_node *node = mem->mm_node;
+
+	if (!node)
+		return;
+
+	spin_lock(&mgr->lock);
+	if (node->node.start != GSGPU_BO_INVALID_OFFSET)
+		drm_mm_remove_node(&node->node);
+	spin_unlock(&mgr->lock);
+	atomic64_add(mem->num_pages, &mgr->available);
+
+	kfree(node);
+	mem->mm_node = NULL;
+}
+
+/**
+ * gsgpu_gtt_mgr_usage - return usage of GTT domain
+ *
+ * @man: TTM memory type manager
+ *
+ * Return how many bytes are used in the GTT domain
+ */
+uint64_t gsgpu_gtt_mgr_usage(struct ttm_mem_type_manager *man)
+{
+	struct gsgpu_gtt_mgr *mgr = man->priv;
+	s64 result = man->size - atomic64_read(&mgr->available);
+
+	return (result > 0 ? result : 0) * PAGE_SIZE;
+}
+
+int gsgpu_gtt_mgr_recover(struct ttm_mem_type_manager *man)
+{
+	struct gsgpu_gtt_mgr *mgr = man->priv;
+	struct gsgpu_gtt_node *node;
+	struct drm_mm_node *mm_node;
+	int r = 0;
+
+	spin_lock(&mgr->lock);
+	drm_mm_for_each_node(mm_node, &mgr->mm) {
+		node = container_of(mm_node, struct gsgpu_gtt_node, node);
+		r = gsgpu_ttm_recover_gart(node->tbo);
+		if (r)
+			break;
+	}
+	spin_unlock(&mgr->lock);
+
+	return r;
+}
+
+/**
+ * gsgpu_gtt_mgr_debug - dump VRAM table
+ *
+ * @man: TTM memory type manager
+ * @printer: DRM printer to use
+ *
+ * Dump the table content using printk.
+ */
+static void gsgpu_gtt_mgr_debug(struct ttm_mem_type_manager *man,
+				 struct drm_printer *printer)
+{
+	struct gsgpu_gtt_mgr *mgr = man->priv;
+
+	spin_lock(&mgr->lock);
+	drm_mm_print(&mgr->mm, printer);
+	spin_unlock(&mgr->lock);
+
+	drm_printf(printer, "man size:%llu pages, gtt available:%lld pages, usage:%lluMB\n",
+		   man->size, (u64)atomic64_read(&mgr->available),
+		   gsgpu_gtt_mgr_usage(man) >> 20);
+}
+
+const struct ttm_mem_type_manager_func gsgpu_gtt_mgr_func = {
+	.init = gsgpu_gtt_mgr_init,
+	.takedown = gsgpu_gtt_mgr_fini,
+	.get_node = gsgpu_gtt_mgr_new,
+	.put_node = gsgpu_gtt_mgr_del,
+	.debug = gsgpu_gtt_mgr_debug
+};
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_hw_sema.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_hw_sema.c
new file mode 100644
index 000000000000..a65132d4a9ee
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_hw_sema.c
@@ -0,0 +1,184 @@
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+#include "gsgpu_hw_sema.h"
+
+
+static void gsgpu_sema_reset(struct gsgpu_device *adev, unsigned id)
+{
+	struct gsgpu_hw_sema_mgr *sema_mgr = &adev->hw_sema_mgr;
+	struct gsgpu_hw_sema *sema = &sema_mgr->sema[id];
+
+	mutex_lock(&sema_mgr->lock);
+	sema->own = false;
+	sema->pasid = 0ULL;
+	sema->ctx = 0ULL;
+	sema->vm = NULL;
+	mutex_unlock(&sema_mgr->lock);
+}
+
+static int gsgpu_sema_grab_new(struct gsgpu_device *adev,
+			       struct gsgpu_vm *vm,
+			       struct drm_gsgpu_hw_sema *drm_sema)
+{
+
+	struct gsgpu_hw_sema_mgr *sema_mgr = &adev->hw_sema_mgr;
+	struct gsgpu_hw_sema *idle;
+	uint32_t ret = 0;
+
+	mutex_lock(&sema_mgr->lock);
+
+	list_for_each_entry(idle, &sema_mgr->sema_list, list) {
+		if (idle->own == false)
+			goto get_id;
+	}
+
+	if (&idle->list == &sema_mgr->sema_list) {
+		ret = -ENODATA;
+		drm_sema->id = ~0ULL;
+		goto error;
+	}
+
+get_id:
+	if (vm) {
+		gsgpu_vm_set_task_info(vm);
+		idle->pasid = vm->pasid;
+		idle->vm = vm;
+	}
+
+	idle->ctx = drm_sema->ctx_id;
+	idle->own = true;
+
+	drm_sema->id = (idle - sema_mgr->sema);
+
+error:
+	mutex_unlock(&sema_mgr->lock);
+
+	return ret;
+}
+
+int gsgpu_hw_sema_op_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)
+{
+	uint32_t ret = 0;
+	uint64_t ops;
+	struct gsgpu_device *adev = dev->dev_private;
+	struct drm_gsgpu_hw_sema  *drm_sema = data;
+	struct gsgpu_fpriv *fpriv = filp->driver_priv;
+	struct gsgpu_vm *vm = &fpriv->vm;
+
+	ops = drm_sema->ops;
+
+	switch (ops) {
+	case GSGPU_HW_SEMA_GET:
+		ret = gsgpu_sema_grab_new(adev, vm, drm_sema);
+		break;
+	case GSGPU_HW_SEMA_PUT:
+		gsgpu_sema_reset(adev, drm_sema->id);
+		ret = 0;
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+void gsgpu_sema_free(struct gsgpu_device *adev, struct gsgpu_vm *vm)
+{
+	struct gsgpu_hw_sema_mgr *sema_mgr = &adev->hw_sema_mgr;
+	struct gsgpu_hw_sema *hw_sema;
+
+	list_for_each_entry(hw_sema, &sema_mgr->sema_list, list) {
+		if (hw_sema->vm == vm)
+			gsgpu_sema_reset(adev, (hw_sema-sema_mgr->sema));
+	}
+}
+
+int gsgpu_hw_sema_mgr_init(struct gsgpu_device *adev)
+{
+	unsigned i;
+
+	struct gsgpu_hw_sema_mgr *sema_mgr = &adev->hw_sema_mgr;
+
+	/* TODO Should get from EC*/
+	sema_mgr->num_ids = GSGPU_NUM_SEMA;
+
+	mutex_init(&sema_mgr->lock);
+	INIT_LIST_HEAD(&sema_mgr->sema_list);
+
+	for (i = 0; i < sema_mgr->num_ids; ++i) {
+		gsgpu_sema_reset(adev, i);
+		list_add_tail(&sema_mgr->sema[i].list, &sema_mgr->sema_list);
+	}
+
+	return 0;
+}
+
+void gsgpu_hw_sema_mgr_fini(struct gsgpu_device *adev)
+{
+
+	unsigned i;
+
+	struct gsgpu_hw_sema_mgr *sema_mgr = &adev->hw_sema_mgr;
+
+	mutex_destroy(&sema_mgr->lock);
+	for (i = 0; i < GSGPU_NUM_SEMA; ++i) {
+		gsgpu_sema_reset(adev, i);
+	}
+
+}
+
+
+#if defined(CONFIG_DEBUG_FS)
+
+static int gsgpu_debugfs_sema_info(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *)m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_hw_sema_mgr *sema_mgr = &adev->hw_sema_mgr;
+	struct gsgpu_hw_sema *sema;
+
+
+	mutex_lock(&sema_mgr->lock);
+
+	rcu_read_lock();
+	list_for_each_entry(sema, &sema_mgr->sema_list, list) {
+
+		if (sema->own) {
+
+			if (sema->vm) {
+				struct gsgpu_task_info task_info;
+				gsgpu_vm_get_task_info(adev, sema->pasid, &task_info);
+				seq_printf(m, "pid %8d process:%s \t",
+					   task_info.pid,
+					   task_info.task_name);
+			}
+
+			seq_printf(m, "id %d \tctx_id %d\n",
+				   (u32)(sema - sema_mgr->sema), sema->ctx);
+
+		} else
+			seq_printf(m, "id %d available\n", (u32)(sema - sema_mgr->sema));
+
+	}
+
+	rcu_read_unlock();
+
+	mutex_unlock(&sema_mgr->lock);
+
+	return 0;
+}
+
+static const struct drm_info_list gsgpu_debugfs_sema_list[] = {
+	{"gsgpu_hw_sema_info", &gsgpu_debugfs_sema_info, 0, NULL},
+};
+#endif
+int gsgpu_debugfs_sema_init(struct gsgpu_device *adev)
+{
+#if defined(CONFIG_DEBUG_FS)
+	return gsgpu_debugfs_add_files(adev, gsgpu_debugfs_sema_list, 1);
+#endif
+	return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ib.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ib.c
new file mode 100644
index 000000000000..063d1825944e
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ib.c
@@ -0,0 +1,322 @@
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+
+#define GSGPU_IB_TEST_TIMEOUT	msecs_to_jiffies(1000)
+
+/*
+ * IB
+ * IBs (Indirect Buffers) and areas of GPU accessible memory where
+ * commands are stored.  You can put a pointer to the IB in the
+ * command ring and the hw will fetch the commands from the IB
+ * and execute them.  Generally userspace acceleration drivers
+ * produce command buffers which are send to the kernel and
+ * put in IBs for execution by the requested ring.
+ */
+static int gsgpu_debugfs_sa_init(struct gsgpu_device *adev);
+
+/**
+ * gsgpu_ib_get - request an IB (Indirect Buffer)
+ *
+ * @ring: ring index the IB is associated with
+ * @size: requested IB size
+ * @ib: IB object returned
+ *
+ * Request an IB (all asics).  IBs are allocated using the
+ * suballocator.
+ * Returns 0 on success, error on failure.
+ */
+int gsgpu_ib_get(struct gsgpu_device *adev, struct gsgpu_vm *vm,
+		  unsigned size, struct gsgpu_ib *ib)
+{
+	int r;
+
+	if (size) {
+		r = gsgpu_sa_bo_new(&adev->ring_tmp_bo,
+				      &ib->sa_bo, size, 256);
+		if (r) {
+			dev_err(adev->dev, "failed to get a new IB (%d)\n", r);
+			return r;
+		}
+
+		ib->ptr = gsgpu_sa_bo_cpu_addr(ib->sa_bo);
+
+		if (!vm)
+			ib->gpu_addr = gsgpu_sa_bo_gpu_addr(ib->sa_bo);
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_ib_free - free an IB (Indirect Buffer)
+ *
+ * @adev: gsgpu_device pointer
+ * @ib: IB object to free
+ * @f: the fence SA bo need wait on for the ib alloation
+ *
+ * Free an IB (all asics).
+ */
+void gsgpu_ib_free(struct gsgpu_device *adev, struct gsgpu_ib *ib,
+		    struct dma_fence *f)
+{
+	gsgpu_sa_bo_free(adev, &ib->sa_bo, f);
+}
+
+/**
+ * gsgpu_ib_schedule - schedule an IB (Indirect Buffer) on the ring
+ *
+ * @adev: gsgpu_device pointer
+ * @num_ibs: number of IBs to schedule
+ * @ibs: IB objects to schedule
+ * @f: fence created during this submission
+ *
+ * Schedule an IB on the associated ring (all asics).
+ * Returns 0 on success, error on failure.
+ *
+ * On SI, there are two parallel engines fed from the primary ring,
+ * the CE (Constant Engine) and the DE (Drawing Engine).  Since
+ * resource descriptors have moved to memory, the CE allows you to
+ * prime the caches while the DE is updating register state so that
+ * the resource descriptors will be already in cache when the draw is
+ * processed.  To accomplish this, the userspace driver submits two
+ * IBs, one for the CE and one for the DE.  If there is a CE IB (called
+ * a CONST_IB), it will be put on the ring prior to the DE IB.  Prior
+ * to SI there was just a DE IB.
+ */
+int gsgpu_ib_schedule(struct gsgpu_ring *ring, unsigned num_ibs,
+		       struct gsgpu_ib *ibs, struct gsgpu_job *job,
+		       struct dma_fence **f)
+{
+	struct gsgpu_device *adev = ring->adev;
+	struct gsgpu_ib *ib = &ibs[0];
+	struct dma_fence *tmp = NULL;
+	bool skip_preamble, need_ctx_switch;
+	struct gsgpu_vm *vm;
+	uint64_t fence_ctx;
+	uint32_t status = 0, alloc_size;
+	unsigned fence_flags = 0;
+
+	unsigned i;
+	int r = 0;
+	bool need_pipe_sync = false;
+
+	if (num_ibs == 0)
+		return -EINVAL;
+
+	/* ring tests don't use a job */
+	if (job) {
+		vm = job->vm;
+		fence_ctx = job->base.s_fence->scheduled.context;
+	} else {
+		vm = NULL;
+		fence_ctx = 0;
+	}
+
+	if (!ring->ready) {
+		dev_err(adev->dev, "couldn't schedule ib on ring <%s>\n", ring->name);
+		return -EINVAL;
+	}
+
+	if (vm && !job->vmid) {
+		dev_err(adev->dev, "VM IB without ID\n");
+		return -EINVAL;
+	}
+
+	alloc_size = ring->funcs->emit_frame_size + num_ibs *
+		ring->funcs->emit_ib_size;
+
+	r = gsgpu_ring_alloc(ring, alloc_size);
+	if (r) {
+		dev_err(adev->dev, "scheduling IB failed (%d).\n", r);
+		return r;
+	}
+
+	need_ctx_switch = ring->current_ctx != fence_ctx;
+	if (ring->funcs->emit_pipeline_sync && job &&
+	    ((tmp = gsgpu_sync_get_fence(&job->sched_sync, NULL)) ||
+	    gsgpu_vm_need_pipeline_sync(ring, job))) {
+		need_pipe_sync = true;
+		dma_fence_put(tmp);
+	}
+
+	if (job) {
+		r = gsgpu_vm_flush(ring, job, need_pipe_sync);
+		if (r) {
+			gsgpu_ring_undo(ring);
+			return r;
+		}
+	}
+
+	skip_preamble = ring->current_ctx == fence_ctx;
+	if (job && ring->funcs->emit_cntxcntl) {
+		if (need_ctx_switch)
+			status |= GSGPU_HAVE_CTX_SWITCH;
+		status |= job->preamble_status;
+
+		gsgpu_ring_emit_cntxcntl(ring, status);
+	}
+
+	for (i = 0; i < num_ibs; ++i) {
+		ib = &ibs[i];
+
+		gsgpu_ring_emit_ib(ring, ib, job ? job->vmid : 0,
+				    need_ctx_switch);
+		need_ctx_switch = false;
+	}
+
+	if (ib->flags & GSGPU_IB_FLAG_TC_WB_NOT_INVALIDATE)
+		fence_flags |= GSGPU_FENCE_FLAG_TC_WB_ONLY;
+
+	/* wrap the last IB with fence */
+	if (job && job->uf_addr) {
+		gsgpu_ring_emit_fence(ring, job->uf_addr, job->uf_sequence,
+				       fence_flags | GSGPU_FENCE_FLAG_64BIT);
+	}
+
+	r = gsgpu_fence_emit(ring, f, fence_flags);
+	if (r) {
+		dev_err(adev->dev, "failed to emit fence (%d)\n", r);
+		if (job && job->vmid)
+			gsgpu_vmid_reset(adev, job->vmid);
+		gsgpu_ring_undo(ring);
+		return r;
+	}
+
+	if (ring->funcs->insert_end)
+		ring->funcs->insert_end(ring);
+
+	ring->current_ctx = fence_ctx;
+	//if (vm && ring->funcs->emit_switch_buffer)
+	//	gsgpu_ring_emit_switch_buffer(ring);
+	gsgpu_ring_commit(ring);
+	return 0;
+}
+
+/**
+ * gsgpu_ib_pool_init - Init the IB (Indirect Buffer) pool
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Initialize the suballocator to manage a pool of memory
+ * for use as IBs (all asics).
+ * Returns 0 on success, error on failure.
+ */
+int gsgpu_ib_pool_init(struct gsgpu_device *adev)
+{
+	int r;
+
+	if (adev->ib_pool_ready) {
+		return 0;
+	}
+	r = gsgpu_sa_bo_manager_init(adev, &adev->ring_tmp_bo,
+				      GSGPU_IB_POOL_SIZE*64*1024,
+				      GSGPU_GPU_PAGE_SIZE,
+				      GSGPU_GEM_DOMAIN_GTT);
+	if (r) {
+		return r;
+	}
+
+	adev->ib_pool_ready = true;
+	if (gsgpu_debugfs_sa_init(adev)) {
+		dev_err(adev->dev, "failed to register debugfs file for SA\n");
+	}
+	return 0;
+}
+
+/**
+ * gsgpu_ib_pool_fini - Free the IB (Indirect Buffer) pool
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Tear down the suballocator managing the pool of memory
+ * for use as IBs (all asics).
+ */
+void gsgpu_ib_pool_fini(struct gsgpu_device *adev)
+{
+	if (adev->ib_pool_ready) {
+		gsgpu_sa_bo_manager_fini(adev, &adev->ring_tmp_bo);
+		adev->ib_pool_ready = false;
+	}
+}
+
+/**
+ * gsgpu_ib_ring_tests - test IBs on the rings
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Test an IB (Indirect Buffer) on each ring.
+ * If the test fails, disable the ring.
+ * Returns 0 on success, error if the primary GFX ring
+ * IB test fails.
+ */
+int gsgpu_ib_ring_tests(struct gsgpu_device *adev)
+{
+	unsigned i;
+	int r, ret = 0;
+	long tmo_gfx, tmo_mm;
+
+	tmo_mm = tmo_gfx = GSGPU_IB_TEST_TIMEOUT;
+
+	for (i = 0; i < GSGPU_MAX_RINGS; ++i) {
+		struct gsgpu_ring *ring = adev->rings[i];
+		long tmo;
+
+		if (!ring || !ring->ready)
+			continue;
+
+		tmo = tmo_gfx;
+
+		r = gsgpu_ring_test_ib(ring, tmo);
+		if (r) {
+			ring->ready = false;
+
+			if (ring == &adev->gfx.gfx_ring[0]) {
+				/* oh, oh, that's really bad */
+				DRM_ERROR("gsgpu: failed testing IB on GFX ring (%d).\n", r);
+				adev->accel_working = false;
+				return r;
+
+			} else {
+				/* still not good, but we can live with it */
+				DRM_ERROR("gsgpu: failed testing IB on ring %d (%d).\n", i, r);
+				ret = r;
+			}
+		}
+	}
+	return ret;
+}
+
+/*
+ * Debugfs info
+ */
+#if defined(CONFIG_DEBUG_FS)
+
+static int gsgpu_debugfs_sa_info(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+
+	gsgpu_sa_bo_dump_debug_info(&adev->ring_tmp_bo, m);
+
+	return 0;
+
+}
+
+static const struct drm_info_list gsgpu_debugfs_sa_list[] = {
+	{"gsgpu_sa_info", &gsgpu_debugfs_sa_info, 0, NULL},
+};
+
+#endif
+
+static int gsgpu_debugfs_sa_init(struct gsgpu_device *adev)
+{
+#if defined(CONFIG_DEBUG_FS)
+	return gsgpu_debugfs_add_files(adev, gsgpu_debugfs_sa_list, 1);
+#else
+	return 0;
+#endif
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ids.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ids.c
new file mode 100644
index 000000000000..876be19c972f
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ids.c
@@ -0,0 +1,562 @@
+#include "gsgpu_ids.h"
+
+#include <linux/idr.h>
+#include <linux/dma-fence-array.h>
+#include <drm/drmP.h>
+
+#include "gsgpu.h"
+#include "gsgpu_trace.h"
+
+/*
+ * PASID manager
+ *
+ * PASIDs are global address space identifiers that can be shared
+ * between the GPU, an IOMMU and the driver. VMs on different devices
+ * may use the same PASID if they share the same address
+ * space. Therefore PASIDs are allocated using a global IDA. VMs are
+ * looked up from the PASID per gsgpu_device.
+ */
+static DEFINE_IDA(gsgpu_pasid_ida);
+
+/* Helper to free pasid from a fence callback */
+struct gsgpu_pasid_cb {
+	struct dma_fence_cb cb;
+	unsigned int pasid;
+};
+
+/**
+ * gsgpu_pasid_alloc - Allocate a PASID
+ * @bits: Maximum width of the PASID in bits, must be at least 1
+ *
+ * Allocates a PASID of the given width while keeping smaller PASIDs
+ * available if possible.
+ *
+ * Returns a positive integer on success. Returns %-EINVAL if bits==0.
+ * Returns %-ENOSPC if no PASID was available. Returns %-ENOMEM on
+ * memory allocation failure.
+ */
+int gsgpu_pasid_alloc(unsigned int bits)
+{
+	int pasid = -EINVAL;
+
+	for (bits = min(bits, 31U); bits > 0; bits--) {
+		pasid = ida_simple_get(&gsgpu_pasid_ida,
+				       1U << (bits - 1), 1U << bits,
+				       GFP_KERNEL);
+		if (pasid != -ENOSPC)
+			break;
+	}
+
+	if (pasid >= 0)
+		trace_gsgpu_pasid_allocated(pasid);
+
+	return pasid;
+}
+
+/**
+ * gsgpu_pasid_free - Free a PASID
+ * @pasid: PASID to free
+ */
+void gsgpu_pasid_free(unsigned int pasid)
+{
+	trace_gsgpu_pasid_freed(pasid);
+	ida_simple_remove(&gsgpu_pasid_ida, pasid);
+}
+
+static void gsgpu_pasid_free_cb(struct dma_fence *fence,
+				 struct dma_fence_cb *_cb)
+{
+	struct gsgpu_pasid_cb *cb =
+		container_of(_cb, struct gsgpu_pasid_cb, cb);
+
+	gsgpu_pasid_free(cb->pasid);
+	dma_fence_put(fence);
+	kfree(cb);
+}
+
+/**
+ * gsgpu_pasid_free_delayed - free pasid when fences signal
+ *
+ * @resv: reservation object with the fences to wait for
+ * @pasid: pasid to free
+ *
+ * Free the pasid only after all the fences in resv are signaled.
+ */
+void gsgpu_pasid_free_delayed(struct reservation_object *resv,
+			       unsigned int pasid)
+{
+	struct dma_fence *fence, **fences;
+	struct gsgpu_pasid_cb *cb;
+	unsigned count;
+	int r;
+
+	r = reservation_object_get_fences_rcu(resv, NULL, &count, &fences);
+	if (r)
+		goto fallback;
+
+	if (count == 0) {
+		gsgpu_pasid_free(pasid);
+		return;
+	}
+
+	if (count == 1) {
+		fence = fences[0];
+		kfree(fences);
+	} else {
+		uint64_t context = dma_fence_context_alloc(1);
+		struct dma_fence_array *array;
+
+		array = dma_fence_array_create(count, fences, context,
+					       1, false);
+		if (!array) {
+			kfree(fences);
+			goto fallback;
+		}
+		fence = &array->base;
+	}
+
+	cb = kmalloc(sizeof(*cb), GFP_KERNEL);
+	if (!cb) {
+		/* Last resort when we are OOM */
+		dma_fence_wait(fence, false);
+		dma_fence_put(fence);
+		gsgpu_pasid_free(pasid);
+	} else {
+		cb->pasid = pasid;
+		if (dma_fence_add_callback(fence, &cb->cb,
+					   gsgpu_pasid_free_cb))
+			gsgpu_pasid_free_cb(fence, &cb->cb);
+	}
+
+	return;
+
+fallback:
+	/* Not enough memory for the delayed delete, as last resort
+	 * block for all the fences to complete.
+	 */
+	reservation_object_wait_timeout_rcu(resv, true, false,
+					    MAX_SCHEDULE_TIMEOUT);
+	gsgpu_pasid_free(pasid);
+}
+
+/*
+ * VMID manager
+ *
+ * VMIDs are an identifier for page tables handling.
+ */
+
+/**
+ * gsgpu_vmid_had_gpu_reset - check if reset occured since last use
+ *
+ * @adev: gsgpu_device pointer
+ * @id: VMID structure
+ *
+ * Check if GPU reset occured since last use of the VMID.
+ */
+bool gsgpu_vmid_had_gpu_reset(struct gsgpu_device *adev,
+			       struct gsgpu_vmid *id)
+{
+	return id->current_gpu_reset_count !=
+		atomic_read(&adev->gpu_reset_counter);
+}
+
+/**
+ * gsgpu_vm_grab_idle - grab idle VMID
+ *
+ * @vm: vm to allocate id for
+ * @ring: ring we want to submit job to
+ * @sync: sync object where we add dependencies
+ * @idle: resulting idle VMID
+ *
+ * Try to find an idle VMID, if none is idle add a fence to wait to the sync
+ * object. Returns -ENOMEM when we are out of memory.
+ */
+static int gsgpu_vmid_grab_idle(struct gsgpu_vm *vm,
+				 struct gsgpu_ring *ring,
+				 struct gsgpu_sync *sync,
+				 struct gsgpu_vmid **idle)
+{
+	struct gsgpu_device *adev = ring->adev;
+	struct gsgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr;
+	struct dma_fence **fences;
+	unsigned i;
+	int r;
+
+	if (ring->vmid_wait && !dma_fence_is_signaled(ring->vmid_wait))
+		return gsgpu_sync_fence(adev, sync, ring->vmid_wait, false);
+
+	fences = kmalloc_array(sizeof(void *), id_mgr->num_ids, GFP_KERNEL);
+	if (!fences)
+		return -ENOMEM;
+
+	/* Check if we have an idle VMID */
+	i = 0;
+	list_for_each_entry((*idle), &id_mgr->ids_lru, list) {
+		fences[i] = gsgpu_sync_peek_fence(&(*idle)->active, ring);
+		if (!fences[i])
+			break;
+		++i;
+	}
+
+	/* If we can't find a idle VMID to use, wait till one becomes available */
+	if (&(*idle)->list == &id_mgr->ids_lru) {
+		u64 fence_context = adev->vm_manager.fence_context + ring->idx;
+		unsigned seqno = ++adev->vm_manager.seqno[ring->idx];
+		struct dma_fence_array *array;
+		unsigned j;
+
+		*idle = NULL;
+		for (j = 0; j < i; ++j)
+			dma_fence_get(fences[j]);
+
+		array = dma_fence_array_create(i, fences, fence_context,
+					       seqno, true);
+		if (!array) {
+			for (j = 0; j < i; ++j)
+				dma_fence_put(fences[j]);
+			kfree(fences);
+			return -ENOMEM;
+		}
+
+		r = gsgpu_sync_fence(adev, sync, &array->base, false);
+		dma_fence_put(ring->vmid_wait);
+		ring->vmid_wait = &array->base;
+		return r;
+	}
+	kfree(fences);
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_grab_reserved - try to assign reserved VMID
+ *
+ * @vm: vm to allocate id for
+ * @ring: ring we want to submit job to
+ * @sync: sync object where we add dependencies
+ * @fence: fence protecting ID from reuse
+ * @job: job who wants to use the VMID
+ *
+ * Try to assign a reserved VMID.
+ */
+static int gsgpu_vmid_grab_reserved(struct gsgpu_vm *vm,
+				     struct gsgpu_ring *ring,
+				     struct gsgpu_sync *sync,
+				     struct dma_fence *fence,
+				     struct gsgpu_job *job,
+				     struct gsgpu_vmid **id)
+{
+	struct gsgpu_device *adev = ring->adev;
+	uint64_t fence_context = adev->fence_context + ring->idx;
+	struct dma_fence *updates = sync->last_vm_update;
+	bool needs_flush = vm->use_cpu_for_update;
+	int r = 0;
+
+	*id = vm->reserved_vmid;
+	if (updates && (*id)->flushed_updates &&
+	    updates->context == (*id)->flushed_updates->context &&
+	    !dma_fence_is_later(updates, (*id)->flushed_updates))
+	    updates = NULL;
+
+	if ((*id)->owner != vm->entity.fence_context ||
+	    job->vm_pd_addr != (*id)->pd_gpu_addr ||
+	    updates || !(*id)->last_flush ||
+	    ((*id)->last_flush->context != fence_context &&
+	     !dma_fence_is_signaled((*id)->last_flush))) {
+		struct dma_fence *tmp;
+
+		/* to prevent one context starved by another context */
+		(*id)->pd_gpu_addr = 0;
+		tmp = gsgpu_sync_peek_fence(&(*id)->active, ring);
+		if (tmp) {
+			*id = NULL;
+			r = gsgpu_sync_fence(adev, sync, tmp, false);
+			return r;
+		}
+		needs_flush = true;
+	}
+
+	/* Good we can use this VMID. Remember this submission as
+	* user of the VMID.
+	*/
+	r = gsgpu_sync_fence(ring->adev, &(*id)->active, fence, false);
+	if (r)
+		return r;
+
+	if (updates) {
+		dma_fence_put((*id)->flushed_updates);
+		(*id)->flushed_updates = dma_fence_get(updates);
+	}
+	job->vm_needs_flush = needs_flush;
+	return 0;
+}
+
+/**
+ * gsgpu_vm_grab_used - try to reuse a VMID
+ *
+ * @vm: vm to allocate id for
+ * @ring: ring we want to submit job to
+ * @sync: sync object where we add dependencies
+ * @fence: fence protecting ID from reuse
+ * @job: job who wants to use the VMID
+ * @id: resulting VMID
+ *
+ * Try to reuse a VMID for this submission.
+ */
+static int gsgpu_vmid_grab_used(struct gsgpu_vm *vm,
+				 struct gsgpu_ring *ring,
+				 struct gsgpu_sync *sync,
+				 struct dma_fence *fence,
+				 struct gsgpu_job *job,
+				 struct gsgpu_vmid **id)
+{
+	struct gsgpu_device *adev = ring->adev;
+	struct gsgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr;
+	uint64_t fence_context = adev->fence_context + ring->idx;
+	struct dma_fence *updates = sync->last_vm_update;
+	int r;
+
+	job->vm_needs_flush = vm->use_cpu_for_update;
+
+	/* Check if we can use a VMID already assigned to this VM */
+	list_for_each_entry_reverse((*id), &id_mgr->ids_lru, list) {
+		bool needs_flush = vm->use_cpu_for_update;
+		struct dma_fence *flushed;
+
+		/* Check all the prerequisites to using this VMID */
+		if ((*id)->owner != vm->entity.fence_context)
+			continue;
+
+		if ((*id)->pd_gpu_addr != job->vm_pd_addr)
+			continue;
+
+		if (!(*id)->last_flush ||
+		    ((*id)->last_flush->context != fence_context &&
+		     !dma_fence_is_signaled((*id)->last_flush)))
+			needs_flush = true;
+
+		flushed  = (*id)->flushed_updates;
+		if (updates && (!flushed || dma_fence_is_later(updates, flushed)))
+			needs_flush = true;
+
+		/* Concurrent flushes are only possible starting with Vega10 */
+		// if (needs_flush)
+		// 	continue;
+
+		/* Good, we can use this VMID. Remember this submission as
+		 * user of the VMID.
+		 */
+		r = gsgpu_sync_fence(ring->adev, &(*id)->active, fence, false);
+		if (r)
+			return r;
+
+		if (updates && (!flushed || dma_fence_is_later(updates, flushed))) {
+			dma_fence_put((*id)->flushed_updates);
+			(*id)->flushed_updates = dma_fence_get(updates);
+		}
+
+		job->vm_needs_flush |= needs_flush;
+		return 0;
+	}
+
+	*id = NULL;
+	return 0;
+}
+
+/**
+ * gsgpu_vm_grab_id - allocate the next free VMID
+ *
+ * @vm: vm to allocate id for
+ * @ring: ring we want to submit job to
+ * @sync: sync object where we add dependencies
+ * @fence: fence protecting ID from reuse
+ * @job: job who wants to use the VMID
+ *
+ * Allocate an id for the vm, adding fences to the sync obj as necessary.
+ */
+int gsgpu_vmid_grab(struct gsgpu_vm *vm, struct gsgpu_ring *ring,
+		     struct gsgpu_sync *sync, struct dma_fence *fence,
+		     struct gsgpu_job *job)
+{
+	struct gsgpu_device *adev = ring->adev;
+	struct gsgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr;
+	struct gsgpu_vmid *idle = NULL;
+	struct gsgpu_vmid *id = NULL;
+	int r = 0;
+
+	mutex_lock(&id_mgr->lock);
+	r = gsgpu_vmid_grab_idle(vm, ring, sync, &idle);
+	if (r || !idle)
+		goto error;
+
+	if (vm->reserved_vmid) {
+		r = gsgpu_vmid_grab_reserved(vm, ring, sync, fence, job, &id);
+		if (r || !id)
+			goto error;
+	} else {
+		r = gsgpu_vmid_grab_used(vm, ring, sync, fence, job, &id);
+		if (r)
+			goto error;
+
+		if (!id) {
+			struct dma_fence *updates = sync->last_vm_update;
+
+			/* Still no ID to use? Then use the idle one found earlier */
+			id = idle;
+
+			/* Remember this submission as user of the VMID */
+			r = gsgpu_sync_fence(ring->adev, &id->active,
+					      fence, false);
+			if (r)
+				goto error;
+
+			dma_fence_put(id->flushed_updates);
+			id->flushed_updates = dma_fence_get(updates);
+			job->vm_needs_flush = true;
+		}
+
+		list_move_tail(&id->list, &id_mgr->ids_lru);
+	}
+
+	id->pd_gpu_addr = job->vm_pd_addr;
+	id->owner = vm->entity.fence_context;
+
+	if (job->vm_needs_flush) {
+		dma_fence_put(id->last_flush);
+		id->last_flush = NULL;
+	}
+	job->vmid = id - id_mgr->ids;
+	job->pasid = vm->pasid;
+	trace_gsgpu_vm_grab_id(vm, ring, job);
+
+error:
+	mutex_unlock(&id_mgr->lock);
+	return r;
+}
+
+int gsgpu_vmid_alloc_reserved(struct gsgpu_device *adev, struct gsgpu_vm *vm)
+{
+	struct gsgpu_vmid_mgr *id_mgr;
+	struct gsgpu_vmid *idle;
+	int r = 0;
+
+	id_mgr = &adev->vm_manager.id_mgr;
+	mutex_lock(&id_mgr->lock);
+	if (vm->reserved_vmid)
+		goto unlock;
+	if (atomic_inc_return(&id_mgr->reserved_vmid_num) >
+	    GSGPU_VM_MAX_RESERVED_VMID) {
+		DRM_ERROR("Over limitation of reserved vmid\n");
+		atomic_dec(&id_mgr->reserved_vmid_num);
+		r = -EINVAL;
+		goto unlock;
+	}
+	/* Select the first entry VMID */
+	idle = list_first_entry(&id_mgr->ids_lru, struct gsgpu_vmid, list);
+	list_del_init(&idle->list);
+	vm->reserved_vmid = idle;
+	mutex_unlock(&id_mgr->lock);
+
+	return 0;
+unlock:
+	mutex_unlock(&id_mgr->lock);
+	return r;
+}
+
+void gsgpu_vmid_free_reserved(struct gsgpu_device *adev, struct gsgpu_vm *vm)
+{
+	struct gsgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr;
+
+	mutex_lock(&id_mgr->lock);
+	if (vm->reserved_vmid) {
+		list_add(&vm->reserved_vmid->list,
+			&id_mgr->ids_lru);
+		vm->reserved_vmid = NULL;
+		atomic_dec(&id_mgr->reserved_vmid_num);
+	}
+	mutex_unlock(&id_mgr->lock);
+}
+
+/**
+ * gsgpu_vmid_reset - reset VMID to zero
+ *
+ * @adev: gsgpu device structure
+ * @vmid: vmid number to use
+ *
+ * Reset saved GDW, GWS and OA to force switch on next flush.
+ */
+void gsgpu_vmid_reset(struct gsgpu_device *adev, unsigned vmid)
+{
+	struct gsgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr;
+	struct gsgpu_vmid *id = &id_mgr->ids[vmid];
+
+	mutex_lock(&id_mgr->lock);
+	id->owner = 0;
+	mutex_unlock(&id_mgr->lock);
+}
+
+/**
+ * gsgpu_vmid_reset_all - reset VMID to zero
+ *
+ * @adev: gsgpu device structure
+ *
+ * Reset VMID to force flush on next use
+ */
+void gsgpu_vmid_reset_all(struct gsgpu_device *adev)
+{
+	unsigned i;
+
+	struct gsgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr;
+
+	for (i = 1; i < id_mgr->num_ids; ++i)
+		gsgpu_vmid_reset(adev, i);
+}
+
+/**
+ * gsgpu_vmid_mgr_init - init the VMID manager
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Initialize the VM manager structures
+ */
+void gsgpu_vmid_mgr_init(struct gsgpu_device *adev)
+{
+	unsigned i;
+
+	struct gsgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr;
+
+	mutex_init(&id_mgr->lock);
+	INIT_LIST_HEAD(&id_mgr->ids_lru);
+	atomic_set(&id_mgr->reserved_vmid_num, 0);
+
+	/* skip over VMID 0, since it is the system VM */
+	for (i = 1; i < id_mgr->num_ids; ++i) {
+		gsgpu_vmid_reset(adev, i);
+		gsgpu_sync_create(&id_mgr->ids[i].active);
+		list_add_tail(&id_mgr->ids[i].list, &id_mgr->ids_lru);
+	}
+}
+
+/**
+ * gsgpu_vmid_mgr_fini - cleanup VM manager
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Cleanup the VM manager and free resources.
+ */
+void gsgpu_vmid_mgr_fini(struct gsgpu_device *adev)
+{
+	unsigned i;
+
+	struct gsgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr;
+
+	mutex_destroy(&id_mgr->lock);
+	for (i = 0; i < GSGPU_NUM_VMID; ++i) {
+		struct gsgpu_vmid *id = &id_mgr->ids[i];
+
+		gsgpu_sync_free(&id->active);
+		dma_fence_put(id->flushed_updates);
+		dma_fence_put(id->last_flush);
+		dma_fence_put(id->pasid_mapping);
+	}
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ih.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ih.c
new file mode 100644
index 000000000000..ffcc3531db97
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ih.c
@@ -0,0 +1,441 @@
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include "gsgpu_ih.h"
+#include "gsgpu_irq.h"
+
+static void gsgpu_ih_set_interrupt_funcs(struct gsgpu_device *adev);
+
+/**
+ * gsgpu_ih_enable_interrupts - Enable the interrupt ring buffer
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Enable the interrupt ring buffer  .
+ */
+static void gsgpu_ih_enable_interrupts(struct gsgpu_device *adev)
+{
+	adev->irq.ih.enabled = true;
+}
+
+/**
+ * gsgpu_ih_disable_interrupts - Disable the interrupt ring buffer
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Disable the interrupt ring buffer  .
+ */
+static void gsgpu_ih_disable_interrupts(struct gsgpu_device *adev)
+{
+	adev->irq.ih.enabled = false;
+	adev->irq.ih.rptr = 0;
+}
+
+/**
+ * gsgpu_ih_irq_init - init and enable the interrupt ring
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Allocate a ring buffer for the interrupt controller,
+ * enable the RLC, disable interrupts, enable the IH
+ * ring buffer and enable it  .
+ * Called at device load and reume.
+ * Returns 0 for success, errors for failure.
+ */
+static int gsgpu_ih_irq_init(struct gsgpu_device *adev)
+{
+	int rb_bufsz;
+	u64 wptr_off;
+
+	/* disable irqs */
+	gsgpu_ih_disable_interrupts(adev);
+
+	rb_bufsz = adev->irq.ih.ring_size / 4;
+	WREG32(GSGPU_INT_CB_SIZE_OFFSET, rb_bufsz);
+
+	/* set the writeback address whether it's enabled or not */
+	if (adev->irq.ih.use_bus_addr)
+		wptr_off = adev->irq.ih.rb_dma_addr;
+	else
+		wptr_off = adev->wb.gpu_addr;
+	WREG32(GSGPU_INT_CB_BASE_LO_OFFSET, lower_32_bits(wptr_off));
+	WREG32(GSGPU_INT_CB_BASE_HI_OFFSET, upper_32_bits(wptr_off));
+
+	/* set rptr, wptr to 0 */
+	WREG32(GSGPU_INT_CB_WPTR_OFFSET, 0);
+	WREG32(GSGPU_INT_CB_RPTR_OFFSET, 0);
+
+	pci_set_master(adev->pdev);
+
+	/* enable interrupts */
+	gsgpu_ih_enable_interrupts(adev);
+
+	return 0;
+}
+
+/**
+ * gsgpu_ih_irq_disable - disable interrupts
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Disable interrupts on the hw  .
+ */
+static void gsgpu_ih_irq_disable(struct gsgpu_device *adev)
+{
+	gsgpu_ih_disable_interrupts(adev);
+
+	/* Wait and acknowledge irq */
+	mdelay(1);
+}
+
+static u32 ih_func_get_wptr(struct gsgpu_device *adev)
+{
+	u32 wptr;
+
+	if (adev->irq.ih.use_bus_addr)
+		wptr = le32_to_cpu(RREG32(GSGPU_INT_CB_WPTR_OFFSET));
+	else
+		wptr = le32_to_cpu(RREG32(GSGPU_INT_CB_WPTR_OFFSET));
+
+	return (wptr & adev->irq.ih.ptr_mask);
+}
+
+static bool ih_func_prescreen_iv(struct gsgpu_device *adev)
+{
+	u32 ring_index = adev->irq.ih.rptr;
+	u16 pasid;
+
+	switch (le32_to_cpu(adev->irq.ih.ring[ring_index]) & 0xff) {
+	case GSGPU_SRCID_GFX_PAGE_INV_FAULT:
+	case GSGPU_SRCID_GFX_MEM_PROT_FAULT:
+		pasid = le32_to_cpu(adev->irq.ih.ring[ring_index + 2]) >> 16;
+		if (!pasid || gsgpu_vm_pasid_fault_credit(adev, pasid))
+			return true;
+		break;
+	default:
+		/* Not a VM fault */
+		return true;
+	}
+
+	adev->irq.ih.rptr += 4;
+	return false;
+}
+
+static void ih_func_decode_iv(struct gsgpu_device *adev,
+				 struct gsgpu_iv_entry *entry)
+{
+	/* wptr/rptr are in bytes! */
+	u32 ring_index = adev->irq.ih.rptr;
+	uint32_t dw[4];
+
+	dw[0] = le32_to_cpu(adev->irq.ih.ring[ring_index + 0]);
+	dw[1] = le32_to_cpu(adev->irq.ih.ring[ring_index + 1]);
+	dw[2] = le32_to_cpu(adev->irq.ih.ring[ring_index + 2]);
+	dw[3] = le32_to_cpu(adev->irq.ih.ring[ring_index + 3]);
+
+	DRM_DEBUG("ih_func_decode_iv dw0 %x dw1 %x dw2 %x dw3 %x \n", dw[0], dw[1], dw[2], dw[3]);
+
+	entry->client_id = GSGPU_IH_CLIENTID_LEGACY;
+	entry->src_id = dw[0] & 0xff;
+	entry->src_data[0] = dw[1] & 0xffffffff;
+	entry->ring_id = dw[2] & 0xff;
+	entry->vmid = (dw[2] >> 8) & 0xff;
+	entry->pasid = (dw[2] >> 16) & 0xffff;
+	entry->src_data[1] = dw[3];
+
+	/* wptr/rptr are in bytes! */
+	adev->irq.ih.rptr += 4;
+}
+
+static void ih_func_set_rptr(struct gsgpu_device *adev)
+{
+	WREG32(GSGPU_INT_CB_RPTR_OFFSET, adev->irq.ih.rptr);
+}
+
+static int gsgpu_ih_early_init(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	gsgpu_ih_set_interrupt_funcs(adev);
+
+	return 0;
+}
+
+static int gsgpu_ih_sw_init(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = gsgpu_ih_ring_init(adev, 4 * 1024, true);
+	if (r)
+		return r;
+
+	r = gsgpu_irq_init(adev);
+	if (r)
+		return r;
+
+	r = gsgpu_hw_sema_mgr_init(adev);
+
+	return r;
+}
+
+static int gsgpu_ih_sw_fini(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	gsgpu_irq_fini(adev);
+	gsgpu_ih_ring_fini(adev);
+	gsgpu_hw_sema_mgr_fini(adev);
+
+	return 0;
+}
+
+static int gsgpu_ih_hw_init(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = gsgpu_ih_irq_init(adev);
+	if (r)
+		return r;
+
+	return 0;
+}
+
+static int gsgpu_ih_hw_fini(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	gsgpu_ih_irq_disable(adev);
+
+	return 0;
+}
+
+static int gsgpu_ih_suspend(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	return gsgpu_ih_hw_fini(adev);
+}
+
+static int gsgpu_ih_resume(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	return gsgpu_ih_hw_init(adev);
+}
+
+static bool gsgpu_ih_is_idle(void *handle)
+{
+
+	return true;
+}
+
+static int gsgpu_ih_wait_for_idle(void *handle)
+{
+	return 0;
+}
+
+static const struct gsgpu_ip_funcs gsgpu_ih_ip_funcs = {
+	.name = "gsgpu_ih",
+	.early_init = gsgpu_ih_early_init,
+	.late_init = NULL,
+	.sw_init = gsgpu_ih_sw_init,
+	.sw_fini = gsgpu_ih_sw_fini,
+	.hw_init = gsgpu_ih_hw_init,
+	.hw_fini = gsgpu_ih_hw_fini,
+	.suspend = gsgpu_ih_suspend,
+	.resume = gsgpu_ih_resume,
+	.is_idle = gsgpu_ih_is_idle,
+	.wait_for_idle = gsgpu_ih_wait_for_idle,
+};
+
+static const struct gsgpu_ih_funcs gsgpu_ih_funcs = {
+	.get_wptr = ih_func_get_wptr,
+	.prescreen_iv = ih_func_prescreen_iv,
+	.decode_iv = ih_func_decode_iv,
+	.set_rptr = ih_func_set_rptr
+};
+
+static void gsgpu_ih_set_interrupt_funcs(struct gsgpu_device *adev)
+{
+	if (adev->irq.ih_funcs == NULL)
+		adev->irq.ih_funcs = &gsgpu_ih_funcs;
+}
+
+const struct gsgpu_ip_block_version gsgpu_ih_ip_block = {
+	.type = GSGPU_IP_BLOCK_TYPE_IH,
+	.major = 3,
+	.minor = 0,
+	.rev = 0,
+	.funcs = &gsgpu_ih_ip_funcs,
+};
+
+/**
+ * gsgpu_ih_ring_alloc - allocate memory for the IH ring
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Allocate a ring buffer for the interrupt controller.
+ * Returns 0 for success, errors for failure.
+ */
+static int gsgpu_ih_ring_alloc(struct gsgpu_device *adev)
+{
+	int r;
+
+	/* Allocate ring buffer */
+	if (adev->irq.ih.ring_obj == NULL) {
+		r = gsgpu_bo_create_kernel(adev, adev->irq.ih.ring_size,
+					    PAGE_SIZE, GSGPU_GEM_DOMAIN_GTT,
+					    &adev->irq.ih.ring_obj,
+					    &adev->irq.ih.gpu_addr,
+					    (void **)&adev->irq.ih.ring);
+		if (r) {
+			DRM_ERROR("gsgpu: failed to create ih ring buffer (%d).\n", r);
+			return r;
+		}
+	}
+	return 0;
+}
+
+/**
+ * gsgpu_ih_ring_init - initialize the IH state
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Initializes the IH state and allocates a buffer
+ * for the IH ring buffer.
+ * Returns 0 for success, errors for failure.
+ */
+int gsgpu_ih_ring_init(struct gsgpu_device *adev, unsigned ring_size,
+			bool use_bus_addr)
+{
+	u32 rb_bufsz;
+	int r;
+
+	/* Align ring size */
+	rb_bufsz = order_base_2(ring_size / 4);
+	ring_size = (1 << rb_bufsz) * 4;
+	adev->irq.ih.ring_size = ring_size;
+	adev->irq.ih.ptr_mask = adev->irq.ih.ring_size / 4 - 1;
+	adev->irq.ih.rptr = 0;
+	adev->irq.ih.use_bus_addr = use_bus_addr;
+
+	if (adev->irq.ih.use_bus_addr) {
+		if (!adev->irq.ih.ring) {
+			/* add 8 bytes for the rptr/wptr shadows and
+			 * add them to the end of the ring allocation.
+			 */
+			adev->irq.ih.ring = pci_alloc_consistent(adev->pdev,
+								 adev->irq.ih.ring_size + 8,
+								 &adev->irq.ih.rb_dma_addr);
+			if (adev->irq.ih.ring == NULL)
+				return -ENOMEM;
+			memset((void *)adev->irq.ih.ring, 0, adev->irq.ih.ring_size + 8);
+			adev->irq.ih.wptr_offs = (adev->irq.ih.ring_size / 4) + 0;
+			adev->irq.ih.rptr_offs = (adev->irq.ih.ring_size / 4) + 1;
+		}
+		return 0;
+	} else {
+		r = gsgpu_device_wb_get(adev, &adev->irq.ih.wptr_offs);
+		if (r) {
+			dev_err(adev->dev, "(%d) ih wptr_offs wb alloc failed\n", r);
+			return r;
+		}
+
+		r = gsgpu_device_wb_get(adev, &adev->irq.ih.rptr_offs);
+		if (r) {
+			gsgpu_device_wb_free(adev, adev->irq.ih.wptr_offs);
+			dev_err(adev->dev, "(%d) ih rptr_offs wb alloc failed\n", r);
+			return r;
+		}
+
+		return gsgpu_ih_ring_alloc(adev);
+	}
+}
+
+/**
+ * gsgpu_ih_ring_fini - tear down the IH state
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Tears down the IH state and frees buffer
+ * used for the IH ring buffer.
+ */
+void gsgpu_ih_ring_fini(struct gsgpu_device *adev)
+{
+	if (adev->irq.ih.use_bus_addr) {
+		if (adev->irq.ih.ring) {
+			/* add 8 bytes for the rptr/wptr shadows and
+			 * add them to the end of the ring allocation.
+			 */
+			pci_free_consistent(adev->pdev, adev->irq.ih.ring_size + 8,
+					    (void *)adev->irq.ih.ring,
+					    adev->irq.ih.rb_dma_addr);
+			adev->irq.ih.ring = NULL;
+		}
+	} else {
+		gsgpu_bo_free_kernel(&adev->irq.ih.ring_obj,
+				      &adev->irq.ih.gpu_addr,
+				      (void **)&adev->irq.ih.ring);
+		gsgpu_device_wb_free(adev, adev->irq.ih.wptr_offs);
+		gsgpu_device_wb_free(adev, adev->irq.ih.rptr_offs);
+	}
+}
+
+/**
+ * gsgpu_ih_process - interrupt handler
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Interrupt hander  , walk the IH ring.
+ * Returns irq process return code.
+ */
+int gsgpu_ih_process(struct gsgpu_device *adev)
+{
+	struct gsgpu_iv_entry entry;
+	u32 wptr;
+
+	if (!adev->irq.ih.enabled || adev->shutdown)
+		return IRQ_NONE;
+
+	if (!adev->irq.msi_enabled)
+		WREG32(GSGPU_HOST_INT, 0);
+
+	wptr = gsgpu_ih_get_wptr(adev);
+
+restart_ih:
+	/* is somebody else already processing irqs? */
+	if (atomic_xchg(&adev->irq.ih.lock, 1))
+		return IRQ_NONE;
+
+	DRM_DEBUG("%s: rptr %d, wptr %d\n", __func__, adev->irq.ih.rptr, wptr);
+
+	/* Order reading of wptr vs. reading of IH ring data */
+	rmb();
+
+	while (adev->irq.ih.rptr != wptr) {
+		u32 ring_index = adev->irq.ih.rptr;
+
+		/* Prescreening of high-frequency interrupts */
+		if (!gsgpu_ih_prescreen_iv(adev)) {
+			adev->irq.ih.rptr &= adev->irq.ih.ptr_mask;
+			continue;
+		}
+
+		entry.iv_entry = (const uint32_t *)
+			&adev->irq.ih.ring[ring_index];
+		gsgpu_ih_decode_iv(adev, &entry);
+		adev->irq.ih.rptr &= adev->irq.ih.ptr_mask;
+
+		gsgpu_irq_dispatch(adev, &entry);
+	}
+	gsgpu_ih_set_rptr(adev);
+	atomic_set(&adev->irq.ih.lock, 0);
+
+	/* make sure wptr hasn't changed while processing */
+	wptr = gsgpu_ih_get_wptr(adev);
+	if (wptr != adev->irq.ih.rptr)
+		goto restart_ih;
+
+	return IRQ_HANDLED;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ioc32.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ioc32.c
new file mode 100644
index 000000000000..2f35cb228803
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ioc32.c
@@ -0,0 +1,18 @@
+#include <linux/compat.h>
+
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu_drv.h"
+
+long gsgpu_kms_compat_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	unsigned int nr = DRM_IOCTL_NR(cmd);
+	int ret;
+
+	if (nr < DRM_COMMAND_BASE)
+		return drm_compat_ioctl(filp, cmd, arg);
+
+	ret = gsgpu_drm_ioctl(filp, cmd, arg);
+
+	return ret;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_irq.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_irq.c
new file mode 100644
index 000000000000..2962a699fd9d
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_irq.c
@@ -0,0 +1,489 @@
+#include <linux/irq.h>
+#include <linux/pm_runtime.h>
+
+#include <drm/drmP.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/gsgpu_drm.h>
+
+#include "gsgpu.h"
+#include "gsgpu_ih.h"
+#include "gsgpu_trace.h"
+#include "gsgpu_dc_irq.h"
+#include "gsgpu_dc_reg.h"
+#include "gsgpu_irq.h"
+
+#define GSGPU_WAIT_IDLE_TIMEOUT 200
+
+/**
+ * gsgpu_irq_reset_work_func - execute GPU reset
+ *
+ * @work: work struct pointer
+ *
+ * Execute scheduled GPU reset (Cayman+).
+ * This function is called when the IRQ handler thinks we need a GPU reset.
+ */
+static void gsgpu_irq_reset_work_func(struct work_struct *work)
+{
+	struct gsgpu_device *adev = container_of(work, struct gsgpu_device,
+						  reset_work);
+
+	gsgpu_device_gpu_recover(adev, NULL, false);
+}
+
+/**
+ * gsgpu_irq_disable_all - disable *all* interrupts
+ *
+ * @adev: gsgpu device pointer
+ *
+ * Disable all types of interrupts from all sources.
+ */
+void gsgpu_irq_disable_all(struct gsgpu_device *adev)
+{
+	unsigned long irqflags;
+	unsigned i, j, k;
+	int r;
+
+	spin_lock_irqsave(&adev->irq.lock, irqflags);
+	for (i = 0; i < GSGPU_IH_CLIENTID_MAX; ++i) {
+		if (!adev->irq.client[i].sources)
+			continue;
+
+		for (j = 0; j < GSGPU_MAX_IRQ_SRC_ID; ++j) {
+			struct gsgpu_irq_src *src = adev->irq.client[i].sources[j];
+
+			if (!src || !src->funcs->set || !src->num_types)
+				continue;
+
+			for (k = 0; k < src->num_types; ++k) {
+				atomic_set(&src->enabled_types[k], 0);
+				r = src->funcs->set(adev, src, k,
+						    GSGPU_IRQ_STATE_DISABLE);
+				if (r)
+					DRM_ERROR("error disabling interrupt (%d)\n",
+						  r);
+			}
+		}
+	}
+	spin_unlock_irqrestore(&adev->irq.lock, irqflags);
+}
+
+/**
+ * gsgpu_irq_handler - IRQ handler
+ *
+ * @irq: IRQ number (unused)
+ * @arg: pointer to DRM device
+ *
+ * IRQ handler for gsgpu driver (all ASICs).
+ *
+ * Returns:
+ * result of handling the IRQ, as defined by &irqreturn_t
+ */
+irqreturn_t gsgpu_irq_handler(int irq, void *arg)
+{
+	struct drm_device *dev = (struct drm_device *) arg;
+	struct gsgpu_device *adev = dev->dev_private;
+	irqreturn_t ret;
+
+	ret = gsgpu_ih_process(adev);
+	if (ret == IRQ_HANDLED)
+		pm_runtime_mark_last_busy(dev->dev);
+	return ret;
+}
+
+static irqreturn_t gsgpu_dc_irq_handler(int irq, void *arg)
+{
+	u32 int_reg;
+	unsigned long base;
+	struct gsgpu_iv_entry entry;
+	struct gsgpu_device *adev = (struct gsgpu_device *)arg;
+	int i = 1;
+
+	base = (unsigned long)(adev->loongson_dc_rmmio_base);
+
+	int_reg = dc_readl(adev, DC_INT_REG);
+	dc_writel(adev, DC_INT_REG, int_reg);
+	entry.client_id = SOC15_IH_CLIENTID_DCE;
+
+	int_reg &= 0xffff;
+	while (int_reg && (i < DC_INT_ID_MAX)) {
+		if (!(int_reg & 0x1)) {
+			int_reg = int_reg >> 1;
+			i++;
+			continue;
+		}
+
+		entry.src_id = i;
+		gsgpu_irq_dispatch(adev, &entry);
+
+		int_reg = int_reg >> 1;
+		i++;
+	}
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * gsgpu_msi_ok - check whether MSI functionality is enabled
+ *
+ * @adev: gsgpu device pointer (unused)
+ *
+ * Checks whether MSI functionality has been disabled via module parameter
+ * (all ASICs).
+ *
+ * Returns:
+ * *true* if MSIs are allowed to be enabled or *false* otherwise
+ */
+static bool gsgpu_msi_ok(struct gsgpu_device *adev)
+{
+	if (gsgpu_msi == 1)
+		return true;
+	else if (gsgpu_msi == 0)
+		return false;
+
+	return true;
+}
+
+/**
+ * gsgpu_irq_init - initialize interrupt handling
+ *
+ * @adev: gsgpu device pointer
+ *
+ * Sets up work functions for hotplug and reset interrupts, enables MSI
+ * functionality, initializes vblank, hotplug and reset interrupt handling.
+ *
+ * Returns:
+ * 0 on success or error code on failure
+ */
+int gsgpu_irq_init(struct gsgpu_device *adev)
+{
+	int r = 0;
+	struct pci_dev *loongson_dc;
+
+	spin_lock_init(&adev->irq.lock);
+
+	/* Enable MSI if not disabled by module parameter */
+	adev->irq.msi_enabled = false;
+
+	if (gsgpu_msi_ok(adev)) {
+		int ret = pci_enable_msi(adev->pdev);
+		if (!ret) {
+			adev->irq.msi_enabled = true;
+			dev_dbg(adev->dev, "gsgpu: using MSI.\n");
+		}
+	}
+
+	INIT_WORK(&adev->reset_work, gsgpu_irq_reset_work_func);
+
+	adev->irq.installed = true;
+	r = drm_irq_install(adev->ddev, adev->ddev->pdev->irq);
+	if (r) {
+		adev->irq.installed = false;
+		cancel_work_sync(&adev->reset_work);
+		return r;
+	}
+	adev->ddev->max_vblank_count = 0x00ffffff;
+
+	loongson_dc = adev->loongson_dc;
+	if (loongson_dc) {
+		u32 dc_irq = loongson_dc->irq;
+		r = request_irq(dc_irq, gsgpu_dc_irq_handler, 0,
+				loongson_dc->driver->name, adev);
+		if (r) {
+			DRM_ERROR("gsgpu register dc irq failed\n");
+			return r;
+		}
+	} else {
+		DRM_ERROR("gsgpu dc device is null\n");
+		return ENODEV;
+	}
+
+	DRM_DEBUG("gsgpu: irq initialized.\n");
+	return 0;
+}
+
+/**
+ * gsgpu_irq_fini - shut down interrupt handling
+ *
+ * @adev: gsgpu device pointer
+ *
+ * Tears down work functions for hotplug and reset interrupts, disables MSI
+ * functionality, shuts down vblank, hotplug and reset interrupt handling,
+ * turns off interrupts from all sources (all ASICs).
+ */
+void gsgpu_irq_fini(struct gsgpu_device *adev)
+{
+	unsigned i, j;
+
+	if (adev->irq.installed) {
+		drm_irq_uninstall(adev->ddev);
+		adev->irq.installed = false;
+		if (adev->irq.msi_enabled)
+			pci_disable_msi(adev->pdev);
+		cancel_work_sync(&adev->reset_work);
+	}
+
+	for (i = 0; i < GSGPU_IH_CLIENTID_MAX; ++i) {
+		if (!adev->irq.client[i].sources)
+			continue;
+
+		for (j = 0; j < GSGPU_MAX_IRQ_SRC_ID; ++j) {
+			struct gsgpu_irq_src *src = adev->irq.client[i].sources[j];
+
+			if (!src)
+				continue;
+
+			kfree(src->enabled_types);
+			src->enabled_types = NULL;
+			if (src->data) {
+				kfree(src->data);
+				kfree(src);
+				adev->irq.client[i].sources[j] = NULL;
+			}
+		}
+		kfree(adev->irq.client[i].sources);
+		adev->irq.client[i].sources = NULL;
+	}
+}
+
+/**
+ * gsgpu_irq_add_id - register IRQ source
+ *
+ * @adev: gsgpu device pointer
+ * @client_id: client id
+ * @src_id: source id
+ * @source: IRQ source pointer
+ *
+ * Registers IRQ source on a client.
+ *
+ * Returns:
+ * 0 on success or error code otherwise
+ */
+int gsgpu_irq_add_id(struct gsgpu_device *adev,
+		      unsigned client_id, unsigned src_id,
+		      struct gsgpu_irq_src *source)
+{
+	if (client_id >= GSGPU_IH_CLIENTID_MAX)
+		return -EINVAL;
+
+	if (src_id >= GSGPU_MAX_IRQ_SRC_ID)
+		return -EINVAL;
+
+	if (!source->funcs)
+		return -EINVAL;
+
+	if (!adev->irq.client[client_id].sources) {
+		adev->irq.client[client_id].sources =
+			kcalloc(GSGPU_MAX_IRQ_SRC_ID,
+				sizeof(struct gsgpu_irq_src *),
+				GFP_KERNEL);
+		if (!adev->irq.client[client_id].sources)
+			return -ENOMEM;
+	}
+
+	if (adev->irq.client[client_id].sources[src_id] != NULL)
+		return -EINVAL;
+
+	if (source->num_types && !source->enabled_types) {
+		atomic_t *types;
+
+		types = kcalloc(source->num_types, sizeof(atomic_t),
+				GFP_KERNEL);
+		if (!types)
+			return -ENOMEM;
+
+		source->enabled_types = types;
+	}
+
+	adev->irq.client[client_id].sources[src_id] = source;
+	return 0;
+}
+
+/**
+ * gsgpu_irq_dispatch - dispatch IRQ to IP blocks
+ *
+ * @adev: gsgpu device pointer
+ * @entry: interrupt vector pointer
+ *
+ * Dispatches IRQ to IP blocks.
+ */
+void gsgpu_irq_dispatch(struct gsgpu_device *adev,
+			 struct gsgpu_iv_entry *entry)
+{
+	unsigned client_id = entry->client_id;
+	unsigned src_id = entry->src_id;
+	struct gsgpu_irq_src *src;
+	int r;
+
+	trace_gsgpu_iv(entry);
+
+	if (client_id >= GSGPU_IH_CLIENTID_MAX) {
+		DRM_DEBUG("Invalid client_id in IV: %d\n", client_id);
+		return;
+	}
+
+	if (src_id >= GSGPU_MAX_IRQ_SRC_ID) {
+		DRM_DEBUG("Invalid src_id in IV: %d\n", src_id);
+		return;
+	}
+
+	if (!adev->irq.client[client_id].sources) {
+		DRM_DEBUG("Unregistered interrupt client_id: %d src_id: %d\n",
+			  client_id, src_id);
+		return;
+	}
+
+	src = adev->irq.client[client_id].sources[src_id];
+	if (!src) {
+		DRM_DEBUG("Unhandled interrupt src_id: %d\n", src_id);
+		return;
+	}
+
+	r = src->funcs->process(adev, src, entry);
+	if (r)
+		DRM_ERROR("error processing interrupt (%d)\n", r);
+}
+
+/**
+ * gsgpu_irq_update - update hardware interrupt state
+ *
+ * @adev: gsgpu device pointer
+ * @src: interrupt source pointer
+ * @type: type of interrupt
+ *
+ * Updates interrupt state for the specific source (all ASICs).
+ */
+int gsgpu_irq_update(struct gsgpu_device *adev,
+			     struct gsgpu_irq_src *src, unsigned type)
+{
+	unsigned long irqflags;
+	enum gsgpu_interrupt_state state;
+	int r;
+
+	spin_lock_irqsave(&adev->irq.lock, irqflags);
+
+	/* We need to determine after taking the lock, otherwise
+	   we might disable just enabled interrupts again */
+	if (gsgpu_irq_enabled(adev, src, type))
+		state = GSGPU_IRQ_STATE_ENABLE;
+	else
+		state = GSGPU_IRQ_STATE_DISABLE;
+
+	r = src->funcs->set(adev, src, type, state);
+	spin_unlock_irqrestore(&adev->irq.lock, irqflags);
+	return r;
+}
+
+/**
+ * gsgpu_irq_gpu_reset_resume_helper - update interrupt states on all sources
+ *
+ * @adev: gsgpu device pointer
+ *
+ * Updates state of all types of interrupts on all sources on resume after
+ * reset.
+ */
+void gsgpu_irq_gpu_reset_resume_helper(struct gsgpu_device *adev)
+{
+	int i, j, k;
+
+	for (i = 0; i < GSGPU_IH_CLIENTID_MAX; ++i) {
+		if (!adev->irq.client[i].sources)
+			continue;
+
+		for (j = 0; j < GSGPU_MAX_IRQ_SRC_ID; ++j) {
+			struct gsgpu_irq_src *src = adev->irq.client[i].sources[j];
+
+			if (!src)
+				continue;
+			for (k = 0; k < src->num_types; k++)
+				gsgpu_irq_update(adev, src, k);
+		}
+	}
+}
+
+/**
+ * gsgpu_irq_get - enable interrupt
+ *
+ * @adev: gsgpu device pointer
+ * @src: interrupt source pointer
+ * @type: type of interrupt
+ *
+ * Enables specified type of interrupt on the specified source (all ASICs).
+ *
+ * Returns:
+ * 0 on success or error code otherwise
+ */
+int gsgpu_irq_get(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
+		   unsigned type)
+{
+	if (!adev->ddev->irq_enabled)
+		return -ENOENT;
+
+	if (type >= src->num_types)
+		return -EINVAL;
+
+	if (!src->enabled_types || !src->funcs->set)
+		return -EINVAL;
+
+	if (atomic_inc_return(&src->enabled_types[type]) == 1)
+		return gsgpu_irq_update(adev, src, type);
+
+	return 0;
+}
+
+/**
+ * gsgpu_irq_put - disable interrupt
+ *
+ * @adev: gsgpu device pointer
+ * @src: interrupt source pointer
+ * @type: type of interrupt
+ *
+ * Enables specified type of interrupt on the specified source (all ASICs).
+ *
+ * Returns:
+ * 0 on success or error code otherwise
+ */
+int gsgpu_irq_put(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
+		   unsigned type)
+{
+	if (!adev->ddev->irq_enabled)
+		return -ENOENT;
+
+	if (type >= src->num_types)
+		return -EINVAL;
+
+	if (!src->enabled_types || !src->funcs->set)
+		return -EINVAL;
+
+	if (atomic_dec_and_test(&src->enabled_types[type]))
+		return gsgpu_irq_update(adev, src, type);
+
+	return 0;
+}
+
+/**
+ * gsgpu_irq_enabled - check whether interrupt is enabled or not
+ *
+ * @adev: gsgpu device pointer
+ * @src: interrupt source pointer
+ * @type: type of interrupt
+ *
+ * Checks whether the given type of interrupt is enabled on the given source.
+ *
+ * Returns:
+ * *true* if interrupt is enabled, *false* if interrupt is disabled or on
+ * invalid parameters
+ */
+bool gsgpu_irq_enabled(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
+			unsigned type)
+{
+	if (!adev->ddev->irq_enabled)
+		return false;
+
+	if (type >= src->num_types)
+		return false;
+
+	if (!src->enabled_types || !src->funcs->set)
+		return false;
+
+	return !!atomic_read(&src->enabled_types[type]);
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_job.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_job.c
new file mode 100644
index 000000000000..4800268da66d
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_job.c
@@ -0,0 +1,216 @@
+#include <linux/kthread.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include "gsgpu_trace.h"
+
+static void gsgpu_job_timedout(struct drm_sched_job *s_job)
+{
+	struct gsgpu_ring *ring = to_gsgpu_ring(s_job->sched);
+	struct gsgpu_job *job = to_gsgpu_job(s_job);
+
+	DRM_ERROR("ring %s timeout, signaled seq=%u, emitted seq=%u\n",
+		  job->base.sched->name, atomic_read(&ring->fence_drv.last_seq),
+		  ring->fence_drv.sync_seq);
+
+	gsgpu_device_gpu_recover(ring->adev, job, false);
+}
+
+int gsgpu_job_alloc(struct gsgpu_device *adev, unsigned num_ibs,
+		     struct gsgpu_job **job, struct gsgpu_vm *vm)
+{
+	size_t size = sizeof(struct gsgpu_job);
+
+	if (num_ibs == 0)
+		return -EINVAL;
+
+	size += sizeof(struct gsgpu_ib) * num_ibs;
+
+	*job = kzalloc(size, GFP_KERNEL);
+	if (!*job)
+		return -ENOMEM;
+
+	/*
+	 * Initialize the scheduler to at least some ring so that we always
+	 * have a pointer to adev.
+	 */
+	(*job)->base.sched = &adev->rings[0]->sched;
+	(*job)->vm = vm;
+	(*job)->ibs = (void *)&(*job)[1];
+	(*job)->num_ibs = num_ibs;
+
+	gsgpu_sync_create(&(*job)->sync);
+	gsgpu_sync_create(&(*job)->sched_sync);
+	(*job)->vram_lost_counter = atomic_read(&adev->vram_lost_counter);
+	(*job)->vm_pd_addr = GSGPU_BO_INVALID_OFFSET;
+
+	return 0;
+}
+
+int gsgpu_job_alloc_with_ib(struct gsgpu_device *adev, unsigned size,
+			     struct gsgpu_job **job)
+{
+	int r;
+
+	r = gsgpu_job_alloc(adev, 1, job, NULL);
+	if (r)
+		return r;
+
+	r = gsgpu_ib_get(adev, NULL, size, &(*job)->ibs[0]);
+	if (r)
+		kfree(*job);
+	else
+		(*job)->vm_pd_addr = adev->gart.table_addr;
+
+	return r;
+}
+
+void gsgpu_job_free_resources(struct gsgpu_job *job)
+{
+	struct gsgpu_ring *ring = to_gsgpu_ring(job->base.sched);
+	struct dma_fence *f;
+	unsigned i;
+
+	/* use sched fence if available */
+	f = job->base.s_fence ? &job->base.s_fence->finished : job->fence;
+
+	for (i = 0; i < job->num_ibs; ++i)
+		gsgpu_ib_free(ring->adev, &job->ibs[i], f);
+}
+
+static void gsgpu_job_free_cb(struct drm_sched_job *s_job)
+{
+	struct gsgpu_ring *ring = to_gsgpu_ring(s_job->sched);
+	struct gsgpu_job *job = to_gsgpu_job(s_job);
+
+	gsgpu_ring_priority_put(ring, s_job->s_priority);
+	dma_fence_put(job->fence);
+	gsgpu_sync_free(&job->sync);
+	gsgpu_sync_free(&job->sched_sync);
+	kfree(job);
+}
+
+void gsgpu_job_free(struct gsgpu_job *job)
+{
+	gsgpu_job_free_resources(job);
+
+	dma_fence_put(job->fence);
+	gsgpu_sync_free(&job->sync);
+	gsgpu_sync_free(&job->sched_sync);
+	kfree(job);
+}
+
+int gsgpu_job_submit(struct gsgpu_job *job, struct drm_sched_entity *entity,
+		      void *owner, struct dma_fence **f)
+{
+	enum drm_sched_priority priority;
+	struct gsgpu_ring *ring;
+	int r;
+
+	if (!f)
+		return -EINVAL;
+
+	r = drm_sched_job_init(&job->base, entity, owner);
+	if (r)
+		return r;
+
+	job->owner = owner;
+	*f = dma_fence_get(&job->base.s_fence->finished);
+	gsgpu_job_free_resources(job);
+	priority = job->base.s_priority;
+	drm_sched_entity_push_job(&job->base, entity);
+
+	ring = to_gsgpu_ring(entity->rq->sched);
+	gsgpu_ring_priority_get(ring, priority);
+
+	return 0;
+}
+
+int gsgpu_job_submit_direct(struct gsgpu_job *job, struct gsgpu_ring *ring,
+			     struct dma_fence **fence)
+{
+	int r;
+
+	job->base.sched = &ring->sched;
+	r = gsgpu_ib_schedule(ring, job->num_ibs, job->ibs, NULL, fence);
+	job->fence = dma_fence_get(*fence);
+	if (r)
+		return r;
+
+	gsgpu_job_free(job);
+	return 0;
+}
+
+static struct dma_fence *gsgpu_job_dependency(struct drm_sched_job *sched_job,
+					       struct drm_sched_entity *s_entity)
+{
+	struct gsgpu_ring *ring = to_gsgpu_ring(s_entity->rq->sched);
+	struct gsgpu_job *job = to_gsgpu_job(sched_job);
+	struct gsgpu_vm *vm = job->vm;
+	struct dma_fence *fence;
+	bool explicit = false;
+	int r;
+
+	fence = gsgpu_sync_get_fence(&job->sync, &explicit);
+	if (fence && explicit) {
+		if (drm_sched_dependency_optimized(fence, s_entity)) {
+			r = gsgpu_sync_fence(ring->adev, &job->sched_sync,
+					      fence, false);
+			if (r)
+				DRM_ERROR("Error adding fence (%d)\n", r);
+		}
+	}
+
+	while (fence == NULL && vm && !job->vmid) {
+		r = gsgpu_vmid_grab(vm, ring, &job->sync,
+				     &job->base.s_fence->finished,
+				     job);
+		if (r)
+			DRM_ERROR("Error getting VM ID (%d)\n", r);
+
+		fence = gsgpu_sync_get_fence(&job->sync, NULL);
+	}
+
+	return fence;
+}
+
+static struct dma_fence *gsgpu_job_run(struct drm_sched_job *sched_job)
+{
+	struct gsgpu_ring *ring = to_gsgpu_ring(sched_job->sched);
+	struct dma_fence *fence = NULL, *finished;
+	struct gsgpu_job *job;
+	int r;
+
+	job = to_gsgpu_job(sched_job);
+	finished = &job->base.s_fence->finished;
+
+	BUG_ON(gsgpu_sync_peek_fence(&job->sync, NULL));
+
+	trace_gsgpu_sched_run_job(job);
+
+	if (job->vram_lost_counter != atomic_read(&ring->adev->vram_lost_counter))
+		dma_fence_set_error(finished, -ECANCELED);/* skip IB as well if VRAM lost */
+
+	if (finished->error < 0) {
+		DRM_INFO("Skip scheduling IBs!\n");
+	} else {
+		r = gsgpu_ib_schedule(ring, job->num_ibs, job->ibs, job,
+				       &fence);
+		if (r)
+			DRM_ERROR("Error scheduling IBs (%d)\n", r);
+	}
+	/* if gpu reset, hw fence will be replaced here */
+	dma_fence_put(job->fence);
+	job->fence = dma_fence_get(fence);
+
+	gsgpu_job_free_resources(job);
+	return fence;
+}
+
+const struct drm_sched_backend_ops gsgpu_sched_ops = {
+	.dependency = gsgpu_job_dependency,
+	.run_job = gsgpu_job_run,
+	.timedout_job = gsgpu_job_timedout,
+	.free_job = gsgpu_job_free_cb
+};
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_kms.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_kms.c
new file mode 100644
index 000000000000..3abea3fc19ee
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_kms.c
@@ -0,0 +1,873 @@
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include <drm/gsgpu_drm.h>
+#include "gsgpu_sched.h"
+
+#include <linux/vga_switcheroo.h>
+#include <linux/slab.h>
+#include <linux/pm_runtime.h>
+
+/**
+ * gsgpu_driver_unload_kms - Main unload function for KMS.
+ *
+ * @dev: drm dev pointer
+ *
+ * This is the main unload function for KMS (all asics).
+ * Returns 0 on success.
+ */
+void gsgpu_driver_unload_kms(struct drm_device *dev)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+
+	if (adev == NULL)
+		return;
+
+	if (adev->rmmio == NULL)
+		goto done_free;
+
+	gsgpu_device_fini(adev);
+
+done_free:
+	kfree(adev);
+	dev->dev_private = NULL;
+}
+
+/**
+ * gsgpu_driver_load_kms - Main load function for KMS.
+ *
+ * @dev: drm dev pointer
+ * @flags: device flags
+ *
+ * This is the main load function for KMS (all asics).
+ * Returns 0 on success, error on failure.
+ */
+int gsgpu_driver_load_kms(struct drm_device *dev, unsigned long flags)
+{
+	struct gsgpu_device *adev;
+	int r;
+
+	*(int *)(0x80000e0010010444) |= 0x10;
+
+	adev = kzalloc(sizeof(struct gsgpu_device), GFP_KERNEL);
+	if (adev == NULL) {
+		return -ENOMEM;
+	}
+	dev->dev_private = (void *)adev;
+
+	if ((gsgpu_runtime_pm != 0) &&
+	    ((flags & GSGPU_IS_APU) == 0))
+		flags |= GSGPU_IS_PX;
+
+	/* gsgpu_device_init should report only fatal error
+	 * like memory allocation failure or iomapping failure,
+	 * or memory manager initialization failure, it must
+	 * properly initialize the GPU MC controller and permit
+	 * VRAM allocation
+	 */
+	r = gsgpu_device_init(adev, dev, dev->pdev, flags);
+	if (r) {
+		dev_err(&dev->pdev->dev, "Fatal error during GPU init\n");
+		goto out;
+	}
+
+out:
+	if (r)
+		gsgpu_driver_unload_kms(dev);
+
+	return r;
+}
+
+static int gsgpu_firmware_info(struct drm_gsgpu_info_firmware *fw_info,
+				struct drm_gsgpu_query_fw *query_fw,
+				struct gsgpu_device *adev)
+{
+	switch (query_fw->fw_type) {
+	case GSGPU_INFO_FW_VCE:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_UVD:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_VCN:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_GMC:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_GFX_ME:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_GFX_PFP:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_GFX_CE:
+		fw_info->ver = adev->gfx.cp_fw_version;
+		fw_info->feature = adev->gfx.cp_feature_version;
+		break;
+	case GSGPU_INFO_FW_GFX_RLC:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_GFX_RLC_RESTORE_LIST_CNTL:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_GFX_RLC_RESTORE_LIST_GPM_MEM:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_GFX_RLC_RESTORE_LIST_SRM_MEM:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_GFX_MEC:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_SMC:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_XDMA:
+		if (query_fw->index >= adev->xdma.num_instances)
+			return -EINVAL;
+		fw_info->ver = adev->xdma.instance[query_fw->index].fw_version;
+		fw_info->feature = adev->xdma.instance[query_fw->index].feature_version;
+		break;
+	case GSGPU_INFO_FW_SOS:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	case GSGPU_INFO_FW_ASD:
+		fw_info->ver = 0;
+		fw_info->feature = 0;
+		break;
+	default:
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * Userspace get information ioctl
+ */
+/**
+ * gsgpu_info_ioctl - answer a device specific request.
+ *
+ * @adev: gsgpu device pointer
+ * @data: request object
+ * @filp: drm filp
+ *
+ * This function is used to pass device specific parameters to the userspace
+ * drivers.  Examples include: pci device id, pipeline parms, tiling params,
+ * etc. (all asics).
+ * Returns 0 on success, -EINVAL on failure.
+ */
+static int gsgpu_info_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	struct drm_gsgpu_info *info = data;
+	struct gsgpu_mode_info *minfo = &adev->mode_info;
+	void __user *out = (void __user *)(uintptr_t)info->return_pointer;
+	uint32_t size = info->return_size;
+	struct drm_crtc *crtc;
+	uint32_t ui32 = 0;
+	uint64_t ui64 = 0;
+	int i, found;
+
+	if (!info->return_size || !info->return_pointer)
+		return -EINVAL;
+
+	switch (info->query) {
+	case GSGPU_INFO_ACCEL_WORKING:
+		ui32 = adev->accel_working;
+		return copy_to_user(out, &ui32, min(size, 4u)) ? -EFAULT : 0;
+	case GSGPU_INFO_CRTC_FROM_ID:
+		for (i = 0, found = 0; i < adev->mode_info.num_crtc; i++) {
+			crtc = (struct drm_crtc *)minfo->crtcs[i];
+			if (crtc && crtc->base.id == info->mode_crtc.id) {
+				struct gsgpu_crtc *gsgpu_crtc = to_gsgpu_crtc(crtc);
+				ui32 = gsgpu_crtc->crtc_id;
+				found = 1;
+				break;
+			}
+		}
+		if (!found) {
+			DRM_DEBUG_KMS("unknown crtc id %d\n", info->mode_crtc.id);
+			return -EINVAL;
+		}
+		return copy_to_user(out, &ui32, min(size, 4u)) ? -EFAULT : 0;
+	case GSGPU_INFO_HW_IP_INFO: {
+		struct drm_gsgpu_info_hw_ip ip = {};
+		enum gsgpu_ip_block_type type;
+		uint32_t ring_mask = 0;
+		uint32_t ib_start_alignment = 0;
+		uint32_t ib_size_alignment = 0;
+
+		if (info->query_hw_ip.ip_instance >= GSGPU_HW_IP_INSTANCE_MAX_COUNT)
+			return -EINVAL;
+
+		switch (info->query_hw_ip.type) {
+		case GSGPU_HW_IP_GFX:
+			type = GSGPU_IP_BLOCK_TYPE_GFX;
+			for (i = 0; i < adev->gfx.num_gfx_rings; i++)
+				ring_mask |= adev->gfx.gfx_ring[i].ready << i;
+			ib_start_alignment = 32;
+			ib_size_alignment = 8;
+			break;
+		case GSGPU_HW_IP_DMA:
+			type = GSGPU_IP_BLOCK_TYPE_XDMA;
+			for (i = 0; i < adev->xdma.num_instances; i++)
+				ring_mask |= adev->xdma.instance[i].ring.ready << i;
+			ib_start_alignment = 256;
+			ib_size_alignment = 8;
+			break;
+		default:
+			return -EINVAL;
+		}
+
+		for (i = 0; i < adev->num_ip_blocks; i++) {
+			if (adev->ip_blocks[i].version->type == type &&
+			    adev->ip_blocks[i].status.valid) {
+				ip.hw_ip_version_major = adev->ip_blocks[i].version->major;
+				ip.hw_ip_version_minor = adev->ip_blocks[i].version->minor;
+				ip.capabilities_flags = 0;
+				ip.available_rings = ring_mask;
+				ip.ib_start_alignment = ib_start_alignment;
+				ip.ib_size_alignment = ib_size_alignment;
+				break;
+			}
+		}
+		return copy_to_user(out, &ip,
+				    min((size_t)size, sizeof(ip))) ? -EFAULT : 0;
+	}
+	case GSGPU_INFO_HW_IP_COUNT: {
+		enum gsgpu_ip_block_type type;
+		uint32_t count = 0;
+
+		switch (info->query_hw_ip.type) {
+		case GSGPU_HW_IP_GFX:
+			type = GSGPU_IP_BLOCK_TYPE_GFX;
+			break;
+		case GSGPU_HW_IP_DMA:
+			type = GSGPU_IP_BLOCK_TYPE_XDMA;
+			break;
+		default:
+			return -EINVAL;
+		}
+
+		for (i = 0; i < adev->num_ip_blocks; i++)
+			if (adev->ip_blocks[i].version->type == type &&
+			    adev->ip_blocks[i].status.valid &&
+			    count < GSGPU_HW_IP_INSTANCE_MAX_COUNT)
+				count++;
+
+		return copy_to_user(out, &count, min(size, 4u)) ? -EFAULT : 0;
+	}
+	case GSGPU_INFO_TIMESTAMP:
+		ui64 = gsgpu_gfx_get_gpu_clock_counter(adev);
+		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+	case GSGPU_INFO_FW_VERSION: {
+		struct drm_gsgpu_info_firmware fw_info;
+		int ret;
+
+		/* We only support one instance of each IP block right now. */
+		if (info->query_fw.ip_instance != 0)
+			return -EINVAL;
+
+		ret = gsgpu_firmware_info(&fw_info, &info->query_fw, adev);
+		if (ret)
+			return ret;
+
+		return copy_to_user(out, &fw_info,
+				    min((size_t)size, sizeof(fw_info))) ? -EFAULT : 0;
+	}
+	case GSGPU_INFO_NUM_BYTES_MOVED:
+		ui64 = atomic64_read(&adev->num_bytes_moved);
+		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+	case GSGPU_INFO_NUM_EVICTIONS:
+		ui64 = atomic64_read(&adev->num_evictions);
+		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+	case GSGPU_INFO_NUM_VRAM_CPU_PAGE_FAULTS:
+		ui64 = atomic64_read(&adev->num_vram_cpu_page_faults);
+		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+	case GSGPU_INFO_VRAM_USAGE:
+		ui64 = gsgpu_vram_mgr_usage(&adev->mman.bdev.man[TTM_PL_VRAM]);
+		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+	case GSGPU_INFO_VIS_VRAM_USAGE:
+		ui64 = gsgpu_vram_mgr_vis_usage(&adev->mman.bdev.man[TTM_PL_VRAM]);
+		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+	case GSGPU_INFO_GTT_USAGE:
+		ui64 = gsgpu_gtt_mgr_usage(&adev->mman.bdev.man[TTM_PL_TT]);
+		return copy_to_user(out, &ui64, min(size, 8u)) ? -EFAULT : 0;
+	case GSGPU_INFO_GDS_CONFIG:
+		return -ENODATA;
+	case GSGPU_INFO_VRAM_GTT: {
+		struct drm_gsgpu_info_vram_gtt vram_gtt;
+
+		vram_gtt.vram_size = adev->gmc.real_vram_size -
+			atomic64_read(&adev->vram_pin_size);
+		vram_gtt.vram_cpu_accessible_size = adev->gmc.visible_vram_size -
+			atomic64_read(&adev->visible_pin_size);
+		vram_gtt.gtt_size = adev->mman.bdev.man[TTM_PL_TT].size;
+		vram_gtt.gtt_size *= PAGE_SIZE;
+		vram_gtt.gtt_size -= atomic64_read(&adev->gart_pin_size);
+		return copy_to_user(out, &vram_gtt,
+				    min((size_t)size, sizeof(vram_gtt))) ? -EFAULT : 0;
+	}
+	case GSGPU_INFO_MEMORY: {
+		struct drm_gsgpu_memory_info mem;
+
+		memset(&mem, 0, sizeof(mem));
+		mem.vram.total_heap_size = adev->gmc.real_vram_size;
+		mem.vram.usable_heap_size = adev->gmc.real_vram_size -
+			atomic64_read(&adev->vram_pin_size);
+		mem.vram.heap_usage =
+			gsgpu_vram_mgr_usage(&adev->mman.bdev.man[TTM_PL_VRAM]);
+		mem.vram.max_allocation = mem.vram.usable_heap_size * 3 / 4;
+
+		mem.cpu_accessible_vram.total_heap_size =
+			adev->gmc.visible_vram_size;
+		mem.cpu_accessible_vram.usable_heap_size = adev->gmc.visible_vram_size -
+			atomic64_read(&adev->visible_pin_size);
+		mem.cpu_accessible_vram.heap_usage =
+			gsgpu_vram_mgr_vis_usage(&adev->mman.bdev.man[TTM_PL_VRAM]);
+		mem.cpu_accessible_vram.max_allocation =
+			mem.cpu_accessible_vram.usable_heap_size * 3 / 4;
+
+		mem.gtt.total_heap_size = adev->mman.bdev.man[TTM_PL_TT].size;
+		mem.gtt.total_heap_size *= PAGE_SIZE;
+		mem.gtt.usable_heap_size = mem.gtt.total_heap_size -
+			atomic64_read(&adev->gart_pin_size);
+		mem.gtt.heap_usage =
+			gsgpu_gtt_mgr_usage(&adev->mman.bdev.man[TTM_PL_TT]);
+		mem.gtt.max_allocation = mem.gtt.usable_heap_size * 3 / 4;
+
+		return copy_to_user(out, &mem,
+				    min((size_t)size, sizeof(mem)))
+				    ? -EFAULT : 0;
+	}
+	case GSGPU_INFO_READ_MMR_REG: {
+		unsigned n, alloc_size;
+		uint32_t *regs;
+		unsigned se_num = (info->read_mmr_reg.instance >>
+				   GSGPU_INFO_MMR_SE_INDEX_SHIFT) &
+				  GSGPU_INFO_MMR_SE_INDEX_MASK;
+		unsigned sh_num = (info->read_mmr_reg.instance >>
+				   GSGPU_INFO_MMR_SH_INDEX_SHIFT) &
+				  GSGPU_INFO_MMR_SH_INDEX_MASK;
+
+		/* set full masks if the userspace set all bits
+		 * in the bitfields */
+		if (se_num == GSGPU_INFO_MMR_SE_INDEX_MASK)
+			se_num = 0xffffffff;
+		if (sh_num == GSGPU_INFO_MMR_SH_INDEX_MASK)
+			sh_num = 0xffffffff;
+
+		regs = kmalloc_array(info->read_mmr_reg.count, sizeof(*regs), GFP_KERNEL);
+		if (!regs)
+			return -ENOMEM;
+		alloc_size = info->read_mmr_reg.count * sizeof(*regs);
+
+		for (i = 0; i < info->read_mmr_reg.count; i++)
+			if (gsgpu_asic_read_register(adev, se_num, sh_num,
+						      info->read_mmr_reg.dword_offset + i,
+						      &regs[i])) {
+				DRM_DEBUG_KMS("unallowed offset %#x\n",
+					      info->read_mmr_reg.dword_offset + i);
+				kfree(regs);
+				return -EFAULT;
+			}
+		n = copy_to_user(out, regs, min(size, alloc_size));
+		kfree(regs);
+		return n ? -EFAULT : 0;
+	}
+	case GSGPU_INFO_DEV_INFO: {
+		struct drm_gsgpu_info_device dev_info = {};
+		uint64_t vm_size;
+
+		dev_info.device_id = dev->pdev->device;
+		dev_info.pci_rev = dev->pdev->revision;
+		dev_info.family = adev->family;
+		dev_info.num_shader_engines = adev->gfx.config.max_shader_engines;
+		dev_info.num_shader_arrays_per_engine = adev->gfx.config.max_sh_per_se;
+		/* return all clocks in KHz */
+		dev_info.gpu_counter_freq = gsgpu_asic_get_clk(adev) * 10;
+
+		dev_info.max_engine_clock = adev->clock.default_sclk * 10;
+		dev_info.max_memory_clock = adev->clock.default_mclk * 10;
+
+		dev_info.enabled_rb_pipes_mask = adev->gfx.config.backend_enable_mask;
+		dev_info.num_rb_pipes = adev->gfx.config.max_backends_per_se *
+			adev->gfx.config.max_shader_engines;
+		dev_info.num_hw_gfx_contexts = adev->gfx.config.max_hw_contexts;
+		dev_info._pad = 0;
+		dev_info.ids_flags = 0;
+		if (adev->flags & GSGPU_IS_APU)
+			dev_info.ids_flags |= GSGPU_IDS_FLAGS_FUSION;
+
+		vm_size = adev->vm_manager.max_pfn * GSGPU_GPU_PAGE_SIZE;
+		vm_size -= GSGPU_VA_RESERVED_SIZE;
+
+		/* Older VCE FW versions are buggy and can handle only 40bits */
+		//if (adev->vce.fw_version < GSGPU_VCE_FW_53_45)
+		//	vm_size = min(vm_size, 1ULL << 40);
+
+		dev_info.virtual_address_offset = GSGPU_VA_RESERVED_SIZE;
+		dev_info.virtual_address_max =
+			min(vm_size, GSGPU_VA_HOLE_START);
+
+		if (vm_size > GSGPU_VA_HOLE_START) {
+			dev_info.high_va_offset = GSGPU_VA_HOLE_END;
+			dev_info.high_va_max = GSGPU_VA_HOLE_END | vm_size;
+		}
+		dev_info.virtual_address_alignment = max((int)PAGE_SIZE, GSGPU_GPU_PAGE_SIZE);
+		dev_info.pte_fragment_size = (1 << adev->vm_manager.fragment_size) * GSGPU_GPU_PAGE_SIZE;
+		dev_info.gart_page_size = max((int)PAGE_SIZE, GSGPU_GPU_PAGE_SIZE);
+		dev_info.cu_active_number = adev->gfx.cu_info.number;
+		dev_info.cu_ao_mask = adev->gfx.cu_info.ao_cu_mask;
+		dev_info.ce_ram_size = adev->gfx.ce_ram_size;
+		memcpy(&dev_info.cu_ao_bitmap[0], &adev->gfx.cu_info.ao_cu_bitmap[0],
+		       sizeof(adev->gfx.cu_info.ao_cu_bitmap));
+		memcpy(&dev_info.cu_bitmap[0], &adev->gfx.cu_info.bitmap[0],
+		       sizeof(adev->gfx.cu_info.bitmap));
+		dev_info.vram_type = adev->gmc.vram_type;
+		dev_info.vram_bit_width = adev->gmc.vram_width;
+		//dev_info.vce_harvest_config = adev->vce.harvest_config;
+		dev_info.gc_double_offchip_lds_buf =
+			adev->gfx.config.double_offchip_lds_buf;
+
+		dev_info.wave_front_size = adev->gfx.cu_info.wave_front_size;
+		dev_info.num_shader_visible_vgprs = adev->gfx.config.max_gprs;
+		dev_info.num_cu_per_sh = adev->gfx.config.max_cu_per_sh;
+		dev_info.num_tcc_blocks = adev->gfx.config.max_texture_channel_caches;
+		dev_info.gs_vgt_table_depth = adev->gfx.config.gs_vgt_table_depth;
+		dev_info.gs_prim_buffer_depth = adev->gfx.config.gs_prim_buffer_depth;
+		dev_info.max_gs_waves_per_vgt = adev->gfx.config.max_gs_threads;
+
+		return copy_to_user(out, &dev_info,
+				    min((size_t)size, sizeof(dev_info))) ? -EFAULT : 0;
+	}
+	case GSGPU_INFO_VCE_CLOCK_TABLE: {
+		return -ENODATA;
+	}
+	case GSGPU_INFO_VBIOS: {
+		uint32_t bios_size = adev->bios_size;
+
+		switch (info->vbios_info.type) {
+		case GSGPU_INFO_VBIOS_SIZE:
+			return copy_to_user(out, &bios_size,
+					min((size_t)size, sizeof(bios_size)))
+					? -EFAULT : 0;
+		case GSGPU_INFO_VBIOS_IMAGE: {
+			uint8_t *bios;
+			uint32_t bios_offset = info->vbios_info.offset;
+
+			if (bios_offset >= bios_size)
+				return -EINVAL;
+
+			bios = adev->bios + bios_offset;
+			return copy_to_user(out, bios,
+					    min((size_t)size, (size_t)(bios_size - bios_offset)))
+					? -EFAULT : 0;
+		}
+		default:
+			DRM_DEBUG_KMS("Invalid request %d\n",
+					info->vbios_info.type);
+			return -EINVAL;
+		}
+	}
+	case GSGPU_INFO_NUM_HANDLES:
+			return -EINVAL;
+	case GSGPU_INFO_SENSOR://pm
+		return -ENOENT;
+	case GSGPU_INFO_VRAM_LOST_COUNTER:
+		ui32 = atomic_read(&adev->vram_lost_counter);
+		return copy_to_user(out, &ui32, min(size, 4u)) ? -EFAULT : 0;
+	default:
+		DRM_DEBUG_KMS("Invalid request %d\n", info->query);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+
+/*
+ * Outdated mess for old drm with Xorg being in charge (void function now).
+ */
+/**
+ * gsgpu_driver_lastclose_kms - drm callback for last close
+ *
+ * @dev: drm dev pointer
+ *
+ * Switch vga_switcheroo state after last close (all asics).
+ */
+void gsgpu_driver_lastclose_kms(struct drm_device *dev)
+{
+	drm_fb_helper_lastclose(dev);
+	vga_switcheroo_process_delayed_switch();
+}
+
+/**
+ * gsgpu_driver_open_kms - drm callback for open
+ *
+ * @dev: drm dev pointer
+ * @file_priv: drm file
+ *
+ * On device open, init vm on cayman+ (all asics).
+ * Returns 0 on success, error on failure.
+ */
+int gsgpu_driver_open_kms(struct drm_device *dev, struct drm_file *file_priv)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_fpriv *fpriv;
+	int r, pasid;
+
+	/* Ensure IB tests are run on ring */
+	flush_delayed_work(&adev->late_init_work);
+
+	file_priv->driver_priv = NULL;
+
+	r = pm_runtime_get_sync(dev->dev);
+	if (r < 0)
+		return r;
+
+	fpriv = kzalloc(sizeof(*fpriv), GFP_KERNEL);
+	if (unlikely(!fpriv)) {
+		r = -ENOMEM;
+		goto out_suspend;
+	}
+
+	pasid = gsgpu_pasid_alloc(16);
+	if (pasid < 0) {
+		dev_warn(adev->dev, "No more PASIDs available!");
+		pasid = 0;
+	}
+	r = gsgpu_vm_init(adev, &fpriv->vm, GSGPU_VM_CONTEXT_GFX, pasid);
+	if (r)
+		goto error_pasid;
+
+	fpriv->prt_va = gsgpu_vm_bo_add(adev, &fpriv->vm, NULL);
+	if (!fpriv->prt_va) {
+		r = -ENOMEM;
+		goto error_vm;
+	}
+
+	mutex_init(&fpriv->bo_list_lock);
+	idr_init(&fpriv->bo_list_handles);
+
+	gsgpu_ctx_mgr_init(&fpriv->ctx_mgr);
+
+	file_priv->driver_priv = fpriv;
+	goto out_suspend;
+
+error_vm:
+	gsgpu_vm_fini(adev, &fpriv->vm);
+
+error_pasid:
+	if (pasid)
+		gsgpu_pasid_free(pasid);
+
+	kfree(fpriv);
+
+out_suspend:
+	pm_runtime_mark_last_busy(dev->dev);
+	pm_runtime_put_autosuspend(dev->dev);
+
+	return r;
+}
+
+/**
+ * gsgpu_driver_postclose_kms - drm callback for post close
+ *
+ * @dev: drm dev pointer
+ * @file_priv: drm file
+ *
+ * On device post close, tear down vm on cayman+ (all asics).
+ */
+void gsgpu_driver_postclose_kms(struct drm_device *dev,
+				 struct drm_file *file_priv)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_fpriv *fpriv = file_priv->driver_priv;
+	struct gsgpu_bo_list *list;
+	struct gsgpu_bo *pd;
+	unsigned int pasid;
+	int handle;
+
+	if (!fpriv)
+		return;
+
+	pm_runtime_get_sync(dev->dev);
+
+	gsgpu_vm_bo_rmv(adev, fpriv->prt_va);
+
+	pasid = fpriv->vm.pasid;
+	pd = gsgpu_bo_ref(fpriv->vm.root.base.bo);
+
+	gsgpu_vm_fini(adev, &fpriv->vm);
+	gsgpu_ctx_mgr_fini(&fpriv->ctx_mgr);
+
+	if (pasid)
+		gsgpu_pasid_free_delayed(pd->tbo.resv, pasid);
+	gsgpu_bo_unref(&pd);
+
+	idr_for_each_entry(&fpriv->bo_list_handles, list, handle)
+		gsgpu_bo_list_put(list);
+
+	idr_destroy(&fpriv->bo_list_handles);
+	mutex_destroy(&fpriv->bo_list_lock);
+
+	kfree(fpriv);
+	file_priv->driver_priv = NULL;
+
+	pm_runtime_mark_last_busy(dev->dev);
+	pm_runtime_put_autosuspend(dev->dev);
+}
+
+/*
+ * VBlank related functions.
+ */
+/**
+ * gsgpu_get_vblank_counter_kms - get frame count
+ *
+ * @dev: drm dev pointer
+ * @pipe: crtc to get the frame count from
+ *
+ * Gets the frame count on the requested crtc (all asics).
+ * Returns frame count on success, -EINVAL on failure.
+ */
+u32 gsgpu_get_vblank_counter_kms(struct drm_device *dev, unsigned int pipe)
+{
+	struct gsgpu_device *adev = dev->dev_private;
+	int vpos, hpos, stat;
+	u32 count;
+
+	if (pipe >= adev->mode_info.num_crtc) {
+		DRM_ERROR("Invalid crtc %u\n", pipe);
+		return -EINVAL;
+	}
+
+	/* The hw increments its frame counter at start of vsync, not at start
+	 * of vblank, as is required by DRM core vblank counter handling.
+	 * Cook the hw count here to make it appear to the caller as if it
+	 * incremented at start of vblank. We measure distance to start of
+	 * vblank in vpos. vpos therefore will be >= 0 between start of vblank
+	 * and start of vsync, so vpos >= 0 means to bump the hw frame counter
+	 * result by 1 to give the proper appearance to caller.
+	 */
+	if (adev->mode_info.crtcs[pipe]) {
+		/* Repeat readout if needed to provide stable result if
+		 * we cross start of vsync during the queries.
+		 */
+		do {
+			count = gsgpu_display_vblank_get_counter(adev, pipe);
+			/* Ask gsgpu_display_get_crtc_scanoutpos to return
+			 * vpos as distance to start of vblank, instead of
+			 * regular vertical scanout pos.
+			 */
+			stat = gsgpu_display_get_crtc_scanoutpos(
+				dev, pipe, GET_DISTANCE_TO_VBLANKSTART,
+				&vpos, &hpos, NULL, NULL,
+				&adev->mode_info.crtcs[pipe]->base.hwmode);
+		} while (count != gsgpu_display_vblank_get_counter(adev, pipe));
+
+		if (((stat & (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_ACCURATE)) !=
+		    (DRM_SCANOUTPOS_VALID | DRM_SCANOUTPOS_ACCURATE))) {
+			DRM_DEBUG_VBL("Query failed! stat %d\n", stat);
+		} else {
+			DRM_DEBUG_VBL("crtc %d: dist from vblank start %d\n",
+				      pipe, vpos);
+
+			/* Bump counter if we are at >= leading edge of vblank,
+			 * but before vsync where vpos would turn negative and
+			 * the hw counter really increments.
+			 */
+			if (vpos >= 0)
+				count++;
+		}
+	} else {
+		/* Fallback to use value as is. */
+		count = gsgpu_display_vblank_get_counter(adev, pipe);
+		DRM_DEBUG_VBL("NULL mode info! Returned count may be wrong.\n");
+	}
+
+	return count;
+}
+
+const struct drm_ioctl_desc gsgpu_ioctls_kms[] = {
+	DRM_IOCTL_DEF_DRV(GSGPU_GEM_CREATE, gsgpu_gem_create_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_CTX, gsgpu_ctx_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_VM, gsgpu_vm_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_SCHED, gsgpu_sched_ioctl, DRM_MASTER),
+	DRM_IOCTL_DEF_DRV(GSGPU_BO_LIST, gsgpu_bo_list_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_FENCE_TO_HANDLE, gsgpu_cs_fence_to_handle_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	/* KMS */
+	DRM_IOCTL_DEF_DRV(GSGPU_GEM_MMAP, gsgpu_gem_mmap_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_GEM_WAIT_IDLE, gsgpu_gem_wait_idle_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_CS, gsgpu_cs_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_INFO, gsgpu_info_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_WAIT_CS, gsgpu_cs_wait_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_WAIT_FENCES, gsgpu_cs_wait_fences_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_GEM_METADATA, gsgpu_gem_metadata_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_GEM_VA, gsgpu_gem_va_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_GEM_OP, gsgpu_gem_op_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_GEM_USERPTR, gsgpu_gem_userptr_ioctl, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(GSGPU_HWSEMA_OP, gsgpu_hw_sema_op_ioctl, DRM_AUTH|DRM_RENDER_ALLOW)
+};
+const int gsgpu_max_kms_ioctl = ARRAY_SIZE(gsgpu_ioctls_kms);
+
+/*
+ * Debugfs info
+ */
+#if defined(CONFIG_DEBUG_FS)
+
+static int gsgpu_debugfs_firmware_info(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+	struct drm_gsgpu_info_firmware fw_info;
+	struct drm_gsgpu_query_fw query_fw;
+	int ret, i;
+
+	/* GMC */
+	query_fw.fw_type = GSGPU_INFO_FW_GMC;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "MC feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* ME */
+	query_fw.fw_type = GSGPU_INFO_FW_GFX_ME;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "ME feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* PFP */
+	query_fw.fw_type = GSGPU_INFO_FW_GFX_PFP;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "PFP feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* CE */
+	query_fw.fw_type = GSGPU_INFO_FW_GFX_CE;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "CE feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* RLC */
+	query_fw.fw_type = GSGPU_INFO_FW_GFX_RLC;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "RLC feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* RLC SAVE RESTORE LIST CNTL */
+	query_fw.fw_type = GSGPU_INFO_FW_GFX_RLC_RESTORE_LIST_CNTL;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "RLC SRLC feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* RLC SAVE RESTORE LIST GPM MEM */
+	query_fw.fw_type = GSGPU_INFO_FW_GFX_RLC_RESTORE_LIST_GPM_MEM;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "RLC SRLG feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* RLC SAVE RESTORE LIST SRM MEM */
+	query_fw.fw_type = GSGPU_INFO_FW_GFX_RLC_RESTORE_LIST_SRM_MEM;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "RLC SRLS feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* MEC */
+	query_fw.fw_type = GSGPU_INFO_FW_GFX_MEC;
+	query_fw.index = 0;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "MEC feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* PSP SOS */
+	query_fw.fw_type = GSGPU_INFO_FW_SOS;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "SOS feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+
+	/* PSP ASD */
+	query_fw.fw_type = GSGPU_INFO_FW_ASD;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "ASD feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* SMC */
+	query_fw.fw_type = GSGPU_INFO_FW_SMC;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "SMC feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	/* XDMA */
+	query_fw.fw_type = GSGPU_INFO_FW_XDMA;
+	for (i = 0; i < adev->xdma.num_instances; i++) {
+		query_fw.index = i;
+		ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+		if (ret)
+			return ret;
+		seq_printf(m, "XDMA%d feature version: %u, firmware version: 0x%08x\n",
+			   i, fw_info.feature, fw_info.ver);
+	}
+
+	/* VCN */
+	query_fw.fw_type = GSGPU_INFO_FW_VCN;
+	ret = gsgpu_firmware_info(&fw_info, &query_fw, adev);
+	if (ret)
+		return ret;
+	seq_printf(m, "VCN feature version: %u, firmware version: 0x%08x\n",
+		   fw_info.feature, fw_info.ver);
+
+	return 0;
+}
+
+static const struct drm_info_list gsgpu_firmware_info_list[] = {
+	{"gsgpu_firmware_info", gsgpu_debugfs_firmware_info, 0, NULL},
+};
+#endif
+
+int gsgpu_debugfs_firmware_init(struct gsgpu_device *adev)
+{
+#if defined(CONFIG_DEBUG_FS)
+	return gsgpu_debugfs_add_files(adev, gsgpu_firmware_info_list,
+					ARRAY_SIZE(gsgpu_firmware_info_list));
+#else
+	return 0;
+#endif
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_mmu.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_mmu.c
new file mode 100644
index 000000000000..3772ed5c6757
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_mmu.c
@@ -0,0 +1,594 @@
+#include <linux/firmware.h>
+#include <drm/drmP.h>
+#include <drm/drm_cache.h>
+#include "gsgpu.h"
+#include "gsgpu_mmu.h"
+#include "gsgpu_irq.h"
+
+#define GSGPU_NUM_OF_VMIDS			4
+
+static void mmu_set_gmc_funcs(struct gsgpu_device *adev);
+static void mmu_set_irq_funcs(struct gsgpu_device *adev);
+
+/**
+ * mmu_vram_gtt_location  gmc locations ram info set
+ *
+ * @adev pionter of struct gsgpu_device
+ * @mc  pionter of gsgpu_gmc
+ */
+static void mmu_vram_gtt_location(struct gsgpu_device *adev,
+				       struct gsgpu_gmc *mc)
+{
+	u64 base = 0;
+
+	/*TODO Use generic register to get vram base address*/
+
+	/*refrence from LS7A2000 page data*/
+	if (adev->chip == dev_7a2000)
+		base = 0x1000000000000;
+	else if (adev->chip == dev_2k2000)
+		base = adev->gmc.aper_base;
+
+	gsgpu_device_vram_location(adev, &adev->gmc, base);
+	gsgpu_device_gart_location(adev, mc);
+}
+
+/**
+ * mmu_mc_init - initialize the memory controller driver params
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Look up the amount of vram, vram width, and decide how to place
+ * vram and gart within the GPU's physical address space ().
+ * Returns 0 for success.
+ */
+static int mmu_mc_init(struct gsgpu_device *adev)
+{
+	adev->gmc.vram_width = 32;
+
+	adev->gmc.mc_mask = 0xffffffffffULL; /* 40 bit MC */
+
+	/* size in MB on gpu ram*/
+	adev->gmc.mc_vram_size = 256 * 1024ULL * 1024ULL;
+	adev->gmc.real_vram_size = 256 * 1024ULL * 1024ULL;
+
+	/*TODO 	pci_resource_start(adev->pdev, 0)
+	 * 		pci_resource_len(adev->pdev, 0)
+	 * */
+
+	if (gsgpu_using_ram) {
+		adev->gmc.aper_base =  0x460000000;
+		adev->gmc.aper_size = 0x10000000;
+	} else {
+		adev->gmc.aper_base = pci_resource_start(adev->pdev, 2);
+		adev->gmc.aper_size = pci_resource_len(adev->pdev, 2);
+	}
+
+	DRM_INFO("aper_base %#llx SIZE %#llx bytes \n", adev->gmc.aper_base, adev->gmc.aper_size);
+	/* In case the PCI BAR is larger than the actual amount of vram */
+	adev->gmc.visible_vram_size = adev->gmc.aper_size;
+	if (adev->gmc.visible_vram_size > adev->gmc.real_vram_size)
+		adev->gmc.visible_vram_size = adev->gmc.real_vram_size;
+
+	/* set the gart size */
+	if (gsgpu_gart_size == -1) {
+		adev->gmc.gart_size = 256ULL << 20;
+	/* base = 0x1000000000000; */
+	} else {
+		adev->gmc.gart_size = (u64)gsgpu_gart_size << 20;
+	}
+
+	adev->gmc.vm_fault_info = kmalloc(sizeof(struct gsgpu_vm_fault_info), GFP_KERNEL);
+
+	if (!adev->gmc.vm_fault_info)
+		return -ENOMEM;
+
+	return 0;
+}
+
+/*
+ * GART
+ * VMID 0 is the physical GPU addresses as used by the kernel.
+ * VMIDs 1-15 are used for userspace clients and are handled
+ * by the gsgpu vm/hsa code.
+ */
+
+/**
+ * mmu_flush_gpu_tlb - gart tlb flush callback
+ *
+ * @adev: gsgpu_device pointer
+ * @vmid: vm instance to flush
+ *
+ * Flush the TLB for the requested page table ().
+ */
+static void mmu_flush_gpu_tlb(struct gsgpu_device *adev,
+					uint32_t vmid)
+{
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_FLUSH), GSGPU_MMU_FLUSH_PKT(vmid, GSGPU_MMU_FLUSH_VMID), 0);
+}
+
+static uint64_t mmu_emit_flush_gpu_tlb(struct gsgpu_ring *ring,
+					    unsigned vmid, uint64_t pd_addr)
+{
+	uint32_t reg;
+	reg = GSGPU_MMU_VMID_OF_PGD(vmid);
+
+	gsgpu_ring_emit_wreg(ring, reg, lower_32_bits(pd_addr));
+	gsgpu_ring_emit_wreg(ring, reg + 4, upper_32_bits(pd_addr));
+
+	gsgpu_ring_emit_wreg(ring, GSGPU_MMU_FLUSH_CTRL_OFFSET, GSGPU_MMU_FLUSH_PKT(vmid, GSGPU_MMU_FLUSH_VMID));
+
+	return pd_addr;
+}
+
+/**
+ * mmu_set_pte_pde - update the page tables using MMIO
+ *
+ * @adev: gsgpu_device pointer
+ * @cpu_pt_addr: cpu address of the page table
+ * @gpu_page_idx: entry in the page table to update
+ * @addr: dst addr to write into pte/pde
+ * @flags: access flags
+ *
+ * Update the page tables using the CPU.
+ */
+static int mmu_set_pte_pde(struct gsgpu_device *adev, void *cpu_pt_addr,
+				uint32_t gpu_page_idx, uint64_t addr,
+				uint64_t flags)
+{
+	void __iomem *ptr = (void *)cpu_pt_addr;
+	uint64_t value;
+
+	/*
+	 * PTE format:
+	 * 63:40 reserved
+	 * 39:12 physical page base address
+	 * 7:4 zinf
+	 * 3 writeable
+	 * 2 exception
+	 * 1 hugepage
+	 * 0 present
+	 *
+	 */
+	//value = addr & adev->vm_manager.dir_mask;
+	//value = addr & 0x000000FFFFFFF000ULL;
+	value = addr & 0xFFFFFFFFFFFFF000ULL;
+	value |= flags;
+
+	writeq(value, ptr + (gpu_page_idx * GSGPU_MMU_PTE_SIZE));
+
+	return 0;
+}
+
+static uint64_t mmu_get_vm_pte_flags(struct gsgpu_device *adev,
+					  uint32_t flags)
+{
+	uint64_t pte_flag = 0;
+
+	if (flags & GSGPU_VM_PAGE_READABLE)
+		pte_flag |= GSGPU_PTE_PRESENT;
+	if (flags & GSGPU_VM_PAGE_WRITEABLE)
+		pte_flag |= GSGPU_PTE_WRITEABLE;
+
+	return pte_flag;
+}
+
+static void mmu_get_vm_pde(struct gsgpu_device *adev, int level,
+				uint64_t *addr, uint64_t *flags)
+{
+	//BUG_ON(*addr & ~adev->vm_manager.dir_mask);
+	//BUG_ON(*addr & 0xFFFFFF0000000FFFULL);
+}
+
+/**
+ * mmu_gart_enable - gart enable
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * This sets up the TLBs, programs the page tables for VMID0,
+ * sets up the hw for VMIDs 1-15 which are allocated on
+ * demand, and sets up the global locations for the LDS, GDS,
+ * and GPUVM for FSA64 clients ().
+ * Returns 0 for success, errors for failure.
+ */
+static int mmu_gart_enable(struct gsgpu_device *adev)
+{
+	int r, i;
+
+	if (adev->gart.robj == NULL) {
+		dev_err(adev->dev, "No VRAM object for PCIE GART.\n");
+		return -EINVAL;
+	}
+	r = gsgpu_gart_table_vram_pin(adev);
+	if (r)
+		return r;
+
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_SET_EXC), 3, 0);
+	gsgpu_cmd_exec(adev, GSCMDi(GSCMD_MMU, MMU_SET_PGD, 0), \
+			lower_32_bits(adev->gart.table_addr), upper_32_bits(adev->gart.table_addr));
+
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_SET_SAFE), \
+			lower_32_bits(adev->dummy_page_addr), upper_32_bits(adev->dummy_page_addr));
+
+	gsgpu_cmd_exec(adev, GSCMDi(GSCMD_MMU, MMU_SET_DIR, 0),
+			GSGPU_MMU_DIR_CTRL_256M_1LVL, 0);
+
+	for (i = 1; i < GSGPU_NUM_OF_VMIDS; i++) {
+		gsgpu_cmd_exec(adev, GSCMDi(GSCMD_MMU, MMU_SET_DIR, i),
+				GSGPU_MMU_DIR_CTRL_1T_3LVL, 0);
+	}
+
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_ENABLE), MMU_ENABLE, ~1);
+
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_FLUSH), GSGPU_MMU_FLUSH_PKT(0, GSGPU_MMU_FLUSH_ALL), 0);
+
+	DRM_INFO("PCIE GART of %uM enabled (table at 0x%016llX).\n",
+		 (unsigned)(adev->gmc.gart_size >> 20),
+		 (unsigned long long)adev->gart.table_addr);
+	adev->gart.ready = true;
+	return 0;
+}
+
+static int mmu_gart_init(struct gsgpu_device *adev)
+{
+	int r;
+
+	if (adev->gart.robj) {
+		WARN(1, "GSGPU PCIE GART already initialized\n");
+		return 0;
+	}
+	/* Initialize common gart structure */
+	r = gsgpu_gart_init(adev);
+	if (r)
+		return r;
+	adev->gart.table_size = adev->gart.num_gpu_pages * GSGPU_MMU_PTE_SIZE;
+	adev->gart.gart_pte_flags = 0;
+	return gsgpu_gart_table_vram_alloc(adev);
+}
+
+/**
+ * mmu_gart_disable - gart disable
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * This disables all VM page table ().
+ */
+static void mmu_gart_disable(struct gsgpu_device *adev)
+{
+	gsgpu_gart_table_vram_unpin(adev);
+}
+
+static int mmu_early_init(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	mmu_set_gmc_funcs(adev);
+	mmu_set_irq_funcs(adev);
+
+	spin_lock_init(&adev->gmc.invalidate_lock);
+
+	return 0;
+}
+
+static int mmu_late_init(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	gsgpu_bo_late_init(adev);
+
+	if (gsgpu_vm_fault_stop != GSGPU_VM_FAULT_STOP_ALWAYS)
+		return gsgpu_irq_get(adev, &adev->gmc.vm_fault, 0);
+	else
+		return 0;
+}
+
+static inline int mmu_irq_set(struct gsgpu_device *adev)
+{
+	int r;
+
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_GFX_PAGE_INV_FAULT, &adev->gmc.vm_fault);
+	if (r)
+		return r;
+
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_GFX_MEM_PROT_FAULT, &adev->gmc.vm_fault);
+	if (r)
+		return r;
+
+	return 0;
+}
+
+static inline void mmu_set_dma_consistent(struct gsgpu_device *adev)
+{
+	int r;
+	int dma_bits;
+
+	/* set DMA mask + need_dma32 flags.
+	 * PCIE - can handle 40-bits.
+	 * PCI - dma32 for legacy pci gart, 40 bits on newer asics
+	 */
+	adev->need_dma32 = false;
+	dma_bits = 40;
+	r = pci_set_dma_mask(adev->pdev, DMA_BIT_MASK(dma_bits));
+	if (r) {
+		adev->need_dma32 = true;
+		dma_bits = 32;
+		pr_warn("gsgpu: No suitable DMA available\n");
+	}
+	r = pci_set_consistent_dma_mask(adev->pdev, DMA_BIT_MASK(dma_bits));
+	if (r) {
+		pci_set_consistent_dma_mask(adev->pdev, DMA_BIT_MASK(32));
+		pr_warn("gsgpu: No coherent DMA available\n");
+	}
+
+	adev->gmc.dma_bits = dma_bits;
+}
+
+static inline int mmu_vm_manager_init(struct gsgpu_device *adev)
+{
+
+	/* Adjust VM size here.
+	 * Currently set to 4GB ((1 << 20) 4k pages).
+	 * Max GPUVM size for cayman and SI is 40 bits.
+	 */
+	gsgpu_vm_adjust_size(adev, 64, 2, 40);
+
+	/*
+	 * number of VMs
+	 * VMID 0 is reserved for System
+	 * gsgpu graphics/compute will use VMIDs 1-7
+	 */
+	adev->vm_manager.id_mgr.num_ids = GSGPU_NUM_OF_VMIDS;
+	gsgpu_vm_manager_init(adev);
+
+	/* base offset of vram pages */
+	if (gsgpu_using_ram)
+		adev->vm_manager.vram_base_offset = adev->gmc.aper_base;
+	else
+		adev->vm_manager.vram_base_offset = 0x1000000000000;
+
+	return 0;
+}
+
+static int mmu_sw_init(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = mmu_irq_set(adev);
+	if (r)
+		return r;
+
+	mmu_set_dma_consistent(adev);
+
+	adev->need_swiotlb = drm_get_max_iomem() > (u64)BIT(adev->gmc.dma_bits);
+
+	r = mmu_mc_init(adev);
+	if (r)
+		return r;
+
+	mmu_vram_gtt_location(adev, &adev->gmc);
+
+	/* Memory manager via ttm*/
+	r = gsgpu_bo_init(adev);
+	if (r)
+		return r;
+
+	r = mmu_gart_init(adev);
+	if (r)
+		return r;
+
+	r = mmu_vm_manager_init(adev);
+	if (r)
+		return r;
+
+	return 0;
+}
+
+static int mmu_sw_fini(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	gsgpu_gem_force_release(adev);
+	gsgpu_vm_manager_fini(adev);
+	kfree(adev->gmc.vm_fault_info);
+	gsgpu_gart_table_vram_free(adev);
+	gsgpu_bo_fini(adev);
+	gsgpu_gart_fini(adev);
+	release_firmware(adev->gmc.fw);
+	adev->gmc.fw = NULL;
+
+	return 0;
+}
+
+static int mmu_hw_init(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = mmu_gart_enable(adev);
+	if (r)
+		return r;
+
+	return r;
+}
+
+static int mmu_hw_fini(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	gsgpu_irq_put(adev, &adev->gmc.vm_fault, 0);
+	mmu_gart_disable(adev);
+
+	return 0;
+}
+
+static int mmu_suspend(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	mmu_hw_fini(adev);
+
+	return 0;
+}
+
+static int mmu_resume(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = mmu_hw_init(adev);
+	if (r)
+		return r;
+
+	gsgpu_vmid_reset_all(adev);
+
+	return 0;
+}
+
+static bool mmu_is_idle(void *handle)
+{
+	return true;
+}
+
+static bool mmu_check_soft_reset(void *handle)
+{
+	return false;
+}
+
+static int mmu_pre_soft_reset(void *handle)
+{
+	return 0;
+}
+
+static int mmu_soft_reset(void *handle)
+{
+	return 0;
+}
+
+static int mmu_post_soft_reset(void *handle)
+{
+	return 0;
+}
+
+static int mmu_vm_fault_interrupt_state(struct gsgpu_device *adev,
+					     struct gsgpu_irq_src *src,
+					     unsigned type,
+					     enum gsgpu_interrupt_state state)
+{
+	switch (state) {
+	case GSGPU_IRQ_STATE_DISABLE:
+		gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_SET_EXC), 0, ~1);
+		break;
+	case GSGPU_IRQ_STATE_ENABLE:
+		gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_SET_EXC), 1, ~1);
+		break;
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+/**
+ * mmu_process_interrupt
+ * TODO
+ * @adev
+ * @source
+ * @entry
+ *
+ * Return:
+
+ */
+static int mmu_process_interrupt(struct gsgpu_device *adev,
+				      struct gsgpu_irq_src *source,
+				      struct gsgpu_iv_entry *entry)
+{
+	u32 addr_hi, addr_lo, status;
+
+	addr_lo = entry->src_data[0];
+	addr_hi = entry->src_data[1];
+	status =  (entry->src_data[1] >> 16);
+
+	if (printk_ratelimit()) {
+		struct gsgpu_task_info task_info;
+
+		gsgpu_vm_get_task_info(adev, entry->pasid, &task_info);
+
+		dev_err(adev->dev, "GPU fault detected: %d  vmid %d pasid %d for process %s pid %d thread %s pid %d\n",
+			entry->src_id, entry->vmid, entry->pasid, task_info.process_name,
+			task_info.tgid, task_info.task_name, task_info.pid);
+		dev_err(adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_ADDR   0x%08x%08x\n",
+			addr_hi, addr_lo);
+		dev_err(adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%04X\n",
+			status);
+	}
+
+	return 0;
+}
+
+static void mmu_emit_pasid_mapping(struct gsgpu_ring *ring, unsigned vmid,
+					unsigned pasid)
+{
+	/**
+	 * let the firmware save the mapping
+	 * relationship between vmid and pasid in DRAM
+	 * By simulating a command stream.
+	 * The true ways is Set regs instead of this way
+	**/
+
+	gsgpu_ring_write(ring, GSPKT(GSPKT_VM_BIND, 3));
+	gsgpu_ring_write(ring, ring->funcs->type);
+	gsgpu_ring_write(ring, vmid);
+	gsgpu_ring_write(ring, pasid);
+}
+
+static const struct gsgpu_ip_funcs mmu_ip_funcs = {
+	.name = "mmu",
+	.early_init = mmu_early_init,
+	.late_init = mmu_late_init,
+	.sw_init = mmu_sw_init,
+	.sw_fini = mmu_sw_fini,
+	.hw_init = mmu_hw_init,
+	.hw_fini = mmu_hw_fini,
+	.suspend = mmu_suspend,
+	.resume = mmu_resume,
+	.is_idle = mmu_is_idle,
+	.wait_for_idle = NULL,
+	.check_soft_reset = mmu_check_soft_reset,
+	.pre_soft_reset = mmu_pre_soft_reset,
+	.soft_reset = mmu_soft_reset,
+	.post_soft_reset = mmu_post_soft_reset,
+};
+
+static const struct gsgpu_gmc_funcs mmu_gmc_funcs = {
+	.flush_gpu_tlb = mmu_flush_gpu_tlb,
+	.emit_flush_gpu_tlb = mmu_emit_flush_gpu_tlb,
+	.emit_pasid_mapping = mmu_emit_pasid_mapping,
+	.set_pte_pde = mmu_set_pte_pde,
+	.get_vm_pte_flags = mmu_get_vm_pte_flags,
+	.get_vm_pde = mmu_get_vm_pde
+};
+
+static const struct gsgpu_irq_src_funcs mmu_irq_funcs = {
+	.set = mmu_vm_fault_interrupt_state,
+	.process = mmu_process_interrupt,
+};
+
+static void mmu_set_gmc_funcs(struct gsgpu_device *adev)
+{
+	if (adev->gmc.gmc_funcs == NULL)
+		adev->gmc.gmc_funcs = &mmu_gmc_funcs;
+}
+
+static void mmu_set_irq_funcs(struct gsgpu_device *adev)
+{
+	adev->gmc.vm_fault.num_types = 1;
+	adev->gmc.vm_fault.funcs = &mmu_irq_funcs;
+}
+const struct gsgpu_ip_block_version mmu_ip_block = {
+	.type = GSGPU_IP_BLOCK_TYPE_GMC,
+	.major = 1,
+	.minor = 0,
+	.rev = 0,
+	.funcs = &mmu_ip_funcs,
+};
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c
new file mode 100644
index 000000000000..1f50d45650eb
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_mn.c
@@ -0,0 +1,433 @@
+#include <linux/firmware.h>
+#include <linux/module.h>
+#include <linux/mmu_notifier.h>
+#include <linux/interval_tree.h>
+#include <drm/drmP.h>
+#include <drm/drm.h>
+
+#include "gsgpu.h"
+
+/**
+ * struct gsgpu_mn
+ *
+ * @adev: gsgpu device pointer
+ * @mm: process address space
+ * @mn: MMU notifier structure
+ * @type: type of MMU notifier
+ * @work: destruction work item
+ * @node: hash table node to find structure by adev and mn
+ * @lock: rw semaphore protecting the notifier nodes
+ * @objects: interval tree containing gsgpu_mn_nodes
+ * @read_lock: mutex for recursive locking of @lock
+ * @recursion: depth of recursion
+ *
+ * Data for each gsgpu device and process address space.
+ */
+struct gsgpu_mn {
+	/* constant after initialisation */
+	struct gsgpu_device	*adev;
+	struct mm_struct	*mm;
+	struct mmu_notifier	mn;
+	enum gsgpu_mn_type	type;
+
+	/* only used on destruction */
+	struct work_struct	work;
+
+	/* protected by adev->mn_lock */
+	struct hlist_node	node;
+
+	/* objects protected by lock */
+	struct rw_semaphore	lock;
+	struct rb_root_cached	objects;
+	struct mutex		read_lock;
+	atomic_t		recursion;
+};
+
+/**
+ * struct gsgpu_mn_node
+ *
+ * @it: interval node defining start-last of the affected address range
+ * @bos: list of all BOs in the affected address range
+ *
+ * Manages all BOs which are affected of a certain range of address space.
+ */
+struct gsgpu_mn_node {
+	struct interval_tree_node	it;
+	struct list_head		bos;
+};
+
+/**
+ * gsgpu_mn_destroy - destroy the MMU notifier
+ *
+ * @work: previously sheduled work item
+ *
+ * Lazy destroys the notifier from a work item
+ */
+static void gsgpu_mn_destroy(struct work_struct *work)
+{
+	struct gsgpu_mn *amn = container_of(work, struct gsgpu_mn, work);
+	struct gsgpu_device *adev = amn->adev;
+	struct gsgpu_mn_node *node, *next_node;
+	struct gsgpu_bo *bo, *next_bo;
+
+	mutex_lock(&adev->mn_lock);
+	down_write(&amn->lock);
+	hash_del(&amn->node);
+	rbtree_postorder_for_each_entry_safe(node, next_node,
+					     &amn->objects.rb_root, it.rb) {
+		list_for_each_entry_safe(bo, next_bo, &node->bos, mn_list) {
+			bo->mn = NULL;
+			list_del_init(&bo->mn_list);
+		}
+		kfree(node);
+	}
+	up_write(&amn->lock);
+	mutex_unlock(&adev->mn_lock);
+	mmu_notifier_unregister_no_release(&amn->mn, amn->mm);
+	kfree(amn);
+}
+
+/**
+ * gsgpu_mn_release - callback to notify about mm destruction
+ *
+ * @mn: our notifier
+ * @mm: the mm this callback is about
+ *
+ * Shedule a work item to lazy destroy our notifier.
+ */
+static void gsgpu_mn_release(struct mmu_notifier *mn,
+			      struct mm_struct *mm)
+{
+	struct gsgpu_mn *amn = container_of(mn, struct gsgpu_mn, mn);
+
+	INIT_WORK(&amn->work, gsgpu_mn_destroy);
+	schedule_work(&amn->work);
+}
+
+
+/**
+ * gsgpu_mn_lock - take the write side lock for this notifier
+ *
+ * @mn: our notifier
+ */
+void gsgpu_mn_lock(struct gsgpu_mn *mn)
+{
+	if (mn)
+		down_write(&mn->lock);
+}
+
+/**
+ * gsgpu_mn_unlock - drop the write side lock for this notifier
+ *
+ * @mn: our notifier
+ */
+void gsgpu_mn_unlock(struct gsgpu_mn *mn)
+{
+	if (mn)
+		up_write(&mn->lock);
+}
+
+/**
+ * gsgpu_mn_read_lock - take the read side lock for this notifier
+ *
+ * @amn: our notifier
+ */
+static int gsgpu_mn_read_lock(struct gsgpu_mn *amn, bool blockable)
+{
+	if (blockable)
+		mutex_lock(&amn->read_lock);
+	else if (!mutex_trylock(&amn->read_lock))
+		return -EAGAIN;
+
+	if (atomic_inc_return(&amn->recursion) == 1)
+		down_read_non_owner(&amn->lock);
+	mutex_unlock(&amn->read_lock);
+
+	return 0;
+}
+
+/**
+ * gsgpu_mn_read_unlock - drop the read side lock for this notifier
+ *
+ * @amn: our notifier
+ */
+static void gsgpu_mn_read_unlock(struct gsgpu_mn *amn)
+{
+	if (atomic_dec_return(&amn->recursion) == 0)
+		up_read_non_owner(&amn->lock);
+}
+
+/**
+ * gsgpu_mn_invalidate_node - unmap all BOs of a node
+ *
+ * @node: the node with the BOs to unmap
+ * @start: start of address range affected
+ * @end: end of address range affected
+ *
+ * Block for operations on BOs to finish and mark pages as accessed and
+ * potentially dirty.
+ */
+static void gsgpu_mn_invalidate_node(struct gsgpu_mn_node *node,
+				      unsigned long start,
+				      unsigned long end)
+{
+	struct gsgpu_bo *bo;
+	long r;
+
+	list_for_each_entry(bo, &node->bos, mn_list) {
+
+		if (!gsgpu_ttm_tt_affect_userptr(bo->tbo.ttm, start, end))
+			continue;
+
+		r = reservation_object_wait_timeout_rcu(bo->tbo.resv,
+			true, false, MAX_SCHEDULE_TIMEOUT);
+		if (r <= 0)
+			DRM_ERROR("(%ld) failed to wait for user bo\n", r);
+
+		gsgpu_ttm_tt_mark_user_pages(bo->tbo.ttm);
+	}
+}
+
+/**
+ * gsgpu_mn_invalidate_range_start_gfx - callback to notify about mm change
+ *
+ * @mn: our notifier
+ * @mm: the mm this callback is about
+ * @start: start of updated range
+ * @end: end of updated range
+ *
+ * Block for operations on BOs to finish and mark pages as accessed and
+ * potentially dirty.
+ */
+static int gsgpu_mn_invalidate_range_start_gfx(struct mmu_notifier *mn,
+						 struct mm_struct *mm,
+						 unsigned long start,
+						 unsigned long end,
+						 bool blockable)
+{
+	struct gsgpu_mn *amn = container_of(mn, struct gsgpu_mn, mn);
+	struct interval_tree_node *it;
+
+	/* notification is exclusive, but interval is inclusive */
+	end -= 1;
+
+	/* TODO we should be able to split locking for interval tree and
+	 * gsgpu_mn_invalidate_node
+	 */
+	if (gsgpu_mn_read_lock(amn, blockable))
+		return -EAGAIN;
+
+	it = interval_tree_iter_first(&amn->objects, start, end);
+	while (it) {
+		struct gsgpu_mn_node *node;
+
+		if (!blockable) {
+			gsgpu_mn_read_unlock(amn);
+			return -EAGAIN;
+		}
+
+		node = container_of(it, struct gsgpu_mn_node, it);
+		it = interval_tree_iter_next(it, start, end);
+
+		gsgpu_mn_invalidate_node(node, start, end);
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_mn_invalidate_range_end - callback to notify about mm change
+ *
+ * @mn: our notifier
+ * @mm: the mm this callback is about
+ * @start: start of updated range
+ * @end: end of updated range
+ *
+ * Release the lock again to allow new command submissions.
+ */
+static void gsgpu_mn_invalidate_range_end(struct mmu_notifier *mn,
+					   struct mm_struct *mm,
+					   unsigned long start,
+					   unsigned long end)
+{
+	struct gsgpu_mn *amn = container_of(mn, struct gsgpu_mn, mn);
+
+	gsgpu_mn_read_unlock(amn);
+}
+
+static const struct mmu_notifier_ops gsgpu_mn_ops[] = {
+	[GSGPU_MN_TYPE_GFX] = {
+		.release = gsgpu_mn_release,
+		.invalidate_range_start = gsgpu_mn_invalidate_range_start_gfx,
+		.invalidate_range_end = gsgpu_mn_invalidate_range_end,
+	},
+};
+
+/* Low bits of any reasonable mm pointer will be unused due to struct
+ * alignment. Use these bits to make a unique key from the mm pointer
+ * and notifier type.
+ */
+#define GSGPU_MN_KEY(mm, type) ((unsigned long)(mm) + (type))
+
+/**
+ * gsgpu_mn_get - create notifier context
+ *
+ * @adev: gsgpu device pointer
+ * @type: type of MMU notifier context
+ *
+ * Creates a notifier context for current->mm.
+ */
+struct gsgpu_mn *gsgpu_mn_get(struct gsgpu_device *adev,
+				enum gsgpu_mn_type type)
+{
+	struct mm_struct *mm = current->mm;
+	struct gsgpu_mn *amn;
+	unsigned long key = GSGPU_MN_KEY(mm, type);
+	int r;
+
+	mutex_lock(&adev->mn_lock);
+	if (down_write_killable(&mm->mmap_sem)) {
+		mutex_unlock(&adev->mn_lock);
+		return ERR_PTR(-EINTR);
+	}
+
+	hash_for_each_possible(adev->mn_hash, amn, node, key)
+		if (GSGPU_MN_KEY(amn->mm, amn->type) == key)
+			goto release_locks;
+
+	amn = kzalloc(sizeof(*amn), GFP_KERNEL);
+	if (!amn) {
+		amn = ERR_PTR(-ENOMEM);
+		goto release_locks;
+	}
+
+	amn->adev = adev;
+	amn->mm = mm;
+	init_rwsem(&amn->lock);
+	amn->type = type;
+	amn->mn.ops = &gsgpu_mn_ops[type];
+	amn->objects = RB_ROOT_CACHED;
+	mutex_init(&amn->read_lock);
+	atomic_set(&amn->recursion, 0);
+
+	r = __mmu_notifier_register(&amn->mn, mm);
+	if (r)
+		goto free_amn;
+
+	hash_add(adev->mn_hash, &amn->node, GSGPU_MN_KEY(mm, type));
+
+release_locks:
+	up_write(&mm->mmap_sem);
+	mutex_unlock(&adev->mn_lock);
+
+	return amn;
+
+free_amn:
+	up_write(&mm->mmap_sem);
+	mutex_unlock(&adev->mn_lock);
+	kfree(amn);
+
+	return ERR_PTR(r);
+}
+
+/**
+ * gsgpu_mn_register - register a BO for notifier updates
+ *
+ * @bo: gsgpu buffer object
+ * @addr: userptr addr we should monitor
+ *
+ * Registers an MMU notifier for the given BO at the specified address.
+ * Returns 0 on success, -ERRNO if anything goes wrong.
+ */
+int gsgpu_mn_register(struct gsgpu_bo *bo, unsigned long addr)
+{
+	unsigned long end = addr + gsgpu_bo_size(bo) - 1;
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	enum gsgpu_mn_type type = GSGPU_MN_TYPE_GFX;
+	struct gsgpu_mn *amn;
+	struct gsgpu_mn_node *node = NULL, *new_node;
+	struct list_head bos;
+	struct interval_tree_node *it;
+
+	amn = gsgpu_mn_get(adev, type);
+	if (IS_ERR(amn))
+		return PTR_ERR(amn);
+
+	new_node = kmalloc(sizeof(*new_node), GFP_KERNEL);
+	if (!new_node)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&bos);
+
+	down_write(&amn->lock);
+
+	while ((it = interval_tree_iter_first(&amn->objects, addr, end))) {
+		kfree(node);
+		node = container_of(it, struct gsgpu_mn_node, it);
+		interval_tree_remove(&node->it, &amn->objects);
+		addr = min(it->start, addr);
+		end = max(it->last, end);
+		list_splice(&node->bos, &bos);
+	}
+
+	if (!node)
+		node = new_node;
+	else
+		kfree(new_node);
+
+	bo->mn = amn;
+
+	node->it.start = addr;
+	node->it.last = end;
+	INIT_LIST_HEAD(&node->bos);
+	list_splice(&bos, &node->bos);
+	list_add(&bo->mn_list, &node->bos);
+
+	interval_tree_insert(&node->it, &amn->objects);
+
+	up_write(&amn->lock);
+
+	return 0;
+}
+
+/**
+ * gsgpu_mn_unregister - unregister a BO for notifier updates
+ *
+ * @bo: gsgpu buffer object
+ *
+ * Remove any registration of MMU notifier updates from the buffer object.
+ */
+void gsgpu_mn_unregister(struct gsgpu_bo *bo)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	struct gsgpu_mn *amn;
+	struct list_head *head;
+
+	mutex_lock(&adev->mn_lock);
+
+	amn = bo->mn;
+	if (amn == NULL) {
+		mutex_unlock(&adev->mn_lock);
+		return;
+	}
+
+	down_write(&amn->lock);
+
+	/* save the next list entry for later */
+	head = bo->mn_list.next;
+
+	bo->mn = NULL;
+	list_del_init(&bo->mn_list);
+
+	if (list_empty(head)) {
+		struct gsgpu_mn_node *node;
+
+		node = container_of(head, struct gsgpu_mn_node, bos);
+		interval_tree_remove(&node->it, &amn->objects);
+		kfree(node);
+	}
+
+	up_write(&amn->lock);
+	mutex_unlock(&adev->mn_lock);
+}
+
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c
new file mode 100644
index 000000000000..fb8d1c606154
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_object.c
@@ -0,0 +1,1323 @@
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include <drm/drm_cache.h>
+#include <asm/dma.h>
+#include "gsgpu.h"
+#include "gsgpu_trace.h"
+
+/**
+ * DOC: gsgpu_object
+ *
+ * This defines the interfaces to operate on an &gsgpu_bo buffer object which
+ * represents memory used by driver (VRAM, system memory, etc.). The driver
+ * provides DRM/GEM APIs to userspace. DRM/GEM APIs then use these interfaces
+ * to create/destroy/set buffer object which are then managed by the kernel TTM
+ * memory manager.
+ * The interfaces are also used internally by kernel clients, including gfx,
+ * uvd, etc. for kernel managed allocations used by the GPU.
+ *
+ */
+
+static bool gsgpu_bo_need_backup(struct gsgpu_device *adev)
+{
+	if (adev->flags & GSGPU_IS_APU)
+		return false;
+
+	if (gsgpu_gpu_recovery == 0 || gsgpu_gpu_recovery == -1)
+		return false;
+
+	return true;
+}
+
+/**
+ * gsgpu_bo_subtract_pin_size - Remove BO from pin_size accounting
+ *
+ * @bo: &gsgpu_bo buffer object
+ *
+ * This function is called when a BO stops being pinned, and updates the
+ * &gsgpu_device pin_size values accordingly.
+ */
+static void gsgpu_bo_subtract_pin_size(struct gsgpu_bo *bo)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+
+	if (bo->tbo.mem.mem_type == TTM_PL_VRAM) {
+		atomic64_sub(gsgpu_bo_size(bo), &adev->vram_pin_size);
+		atomic64_sub(gsgpu_vram_mgr_bo_visible_size(bo),
+			     &adev->visible_pin_size);
+	} else if (bo->tbo.mem.mem_type == TTM_PL_TT) {
+		atomic64_sub(gsgpu_bo_size(bo), &adev->gart_pin_size);
+	}
+}
+
+static void gsgpu_bo_destroy(struct ttm_buffer_object *tbo)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(tbo->bdev);
+	struct gsgpu_bo *bo = ttm_to_gsgpu_bo(tbo);
+
+	if (bo->pin_count > 0)
+		gsgpu_bo_subtract_pin_size(bo);
+
+	gsgpu_bo_kunmap(bo);
+
+	if (bo->gem_base.import_attach)
+		drm_prime_gem_destroy(&bo->gem_base, bo->tbo.sg);
+	drm_gem_object_release(&bo->gem_base);
+	gsgpu_bo_unref(&bo->parent);
+	if (!list_empty(&bo->shadow_list)) {
+		mutex_lock(&adev->shadow_list_lock);
+		list_del_init(&bo->shadow_list);
+		mutex_unlock(&adev->shadow_list_lock);
+	}
+	kfree(bo->metadata);
+	kfree(bo);
+}
+
+/**
+ * gsgpu_bo_is_gsgpu_bo - check if the buffer object is an &gsgpu_bo
+ * @bo: buffer object to be checked
+ *
+ * Uses destroy function associated with the object to determine if this is
+ * an &gsgpu_bo.
+ *
+ * Returns:
+ * true if the object belongs to &gsgpu_bo, false if not.
+ */
+bool gsgpu_bo_is_gsgpu_bo(struct ttm_buffer_object *bo)
+{
+	if (bo->destroy == &gsgpu_bo_destroy)
+		return true;
+	return false;
+}
+
+/**
+ * gsgpu_bo_placement_from_domain - set buffer's placement
+ * @abo: &gsgpu_bo buffer object whose placement is to be set
+ * @domain: requested domain
+ *
+ * Sets buffer's placement according to requested domain and the buffer's
+ * flags.
+ */
+void gsgpu_bo_placement_from_domain(struct gsgpu_bo *abo, u32 domain)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(abo->tbo.bdev);
+	struct ttm_placement *placement = &abo->placement;
+	struct ttm_place *places = abo->placements;
+	u64 flags = abo->flags;
+	u32 c = 0;
+
+	if (domain & GSGPU_GEM_DOMAIN_VRAM) {
+		unsigned visible_pfn = adev->gmc.visible_vram_size >> PAGE_SHIFT;
+
+		places[c].fpfn = 0;
+		places[c].lpfn = 0;
+		places[c].flags = TTM_PL_FLAG_UNCACHED | TTM_PL_FLAG_VRAM | TTM_PL_FLAG_WC;
+
+		if (flags & GSGPU_GEM_CREATE_COMPRESSED_MASK)
+			places[c].flags |= TTM_PL_FLAG_NO_EVICT;
+
+		if (flags & GSGPU_GEM_CREATE_CPU_GTT_USWC)
+			places[c].flags |= TTM_PL_FLAG_WC;
+
+		if (flags & GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)
+			places[c].lpfn = visible_pfn;
+		else
+			places[c].flags |= TTM_PL_FLAG_TOPDOWN;
+
+		if (flags & GSGPU_GEM_CREATE_VRAM_CONTIGUOUS)
+			places[c].flags |= TTM_PL_FLAG_CONTIGUOUS;
+		c++;
+	}
+
+	if (domain & GSGPU_GEM_DOMAIN_GTT) {
+		places[c].fpfn = 0;
+		if (flags & GSGPU_GEM_CREATE_SHADOW)
+			places[c].lpfn = adev->gmc.gart_size >> PAGE_SHIFT;
+		else
+			places[c].lpfn = 0;
+		places[c].flags = TTM_PL_FLAG_TT;
+		if (flags & GSGPU_GEM_CREATE_CPU_GTT_USWC)
+			places[c].flags |= TTM_PL_FLAG_WC |
+				TTM_PL_FLAG_UNCACHED;
+		else if (dev_is_coherent(NULL))
+			places[c].flags |= TTM_PL_FLAG_CACHED;
+		else
+			places[c].flags |= TTM_PL_FLAG_UNCACHED;
+		c++;
+	}
+
+	if (domain & GSGPU_GEM_DOMAIN_CPU) {
+		places[c].fpfn = 0;
+		places[c].lpfn = 0;
+		places[c].flags = TTM_PL_FLAG_SYSTEM;
+		if (flags & GSGPU_GEM_CREATE_CPU_GTT_USWC)
+			places[c].flags |= TTM_PL_FLAG_WC |
+				TTM_PL_FLAG_UNCACHED;
+		else if (dev_is_coherent(NULL))
+			places[c].flags |= TTM_PL_FLAG_CACHED;
+		else
+			places[c].flags |= TTM_PL_FLAG_UNCACHED;
+		c++;
+	}
+
+	if (!c) {
+		places[c].fpfn = 0;
+		places[c].lpfn = 0;
+		if (dev_is_coherent(NULL))
+			places[c].flags = TTM_PL_MASK_CACHING | TTM_PL_FLAG_SYSTEM;
+		else
+			places[c].flags = TTM_PL_FLAG_WC | TTM_PL_FLAG_UNCACHED | TTM_PL_FLAG_SYSTEM;
+		c++;
+	}
+
+	BUG_ON(c >= GSGPU_BO_MAX_PLACEMENTS);
+
+	placement->num_placement = c;
+	placement->placement = places;
+
+	placement->num_busy_placement = c;
+	placement->busy_placement = places;
+}
+
+/**
+ * gsgpu_bo_create_reserved - create reserved BO for kernel use
+ *
+ * @adev: gsgpu device object
+ * @size: size for the new BO
+ * @align: alignment for the new BO
+ * @domain: where to place it
+ * @bo_ptr: used to initialize BOs in structures
+ * @gpu_addr: GPU addr of the pinned BO
+ * @cpu_addr: optional CPU address mapping
+ *
+ * Allocates and pins a BO for kernel internal use, and returns it still
+ * reserved.
+ *
+ * Note: For bo_ptr new BO is only created if bo_ptr points to NULL.
+ *
+ * Returns:
+ * 0 on success, negative error code otherwise.
+ */
+int gsgpu_bo_create_reserved(struct gsgpu_device *adev,
+			      unsigned long size, int align,
+			      u32 domain, struct gsgpu_bo **bo_ptr,
+			      u64 *gpu_addr, void **cpu_addr)
+{
+	struct gsgpu_bo_param bp;
+	bool free = false;
+	int r;
+
+	memset(&bp, 0, sizeof(bp));
+	bp.size = size;
+	bp.byte_align = align;
+	bp.domain = domain;
+	bp.flags = GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
+		GSGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	bp.type = ttm_bo_type_kernel;
+	bp.resv = NULL;
+
+	if (!*bo_ptr) {
+		r = gsgpu_bo_create(adev, &bp, bo_ptr);
+		if (r) {
+			dev_err(adev->dev, "(%d) failed to allocate kernel bo\n",
+				r);
+			return r;
+		}
+		free = true;
+	}
+
+	r = gsgpu_bo_reserve(*bo_ptr, false);
+	if (r) {
+		dev_err(adev->dev, "(%d) failed to reserve kernel bo\n", r);
+		goto error_free;
+	}
+
+	r = gsgpu_bo_pin(*bo_ptr, domain);
+	if (r) {
+		dev_err(adev->dev, "(%d) kernel bo pin failed\n", r);
+		goto error_unreserve;
+	}
+
+	r = gsgpu_ttm_alloc_gart(&(*bo_ptr)->tbo);
+	if (r) {
+		dev_err(adev->dev, "%p bind failed\n", *bo_ptr);
+		goto error_unpin;
+	}
+
+	if (gpu_addr)
+		*gpu_addr = gsgpu_bo_gpu_offset(*bo_ptr);
+
+	if (cpu_addr) {
+		r = gsgpu_bo_kmap(*bo_ptr, cpu_addr);
+		if (r) {
+			dev_err(adev->dev, "(%d) kernel bo map failed\n", r);
+			goto error_unpin;
+		}
+	}
+
+	return 0;
+
+error_unpin:
+	gsgpu_bo_unpin(*bo_ptr);
+error_unreserve:
+	gsgpu_bo_unreserve(*bo_ptr);
+
+error_free:
+	if (free)
+		gsgpu_bo_unref(bo_ptr);
+
+	return r;
+}
+
+/**
+ * gsgpu_bo_create_kernel - create BO for kernel use
+ *
+ * @adev: gsgpu device object
+ * @size: size for the new BO
+ * @align: alignment for the new BO
+ * @domain: where to place it
+ * @bo_ptr:  used to initialize BOs in structures
+ * @gpu_addr: GPU addr of the pinned BO
+ * @cpu_addr: optional CPU address mapping
+ *
+ * Allocates and pins a BO for kernel internal use.
+ *
+ * Note: For bo_ptr new BO is only created if bo_ptr points to NULL.
+ *
+ * Returns:
+ * 0 on success, negative error code otherwise.
+ */
+int gsgpu_bo_create_kernel(struct gsgpu_device *adev,
+			    unsigned long size, int align,
+			    u32 domain, struct gsgpu_bo **bo_ptr,
+			    u64 *gpu_addr, void **cpu_addr)
+{
+	int r;
+
+	r = gsgpu_bo_create_reserved(adev, size, align, domain, bo_ptr,
+				      gpu_addr, cpu_addr);
+
+	if (r)
+		return r;
+
+	gsgpu_bo_unreserve(*bo_ptr);
+
+	return 0;
+}
+
+/**
+ * gsgpu_bo_free_kernel - free BO for kernel use
+ *
+ * @bo: gsgpu BO to free
+ * @gpu_addr: pointer to where the BO's GPU memory space address was stored
+ * @cpu_addr: pointer to where the BO's CPU memory space address was stored
+ *
+ * unmaps and unpin a BO for kernel internal use.
+ */
+void gsgpu_bo_free_kernel(struct gsgpu_bo **bo, u64 *gpu_addr,
+			   void **cpu_addr)
+{
+	if (*bo == NULL)
+		return;
+
+	if (likely(gsgpu_bo_reserve(*bo, true) == 0)) {
+		if (cpu_addr)
+			gsgpu_bo_kunmap(*bo);
+
+		gsgpu_bo_unpin(*bo);
+		gsgpu_bo_unreserve(*bo);
+	}
+	gsgpu_bo_unref(bo);
+
+	if (gpu_addr)
+		*gpu_addr = 0;
+
+	if (cpu_addr)
+		*cpu_addr = NULL;
+}
+
+/* Validate bo size is bit bigger then the request domain */
+static bool gsgpu_bo_validate_size(struct gsgpu_device *adev,
+					  unsigned long size, u32 domain)
+{
+	struct ttm_mem_type_manager *man = NULL;
+
+	/*
+	 * If GTT is part of requested domains the check must succeed to
+	 * allow fall back to GTT
+	 */
+	if (domain & GSGPU_GEM_DOMAIN_GTT) {
+		man = &adev->mman.bdev.man[TTM_PL_TT];
+
+		if (size < (man->size << PAGE_SHIFT))
+			return true;
+		else
+			goto fail;
+	}
+
+	if (domain & GSGPU_GEM_DOMAIN_VRAM) {
+		man = &adev->mman.bdev.man[TTM_PL_VRAM];
+
+		if (size < (man->size << PAGE_SHIFT))
+			return true;
+		else
+			goto fail;
+	}
+
+
+	/* TODO add more domains checks, such as GSGPU_GEM_DOMAIN_CPU */
+	return true;
+
+fail:
+	DRM_DEBUG("BO size %lu > total memory in domain: %llu\n", size,
+		  man->size << PAGE_SHIFT);
+	return false;
+}
+
+static int gsgpu_bo_do_create(struct gsgpu_device *adev,
+			       struct gsgpu_bo_param *bp,
+			       struct gsgpu_bo **bo_ptr)
+{
+	struct ttm_operation_ctx ctx = {
+		.interruptible = (bp->type != ttm_bo_type_kernel),
+		.no_wait_gpu = false,
+		.resv = bp->resv,
+		.flags = TTM_OPT_FLAG_ALLOW_RES_EVICT
+	};
+	struct gsgpu_bo *bo;
+	unsigned long page_align, size = bp->size;
+	size_t acc_size;
+	int r;
+
+	page_align = roundup(bp->byte_align, PAGE_SIZE) >> PAGE_SHIFT;
+	size = ALIGN(size, PAGE_SIZE);
+
+	if (!gsgpu_bo_validate_size(adev, size, bp->domain))
+		return -ENOMEM;
+
+	*bo_ptr = NULL;
+
+	acc_size = ttm_bo_dma_acc_size(&adev->mman.bdev, size,
+				       sizeof(struct gsgpu_bo));
+
+	bo = kzalloc(sizeof(struct gsgpu_bo), GFP_KERNEL);
+	if (bo == NULL)
+		return -ENOMEM;
+	drm_gem_private_object_init(adev->ddev, &bo->gem_base, size);
+	INIT_LIST_HEAD(&bo->shadow_list);
+	INIT_LIST_HEAD(&bo->va);
+	bo->preferred_domains = bp->preferred_domain ? bp->preferred_domain :
+		bp->domain;
+	bo->allowed_domains = bo->preferred_domains;
+	if (bp->type != ttm_bo_type_kernel &&
+	    bo->allowed_domains == GSGPU_GEM_DOMAIN_VRAM)
+		bo->allowed_domains |= GSGPU_GEM_DOMAIN_GTT;
+
+	bo->flags = bp->flags;
+
+	if (!drm_arch_can_wc_memory())
+		bo->flags &= ~GSGPU_GEM_CREATE_CPU_GTT_USWC;
+
+	bo->tbo.bdev = &adev->mman.bdev;
+	gsgpu_bo_placement_from_domain(bo, bp->domain);
+	if (bp->type == ttm_bo_type_kernel)
+		bo->tbo.priority = 1;
+
+	r = ttm_bo_init_reserved(&adev->mman.bdev, &bo->tbo, size, bp->type,
+				 &bo->placement, page_align, &ctx, acc_size,
+				 NULL, bp->resv, &gsgpu_bo_destroy);
+	if (unlikely(r != 0))
+		return r;
+
+	if (!gsgpu_gmc_vram_full_visible(&adev->gmc) &&
+	    bo->tbo.mem.mem_type == TTM_PL_VRAM &&
+	    bo->tbo.mem.start < adev->gmc.visible_vram_size >> PAGE_SHIFT)
+		gsgpu_cs_report_moved_bytes(adev, ctx.bytes_moved,
+					     ctx.bytes_moved);
+	else
+		gsgpu_cs_report_moved_bytes(adev, ctx.bytes_moved, 0);
+
+	if (bp->flags & GSGPU_GEM_CREATE_VRAM_CLEARED &&
+	    bo->tbo.mem.placement & TTM_PL_FLAG_VRAM) {
+		struct dma_fence *fence;
+
+		r = gsgpu_fill_buffer(bo, 0, bo->tbo.resv, &fence);
+		if (unlikely(r))
+			goto fail_unreserve;
+
+		gsgpu_bo_fence(bo, fence, false);
+		dma_fence_put(bo->tbo.moving);
+		bo->tbo.moving = dma_fence_get(fence);
+		dma_fence_put(fence);
+	}
+
+	if (!bp->resv)
+		gsgpu_bo_unreserve(bo);
+	*bo_ptr = bo;
+
+	trace_gsgpu_bo_create(bo);
+
+	/* Treat CPU_ACCESS_REQUIRED only as a hint if given by UMD */
+	if (bp->type == ttm_bo_type_device)
+		bo->flags &= ~GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+
+	return 0;
+
+fail_unreserve:
+	if (!bp->resv)
+		ww_mutex_unlock(&bo->tbo.resv->lock);
+	gsgpu_bo_unref(&bo);
+	return r;
+}
+
+static int gsgpu_bo_create_shadow(struct gsgpu_device *adev,
+				   unsigned long size, int byte_align,
+				   struct gsgpu_bo *bo)
+{
+	struct gsgpu_bo_param bp;
+	int r;
+
+	if (bo->shadow)
+		return 0;
+
+	memset(&bp, 0, sizeof(bp));
+	bp.size = size;
+	bp.byte_align = byte_align;
+	bp.domain = GSGPU_GEM_DOMAIN_GTT;
+	bp.flags = GSGPU_GEM_CREATE_CPU_GTT_USWC |
+		GSGPU_GEM_CREATE_SHADOW;
+	bp.type = ttm_bo_type_kernel;
+	bp.resv = bo->tbo.resv;
+
+	r = gsgpu_bo_do_create(adev, &bp, &bo->shadow);
+	if (!r) {
+		bo->shadow->parent = gsgpu_bo_ref(bo);
+		mutex_lock(&adev->shadow_list_lock);
+		list_add_tail(&bo->shadow_list, &adev->shadow_list);
+		mutex_unlock(&adev->shadow_list_lock);
+	}
+
+	return r;
+}
+
+/**
+ * gsgpu_bo_create - create an &gsgpu_bo buffer object
+ * @adev: gsgpu device object
+ * @bp: parameters to be used for the buffer object
+ * @bo_ptr: pointer to the buffer object pointer
+ *
+ * Creates an &gsgpu_bo buffer object; and if requested, also creates a
+ * shadow object.
+ * Shadow object is used to backup the original buffer object, and is always
+ * in GTT.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_create(struct gsgpu_device *adev,
+		     struct gsgpu_bo_param *bp,
+		     struct gsgpu_bo **bo_ptr)
+{
+	u64 flags = bp->flags;
+	int r;
+
+	bp->flags = bp->flags & ~GSGPU_GEM_CREATE_SHADOW;
+	r = gsgpu_bo_do_create(adev, bp, bo_ptr);
+	if (r)
+		return r;
+
+	if ((flags & GSGPU_GEM_CREATE_SHADOW) && gsgpu_bo_need_backup(adev)) {
+		if (!bp->resv)
+			WARN_ON(reservation_object_lock((*bo_ptr)->tbo.resv,
+							NULL));
+
+		r = gsgpu_bo_create_shadow(adev, bp->size, bp->byte_align, (*bo_ptr));
+
+		if (!bp->resv)
+			reservation_object_unlock((*bo_ptr)->tbo.resv);
+
+		if (r)
+			gsgpu_bo_unref(bo_ptr);
+	}
+
+	return r;
+}
+
+/**
+ * gsgpu_bo_backup_to_shadow - Backs up an &gsgpu_bo buffer object
+ * @adev: gsgpu device object
+ * @ring: gsgpu_ring for the engine handling the buffer operations
+ * @bo: &gsgpu_bo buffer to be backed up
+ * @resv: reservation object with embedded fence
+ * @fence: dma_fence associated with the operation
+ * @direct: whether to submit the job directly
+ *
+ * Copies an &gsgpu_bo buffer object to its shadow object.
+ * Not used for now.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_backup_to_shadow(struct gsgpu_device *adev,
+			       struct gsgpu_ring *ring,
+			       struct gsgpu_bo *bo,
+			       struct reservation_object *resv,
+			       struct dma_fence **fence,
+			       bool direct)
+
+{
+	struct gsgpu_bo *shadow = bo->shadow;
+	uint64_t bo_addr, shadow_addr;
+	int r;
+
+	if (!shadow)
+		return -EINVAL;
+
+	bo_addr = gsgpu_bo_gpu_offset(bo);
+	shadow_addr = gsgpu_bo_gpu_offset(bo->shadow);
+
+	r = reservation_object_reserve_shared(bo->tbo.resv);
+	if (r)
+		goto err;
+
+	r = gsgpu_copy_buffer(ring, bo_addr, shadow_addr,
+			       gsgpu_bo_size(bo), resv, fence,
+			       direct, false);
+	if (!r)
+		gsgpu_bo_fence(bo, *fence, true);
+
+err:
+	return r;
+}
+
+/**
+ * gsgpu_bo_validate - validate an &gsgpu_bo buffer object
+ * @bo: pointer to the buffer object
+ *
+ * Sets placement according to domain; and changes placement and caching
+ * policy of the buffer object according to the placement.
+ * This is used for validating shadow bos.  It calls ttm_bo_validate() to
+ * make sure the buffer is resident where it needs to be.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_validate(struct gsgpu_bo *bo)
+{
+	struct ttm_operation_ctx ctx = { false, false };
+	uint32_t domain;
+	int r;
+
+	if (bo->pin_count)
+		return 0;
+
+	domain = bo->preferred_domains;
+
+retry:
+	gsgpu_bo_placement_from_domain(bo, domain);
+	r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+	if (unlikely(r == -ENOMEM) && domain != bo->allowed_domains) {
+		domain = bo->allowed_domains;
+		goto retry;
+	}
+
+	return r;
+}
+
+/**
+ * gsgpu_bo_restore_from_shadow - restore an &gsgpu_bo buffer object
+ * @adev: gsgpu device object
+ * @ring: gsgpu_ring for the engine handling the buffer operations
+ * @bo: &gsgpu_bo buffer to be restored
+ * @resv: reservation object with embedded fence
+ * @fence: dma_fence associated with the operation
+ * @direct: whether to submit the job directly
+ *
+ * Copies a buffer object's shadow content back to the object.
+ * This is used for recovering a buffer from its shadow in case of a gpu
+ * reset where vram context may be lost.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_restore_from_shadow(struct gsgpu_device *adev,
+				  struct gsgpu_ring *ring,
+				  struct gsgpu_bo *bo,
+				  struct reservation_object *resv,
+				  struct dma_fence **fence,
+				  bool direct)
+
+{
+	struct gsgpu_bo *shadow = bo->shadow;
+	uint64_t bo_addr, shadow_addr;
+	int r;
+
+	if (!shadow)
+		return -EINVAL;
+
+	bo_addr = gsgpu_bo_gpu_offset(bo);
+	shadow_addr = gsgpu_bo_gpu_offset(bo->shadow);
+
+	r = reservation_object_reserve_shared(bo->tbo.resv);
+	if (r)
+		goto err;
+
+	r = gsgpu_copy_buffer(ring, shadow_addr, bo_addr,
+			       gsgpu_bo_size(bo), resv, fence,
+			       direct, false);
+	if (!r)
+		gsgpu_bo_fence(bo, *fence, true);
+
+err:
+	return r;
+}
+
+/**
+ * gsgpu_bo_kmap - map an &gsgpu_bo buffer object
+ * @bo: &gsgpu_bo buffer object to be mapped
+ * @ptr: kernel virtual address to be returned
+ *
+ * Calls ttm_bo_kmap() to set up the kernel virtual mapping; calls
+ * gsgpu_bo_kptr() to get the kernel virtual address.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_kmap(struct gsgpu_bo *bo, void **ptr)
+{
+	void *kptr;
+	long r;
+
+	if (bo->flags & GSGPU_GEM_CREATE_NO_CPU_ACCESS)
+		return -EPERM;
+
+	kptr = gsgpu_bo_kptr(bo);
+	if (kptr) {
+		if (ptr)
+			*ptr = kptr;
+		return 0;
+	}
+
+	r = reservation_object_wait_timeout_rcu(bo->tbo.resv, false, false,
+						MAX_SCHEDULE_TIMEOUT);
+	if (r < 0)
+		return r;
+
+	r = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.num_pages, &bo->kmap);
+	if (r)
+		return r;
+
+	if (ptr)
+		*ptr = gsgpu_bo_kptr(bo);
+
+	return 0;
+}
+
+/**
+ * gsgpu_bo_kptr - returns a kernel virtual address of the buffer object
+ * @bo: &gsgpu_bo buffer object
+ *
+ * Calls ttm_kmap_obj_virtual() to get the kernel virtual address
+ *
+ * Returns:
+ * the virtual address of a buffer object area.
+ */
+void *gsgpu_bo_kptr(struct gsgpu_bo *bo)
+{
+	bool is_iomem;
+
+	return ttm_kmap_obj_virtual(&bo->kmap, &is_iomem);
+}
+
+/**
+ * gsgpu_bo_kunmap - unmap an &gsgpu_bo buffer object
+ * @bo: &gsgpu_bo buffer object to be unmapped
+ *
+ * Unmaps a kernel map set up by gsgpu_bo_kmap().
+ */
+void gsgpu_bo_kunmap(struct gsgpu_bo *bo)
+{
+	if (bo->kmap.bo)
+		ttm_bo_kunmap(&bo->kmap);
+}
+
+/**
+ * gsgpu_bo_ref - reference an &gsgpu_bo buffer object
+ * @bo: &gsgpu_bo buffer object
+ *
+ * References the contained &ttm_buffer_object.
+ *
+ * Returns:
+ * a refcounted pointer to the &gsgpu_bo buffer object.
+ */
+struct gsgpu_bo *gsgpu_bo_ref(struct gsgpu_bo *bo)
+{
+	if (bo == NULL)
+		return NULL;
+
+	ttm_bo_get(&bo->tbo);
+	return bo;
+}
+
+/**
+ * gsgpu_bo_unref - unreference an &gsgpu_bo buffer object
+ * @bo: &gsgpu_bo buffer object
+ *
+ * Unreferences the contained &ttm_buffer_object and clear the pointer
+ */
+void gsgpu_bo_unref(struct gsgpu_bo **bo)
+{
+	struct ttm_buffer_object *tbo;
+
+	if ((*bo) == NULL)
+		return;
+
+	tbo = &((*bo)->tbo);
+	ttm_bo_put(tbo);
+	*bo = NULL;
+}
+
+/**
+ * gsgpu_bo_pin_restricted - pin an &gsgpu_bo buffer object
+ * @bo: &gsgpu_bo buffer object to be pinned
+ * @domain: domain to be pinned to
+ * @min_offset: the start of requested address range
+ * @max_offset: the end of requested address range
+ *
+ * Pins the buffer object according to requested domain and address range. If
+ * the memory is unbound gart memory, binds the pages into gart table. Adjusts
+ * pin_count and pin_size accordingly.
+ *
+ * Pinning means to lock pages in memory along with keeping them at a fixed
+ * offset. It is required when a buffer can not be moved, for example, when
+ * a display buffer is being scanned out.
+ *
+ * Compared with gsgpu_bo_pin(), this function gives more flexibility on
+ * where to pin a buffer if there are specific restrictions on where a buffer
+ * must be located.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_pin_restricted(struct gsgpu_bo *bo, u32 domain,
+			     u64 min_offset, u64 max_offset)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	struct ttm_operation_ctx ctx = { false, false };
+	int r, i;
+
+	if (gsgpu_ttm_tt_get_usermm(bo->tbo.ttm))
+		return -EPERM;
+
+	if (WARN_ON_ONCE(min_offset > max_offset))
+		return -EINVAL;
+
+	/* A shared bo cannot be migrated to VRAM */
+	if (bo->prime_shared_count) {
+		if (domain & GSGPU_GEM_DOMAIN_GTT)
+			domain = GSGPU_GEM_DOMAIN_GTT;
+		else
+			return -EINVAL;
+	}
+
+	/* This assumes only APU display buffers are pinned with (VRAM|GTT).
+	 * See function gsgpu_display_supported_domains()
+	 */
+	domain = gsgpu_bo_get_preferred_pin_domain(adev, domain);
+
+	if (bo->pin_count) {
+		uint32_t mem_type = bo->tbo.mem.mem_type;
+
+		if (!(domain & gsgpu_mem_type_to_domain(mem_type)))
+			return -EINVAL;
+
+		bo->pin_count++;
+
+		if (max_offset != 0) {
+			u64 domain_start = bo->tbo.bdev->man[mem_type].gpu_offset;
+			WARN_ON_ONCE(max_offset <
+				     (gsgpu_bo_gpu_offset(bo) - domain_start));
+		}
+
+		return 0;
+	}
+
+	bo->flags |= GSGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	/* force to pin into visible video ram */
+	if (!(bo->flags & GSGPU_GEM_CREATE_NO_CPU_ACCESS))
+		bo->flags |= GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+	gsgpu_bo_placement_from_domain(bo, domain);
+	for (i = 0; i < bo->placement.num_placement; i++) {
+		unsigned fpfn, lpfn;
+
+		fpfn = min_offset >> PAGE_SHIFT;
+		lpfn = max_offset >> PAGE_SHIFT;
+
+		if (fpfn > bo->placements[i].fpfn)
+			bo->placements[i].fpfn = fpfn;
+		if (!bo->placements[i].lpfn ||
+		    (lpfn && lpfn < bo->placements[i].lpfn))
+			bo->placements[i].lpfn = lpfn;
+		bo->placements[i].flags |= TTM_PL_FLAG_NO_EVICT;
+	}
+
+	r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+	if (unlikely(r)) {
+		dev_err(adev->dev, "%p pin failed\n", bo);
+		goto error;
+	}
+
+	bo->pin_count = 1;
+
+	domain = gsgpu_mem_type_to_domain(bo->tbo.mem.mem_type);
+	if (domain == GSGPU_GEM_DOMAIN_VRAM) {
+		atomic64_add(gsgpu_bo_size(bo), &adev->vram_pin_size);
+		atomic64_add(gsgpu_vram_mgr_bo_visible_size(bo),
+			     &adev->visible_pin_size);
+	} else if (domain == GSGPU_GEM_DOMAIN_GTT) {
+		atomic64_add(gsgpu_bo_size(bo), &adev->gart_pin_size);
+	}
+
+error:
+	return r;
+}
+
+/**
+ * gsgpu_bo_pin - pin an &gsgpu_bo buffer object
+ * @bo: &gsgpu_bo buffer object to be pinned
+ * @domain: domain to be pinned to
+ *
+ * A simple wrapper to gsgpu_bo_pin_restricted().
+ * Provides a simpler API for buffers that do not have any strict restrictions
+ * on where a buffer must be located.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_pin(struct gsgpu_bo *bo, u32 domain)
+{
+	return gsgpu_bo_pin_restricted(bo, domain, 0, 0);
+}
+
+/**
+ * gsgpu_bo_unpin - unpin an &gsgpu_bo buffer object
+ * @bo: &gsgpu_bo buffer object to be unpinned
+ *
+ * Decreases the pin_count, and clears the flags if pin_count reaches 0.
+ * Changes placement and pin size accordingly.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_unpin(struct gsgpu_bo *bo)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	struct ttm_operation_ctx ctx = { false, false };
+	int r, i;
+
+	if (!bo->pin_count) {
+		dev_warn(adev->dev, "%p unpin not necessary\n", bo);
+		return 0;
+	}
+	bo->pin_count--;
+	if (bo->pin_count)
+		return 0;
+
+	gsgpu_bo_subtract_pin_size(bo);
+
+	for (i = 0; i < bo->placement.num_placement; i++) {
+		bo->placements[i].lpfn = 0;
+		bo->placements[i].flags &= ~TTM_PL_FLAG_NO_EVICT;
+	}
+	r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+	if (unlikely(r))
+		dev_err(adev->dev, "%p validate failed for unpin\n", bo);
+
+	return r;
+}
+
+/**
+ * gsgpu_bo_evict_vram - evict VRAM buffers
+ * @adev: gsgpu device object
+ *
+ * Evicts all VRAM buffers on the lru list of the memory type.
+ * Mainly used for evicting vram at suspend time.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_evict_vram(struct gsgpu_device *adev)
+{
+	/* late 2.6.33 fix IGP hibernate - we need pm ops to do this correct */
+	if (0 && (adev->flags & GSGPU_IS_APU)) {
+		/* Useless to evict on IGP chips */
+		return 0;
+	}
+	return ttm_bo_evict_mm(&adev->mman.bdev, TTM_PL_VRAM);
+}
+
+static const char *gsgpu_vram_names[] = {
+	"UNKNOWN",
+	"GDDR1",
+	"DDR2",
+	"GDDR3",
+	"GDDR4",
+	"GDDR5",
+	"HBM",
+	"DDR3",
+	"DDR4",
+};
+
+/**
+ * gsgpu_bo_init - initialize memory manager
+ * @adev: gsgpu device object
+ *
+ * Calls gsgpu_ttm_init() to initialize gsgpu memory manager.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_init(struct gsgpu_device *adev)
+{
+	/* reserve PAT memory space to WC for VRAM */
+	arch_io_reserve_memtype_wc(adev->gmc.aper_base,
+				   adev->gmc.aper_size);
+
+	/* Add an MTRR for the VRAM */
+	adev->gmc.vram_mtrr = arch_phys_wc_add(adev->gmc.aper_base,
+					      adev->gmc.aper_size);
+	DRM_INFO("Detected VRAM RAM=%lluM, BAR=%lluM\n",
+		 adev->gmc.mc_vram_size >> 20,
+		 (unsigned long long)adev->gmc.aper_size >> 20);
+	DRM_INFO("RAM width %dbits %s\n",
+		 adev->gmc.vram_width, gsgpu_vram_names[adev->gmc.vram_type]);
+	return gsgpu_ttm_init(adev);
+}
+
+/**
+ * gsgpu_bo_late_init - late init
+ * @adev: gsgpu device object
+ *
+ * Calls gsgpu_ttm_late_init() to free resources used earlier during
+ * initialization.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_late_init(struct gsgpu_device *adev)
+{
+	gsgpu_ttm_late_init(adev);
+
+	return 0;
+}
+
+/**
+ * gsgpu_bo_fini - tear down memory manager
+ * @adev: gsgpu device object
+ *
+ * Reverses gsgpu_bo_init() to tear down memory manager.
+ */
+void gsgpu_bo_fini(struct gsgpu_device *adev)
+{
+	gsgpu_ttm_fini(adev);
+	arch_phys_wc_del(adev->gmc.vram_mtrr);
+	arch_io_free_memtype_wc(adev->gmc.aper_base, adev->gmc.aper_size);
+}
+
+/**
+ * gsgpu_bo_fbdev_mmap - mmap fbdev memory
+ * @bo: &gsgpu_bo buffer object
+ * @vma: vma as input from the fbdev mmap method
+ *
+ * Calls ttm_fbdev_mmap() to mmap fbdev memory if it is backed by a bo.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_fbdev_mmap(struct gsgpu_bo *bo,
+			     struct vm_area_struct *vma)
+{
+	return ttm_fbdev_mmap(vma, &bo->tbo);
+}
+
+/**
+ * gsgpu_bo_set_tiling_flags - set tiling flags
+ * @bo: &gsgpu_bo buffer object
+ * @tiling_flags: new flags
+ *
+ * Sets buffer object's tiling flags with the new one. Used by GEM ioctl or
+ * kernel driver to set the tiling flags on a buffer.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_set_tiling_flags(struct gsgpu_bo *bo, u64 tiling_flags)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+
+	if (adev->family <= GSGPU_FAMILY_CZ &&
+	    GSGPU_TILING_GET(tiling_flags, TILE_SPLIT) > 6)
+		return -EINVAL;
+
+	bo->tiling_flags = tiling_flags;
+	return 0;
+}
+
+/**
+ * gsgpu_bo_get_tiling_flags - get tiling flags
+ * @bo: &gsgpu_bo buffer object
+ * @tiling_flags: returned flags
+ *
+ * Gets buffer object's tiling flags. Used by GEM ioctl or kernel driver to
+ * set the tiling flags on a buffer.
+ */
+void gsgpu_bo_get_tiling_flags(struct gsgpu_bo *bo, u64 *tiling_flags)
+{
+	lockdep_assert_held(&bo->tbo.resv->lock.base);
+
+	if (tiling_flags)
+		*tiling_flags = bo->tiling_flags;
+}
+
+/**
+ * gsgpu_bo_set_metadata - set metadata
+ * @bo: &gsgpu_bo buffer object
+ * @metadata: new metadata
+ * @metadata_size: size of the new metadata
+ * @flags: flags of the new metadata
+ *
+ * Sets buffer object's metadata, its size and flags.
+ * Used via GEM ioctl.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_set_metadata (struct gsgpu_bo *bo, void *metadata,
+			    uint32_t metadata_size, uint64_t flags)
+{
+	void *buffer;
+
+	if (!metadata_size) {
+		if (bo->metadata_size) {
+			kfree(bo->metadata);
+			bo->metadata = NULL;
+			bo->metadata_size = 0;
+		}
+		return 0;
+	}
+
+	if (metadata == NULL)
+		return -EINVAL;
+
+	buffer = kmemdup(metadata, metadata_size, GFP_KERNEL);
+	if (buffer == NULL)
+		return -ENOMEM;
+
+	kfree(bo->metadata);
+	bo->metadata_flags = flags;
+	bo->metadata = buffer;
+	bo->metadata_size = metadata_size;
+
+	return 0;
+}
+
+/**
+ * gsgpu_bo_get_metadata - get metadata
+ * @bo: &gsgpu_bo buffer object
+ * @buffer: returned metadata
+ * @buffer_size: size of the buffer
+ * @metadata_size: size of the returned metadata
+ * @flags: flags of the returned metadata
+ *
+ * Gets buffer object's metadata, its size and flags. buffer_size shall not be
+ * less than metadata_size.
+ * Used via GEM ioctl.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_get_metadata(struct gsgpu_bo *bo, void *buffer,
+			   size_t buffer_size, uint32_t *metadata_size,
+			   uint64_t *flags)
+{
+	if (!buffer && !metadata_size)
+		return -EINVAL;
+
+	if (buffer) {
+		if (buffer_size < bo->metadata_size)
+			return -EINVAL;
+
+		if (bo->metadata_size)
+			memcpy(buffer, bo->metadata, bo->metadata_size);
+	}
+
+	if (metadata_size)
+		*metadata_size = bo->metadata_size;
+	if (flags)
+		*flags = bo->metadata_flags;
+
+	return 0;
+}
+
+/**
+ * gsgpu_bo_move_notify - notification about a memory move
+ * @bo: pointer to a buffer object
+ * @evict: if this move is evicting the buffer from the graphics address space
+ * @new_mem: new information of the bufer object
+ *
+ * Marks the corresponding &gsgpu_bo buffer object as invalid, also performs
+ * bookkeeping.
+ * TTM driver callback which is called when ttm moves a buffer.
+ */
+void gsgpu_bo_move_notify(struct ttm_buffer_object *bo,
+			   bool evict,
+			   struct ttm_mem_reg *new_mem)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->bdev);
+	struct gsgpu_bo *abo;
+	struct ttm_mem_reg *old_mem = &bo->mem;
+
+	if (!gsgpu_bo_is_gsgpu_bo(bo))
+		return;
+
+	abo = ttm_to_gsgpu_bo(bo);
+	gsgpu_vm_bo_invalidate(adev, abo, evict);
+
+	gsgpu_bo_kunmap(abo);
+
+	/* remember the eviction */
+	if (evict)
+		atomic64_inc(&adev->num_evictions);
+
+	/* update statistics */
+	if (!new_mem)
+		return;
+
+	/* move_notify is called before move happens */
+	trace_gsgpu_bo_move(abo, new_mem->mem_type, old_mem->mem_type);
+}
+
+/**
+ * gsgpu_bo_fault_reserve_notify - notification about a memory fault
+ * @bo: pointer to a buffer object
+ *
+ * Notifies the driver we are taking a fault on this BO and have reserved it,
+ * also performs bookkeeping.
+ * TTM driver callback for dealing with vm faults.
+ *
+ * Returns:
+ * 0 for success or a negative error code on failure.
+ */
+int gsgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->bdev);
+	struct ttm_operation_ctx ctx = { false, false };
+	struct gsgpu_bo *abo;
+	unsigned long offset, size;
+	int r;
+
+	if (!gsgpu_bo_is_gsgpu_bo(bo))
+		return 0;
+
+	abo = ttm_to_gsgpu_bo(bo);
+
+	/* Remember that this BO was accessed by the CPU */
+	abo->flags |= GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+
+	if (bo->mem.mem_type != TTM_PL_VRAM)
+		return 0;
+
+	size = bo->mem.num_pages << PAGE_SHIFT;
+	offset = bo->mem.start << PAGE_SHIFT;
+	if ((offset + size) <= adev->gmc.visible_vram_size)
+		return 0;
+
+	/* Can't move a pinned BO to visible VRAM */
+	if (abo->pin_count > 0)
+		return -EINVAL;
+
+	/* hurrah the memory is not visible ! */
+	atomic64_inc(&adev->num_vram_cpu_page_faults);
+	gsgpu_bo_placement_from_domain(abo, GSGPU_GEM_DOMAIN_VRAM |
+					GSGPU_GEM_DOMAIN_GTT);
+
+	/* Avoid costly evictions; only set GTT as a busy placement */
+	abo->placement.num_busy_placement = 1;
+	abo->placement.busy_placement = &abo->placements[1];
+
+	r = ttm_bo_validate(bo, &abo->placement, &ctx);
+	if (unlikely(r != 0))
+		return r;
+
+	offset = bo->mem.start << PAGE_SHIFT;
+	/* this should never happen */
+	if (bo->mem.mem_type == TTM_PL_VRAM &&
+	    (offset + size) > adev->gmc.visible_vram_size)
+		return -EINVAL;
+
+	return 0;
+}
+
+/**
+ * gsgpu_bo_fence - add fence to buffer object
+ *
+ * @bo: buffer object in question
+ * @fence: fence to add
+ * @shared: true if fence should be added shared
+ *
+ */
+void gsgpu_bo_fence(struct gsgpu_bo *bo, struct dma_fence *fence,
+		     bool shared)
+{
+	struct reservation_object *resv = bo->tbo.resv;
+
+	if (shared)
+		reservation_object_add_shared_fence(resv, fence);
+	else
+		reservation_object_add_excl_fence(resv, fence);
+}
+
+/**
+ * gsgpu_bo_gpu_offset - return GPU offset of bo
+ * @bo:	gsgpu object for which we query the offset
+ *
+ * Note: object should either be pinned or reserved when calling this
+ * function, it might be useful to add check for this for debugging.
+ *
+ * Returns:
+ * current GPU offset of the object.
+ */
+u64 gsgpu_bo_gpu_offset(struct gsgpu_bo *bo)
+{
+	WARN_ON_ONCE(bo->tbo.mem.mem_type == TTM_PL_SYSTEM);
+	WARN_ON_ONCE(bo->tbo.mem.mem_type == TTM_PL_TT &&
+		     !gsgpu_gtt_mgr_has_gart_addr(&bo->tbo.mem));
+	WARN_ON_ONCE(!ww_mutex_is_locked(&bo->tbo.resv->lock) &&
+		     !bo->pin_count);
+	WARN_ON_ONCE(bo->tbo.mem.start == GSGPU_BO_INVALID_OFFSET);
+	WARN_ON_ONCE(bo->tbo.mem.mem_type == TTM_PL_VRAM &&
+		     !(bo->flags & GSGPU_GEM_CREATE_VRAM_CONTIGUOUS));
+
+	return bo->tbo.offset;
+}
+
+/**
+ * gsgpu_bo_get_preferred_pin_domain - get preferred domain for scanout
+ * @adev: gsgpu device object
+ * @domain: allowed :ref:`memory domains <gsgpu_memory_domains>`
+ *
+ * Returns:
+ * Which of the allowed domains is preferred for pinning the BO for scanout.
+ */
+uint32_t gsgpu_bo_get_preferred_pin_domain(struct gsgpu_device *adev,
+					    uint32_t domain)
+{
+	if (domain == (GSGPU_GEM_DOMAIN_VRAM | GSGPU_GEM_DOMAIN_GTT)) {
+		domain = GSGPU_GEM_DOMAIN_VRAM;
+		if (adev->gmc.real_vram_size <= GSGPU_SG_THRESHOLD)
+			domain = GSGPU_GEM_DOMAIN_GTT;
+	}
+	return domain;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_pm.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_pm.c
new file mode 100644
index 000000000000..5fa0cbb13b40
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_pm.c
@@ -0,0 +1,194 @@
+#include "gsgpu.h"
+#include "gsgpu_dc_resource.h"
+#include "gsgpu_dc_vbios.h"
+
+#define FREQ_LEVEL0 0xf
+#define FREQ_LEVEL1 0xd
+#define FREQ_LEVEL2 0xb
+#define FREQ_LEVEL3 0x9
+#define FREQ_LEVEL4 0x7
+#define FREQ_LEVEL5 0x5
+#define FREQ_LEVEL6 0x3
+#define FREQ_LEVEL7 0x1
+
+#define SET_LEVEL0  0
+#define SET_LEVEL1  1
+#define SET_LEVEL2  2
+#define SET_LEVEL3  3
+#define SET_LEVEL4  4
+#define SET_LEVEL5  5
+#define SET_LEVEL6  6
+#define SET_LEVEL7  7
+
+#define STATIC_FREQ						0x00
+
+static ssize_t gsgpu_get_gpu_clk(struct device *dev,
+		struct device_attribute *attr,
+		char *buf)
+{
+	struct drm_device *ddev = dev_get_drvdata(dev);
+	struct gsgpu_device *adev = ddev->dev_private;
+	struct gpu_resource *gpu_res = NULL;
+
+	int i, now, size = 0;
+	int number = 8;
+	uint32_t regular_freq_count;
+	uint32_t max_freq_value;
+	uint32_t value;
+	uint32_t freq;
+	uint32_t default_freq_count = 3;
+	uint32_t default_freq_value = 480;
+
+	gpu_res = dc_get_vbios_resource(adev->dc->vbios, 0, GSGPU_RESOURCE_GPU);
+
+	if (NULL == gpu_res) {
+		regular_freq_count = default_freq_count;
+		max_freq_value = default_freq_value;
+	} else {
+		regular_freq_count = gpu_res->count_freq;
+		max_freq_value = gpu_res->shaders_freq;
+
+	}
+	value = max_freq_value;
+	freq = RREG32(GSGPU_FREQ_SCALE);
+
+	switch (freq) {
+	case FREQ_LEVEL0:
+		now = SET_LEVEL0;
+		break;
+	case FREQ_LEVEL1:
+		now = SET_LEVEL1;
+		break;
+	case FREQ_LEVEL2:
+		now = SET_LEVEL2;
+		break;
+	case FREQ_LEVEL3:
+		now = SET_LEVEL3;
+		break;
+	case FREQ_LEVEL4:
+		now = SET_LEVEL4;
+		break;
+	case FREQ_LEVEL5:
+		now = SET_LEVEL5;
+		break;
+	case FREQ_LEVEL6:
+		now = SET_LEVEL6;
+		break;
+	case FREQ_LEVEL7:
+		now = SET_LEVEL7;
+		break;
+	default:
+		now = 0;
+		break;
+	}
+
+	for (i = 0; i < regular_freq_count; i++) {
+		size += sprintf(buf + size, "%d: %uMhz %s\n",
+				i, value,
+				(i == now) ? "*" : "");
+		number = number - 1;
+		value = max_freq_value / 8 * number;
+
+	}
+
+	return size;
+}
+
+static ssize_t gsgpu_read_level(const char *buf, size_t count, uint32_t max_level)
+{
+	int ret;
+	char *tmp;
+	char **str;
+	char *sub_str = NULL;
+	char buf_cpy[100];
+	const char delimiter[3] = {' ', '\n', '\0'};
+
+	memcpy(buf_cpy, buf, count);
+	tmp = buf_cpy;
+
+	if (NULL == tmp)
+		return -EINVAL;
+
+	sub_str = strsep(&tmp, delimiter);
+	if (0 == strlen(sub_str))
+		return -EINVAL;
+
+	ret = simple_strtoul(sub_str, str, 10);
+	if (ret >= 0 && ret <= (max_level - 1))
+		return ret;
+	else
+		return -EINVAL;
+}
+
+static ssize_t gsgpu_set_gpu_clk(struct device *dev,
+		struct device_attribute *attr,
+		const char *buf,
+		size_t count)
+{
+	struct drm_device *ddev = dev_get_drvdata(dev);
+	struct gsgpu_device *adev = ddev->dev_private;
+	struct gpu_resource *gpu_res = NULL;
+	uint32_t level;
+	uint32_t regular_freq_count;
+	uint32_t default_freq_count = 3;
+
+	gpu_res = dc_get_vbios_resource(adev->dc->vbios, 0, GSGPU_RESOURCE_GPU);
+
+	if (NULL == gpu_res)
+		regular_freq_count = default_freq_count;
+	else
+		regular_freq_count = gpu_res->count_freq;
+
+	level = gsgpu_read_level(buf, count, regular_freq_count);
+
+	switch (level) {
+	case SET_LEVEL0:
+		level = FREQ_LEVEL0;
+		break;
+	case SET_LEVEL1:
+		level = FREQ_LEVEL1;
+		break;
+	case SET_LEVEL2:
+		level = FREQ_LEVEL2;
+		break;
+	case SET_LEVEL3:
+		level = FREQ_LEVEL3;
+		break;
+	case SET_LEVEL4:
+		level = FREQ_LEVEL4;
+		break;
+	case SET_LEVEL5:
+		level = FREQ_LEVEL5;
+		break;
+	case SET_LEVEL6:
+		level = FREQ_LEVEL6;
+		break;
+	case SET_LEVEL7:
+		level = FREQ_LEVEL7;
+		break;
+	default:
+		level = FREQ_LEVEL0;
+		break;
+	}
+
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_FREQ, STATIC_FREQ), level, 0);
+
+	return count;
+}
+
+static DEVICE_ATTR(gpu_clk, S_IRUGO | S_IWUSR,
+		gsgpu_get_gpu_clk,
+		gsgpu_set_gpu_clk);
+
+int gsgpu_pm_sysfs_init(struct gsgpu_device *adev)
+{
+	int ret;
+
+    ret = device_create_file(adev->dev, &dev_attr_gpu_clk);
+	if (ret) {
+		DRM_ERROR("failed to create device file for gpu clk\n");
+		return ret;
+	}
+
+    return 0;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c
new file mode 100644
index 000000000000..f99fb129a614
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_prime.c
@@ -0,0 +1,418 @@
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include "gsgpu_display.h"
+#include <drm/gsgpu_drm.h>
+#include <linux/dma-buf.h>
+#include <linux/dma-fence-array.h>
+
+static const struct dma_buf_ops gsgpu_dmabuf_ops;
+
+/**
+ * gsgpu_gem_prime_get_sg_table - &drm_driver.gem_prime_get_sg_table
+ * implementation
+ * @obj: GEM buffer object
+ *
+ * Returns:
+ * A scatter/gather table for the pinned pages of the buffer object's memory.
+ */
+struct sg_table *gsgpu_gem_prime_get_sg_table(struct drm_gem_object *obj)
+{
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(obj);
+	int npages = bo->tbo.num_pages;
+
+	return drm_prime_pages_to_sg(bo->tbo.ttm->pages, npages);
+}
+
+/**
+ * gsgpu_gem_prime_vmap - &dma_buf_ops.vmap implementation
+ * @obj: GEM buffer object
+ *
+ * Sets up an in-kernel virtual mapping of the buffer object's memory.
+ *
+ * Returns:
+ * The virtual address of the mapping or an error pointer.
+ */
+void *gsgpu_gem_prime_vmap(struct drm_gem_object *obj)
+{
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(obj);
+	int ret;
+
+	ret = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.num_pages,
+			  &bo->dma_buf_vmap);
+	if (ret)
+		return ERR_PTR(ret);
+
+	return bo->dma_buf_vmap.virtual;
+}
+
+/**
+ * gsgpu_gem_prime_vunmap - &dma_buf_ops.vunmap implementation
+ * @obj: GEM buffer object
+ * @vaddr: virtual address (unused)
+ *
+ * Tears down the in-kernel virtual mapping of the buffer object's memory.
+ */
+void gsgpu_gem_prime_vunmap(struct drm_gem_object *obj, void *vaddr)
+{
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(obj);
+
+	ttm_bo_kunmap(&bo->dma_buf_vmap);
+}
+
+/**
+ * gsgpu_gem_prime_mmap - &drm_driver.gem_prime_mmap implementation
+ * @obj: GEM buffer object
+ * @vma: virtual memory area
+ *
+ * Sets up a userspace mapping of the buffer object's memory in the given
+ * virtual memory area.
+ *
+ * Returns:
+ * 0 on success or negative error code.
+ */
+int gsgpu_gem_prime_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)
+{
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(obj);
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	unsigned asize = gsgpu_bo_size(bo);
+	int ret;
+
+	if (!vma->vm_file)
+		return -ENODEV;
+
+	if (adev == NULL)
+		return -ENODEV;
+
+	/* Check for valid size. */
+	if (asize < vma->vm_end - vma->vm_start)
+		return -EINVAL;
+
+	if (gsgpu_ttm_tt_get_usermm(bo->tbo.ttm) ||
+	    (bo->flags & GSGPU_GEM_CREATE_NO_CPU_ACCESS)) {
+		return -EPERM;
+	}
+	vma->vm_pgoff += gsgpu_bo_mmap_offset(bo) >> PAGE_SHIFT;
+
+	/* prime mmap does not need to check access, so allow here */
+	ret = drm_vma_node_allow(&obj->vma_node, vma->vm_file->private_data);
+	if (ret)
+		return ret;
+
+	ret = ttm_bo_mmap(vma->vm_file, vma, &adev->mman.bdev);
+	drm_vma_node_revoke(&obj->vma_node, vma->vm_file->private_data);
+
+	return ret;
+}
+
+/**
+ * gsgpu_gem_prime_import_sg_table - &drm_driver.gem_prime_import_sg_table
+ * implementation
+ * @dev: DRM device
+ * @attach: DMA-buf attachment
+ * @sg: Scatter/gather table
+ *
+ * Import shared DMA buffer memory exported by another device.
+ *
+ * Returns:
+ * A new GEM buffer object of the given DRM device, representing the memory
+ * described by the given DMA-buf attachment and scatter/gather table.
+ */
+struct drm_gem_object *
+gsgpu_gem_prime_import_sg_table(struct drm_device *dev,
+				 struct dma_buf_attachment *attach,
+				 struct sg_table *sg)
+{
+	struct reservation_object *resv = attach->dmabuf->resv;
+	struct gsgpu_device *adev = dev->dev_private;
+	struct gsgpu_bo *bo;
+	struct gsgpu_bo_param bp;
+	int ret;
+
+	memset(&bp, 0, sizeof(bp));
+	bp.size = attach->dmabuf->size;
+	bp.byte_align = PAGE_SIZE;
+	bp.domain = GSGPU_GEM_DOMAIN_CPU;
+	bp.flags = 0;
+	bp.type = ttm_bo_type_sg;
+	bp.resv = resv;
+	ww_mutex_lock(&resv->lock, NULL);
+	ret = gsgpu_bo_create(adev, &bp, &bo);
+	if (ret)
+		goto error;
+
+	bo->tbo.sg = sg;
+	bo->tbo.ttm->sg = sg;
+	bo->allowed_domains = GSGPU_GEM_DOMAIN_GTT;
+	bo->preferred_domains = GSGPU_GEM_DOMAIN_GTT;
+	if (attach->dmabuf->ops != &gsgpu_dmabuf_ops)
+		bo->prime_shared_count = 1;
+
+	ww_mutex_unlock(&resv->lock);
+	return &bo->gem_base;
+
+error:
+	ww_mutex_unlock(&resv->lock);
+	return ERR_PTR(ret);
+}
+
+static int
+__reservation_object_make_exclusive(struct reservation_object *obj)
+{
+	struct dma_fence **fences;
+	unsigned int count;
+	int r;
+
+	if (!reservation_object_get_list(obj)) /* no shared fences to convert */
+		return 0;
+
+	r = reservation_object_get_fences_rcu(obj, NULL, &count, &fences);
+	if (r)
+		return r;
+
+	if (count == 0) {
+		/* Now that was unexpected. */
+	} else if (count == 1) {
+		reservation_object_add_excl_fence(obj, fences[0]);
+		dma_fence_put(fences[0]);
+		kfree(fences);
+	} else {
+		struct dma_fence_array *array;
+
+		array = dma_fence_array_create(count, fences,
+					       dma_fence_context_alloc(1), 0,
+					       false);
+		if (!array)
+			goto err_fences_put;
+
+		reservation_object_add_excl_fence(obj, &array->base);
+		dma_fence_put(&array->base);
+	}
+
+	return 0;
+
+err_fences_put:
+	while (count--)
+		dma_fence_put(fences[count]);
+	kfree(fences);
+	return -ENOMEM;
+}
+
+/**
+ * gsgpu_gem_map_attach - &dma_buf_ops.attach implementation
+ * @dma_buf: shared DMA buffer
+ * @attach: DMA-buf attachment
+ *
+ * Makes sure that the shared DMA buffer can be accessed by the target device.
+ * For now, simply pins it to the GTT domain, where it should be accessible by
+ * all DMA devices.
+ *
+ * Returns:
+ * 0 on success or negative error code.
+ */
+static int gsgpu_gem_map_attach(struct dma_buf *dma_buf,
+				 struct dma_buf_attachment *attach)
+{
+	struct drm_gem_object *obj = dma_buf->priv;
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(obj);
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	long r;
+
+	r = drm_gem_map_attach(dma_buf, attach);
+	if (r)
+		return r;
+
+	r = gsgpu_bo_reserve(bo, false);
+	if (unlikely(r != 0))
+		goto error_detach;
+
+
+	if (attach->dev->driver != adev->dev->driver) {
+		/*
+		 * We only create shared fences for internal use, but importers
+		 * of the dmabuf rely on exclusive fences for implicitly
+		 * tracking write hazards. As any of the current fences may
+		 * correspond to a write, we need to convert all existing
+		 * fences on the reservation object into a single exclusive
+		 * fence.
+		 */
+		r = __reservation_object_make_exclusive(bo->tbo.resv);
+		if (r)
+			goto error_unreserve;
+	}
+
+	/* pin buffer into GTT */
+	r = gsgpu_bo_pin(bo, GSGPU_GEM_DOMAIN_GTT);
+	if (r)
+		goto error_unreserve;
+
+	if (attach->dev->driver != adev->dev->driver)
+		bo->prime_shared_count++;
+
+error_unreserve:
+	gsgpu_bo_unreserve(bo);
+
+error_detach:
+	if (r)
+		drm_gem_map_detach(dma_buf, attach);
+	return r;
+}
+
+/**
+ * gsgpu_gem_map_detach - &dma_buf_ops.detach implementation
+ * @dma_buf: shared DMA buffer
+ * @attach: DMA-buf attachment
+ *
+ * This is called when a shared DMA buffer no longer needs to be accessible by
+ * the other device. For now, simply unpins the buffer from GTT.
+ */
+static void gsgpu_gem_map_detach(struct dma_buf *dma_buf,
+				  struct dma_buf_attachment *attach)
+{
+	struct drm_gem_object *obj = dma_buf->priv;
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(obj);
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	int ret = 0;
+
+	ret = gsgpu_bo_reserve(bo, true);
+	if (unlikely(ret != 0))
+		goto error;
+
+	gsgpu_bo_unpin(bo);
+	if (attach->dev->driver != adev->dev->driver && bo->prime_shared_count)
+		bo->prime_shared_count--;
+	gsgpu_bo_unreserve(bo);
+
+error:
+	drm_gem_map_detach(dma_buf, attach);
+}
+
+/**
+ * gsgpu_gem_prime_res_obj - &drm_driver.gem_prime_res_obj implementation
+ * @obj: GEM buffer object
+ *
+ * Returns:
+ * The buffer object's reservation object.
+ */
+struct reservation_object *gsgpu_gem_prime_res_obj(struct drm_gem_object *obj)
+{
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(obj);
+
+	return bo->tbo.resv;
+}
+
+/**
+ * gsgpu_gem_begin_cpu_access - &dma_buf_ops.begin_cpu_access implementation
+ * @dma_buf: shared DMA buffer
+ * @direction: direction of DMA transfer
+ *
+ * This is called before CPU access to the shared DMA buffer's memory. If it's
+ * a read access, the buffer is moved to the GTT domain if possible, for optimal
+ * CPU read performance.
+ *
+ * Returns:
+ * 0 on success or negative error code.
+ */
+static int gsgpu_gem_begin_cpu_access(struct dma_buf *dma_buf,
+				       enum dma_data_direction direction)
+{
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(dma_buf->priv);
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	struct ttm_operation_ctx ctx = { true, false };
+	u32 domain = gsgpu_display_supported_domains(adev);
+	int ret;
+	bool reads = (direction == DMA_BIDIRECTIONAL ||
+		      direction == DMA_FROM_DEVICE);
+
+	if (!reads || !(domain & GSGPU_GEM_DOMAIN_GTT))
+		return 0;
+
+	/* move to gtt */
+	ret = gsgpu_bo_reserve(bo, false);
+	if (unlikely(ret != 0))
+		return ret;
+
+	if (!bo->pin_count && (bo->allowed_domains & GSGPU_GEM_DOMAIN_GTT)) {
+		gsgpu_bo_placement_from_domain(bo, GSGPU_GEM_DOMAIN_GTT);
+		ret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+	}
+
+	gsgpu_bo_unreserve(bo);
+	return ret;
+}
+
+static const struct dma_buf_ops gsgpu_dmabuf_ops = {
+	.attach = gsgpu_gem_map_attach,
+	.detach = gsgpu_gem_map_detach,
+	.map_dma_buf = drm_gem_map_dma_buf,
+	.unmap_dma_buf = drm_gem_unmap_dma_buf,
+	.release = drm_gem_dmabuf_release,
+	.begin_cpu_access = gsgpu_gem_begin_cpu_access,
+	.map = drm_gem_dmabuf_kmap,
+	.unmap = drm_gem_dmabuf_kunmap,
+	.mmap = drm_gem_dmabuf_mmap,
+	.vmap = drm_gem_dmabuf_vmap,
+	.vunmap = drm_gem_dmabuf_vunmap,
+};
+
+/**
+ * gsgpu_gem_prime_export - &drm_driver.gem_prime_export implementation
+ * @dev: DRM device
+ * @gobj: GEM buffer object
+ * @flags: flags like DRM_CLOEXEC and DRM_RDWR
+ *
+ * The main work is done by the &drm_gem_prime_export helper, which in turn
+ * uses &gsgpu_gem_prime_res_obj.
+ *
+ * Returns:
+ * Shared DMA buffer representing the GEM buffer object from the given device.
+ */
+struct dma_buf *gsgpu_gem_prime_export(struct drm_device *dev,
+					struct drm_gem_object *gobj,
+					int flags)
+{
+	struct gsgpu_bo *bo = gem_to_gsgpu_bo(gobj);
+	struct dma_buf *buf;
+
+	if (gsgpu_ttm_tt_get_usermm(bo->tbo.ttm) ||
+	    bo->flags & GSGPU_GEM_CREATE_VM_ALWAYS_VALID)
+		return ERR_PTR(-EPERM);
+
+	buf = drm_gem_prime_export(dev, gobj, flags);
+	if (!IS_ERR(buf)) {
+		buf->file->f_mapping = dev->anon_inode->i_mapping;
+		buf->ops = &gsgpu_dmabuf_ops;
+	}
+
+	return buf;
+}
+
+/**
+ * gsgpu_gem_prime_import - &drm_driver.gem_prime_import implementation
+ * @dev: DRM device
+ * @dma_buf: Shared DMA buffer
+ *
+ * The main work is done by the &drm_gem_prime_import helper, which in turn
+ * uses &gsgpu_gem_prime_import_sg_table.
+ *
+ * Returns:
+ * GEM buffer object representing the shared DMA buffer for the given device.
+ */
+struct drm_gem_object *gsgpu_gem_prime_import(struct drm_device *dev,
+					    struct dma_buf *dma_buf)
+{
+	struct drm_gem_object *obj;
+
+	if (dma_buf->ops == &gsgpu_dmabuf_ops) {
+		obj = dma_buf->priv;
+		if (obj->dev == dev) {
+			/*
+			 * Importing dmabuf exported from out own gem increases
+			 * refcount on gem itself instead of f_count of dmabuf.
+			 */
+			drm_gem_object_get(obj);
+			return obj;
+		}
+	}
+
+	return drm_gem_prime_import(dev, dma_buf);
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_queue_mgr.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_queue_mgr.c
new file mode 100644
index 000000000000..ee171bda19ca
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_queue_mgr.c
@@ -0,0 +1,227 @@
+#include "gsgpu.h"
+#include "gsgpu_ring.h"
+
+static int gsgpu_queue_mapper_init(struct gsgpu_queue_mapper *mapper,
+				    int hw_ip)
+{
+	if (!mapper)
+		return -EINVAL;
+
+	if (hw_ip > GSGPU_MAX_IP_NUM)
+		return -EINVAL;
+
+	mapper->hw_ip = hw_ip;
+	mutex_init(&mapper->lock);
+
+	memset(mapper->queue_map, 0, sizeof(mapper->queue_map));
+
+	return 0;
+}
+
+static struct gsgpu_ring *gsgpu_get_cached_map(struct gsgpu_queue_mapper *mapper,
+					  int ring)
+{
+	return mapper->queue_map[ring];
+}
+
+static int gsgpu_update_cached_map(struct gsgpu_queue_mapper *mapper,
+			     int ring, struct gsgpu_ring *pring)
+{
+	if (WARN_ON(mapper->queue_map[ring])) {
+		DRM_ERROR("Un-expected ring re-map\n");
+		return -EINVAL;
+	}
+
+	mapper->queue_map[ring] = pring;
+
+	return 0;
+}
+
+static int gsgpu_identity_map(struct gsgpu_device *adev,
+			       struct gsgpu_queue_mapper *mapper,
+			       u32 ring,
+			       struct gsgpu_ring **out_ring)
+{
+	switch (mapper->hw_ip) {
+	case GSGPU_HW_IP_GFX:
+		*out_ring = &adev->gfx.gfx_ring[ring];
+		break;
+	case GSGPU_HW_IP_DMA:
+		*out_ring = &adev->xdma.instance[ring].ring;
+		break;
+	default:
+		*out_ring = NULL;
+		DRM_ERROR("unknown HW IP type: %d\n", mapper->hw_ip);
+		return -EINVAL;
+	}
+
+	return gsgpu_update_cached_map(mapper, ring, *out_ring);
+}
+
+static enum gsgpu_ring_type gsgpu_hw_ip_to_ring_type(int hw_ip)
+{
+	switch (hw_ip) {
+	case GSGPU_HW_IP_GFX:
+		return GSGPU_RING_TYPE_GFX;
+	case GSGPU_HW_IP_DMA:
+		return GSGPU_RING_TYPE_XDMA;
+	default:
+		DRM_ERROR("Invalid HW IP specified %d\n", hw_ip);
+		return -1;
+	}
+}
+
+static int gsgpu_lru_map(struct gsgpu_device *adev,
+			  struct gsgpu_queue_mapper *mapper,
+			  u32 user_ring, bool lru_pipe_order,
+			  struct gsgpu_ring **out_ring)
+{
+	int r, i, j;
+	int ring_type = gsgpu_hw_ip_to_ring_type(mapper->hw_ip);
+	int ring_blacklist[GSGPU_MAX_RINGS];
+	struct gsgpu_ring *ring;
+
+	/* 0 is a valid ring index, so initialize to -1 */
+	memset(ring_blacklist, 0xff, sizeof(ring_blacklist));
+
+	for (i = 0, j = 0; i < GSGPU_MAX_RINGS; i++) {
+		ring = mapper->queue_map[i];
+		if (ring)
+			ring_blacklist[j++] = ring->idx;
+	}
+
+	r = gsgpu_ring_lru_get(adev, ring_type, ring_blacklist,
+				j, lru_pipe_order, out_ring);
+	if (r)
+		return r;
+
+	return gsgpu_update_cached_map(mapper, user_ring, *out_ring);
+}
+
+/**
+ * gsgpu_queue_mgr_init - init an gsgpu_queue_mgr struct
+ *
+ * @adev: gsgpu_device pointer
+ * @mgr: gsgpu_queue_mgr structure holding queue information
+ *
+ * Initialize the the selected @mgr (all asics).
+ *
+ * Returns 0 on success, error on failure.
+ */
+int gsgpu_queue_mgr_init(struct gsgpu_device *adev,
+			  struct gsgpu_queue_mgr *mgr)
+{
+	int i, r;
+
+	if (!adev || !mgr)
+		return -EINVAL;
+
+	memset(mgr, 0, sizeof(*mgr));
+
+	for (i = 0; i < GSGPU_MAX_IP_NUM; ++i) {
+		r = gsgpu_queue_mapper_init(&mgr->mapper[i], i);
+		if (r)
+			return r;
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_queue_mgr_fini - de-initialize an gsgpu_queue_mgr struct
+ *
+ * @adev: gsgpu_device pointer
+ * @mgr: gsgpu_queue_mgr structure holding queue information
+ *
+ * De-initialize the the selected @mgr (all asics).
+ *
+ * Returns 0 on success, error on failure.
+ */
+int gsgpu_queue_mgr_fini(struct gsgpu_device *adev,
+			  struct gsgpu_queue_mgr *mgr)
+{
+	return 0;
+}
+
+/**
+ * gsgpu_queue_mgr_map - Map a userspace ring id to an gsgpu_ring
+ *
+ * @adev: gsgpu_device pointer
+ * @mgr: gsgpu_queue_mgr structure holding queue information
+ * @hw_ip: HW IP enum
+ * @instance: HW instance
+ * @ring: user ring id
+ * @our_ring: pointer to mapped gsgpu_ring
+ *
+ * Map a userspace ring id to an appropriate kernel ring. Different
+ * policies are configurable at a HW IP level.
+ *
+ * Returns 0 on success, error on failure.
+ */
+int gsgpu_queue_mgr_map(struct gsgpu_device *adev,
+			 struct gsgpu_queue_mgr *mgr,
+			 u32 hw_ip, u32 instance, u32 ring,
+			 struct gsgpu_ring **out_ring)
+{
+	int r, ip_num_rings = 0;
+	struct gsgpu_queue_mapper *mapper = &mgr->mapper[hw_ip];
+
+	if (!adev || !mgr || !out_ring)
+		return -EINVAL;
+
+	if (hw_ip >= GSGPU_MAX_IP_NUM)
+		return -EINVAL;
+
+	if (ring >= GSGPU_MAX_RINGS)
+		return -EINVAL;
+
+	/* Right now all IPs have only one instance - multiple rings. */
+	if (instance != 0) {
+		DRM_DEBUG("invalid ip instance: %d\n", instance);
+		return -EINVAL;
+	}
+
+	switch (hw_ip) {
+	case GSGPU_HW_IP_GFX:
+		ip_num_rings = adev->gfx.num_gfx_rings;
+		break;
+	case GSGPU_HW_IP_DMA:
+		ip_num_rings = adev->xdma.num_instances;
+		break;
+	default:
+		DRM_DEBUG("unknown ip type: %d\n", hw_ip);
+		return -EINVAL;
+	}
+
+	if (ring >= ip_num_rings) {
+		DRM_DEBUG("Ring index:%d exceeds maximum:%d for ip:%d\n",
+			  ring, ip_num_rings, hw_ip);
+		return -EINVAL;
+	}
+
+	mutex_lock(&mapper->lock);
+
+	*out_ring = gsgpu_get_cached_map(mapper, ring);
+	if (*out_ring) {
+		/* cache hit */
+		r = 0;
+		goto out_unlock;
+	}
+
+	switch (mapper->hw_ip) {
+	case GSGPU_HW_IP_GFX:
+		r = gsgpu_identity_map(adev, mapper, ring, out_ring);
+		break;
+	case GSGPU_HW_IP_DMA:
+		r = gsgpu_lru_map(adev, mapper, ring, false, out_ring);
+		break;
+	default:
+		*out_ring = NULL;
+		r = -EINVAL;
+		DRM_DEBUG("unknown HW IP type: %d\n", mapper->hw_ip);
+	}
+
+out_unlock:
+	mutex_unlock(&mapper->lock);
+	return r;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ring.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ring.c
new file mode 100644
index 000000000000..7ec3f735ec58
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ring.c
@@ -0,0 +1,536 @@
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/debugfs.h>
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+
+/*
+ * Rings
+ * Most engines on the GPU are fed via ring buffers.  Ring
+ * buffers are areas of GPU accessible memory that the host
+ * writes commands into and the GPU reads commands out of.
+ * There is a rptr (read pointer) that determines where the
+ * GPU is currently reading, and a wptr (write pointer)
+ * which determines where the host has written.  When the
+ * pointers are equal, the ring is idle.  When the host
+ * writes commands to the ring buffer, it increments the
+ * wptr.  The GPU then starts fetching commands and executes
+ * them until the pointers are equal again.
+ */
+static int gsgpu_debugfs_ring_init(struct gsgpu_device *adev,
+				    struct gsgpu_ring *ring);
+static void gsgpu_debugfs_ring_fini(struct gsgpu_ring *ring);
+
+/**
+ * gsgpu_ring_alloc - allocate space on the ring buffer
+ *
+ * @adev: gsgpu_device pointer
+ * @ring: gsgpu_ring structure holding ring information
+ * @ndw: number of dwords to allocate in the ring buffer
+ *
+ * Allocate @ndw dwords in the ring buffer (all asics).
+ * Returns 0 on success, error on failure.
+ */
+int gsgpu_ring_alloc(struct gsgpu_ring *ring, unsigned ndw)
+{
+	/* Align requested size with padding so unlock_commit can
+	 * pad safely */
+	ndw = (ndw + ring->funcs->align_mask) & ~ring->funcs->align_mask;
+
+	/* Make sure we aren't trying to allocate more space
+	 * than the maximum for one submission
+	 */
+	if (WARN_ON_ONCE(ndw > ring->max_dw))
+		return -ENOMEM;
+
+	ring->count_dw = ndw;
+	ring->wptr_old = ring->wptr;
+
+	if (ring->funcs->begin_use)
+		ring->funcs->begin_use(ring);
+
+	return 0;
+}
+
+/** gsgpu_ring_insert_nop - insert NOP packets
+ *
+ * @ring: gsgpu_ring structure holding ring information
+ * @count: the number of NOP packets to insert
+ *
+ * This is the generic insert_nop function for rings except XDMA
+ */
+void gsgpu_ring_insert_nop(struct gsgpu_ring *ring, uint32_t count)
+{
+	int i;
+
+	for (i = 0; i < count; i++)
+		gsgpu_ring_write(ring, ring->funcs->nop);
+}
+
+/** gsgpu_ring_generic_pad_ib - pad IB with NOP packets
+ *
+ * @ring: gsgpu_ring structure holding ring information
+ * @ib: IB to add NOP packets to
+ *
+ * This is the generic pad_ib function for rings except XDMA
+ */
+void gsgpu_ring_generic_pad_ib(struct gsgpu_ring *ring, struct gsgpu_ib *ib)
+{
+	while (ib->length_dw & ring->funcs->align_mask)
+		ib->ptr[ib->length_dw++] = ring->funcs->nop;
+}
+
+/**
+ * gsgpu_ring_commit - tell the GPU to execute the new
+ * commands on the ring buffer
+ *
+ * @adev: gsgpu_device pointer
+ * @ring: gsgpu_ring structure holding ring information
+ *
+ * Update the wptr (write pointer) to tell the GPU to
+ * execute new commands on the ring buffer (all asics).
+ */
+void gsgpu_ring_commit(struct gsgpu_ring *ring)
+{
+	uint32_t count;
+
+	/* We pad to match fetch size */
+	count = ring->funcs->align_mask + 1 -
+		(ring->wptr & ring->funcs->align_mask);
+	count %= ring->funcs->align_mask + 1;
+	ring->funcs->insert_nop(ring, count);
+
+	mb();
+	gsgpu_ring_set_wptr(ring);
+
+	if (ring->funcs->end_use)
+		ring->funcs->end_use(ring);
+
+	gsgpu_ring_lru_touch(ring->adev, ring);
+}
+
+/**
+ * gsgpu_ring_undo - reset the wptr
+ *
+ * @ring: gsgpu_ring structure holding ring information
+ *
+ * Reset the driver's copy of the wptr (all asics).
+ */
+void gsgpu_ring_undo(struct gsgpu_ring *ring)
+{
+	ring->wptr = ring->wptr_old;
+
+	if (ring->funcs->end_use)
+		ring->funcs->end_use(ring);
+}
+
+/**
+ * gsgpu_ring_priority_put - restore a ring's priority
+ *
+ * @ring: gsgpu_ring structure holding the information
+ * @priority: target priority
+ *
+ * Release a request for executing at @priority
+ */
+void gsgpu_ring_priority_put(struct gsgpu_ring *ring,
+			      enum drm_sched_priority priority)
+{
+	int i;
+
+	if (!ring->funcs->set_priority)
+		return;
+
+	if (atomic_dec_return(&ring->num_jobs[priority]) > 0)
+		return;
+
+	/* no need to restore if the job is already at the lowest priority */
+	if (priority == DRM_SCHED_PRIORITY_NORMAL)
+		return;
+
+	mutex_lock(&ring->priority_mutex);
+	/* something higher prio is executing, no need to decay */
+	if (ring->priority > priority)
+		goto out_unlock;
+
+	/* decay priority to the next level with a job available */
+	for (i = priority; i >= DRM_SCHED_PRIORITY_MIN; i--) {
+		if (i == DRM_SCHED_PRIORITY_NORMAL
+				|| atomic_read(&ring->num_jobs[i])) {
+			ring->priority = i;
+			ring->funcs->set_priority(ring, i);
+			break;
+		}
+	}
+
+out_unlock:
+	mutex_unlock(&ring->priority_mutex);
+}
+
+/**
+ * gsgpu_ring_priority_get - change the ring's priority
+ *
+ * @ring: gsgpu_ring structure holding the information
+ * @priority: target priority
+ *
+ * Request a ring's priority to be raised to @priority (refcounted).
+ */
+void gsgpu_ring_priority_get(struct gsgpu_ring *ring,
+			      enum drm_sched_priority priority)
+{
+	if (!ring->funcs->set_priority)
+		return;
+
+	if (atomic_inc_return(&ring->num_jobs[priority]) <= 0)
+		return;
+
+	mutex_lock(&ring->priority_mutex);
+	if (priority <= ring->priority)
+		goto out_unlock;
+
+	ring->priority = priority;
+	ring->funcs->set_priority(ring, priority);
+
+out_unlock:
+	mutex_unlock(&ring->priority_mutex);
+}
+
+/**
+ * gsgpu_ring_init - init driver ring struct.
+ *
+ * @adev: gsgpu_device pointer
+ * @ring: gsgpu_ring structure holding ring information
+ * @max_ndw: maximum number of dw for ring alloc
+ * @nop: nop packet for this ring
+ *
+ * Initialize the driver information for the selected ring (all asics).
+ * Returns 0 on success, error on failure.
+ */
+int gsgpu_ring_init(struct gsgpu_device *adev, struct gsgpu_ring *ring,
+		     unsigned max_dw, struct gsgpu_irq_src *irq_src,
+		     unsigned irq_type)
+{
+	int r, i;
+	int sched_hw_submission = gsgpu_sched_hw_submission;
+
+	if (ring->adev == NULL) {
+		if (adev->num_rings >= GSGPU_MAX_RINGS)
+			return -EINVAL;
+
+		ring->adev = adev;
+		ring->idx = adev->num_rings++;
+		adev->rings[ring->idx] = ring;
+		r = gsgpu_fence_driver_init_ring(ring, sched_hw_submission);
+		if (r)
+			return r;
+	}
+
+	r = gsgpu_device_wb_get(adev, &ring->rptr_offs);
+	if (r) {
+		dev_err(adev->dev, "(%d) ring rptr_offs wb alloc failed\n", r);
+		return r;
+	}
+
+	r = gsgpu_device_wb_get(adev, &ring->wptr_offs);
+	if (r) {
+		dev_err(adev->dev, "(%d) ring wptr_offs wb alloc failed\n", r);
+		return r;
+	}
+
+	r = gsgpu_device_wb_get(adev, &ring->fence_offs);
+	if (r) {
+		dev_err(adev->dev, "(%d) ring fence_offs wb alloc failed\n", r);
+		return r;
+	}
+
+	r = gsgpu_device_wb_get(adev, &ring->cond_exe_offs);
+	if (r) {
+		dev_err(adev->dev, "(%d) ring cond_exec_polling wb alloc failed\n", r);
+		return r;
+	}
+	ring->cond_exe_gpu_addr = adev->wb.gpu_addr + (ring->cond_exe_offs * 4);
+	ring->cond_exe_cpu_addr = &adev->wb.wb[ring->cond_exe_offs];
+	/* always set cond_exec_polling to CONTINUE */
+	*ring->cond_exe_cpu_addr = 1;
+
+	r = gsgpu_fence_driver_start_ring(ring, irq_src, irq_type);
+	if (r) {
+		dev_err(adev->dev, "failed initializing fences (%d).\n", r);
+		return r;
+	}
+
+	ring->ring_size = roundup_pow_of_two(max_dw * 4 * sched_hw_submission);
+
+	ring->buf_mask = ring->ring_size/4 - 1;
+	ring->ptr_mask = ring->funcs->support_64bit_ptrs ?
+		0xffffffffffffffff : ring->buf_mask;
+	/* Allocate ring buffer */
+	if (ring->ring_obj == NULL) {
+		r = gsgpu_bo_create_kernel(adev, ring->ring_size + ring->funcs->extra_dw, PAGE_SIZE,
+					    GSGPU_GEM_DOMAIN_GTT,
+					    &ring->ring_obj,
+					    &ring->gpu_addr,
+					    (void **)&ring->ring);
+		if (r) {
+			dev_err(adev->dev, "(%d) ring create failed\n", r);
+			return r;
+		}
+		gsgpu_ring_clear_ring(ring);
+	}
+
+	ring->max_dw = max_dw;
+	ring->priority = DRM_SCHED_PRIORITY_NORMAL;
+	mutex_init(&ring->priority_mutex);
+	INIT_LIST_HEAD(&ring->lru_list);
+	gsgpu_ring_lru_touch(adev, ring);
+
+	for (i = 0; i < DRM_SCHED_PRIORITY_MAX; ++i)
+		atomic_set(&ring->num_jobs[i], 0);
+
+	if (gsgpu_debugfs_ring_init(adev, ring)) {
+		DRM_ERROR("Failed to register debugfs file for rings !\n");
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_ring_fini - tear down the driver ring struct.
+ *
+ * @adev: gsgpu_device pointer
+ * @ring: gsgpu_ring structure holding ring information
+ *
+ * Tear down the driver information for the selected ring (all asics).
+ */
+void gsgpu_ring_fini(struct gsgpu_ring *ring)
+{
+	ring->ready = false;
+
+	/* Not to finish a ring which is not initialized */
+	if (!(ring->adev) || !(ring->adev->rings[ring->idx]))
+		return;
+
+	gsgpu_device_wb_free(ring->adev, ring->rptr_offs);
+	gsgpu_device_wb_free(ring->adev, ring->wptr_offs);
+
+	gsgpu_device_wb_free(ring->adev, ring->cond_exe_offs);
+	gsgpu_device_wb_free(ring->adev, ring->fence_offs);
+
+	gsgpu_bo_free_kernel(&ring->ring_obj,
+			      &ring->gpu_addr,
+			      (void **)&ring->ring);
+
+	gsgpu_debugfs_ring_fini(ring);
+
+	dma_fence_put(ring->vmid_wait);
+	ring->vmid_wait = NULL;
+	ring->me = 0;
+
+	ring->adev->rings[ring->idx] = NULL;
+}
+
+static void gsgpu_ring_lru_touch_locked(struct gsgpu_device *adev,
+					 struct gsgpu_ring *ring)
+{
+	/* list_move_tail handles the case where ring isn't part of the list */
+	list_move_tail(&ring->lru_list, &adev->ring_lru_list);
+}
+
+static bool gsgpu_ring_is_blacklisted(struct gsgpu_ring *ring,
+				       int *blacklist, int num_blacklist)
+{
+	int i;
+
+	for (i = 0; i < num_blacklist; i++) {
+		if (ring->idx == blacklist[i])
+			return true;
+	}
+
+	return false;
+}
+
+/**
+ * gsgpu_ring_lru_get - get the least recently used ring for a HW IP block
+ *
+ * @adev: gsgpu_device pointer
+ * @type: gsgpu_ring_type enum
+ * @blacklist: blacklisted ring ids array
+ * @num_blacklist: number of entries in @blacklist
+ * @lru_pipe_order: find a ring from the least recently used pipe
+ * @ring: output ring
+ *
+ * Retrieve the gsgpu_ring structure for the least recently used ring of
+ * a specific IP block (all asics).
+ * Returns 0 on success, error on failure.
+ */
+int gsgpu_ring_lru_get(struct gsgpu_device *adev, int type,
+			int *blacklist,	int num_blacklist,
+			bool lru_pipe_order, struct gsgpu_ring **ring)
+{
+	struct gsgpu_ring *entry;
+
+	/* List is sorted in LRU order, find first entry corresponding
+	 * to the desired HW IP */
+	*ring = NULL;
+	spin_lock(&adev->ring_lru_list_lock);
+	list_for_each_entry(entry, &adev->ring_lru_list, lru_list) {
+		if (entry->funcs->type != type)
+			continue;
+
+		if (gsgpu_ring_is_blacklisted(entry, blacklist, num_blacklist))
+			continue;
+
+		if (!*ring) {
+			*ring = entry;
+
+			/* We are done for ring LRU */
+			if (!lru_pipe_order)
+				break;
+		}
+
+		/* Move all rings on the same pipe to the end of the list */
+		if (entry->pipe == (*ring)->pipe)
+			gsgpu_ring_lru_touch_locked(adev, entry);
+	}
+
+	/* Move the ring we found to the end of the list */
+	if (*ring)
+		gsgpu_ring_lru_touch_locked(adev, *ring);
+
+	spin_unlock(&adev->ring_lru_list_lock);
+
+	if (!*ring) {
+		DRM_ERROR("Ring LRU contains no entries for ring type:%d\n", type);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_ring_lru_touch - mark a ring as recently being used
+ *
+ * @adev: gsgpu_device pointer
+ * @ring: ring to touch
+ *
+ * Move @ring to the tail of the lru list
+ */
+void gsgpu_ring_lru_touch(struct gsgpu_device *adev, struct gsgpu_ring *ring)
+{
+	spin_lock(&adev->ring_lru_list_lock);
+	gsgpu_ring_lru_touch_locked(adev, ring);
+	spin_unlock(&adev->ring_lru_list_lock);
+}
+
+/**
+ * gsgpu_ring_emit_reg_write_reg_wait_helper - ring helper
+ *
+ * @adev: gsgpu_device pointer
+ * @reg0: register to write
+ * @reg1: register to wait on
+ * @ref: reference value to write/wait on
+ * @mask: mask to wait on
+ *
+ * Helper for rings that don't support write and wait in a
+ * single oneshot packet.
+ */
+void gsgpu_ring_emit_reg_write_reg_wait_helper(struct gsgpu_ring *ring,
+						uint32_t reg0, uint32_t reg1,
+						uint32_t ref, uint32_t mask)
+{
+	gsgpu_ring_emit_wreg(ring, reg0, ref);
+	gsgpu_ring_emit_reg_wait(ring, reg1, mask, mask);
+}
+
+/*
+ * Debugfs info
+ */
+#if defined(CONFIG_DEBUG_FS)
+
+/* Layout of file is 12 bytes consisting of
+ * - rptr
+ * - wptr
+ * - driver's copy of wptr
+ *
+ * followed by n-words of ring data
+ */
+static ssize_t gsgpu_debugfs_ring_read(struct file *f, char __user *buf,
+					size_t size, loff_t *pos)
+{
+	struct gsgpu_ring *ring = file_inode(f)->i_private;
+	int r, i;
+	uint32_t value, result, early[3];
+
+	if (*pos & 3 || size & 3)
+		return -EINVAL;
+
+	result = 0;
+
+	if (*pos < 12) {
+		early[0] = gsgpu_ring_get_rptr(ring) & ring->buf_mask;
+		early[1] = gsgpu_ring_get_wptr(ring) & ring->buf_mask;
+		early[2] = ring->wptr & ring->buf_mask;
+		for (i = *pos / 4; i < 3 && size; i++) {
+			r = put_user(early[i], (uint32_t *)buf);
+			if (r)
+				return r;
+			buf += 4;
+			result += 4;
+			size -= 4;
+			*pos += 4;
+		}
+	}
+
+	while (size) {
+		if (*pos >= (ring->ring_size + 12))
+			return result;
+
+		value = ring->ring[(*pos - 12)/4];
+		r = put_user(value, (uint32_t *)buf);
+		if (r)
+			return r;
+		buf += 4;
+		result += 4;
+		size -= 4;
+		*pos += 4;
+	}
+
+	return result;
+}
+
+static const struct file_operations gsgpu_debugfs_ring_fops = {
+	.owner = THIS_MODULE,
+	.read = gsgpu_debugfs_ring_read,
+	.llseek = default_llseek
+};
+
+#endif
+
+static int gsgpu_debugfs_ring_init(struct gsgpu_device *adev,
+				    struct gsgpu_ring *ring)
+{
+#if defined(CONFIG_DEBUG_FS)
+	struct drm_minor *minor = adev->ddev->primary;
+	struct dentry *ent, *root = minor->debugfs_root;
+	char name[32];
+
+	sprintf(name, "gsgpu_ring_%s", ring->name);
+
+	ent = debugfs_create_file(name,
+				  S_IFREG | S_IRUGO, root,
+				  ring, &gsgpu_debugfs_ring_fops);
+	if (!ent)
+		return -ENOMEM;
+
+	i_size_write(ent->d_inode, ring->ring_size + 12);
+	ring->ent = ent;
+#endif
+	return 0;
+}
+
+static void gsgpu_debugfs_ring_fini(struct gsgpu_ring *ring)
+{
+#if defined(CONFIG_DEBUG_FS)
+	debugfs_remove(ring->ent);
+#endif
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_sa.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_sa.c
new file mode 100644
index 000000000000..b0aba673d108
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_sa.c
@@ -0,0 +1,355 @@
+#include <drm/drmP.h>
+#include "gsgpu.h"
+
+static void gsgpu_sa_bo_remove_locked(struct gsgpu_sa_bo *sa_bo);
+static void gsgpu_sa_bo_try_free(struct gsgpu_sa_manager *sa_manager);
+
+int gsgpu_sa_bo_manager_init(struct gsgpu_device *adev,
+			      struct gsgpu_sa_manager *sa_manager,
+			      unsigned size, u32 align, u32 domain)
+{
+	int i, r;
+
+	init_waitqueue_head(&sa_manager->wq);
+	sa_manager->bo = NULL;
+	sa_manager->size = size;
+	sa_manager->domain = domain;
+	sa_manager->align = align;
+	sa_manager->hole = &sa_manager->olist;
+	INIT_LIST_HEAD(&sa_manager->olist);
+	for (i = 0; i < GSGPU_SA_NUM_FENCE_LISTS; ++i)
+		INIT_LIST_HEAD(&sa_manager->flist[i]);
+
+	r = gsgpu_bo_create_kernel(adev, size, align, domain, &sa_manager->bo,
+				&sa_manager->gpu_addr, &sa_manager->cpu_ptr);
+	if (r) {
+		dev_err(adev->dev, "(%d) failed to allocate bo for manager\n", r);
+		return r;
+	}
+
+	memset(sa_manager->cpu_ptr, 0, sa_manager->size);
+	return r;
+}
+
+void gsgpu_sa_bo_manager_fini(struct gsgpu_device *adev,
+					struct gsgpu_sa_manager *sa_manager)
+{
+	struct gsgpu_sa_bo *sa_bo, *tmp;
+
+	if (sa_manager->bo == NULL) {
+		dev_err(adev->dev, "no bo for sa manager\n");
+		return;
+	}
+
+	if (!list_empty(&sa_manager->olist)) {
+		sa_manager->hole = &sa_manager->olist,
+		gsgpu_sa_bo_try_free(sa_manager);
+		if (!list_empty(&sa_manager->olist)) {
+			dev_err(adev->dev, "sa_manager is not empty, clearing anyway\n");
+		}
+	}
+	list_for_each_entry_safe(sa_bo, tmp, &sa_manager->olist, olist) {
+		gsgpu_sa_bo_remove_locked(sa_bo);
+	}
+
+	gsgpu_bo_free_kernel(&sa_manager->bo, &sa_manager->gpu_addr, &sa_manager->cpu_ptr);
+	sa_manager->size = 0;
+}
+
+static void gsgpu_sa_bo_remove_locked(struct gsgpu_sa_bo *sa_bo)
+{
+	struct gsgpu_sa_manager *sa_manager = sa_bo->manager;
+	if (sa_manager->hole == &sa_bo->olist) {
+		sa_manager->hole = sa_bo->olist.prev;
+	}
+	list_del_init(&sa_bo->olist);
+	list_del_init(&sa_bo->flist);
+	dma_fence_put(sa_bo->fence);
+	kfree(sa_bo);
+}
+
+static void gsgpu_sa_bo_try_free(struct gsgpu_sa_manager *sa_manager)
+{
+	struct gsgpu_sa_bo *sa_bo, *tmp;
+
+	if (sa_manager->hole->next == &sa_manager->olist)
+		return;
+
+	sa_bo = list_entry(sa_manager->hole->next, struct gsgpu_sa_bo, olist);
+	list_for_each_entry_safe_from(sa_bo, tmp, &sa_manager->olist, olist) {
+		if (sa_bo->fence == NULL ||
+		    !dma_fence_is_signaled(sa_bo->fence)) {
+			return;
+		}
+		gsgpu_sa_bo_remove_locked(sa_bo);
+	}
+}
+
+static inline unsigned gsgpu_sa_bo_hole_soffset(struct gsgpu_sa_manager *sa_manager)
+{
+	struct list_head *hole = sa_manager->hole;
+
+	if (hole != &sa_manager->olist) {
+		return list_entry(hole, struct gsgpu_sa_bo, olist)->eoffset;
+	}
+	return 0;
+}
+
+static inline unsigned gsgpu_sa_bo_hole_eoffset(struct gsgpu_sa_manager *sa_manager)
+{
+	struct list_head *hole = sa_manager->hole;
+
+	if (hole->next != &sa_manager->olist) {
+		return list_entry(hole->next, struct gsgpu_sa_bo, olist)->soffset;
+	}
+	return sa_manager->size;
+}
+
+static bool gsgpu_sa_bo_try_alloc(struct gsgpu_sa_manager *sa_manager,
+				   struct gsgpu_sa_bo *sa_bo,
+				   unsigned size, unsigned align)
+{
+	unsigned soffset, eoffset, wasted;
+
+	soffset = gsgpu_sa_bo_hole_soffset(sa_manager);
+	eoffset = gsgpu_sa_bo_hole_eoffset(sa_manager);
+	wasted = (align - (soffset % align)) % align;
+
+	if ((eoffset - soffset) >= (size + wasted)) {
+		soffset += wasted;
+
+		sa_bo->manager = sa_manager;
+		sa_bo->soffset = soffset;
+		sa_bo->eoffset = soffset + size;
+		list_add(&sa_bo->olist, sa_manager->hole);
+		INIT_LIST_HEAD(&sa_bo->flist);
+		sa_manager->hole = &sa_bo->olist;
+		return true;
+	}
+	return false;
+}
+
+/**
+ * gsgpu_sa_event - Check if we can stop waiting
+ *
+ * @sa_manager: pointer to the sa_manager
+ * @size: number of bytes we want to allocate
+ * @align: alignment we need to match
+ *
+ * Check if either there is a fence we can wait for or
+ * enough free memory to satisfy the allocation directly
+ */
+static bool gsgpu_sa_event(struct gsgpu_sa_manager *sa_manager,
+			    unsigned size, unsigned align)
+{
+	unsigned soffset, eoffset, wasted;
+	int i;
+
+	for (i = 0; i < GSGPU_SA_NUM_FENCE_LISTS; ++i)
+		if (!list_empty(&sa_manager->flist[i]))
+			return true;
+
+	soffset = gsgpu_sa_bo_hole_soffset(sa_manager);
+	eoffset = gsgpu_sa_bo_hole_eoffset(sa_manager);
+	wasted = (align - (soffset % align)) % align;
+
+	if ((eoffset - soffset) >= (size + wasted)) {
+		return true;
+	}
+
+	return false;
+}
+
+static bool gsgpu_sa_bo_next_hole(struct gsgpu_sa_manager *sa_manager,
+				   struct dma_fence **fences,
+				   unsigned *tries)
+{
+	struct gsgpu_sa_bo *best_bo = NULL;
+	unsigned i, soffset, best, tmp;
+
+	/* if hole points to the end of the buffer */
+	if (sa_manager->hole->next == &sa_manager->olist) {
+		/* try again with its beginning */
+		sa_manager->hole = &sa_manager->olist;
+		return true;
+	}
+
+	soffset = gsgpu_sa_bo_hole_soffset(sa_manager);
+	/* to handle wrap around we add sa_manager->size */
+	best = sa_manager->size * 2;
+	/* go over all fence list and try to find the closest sa_bo
+	 * of the current last
+	 */
+	for (i = 0; i < GSGPU_SA_NUM_FENCE_LISTS; ++i) {
+		struct gsgpu_sa_bo *sa_bo;
+
+		if (list_empty(&sa_manager->flist[i]))
+			continue;
+
+		sa_bo = list_first_entry(&sa_manager->flist[i],
+					 struct gsgpu_sa_bo, flist);
+
+		if (!dma_fence_is_signaled(sa_bo->fence)) {
+			fences[i] = sa_bo->fence;
+			continue;
+		}
+
+		/* limit the number of tries each ring gets */
+		if (tries[i] > 2) {
+			continue;
+		}
+
+		tmp = sa_bo->soffset;
+		if (tmp < soffset) {
+			/* wrap around, pretend it's after */
+			tmp += sa_manager->size;
+		}
+		tmp -= soffset;
+		if (tmp < best) {
+			/* this sa bo is the closest one */
+			best = tmp;
+			best_bo = sa_bo;
+		}
+	}
+
+	if (best_bo) {
+		uint32_t idx = best_bo->fence->context;
+
+		idx %= GSGPU_SA_NUM_FENCE_LISTS;
+		++tries[idx];
+		sa_manager->hole = best_bo->olist.prev;
+
+		/* we knew that this one is signaled,
+		   so it's save to remote it */
+		gsgpu_sa_bo_remove_locked(best_bo);
+		return true;
+	}
+	return false;
+}
+
+int gsgpu_sa_bo_new(struct gsgpu_sa_manager *sa_manager,
+		     struct gsgpu_sa_bo **sa_bo,
+		     unsigned size, unsigned align)
+{
+	struct dma_fence *fences[GSGPU_SA_NUM_FENCE_LISTS];
+	unsigned tries[GSGPU_SA_NUM_FENCE_LISTS];
+	unsigned count;
+	int i, r;
+	signed long t;
+
+	if (WARN_ON_ONCE(align > sa_manager->align))
+		return -EINVAL;
+
+	if (WARN_ON_ONCE(size > sa_manager->size))
+		return -EINVAL;
+
+	*sa_bo = kmalloc(sizeof(struct gsgpu_sa_bo), GFP_KERNEL);
+	if (!(*sa_bo))
+		return -ENOMEM;
+	(*sa_bo)->manager = sa_manager;
+	(*sa_bo)->fence = NULL;
+	INIT_LIST_HEAD(&(*sa_bo)->olist);
+	INIT_LIST_HEAD(&(*sa_bo)->flist);
+
+	spin_lock(&sa_manager->wq.lock);
+	do {
+		for (i = 0; i < GSGPU_SA_NUM_FENCE_LISTS; ++i) {
+			fences[i] = NULL;
+			tries[i] = 0;
+		}
+
+		do {
+			gsgpu_sa_bo_try_free(sa_manager);
+
+			if (gsgpu_sa_bo_try_alloc(sa_manager, *sa_bo,
+						   size, align)) {
+				spin_unlock(&sa_manager->wq.lock);
+				return 0;
+			}
+
+			/* see if we can skip over some allocations */
+		} while (gsgpu_sa_bo_next_hole(sa_manager, fences, tries));
+
+		for (i = 0, count = 0; i < GSGPU_SA_NUM_FENCE_LISTS; ++i)
+			if (fences[i])
+				fences[count++] = dma_fence_get(fences[i]);
+
+		if (count) {
+			spin_unlock(&sa_manager->wq.lock);
+			t = dma_fence_wait_any_timeout(fences, count, false,
+						       MAX_SCHEDULE_TIMEOUT,
+						       NULL);
+			for (i = 0; i < count; ++i)
+				dma_fence_put(fences[i]);
+
+			r = (t > 0) ? 0 : t;
+			spin_lock(&sa_manager->wq.lock);
+		} else {
+			/* if we have nothing to wait for block */
+			r = wait_event_interruptible_locked(
+				sa_manager->wq,
+				gsgpu_sa_event(sa_manager, size, align)
+			);
+		}
+
+	} while (!r);
+
+	spin_unlock(&sa_manager->wq.lock);
+	kfree(*sa_bo);
+	*sa_bo = NULL;
+	return r;
+}
+
+void gsgpu_sa_bo_free(struct gsgpu_device *adev, struct gsgpu_sa_bo **sa_bo,
+		       struct dma_fence *fence)
+{
+	struct gsgpu_sa_manager *sa_manager;
+
+	if (sa_bo == NULL || *sa_bo == NULL) {
+		return;
+	}
+
+	sa_manager = (*sa_bo)->manager;
+	spin_lock(&sa_manager->wq.lock);
+	if (fence && !dma_fence_is_signaled(fence)) {
+		uint32_t idx;
+
+		(*sa_bo)->fence = dma_fence_get(fence);
+		idx = fence->context % GSGPU_SA_NUM_FENCE_LISTS;
+		list_add_tail(&(*sa_bo)->flist, &sa_manager->flist[idx]);
+	} else {
+		gsgpu_sa_bo_remove_locked(*sa_bo);
+	}
+	wake_up_all_locked(&sa_manager->wq);
+	spin_unlock(&sa_manager->wq.lock);
+	*sa_bo = NULL;
+}
+
+#if defined(CONFIG_DEBUG_FS)
+
+void gsgpu_sa_bo_dump_debug_info(struct gsgpu_sa_manager *sa_manager,
+				  struct seq_file *m)
+{
+	struct gsgpu_sa_bo *i;
+
+	spin_lock(&sa_manager->wq.lock);
+	list_for_each_entry(i, &sa_manager->olist, olist) {
+		uint64_t soffset = i->soffset + sa_manager->gpu_addr;
+		uint64_t eoffset = i->eoffset + sa_manager->gpu_addr;
+		if (&i->olist == sa_manager->hole) {
+			seq_printf(m, ">");
+		} else {
+			seq_printf(m, " ");
+		}
+		seq_printf(m, "[0x%010llx 0x%010llx] size %8lld",
+			   soffset, eoffset, eoffset - soffset);
+
+		if (i->fence)
+			seq_printf(m, " protected by 0x%08x on context %llu",
+				   i->fence->seqno, i->fence->context);
+
+		seq_printf(m, "\n");
+	}
+	spin_unlock(&sa_manager->wq.lock);
+}
+#endif
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_sched.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_sched.c
new file mode 100644
index 000000000000..70adf7fe6c75
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_sched.c
@@ -0,0 +1,76 @@
+#include <linux/fdtable.h>
+#include <linux/pid.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+
+#include "gsgpu_vm.h"
+
+enum drm_sched_priority gsgpu_to_sched_priority(int gsgpu_priority)
+{
+	switch (gsgpu_priority) {
+	case GSGPU_CTX_PRIORITY_VERY_HIGH:
+		return DRM_SCHED_PRIORITY_HIGH_HW;
+	case GSGPU_CTX_PRIORITY_HIGH:
+		return DRM_SCHED_PRIORITY_HIGH_SW;
+	case GSGPU_CTX_PRIORITY_NORMAL:
+		return DRM_SCHED_PRIORITY_NORMAL;
+	case GSGPU_CTX_PRIORITY_LOW:
+	case GSGPU_CTX_PRIORITY_VERY_LOW:
+		return DRM_SCHED_PRIORITY_LOW;
+	case GSGPU_CTX_PRIORITY_UNSET:
+		return DRM_SCHED_PRIORITY_UNSET;
+	default:
+		WARN(1, "Invalid context priority %d\n", gsgpu_priority);
+		return DRM_SCHED_PRIORITY_INVALID;
+	}
+}
+
+static int gsgpu_sched_process_priority_override(struct gsgpu_device *adev,
+						  int fd,
+						  enum drm_sched_priority priority)
+{
+	struct file *filp = fget(fd);
+	struct drm_file *file;
+	struct gsgpu_fpriv *fpriv;
+	struct gsgpu_ctx *ctx;
+	uint32_t id;
+
+	if (!filp)
+		return -EINVAL;
+
+	file = filp->private_data;
+	fpriv = file->driver_priv;
+	idr_for_each_entry(&fpriv->ctx_mgr.ctx_handles, ctx, id)
+		gsgpu_ctx_priority_override(ctx, priority);
+
+	fput(filp);
+
+	return 0;
+}
+
+int gsgpu_sched_ioctl(struct drm_device *dev, void *data,
+		       struct drm_file *filp)
+{
+	union drm_gsgpu_sched *args = data;
+	struct gsgpu_device *adev = dev->dev_private;
+	enum drm_sched_priority priority;
+	int r;
+
+	priority = gsgpu_to_sched_priority(args->in.priority);
+	if (args->in.flags || priority == DRM_SCHED_PRIORITY_INVALID)
+		return -EINVAL;
+
+	switch (args->in.op) {
+	case GSGPU_SCHED_OP_PROCESS_PRIORITY_OVERRIDE:
+		r = gsgpu_sched_process_priority_override(adev,
+							   args->in.fd,
+							   priority);
+		break;
+	default:
+		DRM_ERROR("Invalid sched op specified: %d\n", args->in.op);
+		r = -EINVAL;
+		break;
+	}
+
+	return r;
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_sync.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_sync.c
new file mode 100644
index 000000000000..bffb429e8862
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_sync.c
@@ -0,0 +1,389 @@
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include "gsgpu_trace.h"
+
+struct gsgpu_sync_entry {
+	struct hlist_node	node;
+	struct dma_fence	*fence;
+	bool	explicit;
+};
+
+static struct kmem_cache *gsgpu_sync_slab;
+
+/**
+ * gsgpu_sync_create - zero init sync object
+ *
+ * @sync: sync object to initialize
+ *
+ * Just clear the sync object for now.
+ */
+void gsgpu_sync_create(struct gsgpu_sync *sync)
+{
+	hash_init(sync->fences);
+	sync->last_vm_update = NULL;
+}
+
+/**
+ * gsgpu_sync_same_dev - test if fence belong to us
+ *
+ * @adev: gsgpu device to use for the test
+ * @f: fence to test
+ *
+ * Test if the fence was issued by us.
+ */
+static bool gsgpu_sync_same_dev(struct gsgpu_device *adev,
+				 struct dma_fence *f)
+{
+	struct drm_sched_fence *s_fence = to_drm_sched_fence(f);
+
+	if (s_fence) {
+		struct gsgpu_ring *ring;
+
+		ring = container_of(s_fence->sched, struct gsgpu_ring, sched);
+		return ring->adev == adev;
+	}
+
+	return false;
+}
+
+/**
+ * gsgpu_sync_get_owner - extract the owner of a fence
+ *
+ * @fence: fence get the owner from
+ *
+ * Extract who originally created the fence.
+ */
+static void *gsgpu_sync_get_owner(struct dma_fence *f)
+{
+	struct drm_sched_fence *s_fence;
+
+	if (!f)
+		return GSGPU_FENCE_OWNER_UNDEFINED;
+
+	s_fence = to_drm_sched_fence(f);
+	if (s_fence)
+		return s_fence->owner;
+
+	return GSGPU_FENCE_OWNER_UNDEFINED;
+}
+
+/**
+ * gsgpu_sync_keep_later - Keep the later fence
+ *
+ * @keep: existing fence to test
+ * @fence: new fence
+ *
+ * Either keep the existing fence or the new one, depending which one is later.
+ */
+static void gsgpu_sync_keep_later(struct dma_fence **keep,
+				   struct dma_fence *fence)
+{
+	if (*keep && dma_fence_is_later(*keep, fence))
+		return;
+
+	dma_fence_put(*keep);
+	*keep = dma_fence_get(fence);
+}
+
+/**
+ * gsgpu_sync_add_later - add the fence to the hash
+ *
+ * @sync: sync object to add the fence to
+ * @f: fence to add
+ *
+ * Tries to add the fence to an existing hash entry. Returns true when an entry
+ * was found, false otherwise.
+ */
+static bool gsgpu_sync_add_later(struct gsgpu_sync *sync, struct dma_fence *f, bool explicit)
+{
+	struct gsgpu_sync_entry *e;
+
+	hash_for_each_possible(sync->fences, e, node, f->context) {
+		if (unlikely(e->fence->context != f->context))
+			continue;
+
+		gsgpu_sync_keep_later(&e->fence, f);
+
+		/* Preserve eplicit flag to not loose pipe line sync */
+		e->explicit |= explicit;
+
+		return true;
+	}
+	return false;
+}
+
+/**
+ * gsgpu_sync_fence - remember to sync to this fence
+ *
+ * @sync: sync object to add fence to
+ * @fence: fence to sync to
+ *
+ */
+int gsgpu_sync_fence(struct gsgpu_device *adev, struct gsgpu_sync *sync,
+		      struct dma_fence *f, bool explicit)
+{
+	struct gsgpu_sync_entry *e;
+
+	if (!f)
+		return 0;
+	if (gsgpu_sync_same_dev(adev, f) &&
+	    gsgpu_sync_get_owner(f) == GSGPU_FENCE_OWNER_VM)
+		gsgpu_sync_keep_later(&sync->last_vm_update, f);
+
+	if (gsgpu_sync_add_later(sync, f, explicit))
+		return 0;
+
+	e = kmem_cache_alloc(gsgpu_sync_slab, GFP_KERNEL);
+	if (!e)
+		return -ENOMEM;
+
+	e->explicit = explicit;
+
+	hash_add(sync->fences, &e->node, f->context);
+	e->fence = dma_fence_get(f);
+	return 0;
+}
+
+/**
+ * gsgpu_sync_resv - sync to a reservation object
+ *
+ * @sync: sync object to add fences from reservation object to
+ * @resv: reservation object with embedded fence
+ * @explicit_sync: true if we should only sync to the exclusive fence
+ *
+ * Sync to the fence
+ */
+int gsgpu_sync_resv(struct gsgpu_device *adev,
+		     struct gsgpu_sync *sync,
+		     struct reservation_object *resv,
+		     void *owner, bool explicit_sync)
+{
+	struct reservation_object_list *flist;
+	struct dma_fence *f;
+	void *fence_owner;
+	unsigned i;
+	int r = 0;
+
+	if (resv == NULL)
+		return -EINVAL;
+
+	/* always sync to the exclusive fence */
+	f = reservation_object_get_excl(resv);
+	r = gsgpu_sync_fence(adev, sync, f, false);
+
+	flist = reservation_object_get_list(resv);
+	if (!flist || r)
+		return r;
+
+	for (i = 0; i < flist->shared_count; ++i) {
+		f = rcu_dereference_protected(flist->shared[i],
+					      reservation_object_held(resv));
+		/* We only want to trigger KFD eviction fences on
+		 * evict or move jobs. Skip KFD fences otherwise.
+		 */
+		fence_owner = gsgpu_sync_get_owner(f);
+		if (fence_owner == GSGPU_FENCE_OWNER_KFD &&
+		    owner != GSGPU_FENCE_OWNER_UNDEFINED)
+			continue;
+
+		if (gsgpu_sync_same_dev(adev, f)) {
+			/* VM updates are only interesting
+			 * for other VM updates and moves.
+			 */
+			if ((owner != GSGPU_FENCE_OWNER_UNDEFINED) &&
+			    (fence_owner != GSGPU_FENCE_OWNER_UNDEFINED) &&
+			    ((owner == GSGPU_FENCE_OWNER_VM) !=
+			     (fence_owner == GSGPU_FENCE_OWNER_VM)))
+				continue;
+
+			/* Ignore fence from the same owner and explicit one as
+			 * long as it isn't undefined.
+			 */
+			if (owner != GSGPU_FENCE_OWNER_UNDEFINED &&
+			    (fence_owner == owner || explicit_sync))
+				continue;
+		}
+
+		r = gsgpu_sync_fence(adev, sync, f, false);
+		if (r)
+			break;
+	}
+	return r;
+}
+
+/**
+ * gsgpu_sync_peek_fence - get the next fence not signaled yet
+ *
+ * @sync: the sync object
+ * @ring: optional ring to use for test
+ *
+ * Returns the next fence not signaled yet without removing it from the sync
+ * object.
+ */
+struct dma_fence *gsgpu_sync_peek_fence(struct gsgpu_sync *sync,
+					 struct gsgpu_ring *ring)
+{
+	struct gsgpu_sync_entry *e;
+	struct hlist_node *tmp;
+	int i;
+
+	hash_for_each_safe(sync->fences, i, tmp, e, node) {
+		struct dma_fence *f = e->fence;
+		struct drm_sched_fence *s_fence = to_drm_sched_fence(f);
+
+		if (dma_fence_is_signaled(f)) {
+			hash_del(&e->node);
+			dma_fence_put(f);
+			kmem_cache_free(gsgpu_sync_slab, e);
+			continue;
+		}
+		if (ring && s_fence) {
+			/* For fences from the same ring it is sufficient
+			 * when they are scheduled.
+			 */
+			if (s_fence->sched == &ring->sched) {
+				if (dma_fence_is_signaled(&s_fence->scheduled))
+					continue;
+
+				return &s_fence->scheduled;
+			}
+		}
+
+		return f;
+	}
+
+	return NULL;
+}
+
+/**
+ * gsgpu_sync_get_fence - get the next fence from the sync object
+ *
+ * @sync: sync object to use
+ * @explicit: true if the next fence is explicit
+ *
+ * Get and removes the next fence from the sync object not signaled yet.
+ */
+struct dma_fence *gsgpu_sync_get_fence(struct gsgpu_sync *sync, bool *explicit)
+{
+	struct gsgpu_sync_entry *e;
+	struct hlist_node *tmp;
+	struct dma_fence *f;
+	int i;
+	hash_for_each_safe(sync->fences, i, tmp, e, node) {
+
+		f = e->fence;
+		if (explicit)
+			*explicit = e->explicit;
+
+		hash_del(&e->node);
+		kmem_cache_free(gsgpu_sync_slab, e);
+
+		if (!dma_fence_is_signaled(f))
+			return f;
+
+		dma_fence_put(f);
+	}
+	return NULL;
+}
+
+/**
+ * gsgpu_sync_clone - clone a sync object
+ *
+ * @source: sync object to clone
+ * @clone: pointer to destination sync object
+ *
+ * Adds references to all unsignaled fences in @source to @clone. Also
+ * removes signaled fences from @source while at it.
+ */
+int gsgpu_sync_clone(struct gsgpu_sync *source, struct gsgpu_sync *clone)
+{
+	struct gsgpu_sync_entry *e;
+	struct hlist_node *tmp;
+	struct dma_fence *f;
+	int i, r;
+
+	hash_for_each_safe(source->fences, i, tmp, e, node) {
+		f = e->fence;
+		if (!dma_fence_is_signaled(f)) {
+			r = gsgpu_sync_fence(NULL, clone, f, e->explicit);
+			if (r)
+				return r;
+		} else {
+			hash_del(&e->node);
+			dma_fence_put(f);
+			kmem_cache_free(gsgpu_sync_slab, e);
+		}
+	}
+
+	dma_fence_put(clone->last_vm_update);
+	clone->last_vm_update = dma_fence_get(source->last_vm_update);
+
+	return 0;
+}
+
+int gsgpu_sync_wait(struct gsgpu_sync *sync, bool intr)
+{
+	struct gsgpu_sync_entry *e;
+	struct hlist_node *tmp;
+	int i, r;
+
+	hash_for_each_safe(sync->fences, i, tmp, e, node) {
+		r = dma_fence_wait(e->fence, intr);
+		if (r)
+			return r;
+
+		hash_del(&e->node);
+		dma_fence_put(e->fence);
+		kmem_cache_free(gsgpu_sync_slab, e);
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_sync_free - free the sync object
+ *
+ * @sync: sync object to use
+ *
+ * Free the sync object.
+ */
+void gsgpu_sync_free(struct gsgpu_sync *sync)
+{
+	struct gsgpu_sync_entry *e;
+	struct hlist_node *tmp;
+	unsigned i;
+
+	hash_for_each_safe(sync->fences, i, tmp, e, node) {
+		hash_del(&e->node);
+		dma_fence_put(e->fence);
+		kmem_cache_free(gsgpu_sync_slab, e);
+	}
+
+	dma_fence_put(sync->last_vm_update);
+}
+
+/**
+ * gsgpu_sync_init - init sync object subsystem
+ *
+ * Allocate the slab allocator.
+ */
+int gsgpu_sync_init(void)
+{
+	gsgpu_sync_slab = kmem_cache_create(
+		"gsgpu_sync", sizeof(struct gsgpu_sync_entry), 0,
+		SLAB_HWCACHE_ALIGN, NULL);
+	if (!gsgpu_sync_slab)
+		return -ENOMEM;
+
+	return 0;
+}
+
+/**
+ * gsgpu_sync_fini - fini sync object subsystem
+ *
+ * Free the slab allocator.
+ */
+void gsgpu_sync_fini(void)
+{
+	kmem_cache_destroy(gsgpu_sync_slab);
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_test.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_test.c
new file mode 100644
index 000000000000..b2b8f6a8bb83
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_test.c
@@ -0,0 +1,226 @@
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+
+/* Test BO GTT->VRAM and VRAM->GTT GPU copies across the whole GTT aperture */
+static void gsgpu_do_test_moves(struct gsgpu_device *adev)
+{
+	struct gsgpu_ring *ring = adev->mman.buffer_funcs_ring;
+	struct gsgpu_bo *vram_obj = NULL;
+	struct gsgpu_bo **gtt_obj = NULL;
+	struct gsgpu_bo_param bp;
+	uint64_t gart_addr, vram_addr;
+	unsigned n, size;
+	int i, r;
+
+	size = 1024 * 1024;
+
+	/* Number of tests =
+	 * (Total GTT - IB pool - writeback page - ring buffers) / test size
+	 */
+	n = adev->gmc.gart_size - GSGPU_IB_POOL_SIZE*64*1024;
+	for (i = 0; i < GSGPU_MAX_RINGS; ++i)
+		if (adev->rings[i])
+			n -= adev->rings[i]->ring_size;
+	if (adev->wb.wb_obj)
+		n -= GSGPU_GPU_PAGE_SIZE;
+	if (adev->irq.ih.ring_obj)
+		n -= adev->irq.ih.ring_size;
+	n /= size;
+
+	gtt_obj = kcalloc(n, sizeof(*gtt_obj), GFP_KERNEL);
+	if (!gtt_obj) {
+		DRM_ERROR("Failed to allocate %d pointers\n", n);
+		r = 1;
+		goto out_cleanup;
+	}
+	memset(&bp, 0, sizeof(bp));
+	bp.size = size;
+	bp.byte_align = PAGE_SIZE;
+	bp.domain = GSGPU_GEM_DOMAIN_VRAM;
+	bp.flags = 0;
+	bp.type = ttm_bo_type_kernel;
+	bp.resv = NULL;
+
+	r = gsgpu_bo_create(adev, &bp, &vram_obj);
+	if (r) {
+		DRM_ERROR("Failed to create VRAM object\n");
+		goto out_cleanup;
+	}
+	r = gsgpu_bo_reserve(vram_obj, false);
+	if (unlikely(r != 0))
+		goto out_unref;
+	r = gsgpu_bo_pin(vram_obj, GSGPU_GEM_DOMAIN_VRAM);
+	if (r) {
+		DRM_ERROR("Failed to pin VRAM object\n");
+		goto out_unres;
+	}
+	vram_addr = gsgpu_bo_gpu_offset(vram_obj);
+	for (i = 0; i < n; i++) {
+		void *gtt_map, *vram_map;
+		void **gart_start, **gart_end;
+		void **vram_start, **vram_end;
+		struct dma_fence *fence = NULL;
+
+		bp.domain = GSGPU_GEM_DOMAIN_GTT;
+		r = gsgpu_bo_create(adev, &bp, gtt_obj + i);
+		if (r) {
+			DRM_ERROR("Failed to create GTT object %d\n", i);
+			goto out_lclean;
+		}
+
+		r = gsgpu_bo_reserve(gtt_obj[i], false);
+		if (unlikely(r != 0))
+			goto out_lclean_unref;
+		r = gsgpu_bo_pin(gtt_obj[i], GSGPU_GEM_DOMAIN_GTT);
+		if (r) {
+			DRM_ERROR("Failed to pin GTT object %d\n", i);
+			goto out_lclean_unres;
+		}
+		r = gsgpu_ttm_alloc_gart(&gtt_obj[i]->tbo);
+		if (r) {
+			DRM_ERROR("%p bind failed\n", gtt_obj[i]);
+			goto out_lclean_unpin;
+		}
+		gart_addr = gsgpu_bo_gpu_offset(gtt_obj[i]);
+
+		r = gsgpu_bo_kmap(gtt_obj[i], &gtt_map);
+		if (r) {
+			DRM_ERROR("Failed to map GTT object %d\n", i);
+			goto out_lclean_unpin;
+		}
+
+		for (gart_start = gtt_map, gart_end = gtt_map + size;
+		     gart_start < gart_end;
+		     gart_start++)
+			*gart_start = gart_start;
+
+		gsgpu_bo_kunmap(gtt_obj[i]);
+
+		r = gsgpu_copy_buffer(ring, gart_addr, vram_addr,
+				       size, NULL, &fence, false, false);
+
+		if (r) {
+			DRM_ERROR("Failed GTT->VRAM copy %d\n", i);
+			goto out_lclean_unpin;
+		}
+
+		r = dma_fence_wait(fence, false);
+		if (r) {
+			DRM_ERROR("Failed to wait for GTT->VRAM fence %d\n", i);
+			goto out_lclean_unpin;
+		}
+
+		dma_fence_put(fence);
+
+		r = gsgpu_bo_kmap(vram_obj, &vram_map);
+		if (r) {
+			DRM_ERROR("Failed to map VRAM object after copy %d\n", i);
+			goto out_lclean_unpin;
+		}
+
+		for (gart_start = gtt_map, gart_end = gtt_map + size,
+		     vram_start = vram_map, vram_end = vram_map + size;
+		     vram_start < vram_end;
+		     gart_start++, vram_start++) {
+			if (*vram_start != gart_start) {
+				DRM_ERROR("Incorrect GTT->VRAM copy %d: Got 0x%p, "
+					  "expected 0x%p (GTT/VRAM offset "
+					  "0x%16llx/0x%16llx)\n",
+					  i, *vram_start, gart_start,
+					  (unsigned long long)
+					  (gart_addr - adev->gmc.gart_start +
+					   (void *)gart_start - gtt_map),
+					  (unsigned long long)
+					  (vram_addr - adev->gmc.vram_start +
+					   (void *)gart_start - gtt_map));
+				gsgpu_bo_kunmap(vram_obj);
+				goto out_lclean_unpin;
+			}
+			*vram_start = vram_start;
+		}
+
+		gsgpu_bo_kunmap(vram_obj);
+
+		r = gsgpu_copy_buffer(ring, vram_addr, gart_addr,
+				       size, NULL, &fence, false, false);
+
+		if (r) {
+			DRM_ERROR("Failed VRAM->GTT copy %d\n", i);
+			goto out_lclean_unpin;
+		}
+
+		r = dma_fence_wait(fence, false);
+		if (r) {
+			DRM_ERROR("Failed to wait for VRAM->GTT fence %d\n", i);
+			goto out_lclean_unpin;
+		}
+
+		dma_fence_put(fence);
+
+		r = gsgpu_bo_kmap(gtt_obj[i], &gtt_map);
+		if (r) {
+			DRM_ERROR("Failed to map GTT object after copy %d\n", i);
+			goto out_lclean_unpin;
+		}
+
+		for (gart_start = gtt_map, gart_end = gtt_map + size,
+		     vram_start = vram_map, vram_end = vram_map + size;
+		     gart_start < gart_end;
+		     gart_start++, vram_start++) {
+			if (*gart_start != vram_start) {
+				DRM_ERROR("Incorrect VRAM->GTT copy %d: Got 0x%p, "
+					  "expected 0x%p (VRAM/GTT offset "
+					  "0x%16llx/0x%16llx)\n",
+					  i, *gart_start, vram_start,
+					  (unsigned long long)
+					  (vram_addr - adev->gmc.vram_start +
+					   (void *)vram_start - vram_map),
+					  (unsigned long long)
+					  (gart_addr - adev->gmc.gart_start +
+					   (void *)vram_start - vram_map));
+				gsgpu_bo_kunmap(gtt_obj[i]);
+				goto out_lclean_unpin;
+			}
+		}
+
+		gsgpu_bo_kunmap(gtt_obj[i]);
+
+		DRM_INFO("Tested GTT->VRAM and VRAM->GTT copy for GTT offset 0x%llx\n",
+			 gart_addr - adev->gmc.gart_start);
+		continue;
+
+out_lclean_unpin:
+		gsgpu_bo_unpin(gtt_obj[i]);
+out_lclean_unres:
+		gsgpu_bo_unreserve(gtt_obj[i]);
+out_lclean_unref:
+		gsgpu_bo_unref(&gtt_obj[i]);
+out_lclean:
+		for (--i; i >= 0; --i) {
+			gsgpu_bo_unpin(gtt_obj[i]);
+			gsgpu_bo_unreserve(gtt_obj[i]);
+			gsgpu_bo_unref(&gtt_obj[i]);
+		}
+		if (fence)
+			dma_fence_put(fence);
+		break;
+	}
+
+	gsgpu_bo_unpin(vram_obj);
+out_unres:
+	gsgpu_bo_unreserve(vram_obj);
+out_unref:
+	gsgpu_bo_unref(&vram_obj);
+out_cleanup:
+	kfree(gtt_obj);
+	if (r) {
+		pr_warn("Error while testing BO move\n");
+	}
+}
+
+void gsgpu_test_moves(struct gsgpu_device *adev)
+{
+	if (adev->mman.buffer_funcs)
+		gsgpu_do_test_moves(adev);
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_trace_points.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_trace_points.c
new file mode 100644
index 000000000000..28a80c551543
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_trace_points.c
@@ -0,0 +1,6 @@
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+
+#define CREATE_TRACE_POINTS
+#include "gsgpu_trace.h"
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c
new file mode 100644
index 000000000000..631f059851cc
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c
@@ -0,0 +1,2396 @@
+#include <drm/ttm/ttm_bo_api.h>
+#include <drm/ttm/ttm_bo_driver.h>
+#include <drm/ttm/ttm_placement.h>
+#include <drm/ttm/ttm_module.h>
+#include <drm/ttm/ttm_page_alloc.h>
+#include <drm/drmP.h>
+#include <drm/drm_cache.h>
+#include <drm/gsgpu_drm.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/swiotlb.h>
+#include <linux/swap.h>
+#include <linux/pagemap.h>
+#include <linux/debugfs.h>
+#include <linux/iommu.h>
+#include "gsgpu.h"
+#include "gsgpu_object.h"
+#include "gsgpu_trace.h"
+
+/**TODO
+ *Need fixed this, mmap ops had access handler.
+**/
+#define mmMM_INDEX         0x0
+#define mmMM_INDEX_HI      0x6
+#define mmMM_DATA          0x1
+
+#define DRM_FILE_PAGE_OFFSET (0x100000000ULL >> PAGE_SHIFT)
+
+static int gsgpu_map_buffer(struct ttm_buffer_object *bo,
+			     struct ttm_mem_reg *mem, unsigned num_pages,
+			     uint64_t offset, unsigned window,
+			     struct gsgpu_ring *ring,
+			     uint64_t *addr);
+
+static int gsgpu_ttm_debugfs_init(struct gsgpu_device *adev);
+static void gsgpu_ttm_debugfs_fini(struct gsgpu_device *adev);
+
+/*
+ * Global memory.
+ */
+
+/**
+ * gsgpu_ttm_mem_global_init - Initialize and acquire reference to
+ * memory object
+ *
+ * @ref: Object for initialization.
+ *
+ * This is called by drm_global_item_ref() when an object is being
+ * initialized.
+ */
+static int gsgpu_ttm_mem_global_init(struct drm_global_reference *ref)
+{
+	return ttm_mem_global_init(ref->object);
+}
+
+/**
+ * gsgpu_ttm_mem_global_release - Drop reference to a memory object
+ *
+ * @ref: Object being removed
+ *
+ * This is called by drm_global_item_unref() when an object is being
+ * released.
+ */
+static void gsgpu_ttm_mem_global_release(struct drm_global_reference *ref)
+{
+	ttm_mem_global_release(ref->object);
+}
+
+/**
+ * gsgpu_ttm_global_init - Initialize global TTM memory reference structures.
+ *
+ * @adev: GSGPU device for which the global structures need to be registered.
+ *
+ * This is called as part of the GSGPU ttm init from gsgpu_ttm_init()
+ * during bring up.
+ */
+static int gsgpu_ttm_global_init(struct gsgpu_device *adev)
+{
+	struct drm_global_reference *global_ref;
+	int r;
+
+	/* ensure reference is false in case init fails */
+	adev->mman.mem_global_referenced = false;
+
+	global_ref = &adev->mman.mem_global_ref;
+	global_ref->global_type = DRM_GLOBAL_TTM_MEM;
+	global_ref->size = sizeof(struct ttm_mem_global);
+	global_ref->init = &gsgpu_ttm_mem_global_init;
+	global_ref->release = &gsgpu_ttm_mem_global_release;
+	r = drm_global_item_ref(global_ref);
+	if (r) {
+		DRM_ERROR("Failed setting up TTM memory accounting "
+			  "subsystem.\n");
+		goto error_mem;
+	}
+
+	adev->mman.bo_global_ref.mem_glob =
+		adev->mman.mem_global_ref.object;
+	global_ref = &adev->mman.bo_global_ref.ref;
+	global_ref->global_type = DRM_GLOBAL_TTM_BO;
+	global_ref->size = sizeof(struct ttm_bo_global);
+	global_ref->init = &ttm_bo_global_init;
+	global_ref->release = &ttm_bo_global_release;
+	r = drm_global_item_ref(global_ref);
+	if (r) {
+		DRM_ERROR("Failed setting up TTM BO subsystem.\n");
+		goto error_bo;
+	}
+
+	mutex_init(&adev->mman.gtt_window_lock);
+
+	adev->mman.mem_global_referenced = true;
+
+	return 0;
+
+error_bo:
+	drm_global_item_unref(&adev->mman.mem_global_ref);
+error_mem:
+	return r;
+}
+
+static void gsgpu_ttm_global_fini(struct gsgpu_device *adev)
+{
+	if (adev->mman.mem_global_referenced) {
+		mutex_destroy(&adev->mman.gtt_window_lock);
+		drm_global_item_unref(&adev->mman.bo_global_ref.ref);
+		drm_global_item_unref(&adev->mman.mem_global_ref);
+		adev->mman.mem_global_referenced = false;
+	}
+}
+
+static int gsgpu_invalidate_caches(struct ttm_bo_device *bdev, uint32_t flags)
+{
+	return 0;
+}
+
+/**
+ * gsgpu_init_mem_type - Initialize a memory manager for a specific type of
+ * memory request.
+ *
+ * @bdev: The TTM BO device object (contains a reference to gsgpu_device)
+ * @type: The type of memory requested
+ * @man: The memory type manager for each domain
+ *
+ * This is called by ttm_bo_init_mm() when a buffer object is being
+ * initialized.
+ */
+static int gsgpu_init_mem_type(struct ttm_bo_device *bdev, uint32_t type,
+				struct ttm_mem_type_manager *man)
+{
+	struct gsgpu_device *adev;
+
+	adev = gsgpu_ttm_adev(bdev);
+
+	switch (type) {
+	case TTM_PL_SYSTEM:
+		/* System memory */
+		man->flags = TTM_MEMTYPE_FLAG_MAPPABLE;
+		man->available_caching = TTM_PL_MASK_CACHING;
+		man->default_caching = TTM_PL_FLAG_CACHED;
+		break;
+	case TTM_PL_TT:
+		/* GTT memory  */
+		man->func = &gsgpu_gtt_mgr_func;
+		man->gpu_offset = adev->gmc.gart_start;
+		man->available_caching = TTM_PL_MASK_CACHING;
+		man->default_caching = TTM_PL_FLAG_CACHED;
+		man->flags = TTM_MEMTYPE_FLAG_MAPPABLE | TTM_MEMTYPE_FLAG_CMA;
+		break;
+	case TTM_PL_VRAM:
+		/* "On-card" video ram */
+		man->func = &gsgpu_vram_mgr_func;
+		man->gpu_offset = adev->gmc.vram_start;
+		man->flags = TTM_MEMTYPE_FLAG_FIXED |
+			     TTM_MEMTYPE_FLAG_MAPPABLE;
+		man->available_caching = TTM_PL_FLAG_UNCACHED | TTM_PL_FLAG_WC;
+		man->default_caching = TTM_PL_FLAG_WC;
+		break;
+	default:
+		DRM_ERROR("Unsupported memory type %u\n", (unsigned)type);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/**
+ * gsgpu_evict_flags - Compute placement flags
+ *
+ * @bo: The buffer object to evict
+ * @placement: Possible destination(s) for evicted BO
+ *
+ * Fill in placement data when ttm_bo_evict() is called
+ */
+static void gsgpu_evict_flags(struct ttm_buffer_object *bo,
+				struct ttm_placement *placement)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->bdev);
+	struct gsgpu_bo *abo;
+	static const struct ttm_place placements = {
+		.fpfn = 0,
+		.lpfn = 0,
+		.flags = TTM_PL_MASK_CACHING | TTM_PL_FLAG_SYSTEM
+	};
+
+	/* Don't handle scatter gather BOs */
+	if (bo->type == ttm_bo_type_sg) {
+		placement->num_placement = 0;
+		placement->num_busy_placement = 0;
+		return;
+	}
+
+	/* Object isn't an GSGPU object so ignore */
+	if (!gsgpu_bo_is_gsgpu_bo(bo)) {
+		placement->placement = &placements;
+		placement->busy_placement = &placements;
+		placement->num_placement = 1;
+		placement->num_busy_placement = 1;
+		return;
+	}
+
+	abo = ttm_to_gsgpu_bo(bo);
+	switch (bo->mem.mem_type) {
+	case TTM_PL_VRAM:
+		if (!adev->mman.buffer_funcs_enabled) {
+			/* Move to system memory */
+			gsgpu_bo_placement_from_domain(abo, GSGPU_GEM_DOMAIN_CPU);
+		} else if (!gsgpu_gmc_vram_full_visible(&adev->gmc) &&
+			   !(abo->flags & GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) &&
+			   gsgpu_bo_in_cpu_visible_vram(abo)) {
+
+			/* Try evicting to the CPU inaccessible part of VRAM
+			 * first, but only set GTT as busy placement, so this
+			 * BO will be evicted to GTT rather than causing other
+			 * BOs to be evicted from VRAM
+			 */
+			gsgpu_bo_placement_from_domain(abo, GSGPU_GEM_DOMAIN_VRAM |
+							 GSGPU_GEM_DOMAIN_GTT);
+			abo->placements[0].fpfn = adev->gmc.visible_vram_size >> PAGE_SHIFT;
+			abo->placements[0].lpfn = 0;
+			abo->placement.busy_placement = &abo->placements[1];
+			abo->placement.num_busy_placement = 1;
+		} else {
+			/* Move to GTT memory */
+			gsgpu_bo_placement_from_domain(abo, GSGPU_GEM_DOMAIN_GTT);
+		}
+		break;
+	case TTM_PL_TT:
+	default:
+		gsgpu_bo_placement_from_domain(abo, GSGPU_GEM_DOMAIN_CPU);
+	}
+	*placement = abo->placement;
+}
+
+/**
+ * gsgpu_verify_access - Verify access for a mmap call
+ *
+ * @bo:	The buffer object to map
+ * @filp: The file pointer from the process performing the mmap
+ *
+ * This is called by ttm_bo_mmap() to verify whether a process
+ * has the right to mmap a BO to their process space.
+ */
+static int gsgpu_verify_access(struct ttm_buffer_object *bo, struct file *filp)
+{
+	struct gsgpu_bo *abo = ttm_to_gsgpu_bo(bo);
+
+	if (gsgpu_ttm_tt_get_usermm(bo->ttm))
+		return -EPERM;
+	return drm_vma_node_verify_access(&abo->gem_base.vma_node,
+					  filp->private_data);
+}
+
+/**
+ * gsgpu_move_null - Register memory for a buffer object
+ *
+ * @bo: The bo to assign the memory to
+ * @new_mem: The memory to be assigned.
+ *
+ * Assign the memory from new_mem to the memory of the buffer object bo.
+ */
+static void gsgpu_move_null(struct ttm_buffer_object *bo,
+			     struct ttm_mem_reg *new_mem)
+{
+	struct ttm_mem_reg *old_mem = &bo->mem;
+
+	BUG_ON(old_mem->mm_node != NULL);
+	*old_mem = *new_mem;
+	new_mem->mm_node = NULL;
+}
+
+/**
+ * gsgpu_mm_node_addr - Compute the GPU relative offset of a GTT buffer.
+ *
+ * @bo: The bo to assign the memory to.
+ * @mm_node: Memory manager node for drm allocator.
+ * @mem: The region where the bo resides.
+ *
+ */
+static uint64_t gsgpu_mm_node_addr(struct ttm_buffer_object *bo,
+				    struct drm_mm_node *mm_node,
+				    struct ttm_mem_reg *mem)
+{
+	uint64_t addr = 0;
+
+	if (mem->mem_type != TTM_PL_TT || gsgpu_gtt_mgr_has_gart_addr(mem)) {
+		addr = mm_node->start << PAGE_SHIFT;
+		addr += bo->bdev->man[mem->mem_type].gpu_offset;
+	}
+	return addr;
+}
+
+/**
+ * gsgpu_find_mm_node - Helper function finds the drm_mm_node corresponding to
+ * @offset. It also modifies the offset to be within the drm_mm_node returned
+ *
+ * @mem: The region where the bo resides.
+ * @offset: The offset that drm_mm_node is used for finding.
+ *
+ */
+static struct drm_mm_node *gsgpu_find_mm_node(struct ttm_mem_reg *mem,
+					       unsigned long *offset)
+{
+	struct drm_mm_node *mm_node = mem->mm_node;
+
+	while (*offset >= (mm_node->size << PAGE_SHIFT)) {
+		*offset -= (mm_node->size << PAGE_SHIFT);
+		++mm_node;
+	}
+	return mm_node;
+}
+
+/**
+ * gsgpu_copy_ttm_mem_to_mem - Helper function for copy
+ *
+ * The function copies @size bytes from {src->mem + src->offset} to
+ * {dst->mem + dst->offset}. src->bo and dst->bo could be same BO for a
+ * move and different for a BO to BO copy.
+ *
+ * @f: Returns the last fence if multiple jobs are submitted.
+ */
+int gsgpu_ttm_copy_mem_to_mem(struct gsgpu_device *adev,
+			       struct gsgpu_copy_mem *src,
+			       struct gsgpu_copy_mem *dst,
+			       uint64_t size,
+			       struct reservation_object *resv,
+			       struct dma_fence **f)
+{
+	struct gsgpu_ring *ring = adev->mman.buffer_funcs_ring;
+	struct drm_mm_node *src_mm, *dst_mm;
+	uint64_t src_node_start, dst_node_start, src_node_size,
+		 dst_node_size, src_page_offset, dst_page_offset;
+	struct dma_fence *fence = NULL;
+	int r = 0;
+	const uint64_t GTT_MAX_BYTES = (GSGPU_GTT_MAX_TRANSFER_SIZE *
+					GSGPU_GPU_PAGE_SIZE);
+
+	if (!adev->mman.buffer_funcs_enabled) {
+		DRM_ERROR("Trying to move memory with ring turned off.\n");
+		return -EINVAL;
+	}
+
+	src_mm = gsgpu_find_mm_node(src->mem, &src->offset);
+	src_node_start = gsgpu_mm_node_addr(src->bo, src_mm, src->mem) +
+					     src->offset;
+	src_node_size = (src_mm->size << PAGE_SHIFT) - src->offset;
+	src_page_offset = src_node_start & (PAGE_SIZE - 1);
+
+	dst_mm = gsgpu_find_mm_node(dst->mem, &dst->offset);
+	dst_node_start = gsgpu_mm_node_addr(dst->bo, dst_mm, dst->mem) +
+					     dst->offset;
+	dst_node_size = (dst_mm->size << PAGE_SHIFT) - dst->offset;
+	dst_page_offset = dst_node_start & (PAGE_SIZE - 1);
+
+	mutex_lock(&adev->mman.gtt_window_lock);
+
+	while (size) {
+		unsigned long cur_size;
+		uint64_t from = src_node_start, to = dst_node_start;
+		struct dma_fence *next;
+
+		/* Copy size cannot exceed GTT_MAX_BYTES. So if src or dst
+		 * begins at an offset, then adjust the size accordingly
+		 */
+		cur_size = min3(min(src_node_size, dst_node_size), size,
+				GTT_MAX_BYTES);
+		if (cur_size + src_page_offset > GTT_MAX_BYTES ||
+		    cur_size + dst_page_offset > GTT_MAX_BYTES)
+			cur_size -= max(src_page_offset, dst_page_offset);
+
+		/* Map only what needs to be accessed. Map src to window 0 and
+		 * dst to window 1
+		 */
+		if (src->mem->mem_type == TTM_PL_TT &&
+		    !gsgpu_gtt_mgr_has_gart_addr(src->mem)) {
+			r = gsgpu_map_buffer(src->bo, src->mem,
+					PFN_UP(cur_size + src_page_offset),
+					src_node_start, 0, ring,
+					&from);
+			if (r)
+				goto error;
+			/* Adjust the offset because gsgpu_map_buffer returns
+			 * start of mapped page
+			 */
+			from += src_page_offset;
+		}
+
+		if (dst->mem->mem_type == TTM_PL_TT &&
+		    !gsgpu_gtt_mgr_has_gart_addr(dst->mem)) {
+			r = gsgpu_map_buffer(dst->bo, dst->mem,
+					PFN_UP(cur_size + dst_page_offset),
+					dst_node_start, 1, ring,
+					&to);
+			if (r)
+				goto error;
+			to += dst_page_offset;
+		}
+
+		r = gsgpu_copy_buffer(ring, from, to, cur_size,
+				       resv, &next, false, true);
+		if (r)
+			goto error;
+
+		dma_fence_put(fence);
+		fence = next;
+
+		size -= cur_size;
+		if (!size)
+			break;
+
+		src_node_size -= cur_size;
+		if (!src_node_size) {
+			src_node_start = gsgpu_mm_node_addr(src->bo, ++src_mm,
+							     src->mem);
+			src_node_size = (src_mm->size << PAGE_SHIFT);
+		} else {
+			src_node_start += cur_size;
+			src_page_offset = src_node_start & (PAGE_SIZE - 1);
+		}
+		dst_node_size -= cur_size;
+		if (!dst_node_size) {
+			dst_node_start = gsgpu_mm_node_addr(dst->bo, ++dst_mm,
+							     dst->mem);
+			dst_node_size = (dst_mm->size << PAGE_SHIFT);
+		} else {
+			dst_node_start += cur_size;
+			dst_page_offset = dst_node_start & (PAGE_SIZE - 1);
+		}
+	}
+error:
+	mutex_unlock(&adev->mman.gtt_window_lock);
+	if (f)
+		*f = dma_fence_get(fence);
+	dma_fence_put(fence);
+	return r;
+}
+
+/**
+ * gsgpu_move_blit - Copy an entire buffer to another buffer
+ *
+ * This is a helper called by gsgpu_bo_move() and gsgpu_move_vram_ram() to
+ * help move buffers to and from VRAM.
+ */
+static int gsgpu_move_blit(struct ttm_buffer_object *bo,
+			    bool evict, bool no_wait_gpu,
+			    struct ttm_mem_reg *new_mem,
+			    struct ttm_mem_reg *old_mem)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->bdev);
+	struct gsgpu_copy_mem src, dst;
+	struct dma_fence *fence = NULL;
+	int r;
+
+	src.bo = bo;
+	dst.bo = bo;
+	src.mem = old_mem;
+	dst.mem = new_mem;
+	src.offset = 0;
+	dst.offset = 0;
+
+	r = gsgpu_ttm_copy_mem_to_mem(adev, &src, &dst,
+				       new_mem->num_pages << PAGE_SHIFT,
+				       bo->resv, &fence);
+	if (r)
+		goto error;
+
+	r = ttm_bo_pipeline_move(bo, fence, evict, new_mem);
+	dma_fence_put(fence);
+	return r;
+
+error:
+	if (fence)
+		dma_fence_wait(fence, false);
+	dma_fence_put(fence);
+	return r;
+}
+
+/**
+ * gsgpu_move_vram_ram - Copy VRAM buffer to RAM buffer
+ *
+ * Called by gsgpu_bo_move().
+ */
+static int gsgpu_move_vram_ram(struct ttm_buffer_object *bo, bool evict,
+				struct ttm_operation_ctx *ctx,
+				struct ttm_mem_reg *new_mem)
+{
+	struct gsgpu_device *adev;
+	struct ttm_mem_reg *old_mem = &bo->mem;
+	struct ttm_mem_reg tmp_mem;
+	struct ttm_place placements;
+	struct ttm_placement placement;
+	int r;
+
+	adev = gsgpu_ttm_adev(bo->bdev);
+
+	/* create space/pages for new_mem in GTT space */
+	tmp_mem = *new_mem;
+	tmp_mem.mm_node = NULL;
+	placement.num_placement = 1;
+	placement.placement = &placements;
+	placement.num_busy_placement = 1;
+	placement.busy_placement = &placements;
+	placements.fpfn = 0;
+	placements.lpfn = 0;
+	placements.flags = TTM_PL_MASK_CACHING | TTM_PL_FLAG_TT;
+	r = ttm_bo_mem_space(bo, &placement, &tmp_mem, ctx);
+	if (unlikely(r)) {
+		return r;
+	}
+
+	/* set caching flags */
+	r = ttm_tt_set_placement_caching(bo->ttm, tmp_mem.placement);
+	if (unlikely(r)) {
+		goto out_cleanup;
+	}
+
+	/* Bind the memory to the GTT space */
+	r = ttm_tt_bind(bo->ttm, &tmp_mem, ctx);
+	if (unlikely(r)) {
+		goto out_cleanup;
+	}
+
+	/* blit VRAM to GTT */
+	r = gsgpu_move_blit(bo, evict, ctx->no_wait_gpu, &tmp_mem, old_mem);
+	if (unlikely(r)) {
+		goto out_cleanup;
+	}
+
+	/* move BO (in tmp_mem) to new_mem */
+	r = ttm_bo_move_ttm(bo, ctx, new_mem);
+out_cleanup:
+	ttm_bo_mem_put(bo, &tmp_mem);
+	return r;
+}
+
+/**
+ * gsgpu_move_ram_vram - Copy buffer from RAM to VRAM
+ *
+ * Called by gsgpu_bo_move().
+ */
+static int gsgpu_move_ram_vram(struct ttm_buffer_object *bo, bool evict,
+				struct ttm_operation_ctx *ctx,
+				struct ttm_mem_reg *new_mem)
+{
+	struct gsgpu_device *adev;
+	struct ttm_mem_reg *old_mem = &bo->mem;
+	struct ttm_mem_reg tmp_mem;
+	struct ttm_placement placement;
+	struct ttm_place placements;
+	int r;
+
+	adev = gsgpu_ttm_adev(bo->bdev);
+
+	/* make space in GTT for old_mem buffer */
+	tmp_mem = *new_mem;
+	tmp_mem.mm_node = NULL;
+	placement.num_placement = 1;
+	placement.placement = &placements;
+	placement.num_busy_placement = 1;
+	placement.busy_placement = &placements;
+	placements.fpfn = 0;
+	placements.lpfn = 0;
+	placements.flags = TTM_PL_MASK_CACHING | TTM_PL_FLAG_TT;
+	r = ttm_bo_mem_space(bo, &placement, &tmp_mem, ctx);
+	if (unlikely(r)) {
+		return r;
+	}
+
+	/* move/bind old memory to GTT space */
+	r = ttm_bo_move_ttm(bo, ctx, &tmp_mem);
+	if (unlikely(r)) {
+		goto out_cleanup;
+	}
+
+	/* copy to VRAM */
+	r = gsgpu_move_blit(bo, evict, ctx->no_wait_gpu, new_mem, old_mem);
+	if (unlikely(r)) {
+		goto out_cleanup;
+	}
+out_cleanup:
+	ttm_bo_mem_put(bo, &tmp_mem);
+	return r;
+}
+
+/**
+ * gsgpu_bo_move - Move a buffer object to a new memory location
+ *
+ * Called by ttm_bo_handle_move_mem()
+ */
+static int gsgpu_bo_move(struct ttm_buffer_object *bo, bool evict,
+			  struct ttm_operation_ctx *ctx,
+			  struct ttm_mem_reg *new_mem)
+{
+	struct gsgpu_device *adev;
+	struct gsgpu_bo *abo;
+	struct ttm_mem_reg *old_mem = &bo->mem;
+	int r;
+
+	/* Can't move a pinned BO */
+	abo = ttm_to_gsgpu_bo(bo);
+	if (WARN_ON_ONCE(abo->pin_count > 0))
+		return -EINVAL;
+
+	adev = gsgpu_ttm_adev(bo->bdev);
+
+	if (old_mem->mem_type == TTM_PL_SYSTEM && bo->ttm == NULL) {
+		gsgpu_move_null(bo, new_mem);
+		return 0;
+	}
+	if ((old_mem->mem_type == TTM_PL_TT &&
+	     new_mem->mem_type == TTM_PL_SYSTEM) ||
+	    (old_mem->mem_type == TTM_PL_SYSTEM &&
+	     new_mem->mem_type == TTM_PL_TT)) {
+		/* bind is enough */
+		gsgpu_move_null(bo, new_mem);
+		return 0;
+	}
+
+	if (!adev->mman.buffer_funcs_enabled)
+		goto memcpy;
+
+	if (old_mem->mem_type == TTM_PL_VRAM &&
+	    new_mem->mem_type == TTM_PL_SYSTEM) {
+		r = gsgpu_move_vram_ram(bo, evict, ctx, new_mem);
+	} else if (old_mem->mem_type == TTM_PL_SYSTEM &&
+		   new_mem->mem_type == TTM_PL_VRAM) {
+		r = gsgpu_move_ram_vram(bo, evict, ctx, new_mem);
+	} else {
+		r = gsgpu_move_blit(bo, evict, ctx->no_wait_gpu,
+				     new_mem, old_mem);
+	}
+
+	if (r) {
+memcpy:
+		r = ttm_bo_move_memcpy(bo, ctx, new_mem);
+		if (r) {
+			return r;
+		}
+	}
+
+	if (bo->type == ttm_bo_type_device &&
+	    new_mem->mem_type == TTM_PL_VRAM &&
+	    old_mem->mem_type != TTM_PL_VRAM) {
+		/* gsgpu_bo_fault_reserve_notify will re-set this if the CPU
+		 * accesses the BO after it's moved.
+		 */
+		abo->flags &= ~GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+	}
+
+	/* update statistics */
+	atomic64_add((u64)bo->num_pages << PAGE_SHIFT, &adev->num_bytes_moved);
+	return 0;
+}
+
+/**
+ * gsgpu_ttm_io_mem_reserve - Reserve a block of memory during a fault
+ *
+ * Called by ttm_mem_io_reserve() ultimately via ttm_bo_vm_fault()
+ */
+static int gsgpu_ttm_io_mem_reserve(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)
+{
+	struct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bdev);
+	struct drm_mm_node *mm_node = mem->mm_node;
+
+	mem->bus.addr = NULL;
+	mem->bus.offset = 0;
+	mem->bus.size = mem->num_pages << PAGE_SHIFT;
+	mem->bus.base = 0;
+	mem->bus.is_iomem = false;
+	if (!(man->flags & TTM_MEMTYPE_FLAG_MAPPABLE))
+		return -EINVAL;
+	switch (mem->mem_type) {
+	case TTM_PL_SYSTEM:
+		/* system memory */
+		return 0;
+	case TTM_PL_TT:
+		break;
+	case TTM_PL_VRAM:
+		mem->bus.offset = mem->start << PAGE_SHIFT;
+		/* check if it's visible */
+		if ((mem->bus.offset + mem->bus.size) > adev->gmc.visible_vram_size)
+			return -EINVAL;
+		/* Only physically contiguous buffers apply. In a contiguous
+		 * buffer, size of the first mm_node would match the number of
+		 * pages in ttm_mem_reg.
+		 */
+		if (adev->mman.aper_base_kaddr &&
+		    (mm_node->size == mem->num_pages))
+			mem->bus.addr = (u8 *)adev->mman.aper_base_kaddr +
+					mem->bus.offset;
+
+		mem->bus.base = adev->gmc.aper_base;
+		mem->bus.is_iomem = true;
+		break;
+	default:
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static void gsgpu_ttm_io_mem_free(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)
+{
+}
+
+static unsigned long gsgpu_ttm_io_mem_pfn(struct ttm_buffer_object *bo,
+					   unsigned long page_offset)
+{
+	struct drm_mm_node *mm;
+	unsigned long offset = (page_offset << PAGE_SHIFT);
+
+	mm = gsgpu_find_mm_node(&bo->mem, &offset);
+	return (bo->mem.bus.base >> PAGE_SHIFT) + mm->start +
+		(offset >> PAGE_SHIFT);
+}
+
+/*
+ * TTM backend functions.
+ */
+struct gsgpu_ttm_gup_task_list {
+	struct list_head	list;
+	struct task_struct	*task;
+};
+
+struct gsgpu_ttm_tt {
+	struct ttm_dma_tt	ttm;
+	u64			offset;
+	uint64_t		userptr;
+	struct task_struct	*usertask;
+	uint32_t		userflags;
+	spinlock_t              guptasklock;
+	struct list_head        guptasks;
+	atomic_t		mmu_invalidations;
+	uint32_t		last_set_pages;
+};
+
+/**
+ * gsgpu_ttm_tt_get_user_pages - Pin pages of memory pointed to by a USERPTR
+ * pointer to memory
+ *
+ * Called by gsgpu_gem_userptr_ioctl() and gsgpu_cs_parser_bos().
+ * This provides a wrapper around the get_user_pages() call to provide
+ * device accessible pages that back user memory.
+ */
+int gsgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	struct mm_struct *mm = gtt->usertask->mm;
+	unsigned int flags = 0;
+	unsigned pinned = 0;
+	int r;
+
+	if (!mm) /* Happens during process shutdown */
+		return -ESRCH;
+
+	if (!(gtt->userflags & GSGPU_GEM_USERPTR_READONLY))
+		flags |= FOLL_WRITE;
+
+	down_read(&mm->mmap_sem);
+
+	if (gtt->userflags & GSGPU_GEM_USERPTR_ANONONLY) {
+		/*
+		 * check that we only use anonymous memory to prevent problems
+		 * with writeback
+		 */
+		unsigned long end = gtt->userptr + ttm->num_pages * PAGE_SIZE;
+		struct vm_area_struct *vma;
+
+		vma = find_vma(mm, gtt->userptr);
+		if (!vma || vma->vm_file || vma->vm_end < end) {
+			up_read(&mm->mmap_sem);
+			return -EPERM;
+		}
+	}
+
+	/* loop enough times using contiguous pages of memory */
+	do {
+		unsigned num_pages = ttm->num_pages - pinned;
+		uint64_t userptr = gtt->userptr + pinned * PAGE_SIZE;
+		struct page **p = pages + pinned;
+		struct gsgpu_ttm_gup_task_list guptask;
+
+		guptask.task = current;
+		spin_lock(&gtt->guptasklock);
+		list_add(&guptask.list, &gtt->guptasks);
+		spin_unlock(&gtt->guptasklock);
+
+		if (mm == current->mm)
+			r = get_user_pages(userptr, num_pages, flags, p, NULL);
+		else
+			r = get_user_pages_remote(gtt->usertask,
+					mm, userptr, num_pages,
+					flags, p, NULL, NULL);
+
+		spin_lock(&gtt->guptasklock);
+		list_del(&guptask.list);
+		spin_unlock(&gtt->guptasklock);
+
+		if (r < 0)
+			goto release_pages;
+
+		pinned += r;
+
+	} while (pinned < ttm->num_pages);
+
+	up_read(&mm->mmap_sem);
+	return 0;
+
+release_pages:
+	release_pages(pages, pinned);
+	up_read(&mm->mmap_sem);
+	return r;
+}
+
+/**
+ * gsgpu_ttm_tt_set_user_pages - Copy pages in, putting old pages as necessary.
+ *
+ * Called by gsgpu_cs_list_validate(). This creates the page list
+ * that backs user memory and will ultimately be mapped into the device
+ * address space.
+ */
+void gsgpu_ttm_tt_set_user_pages(struct ttm_tt *ttm, struct page **pages)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	unsigned i;
+
+	gtt->last_set_pages = atomic_read(&gtt->mmu_invalidations);
+	for (i = 0; i < ttm->num_pages; ++i) {
+		if (ttm->pages[i])
+			put_page(ttm->pages[i]);
+
+		ttm->pages[i] = pages ? pages[i] : NULL;
+	}
+}
+
+/**
+ * gsgpu_ttm_tt_mark_user_page - Mark pages as dirty
+ *
+ * Called while unpinning userptr pages
+ */
+void gsgpu_ttm_tt_mark_user_pages(struct ttm_tt *ttm)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	unsigned i;
+
+	for (i = 0; i < ttm->num_pages; ++i) {
+		struct page *page = ttm->pages[i];
+
+		if (!page)
+			continue;
+
+		if (!(gtt->userflags & GSGPU_GEM_USERPTR_READONLY))
+			set_page_dirty(page);
+
+		mark_page_accessed(page);
+	}
+}
+
+/**
+ * gsgpu_ttm_tt_pin_userptr - 	prepare the sg table with the user pages
+ *
+ * Called by gsgpu_ttm_backend_bind()
+ **/
+static int gsgpu_ttm_tt_pin_userptr(struct ttm_tt *ttm)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(ttm->bdev);
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	unsigned nents;
+	int r;
+
+	int write = !(gtt->userflags & GSGPU_GEM_USERPTR_READONLY);
+	enum dma_data_direction direction = write ?
+		DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
+
+	/* Allocate an SG array and squash pages into it */
+	r = sg_alloc_table_from_pages(ttm->sg, ttm->pages, ttm->num_pages, 0,
+				      ttm->num_pages << PAGE_SHIFT,
+				      GFP_KERNEL);
+	if (r)
+		goto release_sg;
+
+	/* Map SG to device */
+	r = -ENOMEM;
+	nents = dma_map_sg(adev->dev, ttm->sg->sgl, ttm->sg->nents, direction);
+	if (nents != ttm->sg->nents)
+		goto release_sg;
+
+	/* convert SG to linear array of pages and dma addresses */
+	drm_prime_sg_to_page_addr_arrays(ttm->sg, ttm->pages,
+					 gtt->ttm.dma_address, ttm->num_pages);
+
+	return 0;
+
+release_sg:
+	kfree(ttm->sg);
+	return r;
+}
+
+/**
+ * gsgpu_ttm_tt_unpin_userptr - Unpin and unmap userptr pages
+ */
+static void gsgpu_ttm_tt_unpin_userptr(struct ttm_tt *ttm)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(ttm->bdev);
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+
+	int write = !(gtt->userflags & GSGPU_GEM_USERPTR_READONLY);
+	enum dma_data_direction direction = write ?
+		DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
+
+	/* double check that we don't free the table twice */
+	if (!ttm->sg->sgl)
+		return;
+
+	/* unmap the pages mapped to the device */
+	dma_unmap_sg(adev->dev, ttm->sg->sgl, ttm->sg->nents, direction);
+
+	/* mark the pages as dirty */
+	gsgpu_ttm_tt_mark_user_pages(ttm);
+
+	sg_free_table(ttm->sg);
+}
+
+int gsgpu_ttm_gart_bind(struct gsgpu_device *adev,
+				struct ttm_buffer_object *tbo,
+				uint64_t flags)
+{
+	struct ttm_tt *ttm = tbo->ttm;
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	int r;
+
+	r = gsgpu_gart_bind(adev, gtt->offset, ttm->num_pages,
+			    ttm->pages, gtt->ttm.dma_address, flags);
+	if (r)
+		DRM_ERROR("failed to bind %lu pages at 0x%08llX\n",
+			  ttm->num_pages, gtt->offset);
+
+	return r;
+}
+
+/**
+ * gsgpu_ttm_backend_bind - Bind GTT memory
+ *
+ * Called by ttm_tt_bind() on behalf of ttm_bo_handle_move_mem().
+ * This handles binding GTT memory to the device address space.
+ */
+static int gsgpu_ttm_backend_bind(struct ttm_tt *ttm,
+				   struct ttm_mem_reg *bo_mem)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(ttm->bdev);
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	uint64_t flags;
+	int r = 0;
+
+	if (gtt->userptr) {
+		r = gsgpu_ttm_tt_pin_userptr(ttm);
+		if (r) {
+			DRM_ERROR("failed to pin userptr\n");
+			return r;
+		}
+	}
+	if (!ttm->num_pages) {
+		WARN(1, "nothing to bind %lu pages for mreg %p back %p!\n",
+		     ttm->num_pages, bo_mem, ttm);
+	}
+
+	if (!gsgpu_gtt_mgr_has_gart_addr(bo_mem)) {
+		gtt->offset = GSGPU_BO_INVALID_OFFSET;
+		return 0;
+	}
+
+	/* compute PTE flags relevant to this BO memory */
+	flags = gsgpu_ttm_tt_pte_flags(adev, ttm, bo_mem);
+
+	/* bind pages into GART page tables */
+	gtt->offset = (u64)bo_mem->start << PAGE_SHIFT;
+	r = gsgpu_gart_bind(adev, gtt->offset, ttm->num_pages,
+		ttm->pages, gtt->ttm.dma_address, flags);
+
+	if (r)
+		DRM_ERROR("failed to bind %lu pages at 0x%08llX\n",
+			  ttm->num_pages, gtt->offset);
+	return r;
+}
+
+/**
+ * gsgpu_ttm_alloc_gart - Allocate GART memory for buffer object
+ */
+int gsgpu_ttm_alloc_gart(struct ttm_buffer_object *bo)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->bdev);
+	struct ttm_operation_ctx ctx = { false, false };
+	struct gsgpu_ttm_tt *gtt = (void *)bo->ttm;
+	struct ttm_mem_reg tmp;
+	struct ttm_placement placement;
+	struct ttm_place placements;
+	uint64_t flags;
+	int r;
+
+	if (bo->mem.mem_type != TTM_PL_TT ||
+	    gsgpu_gtt_mgr_has_gart_addr(&bo->mem))
+		return 0;
+
+	/* allocate GTT space */
+	tmp = bo->mem;
+	tmp.mm_node = NULL;
+	placement.num_placement = 1;
+	placement.placement = &placements;
+	placement.num_busy_placement = 1;
+	placement.busy_placement = &placements;
+	placements.fpfn = 0;
+	placements.lpfn = adev->gmc.gart_size >> PAGE_SHIFT;
+	placements.flags = (bo->mem.placement & ~TTM_PL_MASK_MEM) |
+		TTM_PL_FLAG_TT;
+
+	r = ttm_bo_mem_space(bo, &placement, &tmp, &ctx);
+	if (unlikely(r))
+		return r;
+
+	/* compute PTE flags for this buffer object */
+	flags = gsgpu_ttm_tt_pte_flags(adev, bo->ttm, &tmp);
+
+	/* Bind pages */
+	gtt->offset = (u64)tmp.start << PAGE_SHIFT;
+	r = gsgpu_ttm_gart_bind(adev, bo, flags);
+	if (unlikely(r)) {
+		ttm_bo_mem_put(bo, &tmp);
+		return r;
+	}
+
+	ttm_bo_mem_put(bo, &bo->mem);
+	bo->mem = tmp;
+	bo->offset = (bo->mem.start << PAGE_SHIFT) +
+		bo->bdev->man[bo->mem.mem_type].gpu_offset;
+
+	return 0;
+}
+
+/**
+ * gsgpu_ttm_recover_gart - Rebind GTT pages
+ *
+ * Called by gsgpu_gtt_mgr_recover() from gsgpu_device_reset() to
+ * rebind GTT pages during a GPU reset.
+ */
+int gsgpu_ttm_recover_gart(struct ttm_buffer_object *tbo)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(tbo->bdev);
+	uint64_t flags;
+	int r;
+
+	if (!tbo->ttm)
+		return 0;
+
+	flags = gsgpu_ttm_tt_pte_flags(adev, tbo->ttm, &tbo->mem);
+	r = gsgpu_ttm_gart_bind(adev, tbo, flags);
+
+	return r;
+}
+
+/**
+ * gsgpu_ttm_backend_unbind - Unbind GTT mapped pages
+ *
+ * Called by ttm_tt_unbind() on behalf of ttm_bo_move_ttm() and
+ * ttm_tt_destroy().
+ */
+static int gsgpu_ttm_backend_unbind(struct ttm_tt *ttm)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(ttm->bdev);
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	int r;
+
+	/* if the pages have userptr pinning then clear that first */
+	if (gtt->userptr)
+		gsgpu_ttm_tt_unpin_userptr(ttm);
+
+	if (gtt->offset == GSGPU_BO_INVALID_OFFSET)
+		return 0;
+
+	/* unbind shouldn't be done for GDS/GWS/OA in ttm_bo_clean_mm */
+	r = gsgpu_gart_unbind(adev, gtt->offset, ttm->num_pages);
+	if (r)
+		DRM_ERROR("failed to unbind %lu pages at 0x%08llX\n",
+			  gtt->ttm.ttm.num_pages, gtt->offset);
+	return r;
+}
+
+static void gsgpu_ttm_backend_destroy(struct ttm_tt *ttm)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+
+	if (gtt->usertask)
+		put_task_struct(gtt->usertask);
+
+	ttm_dma_tt_fini(&gtt->ttm);
+	kfree(gtt);
+}
+
+static struct ttm_backend_func gsgpu_backend_func = {
+	.bind = &gsgpu_ttm_backend_bind,
+	.unbind = &gsgpu_ttm_backend_unbind,
+	.destroy = &gsgpu_ttm_backend_destroy,
+};
+
+/**
+ * gsgpu_ttm_tt_create - Create a ttm_tt object for a given BO
+ *
+ * @bo: The buffer object to create a GTT ttm_tt object around
+ *
+ * Called by ttm_tt_create().
+ */
+static struct ttm_tt *gsgpu_ttm_tt_create(struct ttm_buffer_object *bo,
+					   uint32_t page_flags)
+{
+	struct gsgpu_device *adev;
+	struct gsgpu_ttm_tt *gtt;
+
+	adev = gsgpu_ttm_adev(bo->bdev);
+
+	gtt = kzalloc(sizeof(struct gsgpu_ttm_tt), GFP_KERNEL);
+	if (gtt == NULL) {
+		return NULL;
+	}
+	gtt->ttm.ttm.func = &gsgpu_backend_func;
+
+	/* allocate space for the uninitialized page entries */
+	if (ttm_sg_tt_init(&gtt->ttm, bo, page_flags)) {
+		kfree(gtt);
+		return NULL;
+	}
+	return &gtt->ttm.ttm;
+}
+
+/**
+ * gsgpu_ttm_tt_populate - Map GTT pages visible to the device
+ *
+ * Map the pages of a ttm_tt object to an address space visible
+ * to the underlying device.
+ */
+static int gsgpu_ttm_tt_populate(struct ttm_tt *ttm,
+			struct ttm_operation_ctx *ctx)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(ttm->bdev);
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	bool slave = !!(ttm->page_flags & TTM_PAGE_FLAG_SG);
+
+	/* user pages are bound by gsgpu_ttm_tt_pin_userptr() */
+	if (gtt && gtt->userptr) {
+		ttm->sg = kzalloc(sizeof(struct sg_table), GFP_KERNEL);
+		if (!ttm->sg)
+			return -ENOMEM;
+
+		ttm->page_flags |= TTM_PAGE_FLAG_SG;
+		ttm->state = tt_unbound;
+		return 0;
+	}
+
+	if (slave && ttm->sg) {
+		drm_prime_sg_to_page_addr_arrays(ttm->sg, ttm->pages,
+						 gtt->ttm.dma_address,
+						 ttm->num_pages);
+		ttm->state = tt_unbound;
+		return 0;
+	}
+
+#ifdef CONFIG_SWIOTLB
+	if (adev->need_swiotlb && swiotlb_nr_tbl()) {
+		return ttm_dma_populate(&gtt->ttm, adev->dev, ctx);
+	}
+#endif
+
+	/* fall back to generic helper to populate the page array
+	 * and map them to the device */
+	return ttm_populate_and_map_pages(adev->dev, &gtt->ttm, ctx);
+}
+
+/**
+ * gsgpu_ttm_tt_unpopulate - unmap GTT pages and unpopulate page arrays
+ *
+ * Unmaps pages of a ttm_tt object from the device address space and
+ * unpopulates the page array backing it.
+ */
+static void gsgpu_ttm_tt_unpopulate(struct ttm_tt *ttm)
+{
+	struct gsgpu_device *adev;
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	bool slave = !!(ttm->page_flags & TTM_PAGE_FLAG_SG);
+
+	if (gtt && gtt->userptr) {
+		gsgpu_ttm_tt_set_user_pages(ttm, NULL);
+		kfree(ttm->sg);
+		ttm->page_flags &= ~TTM_PAGE_FLAG_SG;
+		return;
+	}
+
+	if (slave)
+		return;
+
+	adev = gsgpu_ttm_adev(ttm->bdev);
+
+#ifdef CONFIG_SWIOTLB
+	if (adev->need_swiotlb && swiotlb_nr_tbl()) {
+		ttm_dma_unpopulate(&gtt->ttm, adev->dev);
+		return;
+	}
+#endif
+
+	/* fall back to generic helper to unmap and unpopulate array */
+	ttm_unmap_and_unpopulate_pages(adev->dev, &gtt->ttm);
+}
+
+/**
+ * gsgpu_ttm_tt_set_userptr - Initialize userptr GTT ttm_tt for the current
+ * task
+ *
+ * @ttm: The ttm_tt object to bind this userptr object to
+ * @addr:  The address in the current tasks VM space to use
+ * @flags: Requirements of userptr object.
+ *
+ * Called by gsgpu_gem_userptr_ioctl() to bind userptr pages
+ * to current task
+ */
+int gsgpu_ttm_tt_set_userptr(struct ttm_tt *ttm, uint64_t addr,
+			      uint32_t flags)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+
+	if (gtt == NULL)
+		return -EINVAL;
+
+	gtt->userptr = addr;
+	gtt->userflags = flags;
+
+	if (gtt->usertask)
+		put_task_struct(gtt->usertask);
+	gtt->usertask = current->group_leader;
+	get_task_struct(gtt->usertask);
+
+	spin_lock_init(&gtt->guptasklock);
+	INIT_LIST_HEAD(&gtt->guptasks);
+	atomic_set(&gtt->mmu_invalidations, 0);
+	gtt->last_set_pages = 0;
+
+	return 0;
+}
+
+/**
+ * gsgpu_ttm_tt_get_usermm - Return memory manager for ttm_tt object
+ */
+struct mm_struct *gsgpu_ttm_tt_get_usermm(struct ttm_tt *ttm)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+
+	if (gtt == NULL)
+		return NULL;
+
+	if (gtt->usertask == NULL)
+		return NULL;
+
+	return gtt->usertask->mm;
+}
+
+/**
+ * gsgpu_ttm_tt_affect_userptr - Determine if a ttm_tt object lays inside an
+ * address range for the current task.
+ *
+ */
+bool gsgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
+				  unsigned long end)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	struct gsgpu_ttm_gup_task_list *entry;
+	unsigned long size;
+
+	if (gtt == NULL || !gtt->userptr)
+		return false;
+
+	/* Return false if no part of the ttm_tt object lies within
+	 * the range
+	 */
+	size = (unsigned long)gtt->ttm.ttm.num_pages * PAGE_SIZE;
+	if (gtt->userptr > end || gtt->userptr + size <= start)
+		return false;
+
+	/* Search the lists of tasks that hold this mapping and see
+	 * if current is one of them.  If it is return false.
+	 */
+	spin_lock(&gtt->guptasklock);
+	list_for_each_entry(entry, &gtt->guptasks, list) {
+		if (entry->task == current) {
+			spin_unlock(&gtt->guptasklock);
+			return false;
+		}
+	}
+	spin_unlock(&gtt->guptasklock);
+
+	atomic_inc(&gtt->mmu_invalidations);
+
+	return true;
+}
+
+/**
+ * gsgpu_ttm_tt_userptr_invalidated - Has the ttm_tt object been invalidated?
+ */
+bool gsgpu_ttm_tt_userptr_invalidated(struct ttm_tt *ttm,
+				       int *last_invalidated)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+	int prev_invalidated = *last_invalidated;
+
+	*last_invalidated = atomic_read(&gtt->mmu_invalidations);
+	return prev_invalidated != *last_invalidated;
+}
+
+/**
+ * gsgpu_ttm_tt_userptr_needs_pages - Have the pages backing this ttm_tt object
+ * been invalidated since the last time they've been set?
+ */
+bool gsgpu_ttm_tt_userptr_needs_pages(struct ttm_tt *ttm)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+
+	if (gtt == NULL || !gtt->userptr)
+		return false;
+
+	return atomic_read(&gtt->mmu_invalidations) != gtt->last_set_pages;
+}
+
+/**
+ * gsgpu_ttm_tt_is_readonly - Is the ttm_tt object read only?
+ */
+bool gsgpu_ttm_tt_is_readonly(struct ttm_tt *ttm)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)ttm;
+
+	if (gtt == NULL)
+		return false;
+
+	return !!(gtt->userflags & GSGPU_GEM_USERPTR_READONLY);
+}
+
+/**
+ * gsgpu_ttm_tt_pte_flags - Compute PTE flags for ttm_tt object
+ *
+ * @ttm: The ttm_tt object to compute the flags for
+ * @mem: The memory registry backing this ttm_tt object
+ */
+uint64_t gsgpu_ttm_tt_pte_flags(struct gsgpu_device *adev, struct ttm_tt *ttm,
+				 struct ttm_mem_reg *mem)
+{
+	uint64_t flags = 0;
+
+	if (mem && mem->mem_type != TTM_PL_SYSTEM)
+		flags |= GSGPU_PTE_PRESENT;
+
+	//if (mem && mem->mem_type == TTM_PL_TT) {
+	//	flags |= GSGPU_PTE_SYSTEM;
+
+	//	if (ttm->caching_state == tt_cached)
+	//		flags |= GSGPU_PTE_SNOOPED;
+	//}
+
+	flags |= adev->gart.gart_pte_flags;
+	//flags |= GSGPU_PTE_READABLE;
+
+	if (!gsgpu_ttm_tt_is_readonly(ttm))
+		flags |= GSGPU_PTE_WRITEABLE;
+
+	return flags;
+}
+
+/**
+ * gsgpu_ttm_bo_eviction_valuable - Check to see if we can evict a buffer
+ * object.
+ *
+ * Return true if eviction is sensible. Called by ttm_mem_evict_first() on
+ * behalf of ttm_bo_mem_force_space() which tries to evict buffer objects until
+ * it can find space for a new object and by ttm_bo_force_list_clean() which is
+ * used to clean out a memory space.
+ */
+static bool gsgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,
+					    const struct ttm_place *place)
+{
+	unsigned long num_pages = bo->mem.num_pages;
+	struct drm_mm_node *node = bo->mem.mm_node;
+
+	switch (bo->mem.mem_type) {
+	case TTM_PL_TT:
+		return true;
+
+	case TTM_PL_VRAM:
+		/* Check each drm MM node individually */
+		while (num_pages) {
+			if (place->fpfn < (node->start + node->size) &&
+			    !(place->lpfn && place->lpfn <= node->start))
+				return true;
+
+			num_pages -= node->size;
+			++node;
+		}
+		return false;
+
+	default:
+		break;
+	}
+
+	return ttm_bo_eviction_valuable(bo, place);
+}
+
+/**
+ * gsgpu_ttm_access_memory - Read or Write memory that backs a buffer object.
+ *
+ * @bo:  The buffer object to read/write
+ * @offset:  Offset into buffer object
+ * @buf:  Secondary buffer to write/read from
+ * @len: Length in bytes of access
+ * @write:  true if writing
+ *
+ * This is used to access VRAM that backs a buffer object via MMIO
+ * access for debugging purposes.
+ */
+static int gsgpu_ttm_access_memory(struct ttm_buffer_object *bo,
+				    unsigned long offset,
+				    void *buf, int len, int write)
+{
+	struct gsgpu_bo *abo = ttm_to_gsgpu_bo(bo);
+	struct gsgpu_device *adev = gsgpu_ttm_adev(abo->tbo.bdev);
+	struct drm_mm_node *nodes;
+	uint32_t value = 0;
+	int ret = 0;
+	uint64_t pos;
+	unsigned long flags;
+
+	if (bo->mem.mem_type != TTM_PL_VRAM)
+		return -EIO;
+
+	DRM_DEBUG_DRIVER("%s Not implemented \n", __FUNCTION__);
+
+	nodes = gsgpu_find_mm_node(&abo->tbo.mem, &offset);
+	pos = (nodes->start << PAGE_SHIFT) + offset;
+
+	while (len && pos < adev->gmc.mc_vram_size) {
+		uint64_t aligned_pos = pos & ~(uint64_t)3;
+		uint32_t bytes = 4 - (pos & 3);
+		uint32_t shift = (pos & 3) * 8;
+		uint32_t mask = 0xffffffff << shift;
+
+		if (len < bytes) {
+			mask &= 0xffffffff >> (bytes - len) * 8;
+			bytes = len;
+		}
+
+		spin_lock_irqsave(&adev->mmio_idx_lock, flags);
+		WREG32_NO_KIQ(mmMM_INDEX, ((uint32_t)aligned_pos) | 0x80000000);
+		WREG32_NO_KIQ(mmMM_INDEX_HI, aligned_pos >> 31);
+		if (!write || mask != 0xffffffff)
+			value = RREG32_NO_KIQ(mmMM_DATA);
+		if (write) {
+			value &= ~mask;
+			value |= (*(uint32_t *)buf << shift) & mask;
+			WREG32_NO_KIQ(mmMM_DATA, value);
+		}
+		spin_unlock_irqrestore(&adev->mmio_idx_lock, flags);
+		if (!write) {
+			value = (value & mask) >> shift;
+			memcpy(buf, &value, bytes);
+		}
+
+		ret += bytes;
+		buf = (uint8_t *)buf + bytes;
+		pos += bytes;
+		len -= bytes;
+		if (pos >= (nodes->start + nodes->size) << PAGE_SHIFT) {
+			++nodes;
+			pos = (nodes->start << PAGE_SHIFT);
+		}
+	}
+
+	return ret;
+}
+
+static struct ttm_bo_driver gsgpu_bo_driver = {
+	.ttm_tt_create = &gsgpu_ttm_tt_create,
+	.ttm_tt_populate = &gsgpu_ttm_tt_populate,
+	.ttm_tt_unpopulate = &gsgpu_ttm_tt_unpopulate,
+	.invalidate_caches = &gsgpu_invalidate_caches,
+	.init_mem_type = &gsgpu_init_mem_type,
+	.eviction_valuable = gsgpu_ttm_bo_eviction_valuable,
+	.evict_flags = &gsgpu_evict_flags,
+	.move = &gsgpu_bo_move,
+	.verify_access = &gsgpu_verify_access,
+	.move_notify = &gsgpu_bo_move_notify,
+	.fault_reserve_notify = &gsgpu_bo_fault_reserve_notify,
+	.io_mem_reserve = &gsgpu_ttm_io_mem_reserve,
+	.io_mem_free = &gsgpu_ttm_io_mem_free,
+	.io_mem_pfn = gsgpu_ttm_io_mem_pfn,
+	.access_memory = &gsgpu_ttm_access_memory
+};
+
+/*
+ * Firmware Reservation functions
+ */
+/**
+ * gsgpu_ttm_fw_reserve_vram_fini - free fw reserved vram
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * free fw reserved vram if it has been reserved.
+ */
+static void gsgpu_ttm_fw_reserve_vram_fini(struct gsgpu_device *adev)
+{
+	gsgpu_bo_free_kernel(&adev->fw_vram_usage.reserved_bo,
+		NULL, &adev->fw_vram_usage.va);
+}
+
+/**
+ * gsgpu_ttm_fw_reserve_vram_init - create bo vram reservation from fw
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * create bo vram reservation from fw.
+ */
+static int gsgpu_ttm_fw_reserve_vram_init(struct gsgpu_device *adev)
+{
+	struct ttm_operation_ctx ctx = { false, false };
+	struct gsgpu_bo_param bp;
+	int r = 0;
+	int i;
+	u64 vram_size = adev->gmc.visible_vram_size;
+	u64 offset = adev->fw_vram_usage.start_offset;
+	u64 size = adev->fw_vram_usage.size;
+	struct gsgpu_bo *bo;
+
+	memset(&bp, 0, sizeof(bp));
+	bp.size = adev->fw_vram_usage.size;
+	bp.byte_align = PAGE_SIZE;
+	bp.domain = GSGPU_GEM_DOMAIN_VRAM;
+	bp.flags = GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
+		GSGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	bp.type = ttm_bo_type_kernel;
+	bp.resv = NULL;
+	adev->fw_vram_usage.va = NULL;
+	adev->fw_vram_usage.reserved_bo = NULL;
+
+	if (adev->fw_vram_usage.size > 0 &&
+		adev->fw_vram_usage.size <= vram_size) {
+
+		r = gsgpu_bo_create(adev, &bp,
+				     &adev->fw_vram_usage.reserved_bo);
+		if (r)
+			goto error_create;
+
+		r = gsgpu_bo_reserve(adev->fw_vram_usage.reserved_bo, false);
+		if (r)
+			goto error_reserve;
+
+		/* remove the original mem node and create a new one at the
+		 * request position
+		 */
+		bo = adev->fw_vram_usage.reserved_bo;
+		offset = ALIGN(offset, PAGE_SIZE);
+		for (i = 0; i < bo->placement.num_placement; ++i) {
+			bo->placements[i].fpfn = offset >> PAGE_SHIFT;
+			bo->placements[i].lpfn = (offset + size) >> PAGE_SHIFT;
+		}
+
+		ttm_bo_mem_put(&bo->tbo, &bo->tbo.mem);
+		r = ttm_bo_mem_space(&bo->tbo, &bo->placement,
+				     &bo->tbo.mem, &ctx);
+		if (r)
+			goto error_pin;
+
+		r = gsgpu_bo_pin_restricted(adev->fw_vram_usage.reserved_bo,
+			GSGPU_GEM_DOMAIN_VRAM,
+			adev->fw_vram_usage.start_offset,
+			(adev->fw_vram_usage.start_offset +
+			adev->fw_vram_usage.size));
+		if (r)
+			goto error_pin;
+		r = gsgpu_bo_kmap(adev->fw_vram_usage.reserved_bo,
+			&adev->fw_vram_usage.va);
+		if (r)
+			goto error_kmap;
+
+		gsgpu_bo_unreserve(adev->fw_vram_usage.reserved_bo);
+	}
+	return r;
+
+error_kmap:
+	gsgpu_bo_unpin(adev->fw_vram_usage.reserved_bo);
+error_pin:
+	gsgpu_bo_unreserve(adev->fw_vram_usage.reserved_bo);
+error_reserve:
+	gsgpu_bo_unref(&adev->fw_vram_usage.reserved_bo);
+error_create:
+	adev->fw_vram_usage.va = NULL;
+	adev->fw_vram_usage.reserved_bo = NULL;
+	return r;
+}
+/**
+ * gsgpu_ttm_init - Init the memory management (ttm) as well as various
+ * gtt/vram related fields.
+ *
+ * This initializes all of the memory space pools that the TTM layer
+ * will need such as the GTT space (system memory mapped to the device),
+ * VRAM (on-board memory), and on-chip memories (GDS, GWS, OA) which
+ * can be mapped per VMID.
+ */
+int gsgpu_ttm_init(struct gsgpu_device *adev)
+{
+	uint64_t gtt_size;
+	int r;
+	u64 vis_vram_limit;
+
+	/* initialize global references for vram/gtt */
+	r = gsgpu_ttm_global_init(adev);
+	if (r) {
+		return r;
+	}
+	/* No others user of address space so set it to 0 */
+	r = ttm_bo_device_init(&adev->mman.bdev,
+			       adev->mman.bo_global_ref.ref.object,
+			       &gsgpu_bo_driver,
+			       adev->ddev->anon_inode->i_mapping,
+			       DRM_FILE_PAGE_OFFSET,
+			       adev->need_dma32);
+	if (r) {
+		DRM_ERROR("failed initializing buffer object driver(%d).\n", r);
+		return r;
+	}
+	adev->mman.initialized = true;
+
+	/* We opt to avoid OOM on system pages allocations */
+	adev->mman.bdev.no_retry = true;
+
+	/* Initialize VRAM pool with all of VRAM divided into pages */
+	r = ttm_bo_init_mm(&adev->mman.bdev, TTM_PL_VRAM,
+				adev->gmc.real_vram_size >> PAGE_SHIFT);
+	if (r) {
+		DRM_ERROR("Failed initializing VRAM heap.\n");
+		return r;
+	}
+
+	/* Reduce size of CPU-visible VRAM if requested */
+	vis_vram_limit = (u64)gsgpu_vis_vram_limit * 1024 * 1024;
+	if (gsgpu_vis_vram_limit > 0 &&
+	    vis_vram_limit <= adev->gmc.visible_vram_size)
+		adev->gmc.visible_vram_size = vis_vram_limit;
+
+	/* Change the size here instead of the init above so only lpfn is affected */
+	gsgpu_ttm_set_buffer_funcs_status(adev, false);
+#ifdef CONFIG_64BIT
+	adev->mman.aper_base_kaddr = ioremap(adev->gmc.aper_base,
+					     adev->gmc.visible_vram_size);
+#endif
+
+	/*
+	 *The reserved vram for firmware must be pinned to the specified
+	 *place on the VRAM, so reserve it early.
+	 */
+	r = gsgpu_ttm_fw_reserve_vram_init(adev);
+	if (r) {
+		return r;
+	}
+
+	/* allocate memory as required for VGA
+	 * This is used for VGA emulation and pre-OS scanout buffers to
+	 * avoid display artifacts while transitioning between pre-OS
+	 * and driver.  */
+	if (adev->gmc.stolen_size) {
+		r = gsgpu_bo_create_kernel(adev, adev->gmc.stolen_size, PAGE_SIZE,
+					    GSGPU_GEM_DOMAIN_VRAM,
+					    &adev->stolen_vga_memory,
+					    NULL, NULL);
+		if (r)
+			return r;
+	}
+	DRM_INFO("gsgpu: %uM of VRAM memory ready\n",
+		 (unsigned) (adev->gmc.real_vram_size / (1024 * 1024)));
+
+	/* Compute GTT size, either bsaed on 3/4th the size of RAM size
+	 * or whatever the user passed on module init */
+	if (gsgpu_gtt_size == -1) {
+		struct sysinfo si;
+
+		si_meminfo(&si);
+		gtt_size = min(max((GSGPU_DEFAULT_GTT_SIZE_MB << 20),
+			       adev->gmc.mc_vram_size),
+			       ((uint64_t)si.totalram * si.mem_unit * 3/4));
+	} else
+		gtt_size = (uint64_t)gsgpu_gtt_size << 20;
+
+	/* Initialize GTT memory pool */
+	r = ttm_bo_init_mm(&adev->mman.bdev, TTM_PL_TT, gtt_size >> PAGE_SHIFT);
+	if (r) {
+		DRM_ERROR("Failed initializing GTT heap.\n");
+		return r;
+	}
+	DRM_INFO("gsgpu: %uM of GTT memory ready.\n",
+		 (unsigned)(gtt_size / (1024 * 1024)));
+
+	/* Register debugfs entries for gsgpu_ttm */
+	r = gsgpu_ttm_debugfs_init(adev);
+	if (r) {
+		DRM_ERROR("Failed to init debugfs\n");
+		return r;
+	}
+	return 0;
+}
+
+/**
+ * gsgpu_ttm_late_init - Handle any late initialization for gsgpu_ttm
+ */
+void gsgpu_ttm_late_init(struct gsgpu_device *adev)
+{
+	/* return the VGA stolen memory (if any) back to VRAM */
+	gsgpu_bo_free_kernel(&adev->stolen_vga_memory, NULL, NULL);
+}
+
+/**
+ * gsgpu_ttm_fini - De-initialize the TTM memory pools
+ */
+void gsgpu_ttm_fini(struct gsgpu_device *adev)
+{
+	if (!adev->mman.initialized)
+		return;
+
+	gsgpu_ttm_debugfs_fini(adev);
+	gsgpu_ttm_fw_reserve_vram_fini(adev);
+	if (adev->mman.aper_base_kaddr)
+		iounmap(adev->mman.aper_base_kaddr);
+	adev->mman.aper_base_kaddr = NULL;
+
+	ttm_bo_clean_mm(&adev->mman.bdev, TTM_PL_VRAM);
+	ttm_bo_clean_mm(&adev->mman.bdev, TTM_PL_TT);
+	ttm_bo_device_release(&adev->mman.bdev);
+	gsgpu_ttm_global_fini(adev);
+	adev->mman.initialized = false;
+	DRM_INFO("gsgpu: ttm finalized\n");
+}
+
+/**
+ * gsgpu_ttm_set_buffer_funcs_status - enable/disable use of buffer functions
+ *
+ * @adev: gsgpu_device pointer
+ * @enable: true when we can use buffer functions.
+ *
+ * Enable/disable use of buffer functions during suspend/resume. This should
+ * only be called at bootup or when userspace isn't running.
+ */
+void gsgpu_ttm_set_buffer_funcs_status(struct gsgpu_device *adev, bool enable)
+{
+	struct ttm_mem_type_manager *man = &adev->mman.bdev.man[TTM_PL_VRAM];
+	uint64_t size;
+	int r;
+
+	if (!adev->mman.initialized || adev->in_gpu_reset ||
+	    adev->mman.buffer_funcs_enabled == enable)
+		return;
+
+	if (enable) {
+		struct gsgpu_ring *ring;
+		struct drm_sched_rq *rq;
+
+		ring = adev->mman.buffer_funcs_ring;
+		rq = &ring->sched.sched_rq[DRM_SCHED_PRIORITY_KERNEL];
+		r = drm_sched_entity_init(&adev->mman.entity, &rq, 1, NULL);
+		if (r) {
+			DRM_ERROR("Failed setting up TTM BO move entity (%d)\n",
+				  r);
+			return;
+		}
+	} else {
+		drm_sched_entity_destroy(&adev->mman.entity);
+		dma_fence_put(man->move);
+		man->move = NULL;
+	}
+
+	/* this just adjusts TTM size idea, which sets lpfn to the correct value */
+	if (enable)
+		size = adev->gmc.real_vram_size;
+	else
+		size = adev->gmc.visible_vram_size;
+	man->size = size >> PAGE_SHIFT;
+	adev->mman.buffer_funcs_enabled = enable;
+}
+
+int gsgpu_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct drm_file *file_priv;
+	struct gsgpu_device *adev;
+
+	if (unlikely(vma->vm_pgoff < DRM_FILE_PAGE_OFFSET))
+		return -EINVAL;
+
+	file_priv = filp->private_data;
+	adev = file_priv->minor->dev->dev_private;
+	if (adev == NULL)
+		return -EINVAL;
+
+	return ttm_bo_mmap(filp, vma, &adev->mman.bdev);
+}
+
+static int gsgpu_map_buffer(struct ttm_buffer_object *bo,
+			     struct ttm_mem_reg *mem, unsigned num_pages,
+			     uint64_t offset, unsigned window,
+			     struct gsgpu_ring *ring,
+			     uint64_t *addr)
+{
+	struct gsgpu_ttm_tt *gtt = (void *)bo->ttm;
+	struct gsgpu_device *adev = ring->adev;
+	struct ttm_tt *ttm = bo->ttm;
+	struct gsgpu_job *job;
+	unsigned num_dw, num_bytes;
+	dma_addr_t *dma_address;
+	struct dma_fence *fence;
+	uint64_t src_addr, dst_addr;
+	uint64_t flags;
+	int r;
+
+	BUG_ON(adev->mman.buffer_funcs->copy_max_bytes <
+	       GSGPU_GTT_MAX_TRANSFER_SIZE * 8);
+
+	*addr = adev->gmc.gart_start;
+	*addr += (u64)window * GSGPU_GTT_MAX_TRANSFER_SIZE *
+		GSGPU_GPU_PAGE_SIZE;
+
+	num_dw = adev->mman.buffer_funcs->copy_num_dw;
+	while (num_dw & 0x7)
+		num_dw++;
+
+	num_bytes = num_pages * 8;
+
+	r = gsgpu_job_alloc_with_ib(adev, num_dw * 4 + num_bytes, &job);
+	if (r)
+		return r;
+
+	src_addr = num_dw * 4;
+	src_addr += job->ibs[0].gpu_addr;
+
+	dst_addr = adev->gart.table_addr;
+	dst_addr += window * GSGPU_GTT_MAX_TRANSFER_SIZE * 8;
+	gsgpu_emit_copy_buffer(adev, &job->ibs[0], src_addr,
+				dst_addr, num_bytes);
+
+	gsgpu_ring_pad_ib(ring, &job->ibs[0]);
+	WARN_ON(job->ibs[0].length_dw > num_dw);
+
+	dma_address = &gtt->ttm.dma_address[offset >> PAGE_SHIFT];
+	flags = gsgpu_ttm_tt_pte_flags(adev, ttm, mem);
+	r = gsgpu_gart_map(adev, 0, num_pages, dma_address, flags,
+			    &job->ibs[0].ptr[num_dw]);
+	if (r)
+		goto error_free;
+
+	r = gsgpu_job_submit(job, &adev->mman.entity,
+			      GSGPU_FENCE_OWNER_UNDEFINED, &fence);
+	if (r)
+		goto error_free;
+
+	dma_fence_put(fence);
+
+	return r;
+
+error_free:
+	gsgpu_job_free(job);
+	return r;
+}
+
+int gsgpu_copy_buffer(struct gsgpu_ring *ring, uint64_t src_offset,
+		       uint64_t dst_offset, uint32_t byte_count,
+		       struct reservation_object *resv,
+		       struct dma_fence **fence, bool direct_submit,
+		       bool vm_needs_flush)
+{
+	struct gsgpu_device *adev = ring->adev;
+	struct gsgpu_job *job;
+
+	uint32_t max_bytes;
+	unsigned num_loops, num_dw;
+	unsigned i;
+	int r;
+
+	if (direct_submit && !ring->ready) {
+		DRM_ERROR("Trying to move memory with ring turned off.\n");
+		return -EINVAL;
+	}
+
+	max_bytes = adev->mman.buffer_funcs->copy_max_bytes;
+	num_loops = DIV_ROUND_UP(byte_count, max_bytes);
+	num_dw = num_loops * adev->mman.buffer_funcs->copy_num_dw;
+
+	/* for IB padding */
+	while (num_dw & 0x7)
+		num_dw++;
+
+	r = gsgpu_job_alloc_with_ib(adev, num_dw * 4, &job);
+	if (r)
+		return r;
+
+	job->vm_needs_flush = vm_needs_flush;
+	if (resv) {
+		r = gsgpu_sync_resv(adev, &job->sync, resv,
+				     GSGPU_FENCE_OWNER_UNDEFINED,
+				     false);
+		if (r) {
+			DRM_ERROR("sync failed (%d).\n", r);
+			goto error_free;
+		}
+	}
+
+	for (i = 0; i < num_loops; i++) {
+		uint32_t cur_size_in_bytes = min(byte_count, max_bytes);
+
+		gsgpu_emit_copy_buffer(adev, &job->ibs[0], src_offset,
+					dst_offset, cur_size_in_bytes);
+
+		src_offset += cur_size_in_bytes;
+		dst_offset += cur_size_in_bytes;
+		byte_count -= cur_size_in_bytes;
+	}
+
+	gsgpu_ring_pad_ib(ring, &job->ibs[0]);
+	WARN_ON(job->ibs[0].length_dw > num_dw);
+	if (direct_submit)
+		r = gsgpu_job_submit_direct(job, ring, fence);
+	else
+		r = gsgpu_job_submit(job, &adev->mman.entity,
+				      GSGPU_FENCE_OWNER_UNDEFINED, fence);
+	if (r)
+		goto error_free;
+
+	return r;
+
+error_free:
+	gsgpu_job_free(job);
+	DRM_ERROR("Error scheduling IBs (%d)\n", r);
+	return r;
+}
+
+int gsgpu_fill_buffer(struct gsgpu_bo *bo,
+		       uint32_t src_data,
+		       struct reservation_object *resv,
+		       struct dma_fence **fence)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	uint32_t max_bytes = adev->mman.buffer_funcs->fill_max_bytes;
+	struct gsgpu_ring *ring = adev->mman.buffer_funcs_ring;
+
+	struct drm_mm_node *mm_node;
+	unsigned long num_pages;
+	unsigned int num_loops, num_dw;
+
+	struct gsgpu_job *job;
+	int r;
+
+	if (!adev->mman.buffer_funcs_enabled) {
+		DRM_ERROR("Trying to clear memory with ring turned off.\n");
+		return -EINVAL;
+	}
+
+	if (bo->tbo.mem.mem_type == TTM_PL_TT) {
+		r = gsgpu_ttm_alloc_gart(&bo->tbo);
+		if (r)
+			return r;
+	}
+
+	num_pages = bo->tbo.num_pages;
+	mm_node = bo->tbo.mem.mm_node;
+	num_loops = 0;
+	while (num_pages) {
+		uint32_t byte_count = mm_node->size << PAGE_SHIFT;
+
+		num_loops += DIV_ROUND_UP(byte_count, max_bytes);
+		num_pages -= mm_node->size;
+		++mm_node;
+	}
+	num_dw = num_loops * adev->mman.buffer_funcs->fill_num_dw;
+
+	/* for IB padding */
+	num_dw += 64;
+
+	r = gsgpu_job_alloc_with_ib(adev, num_dw * 4, &job);
+	if (r)
+		return r;
+
+	if (resv) {
+		r = gsgpu_sync_resv(adev, &job->sync, resv,
+				     GSGPU_FENCE_OWNER_UNDEFINED, false);
+		if (r) {
+			DRM_ERROR("sync failed (%d).\n", r);
+			goto error_free;
+		}
+	}
+
+	num_pages = bo->tbo.num_pages;
+	mm_node = bo->tbo.mem.mm_node;
+
+	while (num_pages) {
+		uint32_t byte_count = mm_node->size << PAGE_SHIFT;
+		uint64_t dst_addr;
+
+		dst_addr = gsgpu_mm_node_addr(&bo->tbo, mm_node, &bo->tbo.mem);
+		while (byte_count) {
+			uint32_t cur_size_in_bytes = min(byte_count, max_bytes);
+
+			gsgpu_emit_fill_buffer(adev, &job->ibs[0], src_data,
+						dst_addr, cur_size_in_bytes);
+
+			dst_addr += cur_size_in_bytes;
+			byte_count -= cur_size_in_bytes;
+		}
+
+		num_pages -= mm_node->size;
+		++mm_node;
+	}
+
+	gsgpu_ring_pad_ib(ring, &job->ibs[0]);
+	WARN_ON(job->ibs[0].length_dw > num_dw);
+	r = gsgpu_job_submit(job, &adev->mman.entity,
+			      GSGPU_FENCE_OWNER_UNDEFINED, fence);
+	if (r)
+		goto error_free;
+
+	return 0;
+
+error_free:
+	gsgpu_job_free(job);
+	return r;
+}
+
+#if defined(CONFIG_DEBUG_FS)
+
+static int gsgpu_mm_dump_table(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *)m->private;
+	unsigned ttm_pl = *(int *)node->info_ent->data;
+	struct drm_device *dev = node->minor->dev;
+	struct gsgpu_device *adev = dev->dev_private;
+	struct ttm_mem_type_manager *man = &adev->mman.bdev.man[ttm_pl];
+	struct drm_printer p = drm_seq_file_printer(m);
+
+	man->func->debug(man, &p);
+	return 0;
+}
+
+static int ttm_pl_vram = TTM_PL_VRAM;
+static int ttm_pl_tt = TTM_PL_TT;
+
+static const struct drm_info_list gsgpu_ttm_debugfs_list[] = {
+	{"gsgpu_vram_mm", gsgpu_mm_dump_table, 0, &ttm_pl_vram},
+	{"gsgpu_gtt_mm", gsgpu_mm_dump_table, 0, &ttm_pl_tt},
+	{"ttm_page_pool", ttm_page_alloc_debugfs, 0, NULL},
+#ifdef CONFIG_SWIOTLB
+	{"ttm_dma_page_pool", ttm_dma_page_alloc_debugfs, 0, NULL}
+#endif
+};
+
+/**
+ * gsgpu_ttm_vram_read - Linear read access to VRAM
+ *
+ * Accesses VRAM via MMIO for debugging purposes.
+ */
+static ssize_t gsgpu_ttm_vram_read(struct file *f, char __user *buf,
+				    size_t size, loff_t *pos)
+{
+	struct gsgpu_device *adev = file_inode(f)->i_private;
+	ssize_t result = 0;
+	int r;
+
+	if (size & 0x3 || *pos & 0x3)
+		return -EINVAL;
+
+	if (*pos >= adev->gmc.mc_vram_size)
+		return -ENXIO;
+
+	while (size) {
+		unsigned long flags;
+		uint32_t value;
+
+		if (*pos >= adev->gmc.mc_vram_size)
+			return result;
+
+		spin_lock_irqsave(&adev->mmio_idx_lock, flags);
+		WREG32_NO_KIQ(mmMM_INDEX, ((uint32_t)*pos) | 0x80000000);
+		WREG32_NO_KIQ(mmMM_INDEX_HI, *pos >> 31);
+		value = RREG32_NO_KIQ(mmMM_DATA);
+		spin_unlock_irqrestore(&adev->mmio_idx_lock, flags);
+
+		r = put_user(value, (uint32_t *)buf);
+		if (r)
+			return r;
+
+		result += 4;
+		buf += 4;
+		*pos += 4;
+		size -= 4;
+	}
+
+	return result;
+}
+
+/**
+ * gsgpu_ttm_vram_write - Linear write access to VRAM
+ *
+ * Accesses VRAM via MMIO for debugging purposes.
+ */
+static ssize_t gsgpu_ttm_vram_write(struct file *f, const char __user *buf,
+				    size_t size, loff_t *pos)
+{
+	struct gsgpu_device *adev = file_inode(f)->i_private;
+	ssize_t result = 0;
+	int r;
+
+	if (size & 0x3 || *pos & 0x3)
+		return -EINVAL;
+
+	if (*pos >= adev->gmc.mc_vram_size)
+		return -ENXIO;
+
+	while (size) {
+		unsigned long flags;
+		uint32_t value;
+
+		if (*pos >= adev->gmc.mc_vram_size)
+			return result;
+
+		r = get_user(value, (uint32_t *)buf);
+		if (r)
+			return r;
+
+		spin_lock_irqsave(&adev->mmio_idx_lock, flags);
+		WREG32_NO_KIQ(mmMM_INDEX, ((uint32_t)*pos) | 0x80000000);
+		WREG32_NO_KIQ(mmMM_INDEX_HI, *pos >> 31);
+		WREG32_NO_KIQ(mmMM_DATA, value);
+		spin_unlock_irqrestore(&adev->mmio_idx_lock, flags);
+
+		result += 4;
+		buf += 4;
+		*pos += 4;
+		size -= 4;
+	}
+
+	return result;
+}
+
+static const struct file_operations gsgpu_ttm_vram_fops = {
+	.owner = THIS_MODULE,
+	.read = gsgpu_ttm_vram_read,
+	.write = gsgpu_ttm_vram_write,
+	.llseek = default_llseek,
+};
+
+#ifdef CONFIG_DRM_GSGPU_GART_DEBUGFS
+
+/**
+ * gsgpu_ttm_gtt_read - Linear read access to GTT memory
+ */
+static ssize_t gsgpu_ttm_gtt_read(struct file *f, char __user *buf,
+				   size_t size, loff_t *pos)
+{
+	struct gsgpu_device *adev = file_inode(f)->i_private;
+	ssize_t result = 0;
+	int r;
+
+	while (size) {
+		loff_t p = *pos / PAGE_SIZE;
+		unsigned off = *pos & ~PAGE_MASK;
+		size_t cur_size = min_t(size_t, size, PAGE_SIZE - off);
+		struct page *page;
+		void *ptr;
+
+		if (p >= adev->gart.num_cpu_pages)
+			return result;
+
+		page = adev->gart.pages[p];
+		if (page) {
+			ptr = kmap(page);
+			ptr += off;
+
+			r = copy_to_user(buf, ptr, cur_size);
+			kunmap(adev->gart.pages[p]);
+		} else
+			r = clear_user(buf, cur_size);
+
+		if (r)
+			return -EFAULT;
+
+		result += cur_size;
+		buf += cur_size;
+		*pos += cur_size;
+		size -= cur_size;
+	}
+
+	return result;
+}
+
+static const struct file_operations gsgpu_ttm_gtt_fops = {
+	.owner = THIS_MODULE,
+	.read = gsgpu_ttm_gtt_read,
+	.llseek = default_llseek
+};
+
+#endif
+
+/**
+ * gsgpu_iomem_read - Virtual read access to GPU mapped memory
+ *
+ * This function is used to read memory that has been mapped to the
+ * GPU and the known addresses are not physical addresses but instead
+ * bus addresses (e.g., what you'd put in an IB or ring buffer).
+ */
+static ssize_t gsgpu_iomem_read(struct file *f, char __user *buf,
+				 size_t size, loff_t *pos)
+{
+	struct gsgpu_device *adev = file_inode(f)->i_private;
+	struct iommu_domain *dom;
+	ssize_t result = 0;
+	int r;
+
+	/* retrieve the IOMMU domain if any for this device */
+	dom = iommu_get_domain_for_dev(adev->dev);
+
+	while (size) {
+		phys_addr_t addr = *pos & PAGE_MASK;
+		loff_t off = *pos & ~PAGE_MASK;
+		size_t bytes = PAGE_SIZE - off;
+		unsigned long pfn;
+		struct page *p;
+		void *ptr;
+
+		bytes = bytes < size ? bytes : size;
+
+		/* Translate the bus address to a physical address.  If
+		 * the domain is NULL it means there is no IOMMU active
+		 * and the address translation is the identity
+		 */
+		addr = dom ? iommu_iova_to_phys(dom, addr) : addr;
+
+		pfn = addr >> PAGE_SHIFT;
+		if (!pfn_valid(pfn))
+			return -EPERM;
+
+		p = pfn_to_page(pfn);
+		if (p->mapping != adev->mman.bdev.dev_mapping)
+			return -EPERM;
+
+		ptr = kmap(p);
+		r = copy_to_user(buf, ptr + off, bytes);
+		kunmap(p);
+		if (r)
+			return -EFAULT;
+
+		size -= bytes;
+		*pos += bytes;
+		result += bytes;
+	}
+
+	return result;
+}
+
+/**
+ * gsgpu_iomem_write - Virtual write access to GPU mapped memory
+ *
+ * This function is used to write memory that has been mapped to the
+ * GPU and the known addresses are not physical addresses but instead
+ * bus addresses (e.g., what you'd put in an IB or ring buffer).
+ */
+static ssize_t gsgpu_iomem_write(struct file *f, const char __user *buf,
+				 size_t size, loff_t *pos)
+{
+	struct gsgpu_device *adev = file_inode(f)->i_private;
+	struct iommu_domain *dom;
+	ssize_t result = 0;
+	int r;
+
+	dom = iommu_get_domain_for_dev(adev->dev);
+
+	while (size) {
+		phys_addr_t addr = *pos & PAGE_MASK;
+		loff_t off = *pos & ~PAGE_MASK;
+		size_t bytes = PAGE_SIZE - off;
+		unsigned long pfn;
+		struct page *p;
+		void *ptr;
+
+		bytes = bytes < size ? bytes : size;
+
+		addr = dom ? iommu_iova_to_phys(dom, addr) : addr;
+
+		pfn = addr >> PAGE_SHIFT;
+		if (!pfn_valid(pfn))
+			return -EPERM;
+
+		p = pfn_to_page(pfn);
+		if (p->mapping != adev->mman.bdev.dev_mapping)
+			return -EPERM;
+
+		ptr = kmap(p);
+		r = copy_from_user(ptr + off, buf, bytes);
+		kunmap(p);
+		if (r)
+			return -EFAULT;
+
+		size -= bytes;
+		*pos += bytes;
+		result += bytes;
+	}
+
+	return result;
+}
+
+static const struct file_operations gsgpu_ttm_iomem_fops = {
+	.owner = THIS_MODULE,
+	.read = gsgpu_iomem_read,
+	.write = gsgpu_iomem_write,
+	.llseek = default_llseek
+};
+
+static const struct {
+	char *name;
+	const struct file_operations *fops;
+	int domain;
+} ttm_debugfs_entries[] = {
+	{ "gsgpu_vram", &gsgpu_ttm_vram_fops, TTM_PL_VRAM },
+#ifdef CONFIG_DRM_GSGPU_GART_DEBUGFS
+	{ "gsgpu_gtt", &gsgpu_ttm_gtt_fops, TTM_PL_TT },
+#endif
+	{ "gsgpu_iomem", &gsgpu_ttm_iomem_fops, TTM_PL_SYSTEM },
+};
+
+#endif
+
+static int gsgpu_ttm_debugfs_init(struct gsgpu_device *adev)
+{
+#if defined(CONFIG_DEBUG_FS)
+	unsigned count;
+
+	struct drm_minor *minor = adev->ddev->primary;
+	struct dentry *ent, *root = minor->debugfs_root;
+
+	for (count = 0; count < ARRAY_SIZE(ttm_debugfs_entries); count++) {
+		ent = debugfs_create_file(
+				ttm_debugfs_entries[count].name,
+				S_IFREG | S_IRUGO, root,
+				adev,
+				ttm_debugfs_entries[count].fops);
+		if (IS_ERR(ent))
+			return PTR_ERR(ent);
+		if (ttm_debugfs_entries[count].domain == TTM_PL_VRAM)
+			i_size_write(ent->d_inode, adev->gmc.mc_vram_size);
+		else if (ttm_debugfs_entries[count].domain == TTM_PL_TT)
+			i_size_write(ent->d_inode, adev->gmc.gart_size);
+		adev->mman.debugfs_entries[count] = ent;
+	}
+
+	count = ARRAY_SIZE(gsgpu_ttm_debugfs_list);
+
+#ifdef CONFIG_SWIOTLB
+	if (!(adev->need_swiotlb && swiotlb_nr_tbl()))
+		--count;
+#endif
+
+	return gsgpu_debugfs_add_files(adev, gsgpu_ttm_debugfs_list, count);
+#else
+	return 0;
+#endif
+}
+
+static void gsgpu_ttm_debugfs_fini(struct gsgpu_device *adev)
+{
+#if defined(CONFIG_DEBUG_FS)
+	unsigned i;
+
+	for (i = 0; i < ARRAY_SIZE(ttm_debugfs_entries); i++)
+		debugfs_remove(adev->mman.debugfs_entries[i]);
+#endif
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c
new file mode 100644
index 000000000000..281daabd2584
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm.c
@@ -0,0 +1,2567 @@
+#include <linux/dma-fence-array.h>
+#include <linux/interval_tree_generic.h>
+#include <linux/idr.h>
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+#include "gsgpu_trace.h"
+#include "gsgpu_vm_it.h"
+#include "gsgpu_gmc.h"
+
+/**
+ * DOC: GPUVM
+ *
+ * GPUVM is similar to the legacy gart on older asics, however
+ * rather than there being a single global gart table
+ * for the entire GPU, there are multiple VM page tables active
+ * at any given time.  The VM page tables can contain a mix
+ * vram pages and system memory pages and system memory pages
+ * can be mapped as snooped (cached system pages) or unsnooped
+ * (uncached system pages).
+ * Each VM has an ID associated with it and there is a page table
+ * associated with each VMID.  When execting a command buffer,
+ * the kernel tells the the ring what VMID to use for that command
+ * buffer.  VMIDs are allocated dynamically as commands are submitted.
+ * The userspace drivers maintain their own address space and the kernel
+ * sets up their pages tables accordingly when they submit their
+ * command buffers and a VMID is assigned.
+ * Cayman/Trinity support up to 8 active VMs at any given time;
+ * SI supports 16.
+ */
+
+/**
+ * struct gsgpu_pte_update_params - Local structure
+ *
+ * Encapsulate some VM table update parameters to reduce
+ * the number of function parameters
+ *
+ */
+struct gsgpu_pte_update_params {
+
+	/**
+	 * @ldev: gsgpu device we do this update for
+	 */
+	struct gsgpu_device *ldev;
+
+	/**
+	 * @vm: optional gsgpu_vm we do this update for
+	 */
+	struct gsgpu_vm *vm;
+
+	/**
+	 * @src: address where to copy page table entries from
+	 */
+	u64 src;
+
+	/**
+	 * @ib: indirect buffer to fill with commands
+	 */
+	struct gsgpu_ib *ib;
+
+	/**
+	 * @func: Function which actually does the update
+	 */
+	void (*func)(struct gsgpu_pte_update_params *params,
+		     struct gsgpu_bo *bo, u64 pe,
+		     u64 addr, unsigned count, u32 incr,
+		     u64 flags);
+	/**
+	 * @pages_addr:
+	 *
+	 * DMA addresses to use for mapping, used during VM update by CPU
+	 */
+	dma_addr_t *pages_addr;
+
+	/**
+	 * @kptr:
+	 *
+	 * Kernel pointer of PD/PT BO that needs to be updated,
+	 * used during VM update by CPU
+	 */
+	void *kptr;
+};
+
+/**
+ * struct gsgpu_prt_cb - Helper to disable partial resident texture feature from a fence callback
+ */
+struct gsgpu_prt_cb {
+
+	/**
+	 * @ldev: gsgpu device
+	 */
+	struct gsgpu_device *ldev;
+
+	/**
+	 * @cb: callback
+	 */
+	struct dma_fence_cb cb;
+};
+
+/**
+ * gsgpu_vm_bo_base_init - Adds bo to the list of bos associated with the vm
+ *
+ * @base: base structure for tracking BO usage in a VM
+ * @vm: vm to which bo is to be added
+ * @bo: gsgpu buffer object
+ *
+ * Initialize a bo_va_base structure and add it to the appropriate lists
+ *
+ */
+static void gsgpu_vm_bo_base_init(struct gsgpu_vm_bo_base *base,
+				   struct gsgpu_vm *vm,
+				   struct gsgpu_bo *bo)
+{
+	base->vm = vm;
+	base->bo = bo;
+	INIT_LIST_HEAD(&base->bo_list);
+	INIT_LIST_HEAD(&base->vm_status);
+
+	if (!bo)
+		return;
+
+	list_add_tail(&base->bo_list, &bo->va);
+
+	if (bo->tbo.type == ttm_bo_type_kernel)
+		list_move(&base->vm_status, &vm->relocated);
+
+	if (bo->tbo.resv != vm->root.base.bo->tbo.resv)
+		return;
+
+	if (bo->preferred_domains &
+	    gsgpu_mem_type_to_domain(bo->tbo.mem.mem_type))
+		return;
+
+	/*
+	 * we checked all the prerequisites, but it looks like this per vm bo
+	 * is currently evicted. add the bo to the evicted list to make sure it
+	 * is validated on next vm use to avoid fault.
+	 * */
+	list_move_tail(&base->vm_status, &vm->evicted);
+	base->moved = true;
+}
+
+static u32 gsgpu_get_pde_pte_size (struct gsgpu_device *ldev)
+{
+	return ldev->vm_manager.pde_pte_bytes;
+}
+
+/**
+ * gsgpu_vm_level_shift - return the addr shift for each level
+ *
+ * @ldev: gsgpu_device pointer
+ * @level: VMPT level
+ *
+ * Returns:
+ * The number of bits the pfn needs to be right shifted for a level.
+ */
+static unsigned gsgpu_vm_level_shift(struct gsgpu_device *ldev,
+				      unsigned level)
+{
+	unsigned shift = 0xff;
+
+	switch (level) {
+	case GSGPU_VM_DIR0:
+		shift = ldev->vm_manager.dir0_shift - ldev->vm_manager.dir2_shift;
+		break;
+	case GSGPU_VM_DIR1:
+		shift = ldev->vm_manager.dir1_shift - ldev->vm_manager.dir2_shift;
+		break;
+	case GSGPU_VM_DIR2:
+		shift = 0;
+		break;
+	default:
+		dev_err(ldev->dev, "the level%d isn't supported.\n", level);
+	}
+
+	return shift;
+}
+
+/**
+ * gsgpu_vm_num_entries - return the number of entries in a PD/PT
+ *
+ * @ldev: gsgpu_device pointer
+ * @level: VMPT level
+ *
+ * Returns:
+ * The number of entries in a page directory or page table.
+ */
+static unsigned gsgpu_vm_num_entries(struct gsgpu_device *ldev,
+				      unsigned level)
+{
+	unsigned width = 0;
+
+	switch (level) {
+	case GSGPU_VM_DIR0:
+		width = ldev->vm_manager.dir0_width;
+		break;
+	case GSGPU_VM_DIR1:
+		width = ldev->vm_manager.dir1_width;
+		break;
+	case GSGPU_VM_DIR2:
+		width = ldev->vm_manager.dir2_width;
+		break;
+	default:
+		dev_err(ldev->dev, "the level%d isn't supported.\n", level);
+	}
+
+	return 1 << width;
+}
+
+/**
+ * gsgpu_vm_bo_size - returns the size of the BOs in bytes
+ *
+ * @ldev: gsgpu_device pointer
+ * @level: VMPT level
+ *
+ * Returns:
+ * The size of the BO for a page directory or page table in bytes.
+ */
+static unsigned gsgpu_vm_bo_size(struct gsgpu_device *ldev, unsigned level)
+{
+	return GSGPU_GPU_PAGE_ALIGN(gsgpu_vm_num_entries(ldev, level) *
+				gsgpu_get_pde_pte_size(ldev));
+}
+
+/**
+ * gsgpu_vm_get_pd_bo - add the VM PD to a validation list
+ *
+ * @vm: vm providing the BOs
+ * @validated: head of validation list
+ * @entry: entry to add
+ *
+ * Add the page directory to the list of BOs to
+ * validate for command submission.
+ */
+void gsgpu_vm_get_pd_bo(struct gsgpu_vm *vm,
+			 struct list_head *validated,
+			 struct gsgpu_bo_list_entry *entry)
+{
+	entry->robj = vm->root.base.bo;
+	entry->priority = 0;
+	entry->tv.bo = &entry->robj->tbo;
+	entry->tv.shared = true;
+	entry->user_pages = NULL;
+	list_add(&entry->tv.head, validated);
+}
+
+/**
+ * gsgpu_vm_validate_pt_bos - validate the page table BOs
+ *
+ * @ldev: gsgpu device pointer
+ * @vm: vm providing the BOs
+ * @validate: callback to do the validation
+ * @param: parameter for the validation callback
+ *
+ * Validate the page table BOs on command submission if neccessary.
+ *
+ * Returns:
+ * Validation result.
+ */
+int gsgpu_vm_validate_pt_bos(struct gsgpu_device *ldev, struct gsgpu_vm *vm,
+			      int (*validate)(void *p, struct gsgpu_bo *bo),
+			      void *param)
+{
+	struct ttm_bo_global *glob = ldev->mman.bdev.glob;
+	struct gsgpu_vm_bo_base *bo_base, *tmp;
+	int r = 0;
+
+	list_for_each_entry_safe(bo_base, tmp, &vm->evicted, vm_status) {
+		struct gsgpu_bo *bo = bo_base->bo;
+
+		if (bo->parent) {
+			r = validate(param, bo);
+			if (r)
+				break;
+
+			spin_lock(&glob->lru_lock);
+			ttm_bo_move_to_lru_tail(&bo->tbo);
+			if (bo->shadow)
+				ttm_bo_move_to_lru_tail(&bo->shadow->tbo);
+			spin_unlock(&glob->lru_lock);
+		}
+
+		if (bo->tbo.type != ttm_bo_type_kernel) {
+			spin_lock(&vm->moved_lock);
+			list_move(&bo_base->vm_status, &vm->moved);
+			spin_unlock(&vm->moved_lock);
+		} else {
+			list_move(&bo_base->vm_status, &vm->relocated);
+		}
+	}
+
+	spin_lock(&glob->lru_lock);
+	list_for_each_entry(bo_base, &vm->idle, vm_status) {
+		struct gsgpu_bo *bo = bo_base->bo;
+
+		if (!bo->parent)
+			continue;
+
+		ttm_bo_move_to_lru_tail(&bo->tbo);
+		if (bo->shadow)
+			ttm_bo_move_to_lru_tail(&bo->shadow->tbo);
+	}
+	spin_unlock(&glob->lru_lock);
+
+	return r;
+}
+
+/**
+ * gsgpu_vm_ready - check VM is ready for updates
+ *
+ * @vm: VM to check
+ *
+ * Check if all VM PDs/PTs are ready for updates
+ *
+ * Returns:
+ * True if eviction list is empty.
+ */
+bool gsgpu_vm_ready(struct gsgpu_vm *vm)
+{
+	return list_empty(&vm->evicted);
+}
+
+/**
+ * gsgpu_vm_clear_bo - initially clear the PDs/PTs
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: VM to clear BO from
+ * @bo: BO to clear
+ * @level: level this BO is at
+ *
+ * Root PD needs to be reserved when calling this.
+ *
+ * Returns:
+ * 0 on success, errno otherwise.
+ */
+static int gsgpu_vm_clear_bo(struct gsgpu_device *ldev,
+			      struct gsgpu_vm *vm, struct gsgpu_bo *bo,
+			      unsigned level)
+{
+	struct ttm_operation_ctx ctx = { true, false };
+	struct dma_fence *fence = NULL;
+	unsigned entries;
+	struct gsgpu_ring *ring;
+	struct gsgpu_job *job;
+	u64 addr;
+	int r;
+
+	entries = gsgpu_bo_size(bo) / gsgpu_get_pde_pte_size(ldev);
+
+	ring = container_of(vm->entity.rq->sched, struct gsgpu_ring, sched);
+
+	r = reservation_object_reserve_shared(bo->tbo.resv);
+	if (r)
+		return r;
+
+	r = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);
+	if (r)
+		goto error;
+
+	r = gsgpu_job_alloc_with_ib(ldev, 64, &job);
+	if (r)
+		goto error;
+
+	addr = gsgpu_bo_gpu_offset(bo);
+
+	if (entries)
+		gsgpu_vm_set_pte_pde(ldev, &job->ibs[0], addr, 0,
+				      entries, GSGPU_GPU_PAGE_SIZE, 0);
+
+	gsgpu_ring_pad_ib(ring, &job->ibs[0]);
+
+	WARN_ON(job->ibs[0].length_dw > 64);
+	r = gsgpu_sync_resv(ldev, &job->sync, bo->tbo.resv,
+			     GSGPU_FENCE_OWNER_UNDEFINED, false);
+	if (r)
+		goto error_free;
+
+	r = gsgpu_job_submit(job, &vm->entity, GSGPU_FENCE_OWNER_UNDEFINED, &fence);
+	if (r)
+		goto error_free;
+
+	gsgpu_bo_fence(bo, fence, true);
+	dma_fence_put(fence);
+
+	if (bo->shadow)
+		return gsgpu_vm_clear_bo(ldev, vm, bo->shadow, level);
+
+	return 0;
+
+error_free:
+	gsgpu_job_free(job);
+
+error:
+	return r;
+}
+
+/**
+ * gsgpu_vm_alloc_levels - allocate the PD/PT levels
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: requested vm
+ * @parent: parent PT
+ * @saddr: start of the address range
+ * @eaddr: end of the address range
+ * @level: VMPT level
+ *
+ * Make sure the page directories and page tables are allocated
+ *
+ * Returns:
+ * 0 on success, errno otherwise.
+ */
+static int gsgpu_vm_alloc_levels(struct gsgpu_device *ldev,
+				  struct gsgpu_vm *vm,
+				  struct gsgpu_vm_pt *parent,
+				  u64 saddr, u64 eaddr,
+				  unsigned level)
+{
+	unsigned shift = gsgpu_vm_level_shift(ldev, level);
+	unsigned pt_idx, from, to;
+	u64 flags;
+	int r;
+
+	if (!parent->entries) {
+		unsigned num_entries = gsgpu_vm_num_entries(ldev, level);
+
+		parent->entries = kvmalloc_array(num_entries,
+						   sizeof(struct gsgpu_vm_pt),
+						   GFP_KERNEL | __GFP_ZERO);
+		if (!parent->entries)
+			return -ENOMEM;
+	}
+
+	from = saddr >> shift;
+	to = eaddr >> shift;
+	if (from >= gsgpu_vm_num_entries(ldev, level) ||
+	    to >= gsgpu_vm_num_entries(ldev, level))
+		return -EINVAL;
+
+	++level;
+	saddr = saddr & ((1 << shift) - 1);
+	eaddr = eaddr & ((1 << shift) - 1);
+
+	flags = GSGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	if (vm->root.base.bo->shadow)
+		flags |= GSGPU_GEM_CREATE_SHADOW;
+	if (vm->use_cpu_for_update)
+		flags |= GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+	else
+		flags |= GSGPU_GEM_CREATE_NO_CPU_ACCESS;
+
+	/**
+	 * walk over the address space and allocate the page tables
+	 */
+	for (pt_idx = from; pt_idx <= to; ++pt_idx) {
+		struct reservation_object *resv = vm->root.base.bo->tbo.resv;
+		struct gsgpu_vm_pt *entry = &parent->entries[pt_idx];
+		struct gsgpu_bo *pt;
+
+		if (!entry->base.bo) {
+			struct gsgpu_bo_param bp;
+
+			memset(&bp, 0, sizeof(bp));
+			bp.size = gsgpu_vm_bo_size(ldev, level);
+			bp.byte_align = GSGPU_GPU_PAGE_SIZE;
+			bp.domain = GSGPU_GEM_DOMAIN_VRAM;
+			bp.flags = flags;
+			bp.type = ttm_bo_type_kernel;
+			bp.resv = resv;
+			r = gsgpu_bo_create(ldev, &bp, &pt);
+			if (r)
+				return r;
+
+			r = gsgpu_vm_clear_bo(ldev, vm, pt, level);
+			if (r) {
+				gsgpu_bo_unref(&pt->shadow);
+				gsgpu_bo_unref(&pt);
+				return r;
+			}
+
+			if (vm->use_cpu_for_update) {
+				r = gsgpu_bo_kmap(pt, NULL);
+				if (r) {
+					gsgpu_bo_unref(&pt->shadow);
+					gsgpu_bo_unref(&pt);
+					return r;
+				}
+			}
+
+			/**
+			 * Keep a reference to the root directory to avoid
+			 * freeing them up in the wrong order.
+			 */
+			pt->parent = gsgpu_bo_ref(parent->base.bo);
+
+			gsgpu_vm_bo_base_init(&entry->base, vm, pt);
+		}
+
+		if (level < GSGPU_VM_DIR2) {
+			u64 sub_saddr = (pt_idx == from) ? saddr : 0;
+			u64 sub_eaddr = (pt_idx == to) ? eaddr :
+				((1 << shift) - 1);
+			r = gsgpu_vm_alloc_levels(ldev, vm, entry, sub_saddr,
+						   sub_eaddr, level);
+			if (r)
+				return r;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_alloc_pts - Allocate page tables.
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: VM to allocate page tables for
+ * @saddr: Start address which needs to be allocated
+ * @size: Size from start address we need.
+ *
+ * Make sure the page tables are allocated.
+ *
+ * Returns:
+ * 0 on success, errno otherwise.
+ */
+int gsgpu_vm_alloc_pts(struct gsgpu_device *ldev,
+			struct gsgpu_vm *vm,
+			u64 saddr, u64 size)
+{
+	u64 eaddr;
+
+	/**
+	 * validate the parameters
+	 */
+	if (saddr & GSGPU_GPU_PAGE_MASK || size & GSGPU_GPU_PAGE_MASK)
+		return -EINVAL;
+
+	eaddr = saddr + size - 1;
+
+	saddr /= GSGPU_GPU_PAGE_SIZE;
+	eaddr /= GSGPU_GPU_PAGE_SIZE;
+
+	if (eaddr >= ldev->vm_manager.max_pfn) {
+		dev_err(ldev->dev, "va above limit (0x%08llX >= 0x%08llX)\n",
+			eaddr, ldev->vm_manager.max_pfn);
+		return -EINVAL;
+	}
+
+	return gsgpu_vm_alloc_levels(ldev, vm, &vm->root, saddr, eaddr,
+				      ldev->vm_manager.root_level);
+}
+
+/**
+ * gsgpu_vm_need_pipeline_sync - Check if pipe sync is needed for job.
+ *
+ * @ring: ring on which the job will be submitted
+ * @job: job to submit
+ *
+ * Returns:
+ * True if sync is needed.
+ */
+bool gsgpu_vm_need_pipeline_sync(struct gsgpu_ring *ring,
+				  struct gsgpu_job *job)
+{
+	struct gsgpu_device *ldev = ring->adev;
+	struct gsgpu_vmid_mgr *id_mgr = &ldev->vm_manager.id_mgr;
+	struct gsgpu_vmid *id;
+	bool vm_flush_needed = job->vm_needs_flush;
+
+	if (job->vmid == 0)
+		return false;
+
+	id = &id_mgr->ids[job->vmid];
+
+	if (gsgpu_vmid_had_gpu_reset(ldev, id))
+		return true;
+
+	return vm_flush_needed;
+}
+
+/**
+ * gsgpu_vm_flush - hardware flush the vm
+ *
+ * @ring: ring to use for flush
+ * @job:  related job
+ * @need_pipe_sync: is pipe sync needed
+ *
+ * Emit a VM flush when it is necessary.
+ *
+ * Returns:
+ * 0 on success, errno otherwise.
+ */
+int gsgpu_vm_flush(struct gsgpu_ring *ring, struct gsgpu_job *job, bool need_pipe_sync)
+{
+	struct gsgpu_device *ldev = ring->adev;
+	struct gsgpu_vmid_mgr *id_mgr = &ldev->vm_manager.id_mgr;
+	struct gsgpu_vmid *id = &id_mgr->ids[job->vmid];
+	bool vm_flush_needed = job->vm_needs_flush;
+	struct dma_fence *fence = NULL;
+	int r;
+	bool pasid_mapping_needed = false;
+
+	if (gsgpu_vmid_had_gpu_reset(ldev, id)) {
+		vm_flush_needed = true;
+		pasid_mapping_needed = true;
+	}
+
+	mutex_lock(&id_mgr->lock);
+	if (id->pasid != job->pasid || !id->pasid_mapping ||
+	    !dma_fence_is_signaled(id->pasid_mapping))
+		pasid_mapping_needed = true;
+	mutex_unlock(&id_mgr->lock);
+
+	vm_flush_needed &= !!ring->funcs->emit_vm_flush  &&
+			job->vm_pd_addr != GSGPU_BO_INVALID_OFFSET;
+
+	pasid_mapping_needed &= ldev->gmc.gmc_funcs->emit_pasid_mapping &&
+		ring->funcs->emit_wreg;
+
+	if (!vm_flush_needed && !need_pipe_sync)
+		return 0;
+
+	// if (need_pipe_sync)
+	// 	gsgpu_ring_emit_pipeline_sync(ring);
+
+	if (pasid_mapping_needed)
+		gsgpu_gmc_emit_pasid_mapping(ring, job->vmid, job->pasid);
+
+	if (vm_flush_needed || pasid_mapping_needed) {
+		trace_gsgpu_vm_flush(ring, job->vmid, job->vm_pd_addr);
+		gsgpu_ring_emit_vm_flush(ring, job->vmid, job->vm_pd_addr);
+	}
+
+	if (vm_flush_needed) {
+		r = gsgpu_fence_emit(ring, &fence, 0);
+		if (r)
+			return r;
+	}
+
+	if (vm_flush_needed) {
+		mutex_lock(&id_mgr->lock);
+		dma_fence_put(id->last_flush);
+		id->last_flush = dma_fence_get(fence);
+		id->current_gpu_reset_count =
+			atomic_read(&ldev->gpu_reset_counter);
+		mutex_unlock(&id_mgr->lock);
+	}
+
+
+	if (pasid_mapping_needed) {
+		mutex_lock(&id_mgr->lock);
+		id->pasid = job->pasid;
+		dma_fence_put(id->pasid_mapping);
+		id->pasid_mapping = dma_fence_get(fence);
+		mutex_unlock(&id_mgr->lock);
+	}
+
+	dma_fence_put(fence);
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_bo_find - find the bo_va for a specific vm & bo
+ *
+ * @vm: requested vm
+ * @bo: requested buffer object
+ *
+ * Find @bo inside the requested vm.
+ * Search inside the @bos vm list for the requested vm
+ * Returns the found bo_va or NULL if none is found
+ *
+ * Object has to be reserved!
+ *
+ * Returns:
+ * Found bo_va or NULL.
+ */
+struct gsgpu_bo_va *gsgpu_vm_bo_find(struct gsgpu_vm *vm,
+				       struct gsgpu_bo *bo)
+{
+	struct gsgpu_bo_va *bo_va;
+
+	list_for_each_entry(bo_va, &bo->va, base.bo_list) {
+		if (bo_va->base.vm == vm) {
+			return bo_va;
+		}
+	}
+	return NULL;
+}
+
+/**
+ * gsgpu_vm_do_set_ptes - helper to call the right asic function
+ *
+ * @params: see gsgpu_pte_update_params definition
+ * @bo: PD/PT to update
+ * @pe: addr of the page entry
+ * @addr: dst addr to write into pe
+ * @count: number of page entries to update
+ * @incr: increase next addr by incr bytes
+ * @flags: hw access flags
+ *
+ * Traces the parameters and calls the right asic functions
+ * to setup the page table using the DMA.
+ */
+static void gsgpu_vm_do_set_ptes(struct gsgpu_pte_update_params *params,
+				  struct gsgpu_bo *bo,
+				  u64 pe, u64 addr,
+				  unsigned count, u32 incr,
+				  u64 flags)
+{
+	pe += gsgpu_bo_gpu_offset(bo);
+	trace_gsgpu_vm_set_ptes(pe, addr, count, incr, flags);
+
+	gsgpu_vm_set_pte_pde(params->ldev, params->ib, pe, addr,
+			      count, incr, flags);
+}
+
+/**
+ * gsgpu_vm_do_copy_ptes - copy the PTEs from the GART
+ *
+ * @params: see gsgpu_pte_update_params definition
+ * @bo: PD/PT to update
+ * @pe: addr of the page entry
+ * @addr: dst addr to write into pe
+ * @count: number of page entries to update
+ * @incr: increase next addr by incr bytes
+ * @flags: hw access flags
+ *
+ * Traces the parameters and calls the DMA function to copy the PTEs.
+ */
+static void gsgpu_vm_do_copy_ptes(struct gsgpu_pte_update_params *params,
+				   struct gsgpu_bo *bo,
+				   u64 pe, u64 addr,
+				   unsigned count, u32 incr,
+				   u64 flags)
+{
+	u64 src = (params->src +
+				(addr >> GSGPU_GPU_PAGE_SHIFT) *
+				gsgpu_get_pde_pte_size(params->ldev));
+
+	pe += gsgpu_bo_gpu_offset(bo);
+	trace_gsgpu_vm_copy_ptes(pe, src, count);
+
+	gsgpu_vm_copy_pte(params->ldev, params->ib, pe, src, count);
+}
+
+/**
+ * gsgpu_vm_map_gart - Resolve gart mapping of addr
+ *
+ * @pages_addr: optional DMA address to use for lookup
+ * @addr: the unmapped addr
+ *
+ * Look up the physical address of the page that the pte resolves
+ * to.
+ *
+ * Returns:
+ * The pointer for the page table entry.
+ */
+static u64 gsgpu_vm_map_gart(const dma_addr_t *pages_addr, u64 addr)
+{
+	u64 result;
+
+	/* page table offset */
+	result = pages_addr[addr >> PAGE_SHIFT];
+
+	/* in case cpu page size != gpu page size*/
+	result |= addr & (~PAGE_MASK);
+
+	result &= ~((1ULL << GSGPU_GPU_PAGE_SHIFT) - 1);
+
+	return result;
+}
+
+/**
+ * gsgpu_vm_cpu_set_ptes - helper to update page tables via CPU
+ *
+ * @params: see gsgpu_pte_update_params definition
+ * @bo: PD/PT to update
+ * @pe: kmap addr of the page entry
+ * @addr: dst addr to write into pe
+ * @count: number of page entries to update
+ * @incr: increase next addr by incr bytes
+ * @flags: hw access flags
+ *
+ * Write count number of PT/PD entries directly.
+ */
+static void gsgpu_vm_cpu_set_ptes(struct gsgpu_pte_update_params *params,
+				   struct gsgpu_bo *bo,
+				   u64 pe, u64 addr,
+				   unsigned count, u32 incr,
+				   u64 flags)
+{
+	unsigned int i, r;
+	void *kptr;
+	u64 value;
+
+	r = gsgpu_bo_kmap(bo, &kptr);
+	if (r)
+		return;
+	pe += (u64)kptr;
+
+	trace_gsgpu_vm_set_ptes(pe, addr, count, incr, flags);
+
+	for (i = 0; i < count; i++) {
+		value = params->pages_addr ?
+			gsgpu_vm_map_gart(params->pages_addr, addr) : addr;
+		gsgpu_gmc_set_pte_pde(params->ldev, (void *)(uintptr_t)pe,
+				       i, value, flags);
+		addr += incr;
+	}
+}
+
+
+/**
+ * gsgpu_vm_wait_pd - Wait for PT BOs to be free.
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: related vm
+ * @owner: fence owner
+ *
+ * Returns:
+ * 0 on success, errno otherwise.
+ */
+static int gsgpu_vm_wait_pd(struct gsgpu_device *ldev, struct gsgpu_vm *vm,
+			     void *owner)
+{
+	struct gsgpu_sync sync;
+	int r;
+
+	gsgpu_sync_create(&sync);
+	gsgpu_sync_resv(ldev, &sync, vm->root.base.bo->tbo.resv, owner, false);
+	r = gsgpu_sync_wait(&sync, true);
+	gsgpu_sync_free(&sync);
+
+	return r;
+}
+
+/*
+ * gsgpu_vm_update_pde - update a single level in the hierarchy
+ *
+ * @param: parameters for the update
+ * @vm: requested vm
+ * @parent: parent directory
+ * @entry: entry to update
+ *
+ * Makes sure the requested entry in parent is up to date.
+ */
+static void gsgpu_vm_update_pde(struct gsgpu_pte_update_params *params,
+				 struct gsgpu_vm *vm,
+				 struct gsgpu_vm_pt *parent,
+				 struct gsgpu_vm_pt *entry)
+{
+	struct gsgpu_bo *bo = parent->base.bo, *pbo;
+	u64 pde, pt, flags;
+	unsigned level;
+
+	/* Don't update huge pages here */
+	if (entry->huge)
+		return;
+
+	for (level = 0, pbo = bo->parent; pbo; ++level)
+		pbo = pbo->parent;
+
+	level += params->ldev->vm_manager.root_level;
+	pt = gsgpu_bo_gpu_offset(entry->base.bo);
+	flags = GSGPU_PTE_PRESENT;
+	gsgpu_gmc_get_vm_pde(params->ldev, level, &pt, &flags);
+	pde = (entry - parent->entries) * gsgpu_get_pde_pte_size(params->ldev);
+	if (bo->shadow)
+		params->func(params, bo->shadow, pde, pt, 1, 0, flags);
+	params->func(params, bo, pde, pt, 1, 0, flags);
+}
+
+/*
+ * gsgpu_vm_invalidate_level - mark all PD levels as invalid
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: related vm
+ * @parent: parent PD
+ * @level: VMPT level
+ *
+ * Mark all PD level as invalid after an error.
+ */
+static void gsgpu_vm_invalidate_level(struct gsgpu_device *ldev,
+				       struct gsgpu_vm *vm,
+				       struct gsgpu_vm_pt *parent,
+				       unsigned level)
+{
+	unsigned pt_idx, num_entries;
+
+	/*
+	 * Recurse into the subdirectories. This recursion is harmless because
+	 * we only have a maximum of 5 layers.
+	 */
+	num_entries = gsgpu_vm_num_entries(ldev, level);
+	for (pt_idx = 0; pt_idx < num_entries; ++pt_idx) {
+		struct gsgpu_vm_pt *entry = &parent->entries[pt_idx];
+
+		if (!entry->base.bo)
+			continue;
+
+		if (!entry->base.moved)
+			list_move(&entry->base.vm_status, &vm->relocated);
+		gsgpu_vm_invalidate_level(ldev, vm, entry, level + 1);
+	}
+}
+
+/*
+ * gsgpu_vm_update_directories - make sure that all directories are valid
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: requested vm
+ *
+ * Makes sure all directories are up to date.
+ *
+ * Returns:
+ * 0 for success, error for failure.
+ */
+int gsgpu_vm_update_directories(struct gsgpu_device *ldev,
+				 struct gsgpu_vm *vm)
+{
+	struct gsgpu_pte_update_params params;
+	struct gsgpu_job *job;
+	unsigned ndw = 0;
+	int r = 0;
+
+	if (list_empty(&vm->relocated))
+		return 0;
+
+restart:
+	memset(&params, 0, sizeof(params));
+	params.ldev = ldev;
+
+	if (vm->use_cpu_for_update) {
+		struct gsgpu_vm_bo_base *bo_base;
+
+		list_for_each_entry(bo_base, &vm->relocated, vm_status) {
+			r = gsgpu_bo_kmap(bo_base->bo, NULL);
+			if (unlikely(r))
+				return r;
+		}
+
+		r = gsgpu_vm_wait_pd(ldev, vm, GSGPU_FENCE_OWNER_VM);
+		if (unlikely(r))
+			return r;
+
+		params.func = gsgpu_vm_cpu_set_ptes;
+	} else {
+		ndw = 512 * 8; //TODO
+		r = gsgpu_job_alloc_with_ib(ldev, ndw * GSGPU_BYTES_PER_DW, &job);
+		if (r)
+			return r;
+
+		params.ib = &job->ibs[0];
+		params.func = gsgpu_vm_do_set_ptes;
+	}
+
+	while (!list_empty(&vm->relocated)) {
+		struct gsgpu_vm_bo_base *bo_base, *parent;
+		struct gsgpu_vm_pt *pt, *entry;
+		struct gsgpu_bo *bo;
+
+		bo_base = list_first_entry(&vm->relocated,
+					   struct gsgpu_vm_bo_base,
+					   vm_status);
+		bo_base->moved = false;
+		list_del_init(&bo_base->vm_status);
+
+		bo = bo_base->bo->parent;
+		if (!bo)
+			continue;
+
+		parent = list_first_entry(&bo->va, struct gsgpu_vm_bo_base,
+					  bo_list);
+		pt = container_of(parent, struct gsgpu_vm_pt, base);
+		entry = container_of(bo_base, struct gsgpu_vm_pt, base);
+
+		gsgpu_vm_update_pde(&params, vm, pt, entry);
+
+		if (!vm->use_cpu_for_update &&
+		    (ndw - params.ib->length_dw) < 32)
+			break;
+	}
+
+	if (vm->use_cpu_for_update) {
+		/* Flush HDP */
+		mb();
+		//TODO make sure cpu write pte is coherent
+	} else if (params.ib->length_dw == 0) {
+		gsgpu_job_free(job);
+	} else {
+		struct gsgpu_bo *root = vm->root.base.bo;
+		struct gsgpu_ring *ring;
+		struct dma_fence *fence;
+
+		ring = container_of(vm->entity.rq->sched, struct gsgpu_ring,
+				    sched);
+
+		gsgpu_ring_pad_ib(ring, params.ib);
+		gsgpu_sync_resv(ldev, &job->sync, root->tbo.resv,
+				 GSGPU_FENCE_OWNER_VM, false);
+		WARN_ON(params.ib->length_dw > ndw);
+		r = gsgpu_job_submit(job, &vm->entity, GSGPU_FENCE_OWNER_VM, &fence);
+		if (r)
+			goto error;
+
+		gsgpu_bo_fence(root, fence, true);
+		dma_fence_put(vm->last_update);
+		vm->last_update = fence;
+	}
+
+	if (!list_empty(&vm->relocated))
+		goto restart;
+
+	return 0;
+
+error:
+	gsgpu_vm_invalidate_level(ldev, vm, &vm->root,
+				   ldev->vm_manager.root_level);
+	gsgpu_job_free(job);
+	return r;
+}
+
+/**
+ * gsgpu_vm_find_entry - find the entry for an address
+ *
+ * @p: see gsgpu_pte_update_params definition
+ * @addr: virtual address in question
+ * @entry: resulting entry or NULL
+ * @parent: parent entry
+ *
+ * Find the vm_pt entry and it's parent for the given address.
+ */
+void gsgpu_vm_get_entry(struct gsgpu_pte_update_params *p, u64 addr,
+			 struct gsgpu_vm_pt **entry,
+			 struct gsgpu_vm_pt **parent)
+{
+	unsigned level = p->ldev->vm_manager.root_level;
+
+	*parent = NULL;
+	*entry = &p->vm->root;
+
+	while ((*entry)->entries) {
+		unsigned shift = gsgpu_vm_level_shift(p->ldev, level++);
+
+		*parent = *entry;
+		*entry = &(*entry)->entries[addr >> shift];
+		addr &= (1ULL << shift) - 1;
+	}
+
+	if (level != GSGPU_VM_DIR2)
+		*entry = NULL;
+}
+
+/**
+ * gsgpu_vm_handle_huge_pages - handle updating the PD with huge pages
+ *
+ * @p: see gsgpu_pte_update_params definition
+ * @entry: vm_pt entry to check
+ * @parent: parent entry
+ * @nptes: number of PTEs updated with this operation
+ * @dst: destination address where the PTEs should point to
+ * @flags: access flags fro the PTEs
+ *
+ * Check if we can update the PD with a huge page.
+ */
+static void gsgpu_vm_handle_huge_pages(struct gsgpu_pte_update_params *p,
+					struct gsgpu_vm_pt *entry,
+					struct gsgpu_vm_pt *parent,
+					unsigned nptes, u64 dst,
+					u64 flags)
+{
+	u64 pde;
+
+	/**
+	 * In the case of a mixed PT the PDE must point to it
+	 */
+	if (!p->src && nptes == GSGPU_VM_PTE_COUNT(p->ldev)) {
+		/**
+		 * Set the huge page flag to stop scanning at this PDE
+		 */
+		flags |= GSGPU_PTE_HUGEPAGE;
+	}
+
+	if (!(flags & GSGPU_PTE_HUGEPAGE)) {
+		if (entry->huge) {
+			/**
+			 * Add the entry to the relocated list to update it.
+			 */
+			entry->huge = false;
+			list_move(&entry->base.vm_status, &p->vm->relocated);
+		}
+		return;
+	}
+
+	entry->huge = true;
+	gsgpu_gmc_get_vm_pde(p->ldev, GSGPU_VM_DIR2, &dst, &flags);
+
+	pde = (entry - parent->entries) * gsgpu_get_pde_pte_size(p->ldev);
+	if (parent->base.bo->shadow)
+		p->func(p, parent->base.bo->shadow, pde, dst, 1, 0, flags);
+	p->func(p, parent->base.bo, pde, dst, 1, 0, flags);
+}
+
+/**
+ * gsgpu_vm_update_ptes - make sure that page tables are valid
+ *
+ * @params: see gsgpu_pte_update_params definition
+ * @start: start of GPU address range
+ * @end: end of GPU address range
+ * @dst: destination address to map to, the next dst inside the function
+ * @flags: mapping flags
+ *
+ * Update the page tables in the range @start - @end.
+ *
+ * Returns:
+ * 0 for success, -EINVAL for failure.
+ */
+static int gsgpu_vm_update_ptes(struct gsgpu_pte_update_params *params,
+				  u64 start, u64 end,
+				  u64 dst, u64 flags)
+{
+	struct gsgpu_device *ldev = params->ldev;
+	const u64 mask = GSGPU_VM_PTE_COUNT(ldev) - 1;
+
+	u64 addr, pe_start;
+	struct gsgpu_bo *pt;
+	unsigned nptes;
+
+	/**
+	 * walk over the address space and update the page tables
+	 */
+	for (addr = start; addr < end; addr += nptes,
+	     dst += nptes * GSGPU_GPU_PAGE_SIZE) {
+		struct gsgpu_vm_pt *entry, *parent;
+
+		gsgpu_vm_get_entry(params, addr, &entry, &parent);
+		if (!entry)
+			return -ENOENT;
+
+		if ((addr & ~mask) == (end & ~mask))
+			nptes = end - addr;
+		else
+			nptes = GSGPU_VM_PTE_COUNT(ldev) - (addr & mask);
+
+		gsgpu_vm_handle_huge_pages(params, entry, parent,
+					    nptes, dst, flags);
+		if (entry->huge)
+			continue;
+
+		pt = entry->base.bo;
+		pe_start = (addr & mask) * gsgpu_get_pde_pte_size(params->ldev);
+		if (pt->shadow)
+			params->func(params, pt->shadow, pe_start, dst, nptes,
+				     GSGPU_GPU_PAGE_SIZE, flags);
+		params->func(params, pt, pe_start, dst, nptes,
+			     GSGPU_GPU_PAGE_SIZE, flags);
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_bo_update_mapping - update a mapping in the vm page table
+ *
+ * @ldev: gsgpu_device pointer
+ * @exclusive: fence we need to sync to
+ * @pages_addr: DMA addresses to use for mapping
+ * @vm: requested vm
+ * @start: start of mapped range
+ * @last: last mapped entry
+ * @flags: flags for the entries
+ * @addr: addr to set the area to
+ * @fence: optional resulting fence
+ *
+ * Fill in the page table entries between @start and @last.
+ *
+ * Returns:
+ * 0 for success, -EINVAL for failure.
+ */
+static int gsgpu_vm_bo_update_mapping(struct gsgpu_device *ldev,
+				       struct dma_fence *exclusive,
+				       dma_addr_t *pages_addr,
+				       struct gsgpu_vm *vm,
+				       u64 start, u64 last,
+				       u64 flags, u64 addr,
+				       struct dma_fence **fence)
+{
+	struct gsgpu_ring *ring;
+	void *owner = GSGPU_FENCE_OWNER_VM;
+	unsigned nptes, ncmds, ndw;
+	struct gsgpu_job *job;
+	struct gsgpu_pte_update_params params;
+	struct dma_fence *f = NULL;
+	int r;
+
+	memset(&params, 0, sizeof(params));
+	params.ldev = ldev;
+	params.vm = vm;
+
+	/**
+	 * sync to everything on unmapping
+	 */
+	if (!(flags & GSGPU_PTE_PRESENT))
+		owner = GSGPU_FENCE_OWNER_UNDEFINED;
+
+	if (vm->use_cpu_for_update) {
+		/**
+		 * params.src is used as flag to indicate system Memory
+		 */
+		if (pages_addr)
+			params.src = ~0;
+
+		/**
+		 * Wait for PT BOs to be free. PTs share the same resv. object
+		 * as the root PD BO
+		 */
+		r = gsgpu_vm_wait_pd(ldev, vm, owner);
+		if (unlikely(r))
+			return r;
+
+		params.func = gsgpu_vm_cpu_set_ptes;
+		params.pages_addr = pages_addr;
+		return gsgpu_vm_update_ptes(&params, start, last + 1,
+					   addr, flags);
+	}
+
+	ring = container_of(vm->entity.rq->sched, struct gsgpu_ring, sched);
+
+	nptes = last - start + 1;
+
+	/**
+	 * reserve space for two commands every (1 << BLOCK_SIZE)
+	 * entries or 2k dwords (whatever is smaller)
+     *
+     * The second command is for the shadow pagetables.
+	 *
+	 * formula - gsgpu_vm_update_ptes
+	 */
+	if (vm->root.base.bo->shadow)
+		ncmds = ((nptes >> min(ldev->vm_manager.block_size, 11u)) + 2) * 2;
+	else
+		ncmds = ((nptes >> min(ldev->vm_manager.block_size, 11u)) + 2);
+
+	/* ib padding, default is 8 bytes, but reserved 64 bytes. */
+	ndw = 64;
+
+	if (pages_addr) {
+		/**
+		 * copy commands needed
+		 */
+		ndw += ncmds * ldev->vm_manager.vm_pte_funcs->copy_pte_num_dw;
+
+		/**
+		 * and also PTEs
+		 */
+		ndw += nptes * (gsgpu_get_pde_pte_size(ldev) / GSGPU_BYTES_PER_DW);
+
+		params.func = gsgpu_vm_do_copy_ptes;
+
+	} else {
+		/**
+		 * set page commands needed
+		 */
+		ndw += ncmds * ldev->vm_manager.vm_pte_funcs->set_pte_pde_num_dw;
+
+		params.func = gsgpu_vm_do_set_ptes;
+	}
+
+	r = gsgpu_job_alloc_with_ib(ldev, ndw * GSGPU_BYTES_PER_DW, &job);
+	if (r)
+		return r;
+
+	params.ib = &job->ibs[0];
+
+	if (pages_addr) {
+		u64 *pte;
+		unsigned i;
+
+		/*
+		 * Put the PTEs at the end of the IB
+		 */
+		i = ndw - nptes * (gsgpu_get_pde_pte_size(ldev) / GSGPU_BYTES_PER_DW);
+		pte = (u64 *)&(job->ibs->ptr[i]);
+		params.src = job->ibs->gpu_addr + i * GSGPU_BYTES_PER_DW;
+
+		for (i = 0; i < nptes; ++i) {
+			pte[i] = gsgpu_vm_map_gart(pages_addr, addr + i *
+						    GSGPU_GPU_PAGE_SIZE);
+			pte[i] |= flags;
+		}
+		addr = 0;
+	}
+
+	r = gsgpu_sync_fence(ldev, &job->sync, exclusive, false);
+	if (r)
+		goto error_free;
+
+	r = gsgpu_sync_resv(ldev, &job->sync, vm->root.base.bo->tbo.resv,
+			     owner, false);
+	if (r)
+		goto error_free;
+
+	r = reservation_object_reserve_shared(vm->root.base.bo->tbo.resv);
+	if (r)
+		goto error_free;
+
+	r = gsgpu_vm_update_ptes(&params, start, last + 1, addr, flags);
+	if (r)
+		goto error_free;
+
+	gsgpu_ring_pad_ib(ring, params.ib);
+	WARN_ON(params.ib->length_dw > ndw);
+	r = gsgpu_job_submit(job, &vm->entity, GSGPU_FENCE_OWNER_VM, &f);
+	if (r)
+		goto error_free;
+
+	gsgpu_bo_fence(vm->root.base.bo, f, true);
+	dma_fence_put(*fence);
+	*fence = f;
+
+	return 0;
+
+error_free:
+	gsgpu_job_free(job);
+	return r;
+}
+
+/**
+ * gsgpu_vm_bo_split_mapping - split a mapping into smaller chunks
+ *
+ * @ldev: gsgpu_device pointer
+ * @exclusive: fence we need to sync to
+ * @pages_addr: DMA addresses to use for mapping
+ * @vm: requested vm
+ * @mapping: mapped range and flags to use for the update
+ * @flags: HW flags for the mapping
+ * @nodes: array of drm_mm_nodes with the MC addresses
+ * @fence: optional resulting fence
+ *
+ * Split the mapping into smaller chunks so that each update fits
+ * into a SDMA IB.
+ *
+ * Returns:
+ * 0 for success, -EINVAL for failure.
+ */
+static int gsgpu_vm_bo_split_mapping(struct gsgpu_device *ldev,
+				      struct dma_fence *exclusive,
+				      dma_addr_t *pages_addr,
+				      struct gsgpu_vm *vm,
+				      struct gsgpu_bo_va_mapping *mapping,
+				      u64 flags,
+				      struct drm_mm_node *nodes,
+				      struct dma_fence **fence)
+{
+	u64 pfn, start = mapping->start;
+	int r;
+
+	if (!(mapping->flags & GSGPU_PTE_WRITEABLE))
+		flags &= ~GSGPU_PTE_WRITEABLE;
+
+	trace_gsgpu_vm_bo_update(mapping);
+	pfn = mapping->offset >> PAGE_SHIFT;
+
+	/* vram - find the node and pfn with mapping->offset */
+	if ((!pages_addr) && nodes) {
+		while (pfn >= nodes->size) {
+			pfn -= nodes->size;
+			++nodes;
+		}
+	}
+
+	do {
+		dma_addr_t *dma_addr;
+		u64 max_entries;
+		u64 addr, last;
+
+		if (pages_addr) {
+			addr = pfn << PAGE_SHIFT;
+			max_entries = GSGPU_VM_MAX_UPDATE_SIZE;
+			dma_addr = pages_addr;
+		} else if (nodes) {
+			addr = nodes->start << PAGE_SHIFT;
+			addr += pfn << PAGE_SHIFT;
+			addr += ldev->vm_manager.vram_base_offset;
+			max_entries = (nodes->size - pfn) * GSGPU_GPU_PAGES_IN_CPU_PAGE;
+			dma_addr = NULL;
+		} else {
+			addr = 0;
+			dma_addr = NULL;
+		}
+
+		last = min(mapping->last, start + max_entries - 1);
+		r = gsgpu_vm_bo_update_mapping(ldev, exclusive, dma_addr, vm, start, last, flags, addr, fence);
+		if (r)
+			return r;
+
+		pfn += (last - start + 1) / GSGPU_GPU_PAGES_IN_CPU_PAGE;
+
+		if ((!pages_addr) && nodes && nodes->size == pfn) {
+			pfn = 0;
+			++nodes;
+		}
+
+		start = last + 1;
+
+	} while (unlikely(start != mapping->last + 1));
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_bo_update - update all BO mappings in the vm page table
+ *
+ * @ldev: gsgpu_device pointer
+ * @bo_va: requested BO and VM object
+ * @clear: if true clear the entries
+ *
+ * Fill in the page table entries for @bo_va.
+ *
+ * Returns:
+ * 0 for success, -EINVAL for failure.
+ */
+int gsgpu_vm_bo_update(struct gsgpu_device *ldev,
+			struct gsgpu_bo_va *bo_va,
+			bool clear)
+{
+	struct gsgpu_bo *bo = bo_va->base.bo;
+	struct gsgpu_vm *vm = bo_va->base.vm;
+	struct gsgpu_bo_va_mapping *mapping;
+	dma_addr_t *pages_addr = NULL;
+	struct ttm_mem_reg *mem;
+	struct drm_mm_node *nodes;
+	struct dma_fence *exclusive, **last_update;
+	u64 flags;
+	int r;
+
+	if (clear || !bo) {
+		mem = NULL;
+		nodes = NULL;
+		exclusive = NULL;
+	} else {
+		struct ttm_dma_tt *ttm;
+
+		mem = &bo->tbo.mem;
+		nodes = mem->mm_node;
+		if (mem->mem_type == TTM_PL_TT) {
+			ttm = container_of(bo->tbo.ttm, struct ttm_dma_tt, ttm);
+			pages_addr = ttm->dma_address;
+		}
+
+		exclusive = reservation_object_get_excl(bo->tbo.resv);
+	}
+
+	if (bo)
+		flags = gsgpu_ttm_tt_pte_flags(ldev, bo->tbo.ttm, mem);
+	else
+		flags = 0x0;
+
+	if (bo && bo->flags & GSGPU_GEM_CREATE_COMPRESSED_MASK)
+		flags |= (bo->flags & GSGPU_GEM_CREATE_COMPRESSED_MASK)
+				  >> GSGPU_PTE_COMPRESSED_SHIFT;
+
+	if (clear || (bo && bo->tbo.resv == vm->root.base.bo->tbo.resv))
+		last_update = &vm->last_update;
+	else
+		last_update = &bo_va->last_pt_update;
+
+	if (!clear && bo_va->base.moved) {
+		bo_va->base.moved = false;
+		list_splice_init(&bo_va->valids, &bo_va->invalids);
+
+	} else if (bo_va->cleared != clear) {
+		list_splice_init(&bo_va->valids, &bo_va->invalids);
+	}
+
+	list_for_each_entry(mapping, &bo_va->invalids, list) {
+		r = gsgpu_vm_bo_split_mapping(ldev, exclusive, pages_addr, vm,
+					       mapping, flags, nodes,
+					       last_update);
+		if (r)
+			return r;
+	}
+
+
+	spin_lock(&vm->moved_lock);
+	list_del_init(&bo_va->base.vm_status);
+	spin_unlock(&vm->moved_lock);
+
+	/*
+	 * If the BO is not in its preferred location add it back to
+	 * the evicted list so that it gets validated again on the
+	 * next command submission.
+	 */
+	if (bo && bo->tbo.resv == vm->root.base.bo->tbo.resv) {
+		u32 mem_type = bo->tbo.mem.mem_type;
+
+		if (!(bo->preferred_domains & gsgpu_mem_type_to_domain(mem_type)))
+			list_add_tail(&bo_va->base.vm_status, &vm->evicted);
+		else
+			list_add(&bo_va->base.vm_status, &vm->idle);
+	}
+
+	list_splice_init(&bo_va->invalids, &bo_va->valids);
+	bo_va->cleared = clear;
+
+	if (trace_gsgpu_vm_bo_mapping_enabled()) {
+		list_for_each_entry(mapping, &bo_va->valids, list)
+			trace_gsgpu_vm_bo_mapping(mapping);
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_free_mapping - free a mapping
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: requested vm
+ * @mapping: mapping to be freed
+ * @fence: fence of the unmap operation
+ *
+ * Free a mapping and make sure we decrease the PRT usage count if applicable.
+ */
+static void gsgpu_vm_free_mapping(struct gsgpu_device *ldev,
+				   struct gsgpu_vm *vm,
+				   struct gsgpu_bo_va_mapping *mapping,
+				   struct dma_fence *fence)
+{
+	kfree(mapping);
+}
+
+/**
+ * gsgpu_vm_clear_freed - clear freed BOs in the PT
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: requested vm
+ * @fence: optional resulting fence (unchanged if no work needed to be done
+ * or if an error occurred)
+ *
+ * Make sure all freed BOs are cleared in the PT.
+ * PTs have to be reserved and mutex must be locked!
+ *
+ * Returns:
+ * 0 for success.
+ *
+ */
+int gsgpu_vm_clear_freed(struct gsgpu_device *ldev,
+			  struct gsgpu_vm *vm,
+			  struct dma_fence **fence)
+{
+	struct gsgpu_bo_va_mapping *mapping;
+	u64 init_pte_value = 0;
+	struct dma_fence *f = NULL;
+	int r;
+
+	while (!list_empty(&vm->freed)) {
+		mapping = list_first_entry(&vm->freed,
+			struct gsgpu_bo_va_mapping, list);
+		list_del(&mapping->list);
+
+		r = gsgpu_vm_bo_update_mapping(ldev, NULL, NULL, vm,
+						mapping->start, mapping->last,
+						init_pte_value, 0, &f);
+		gsgpu_vm_free_mapping(ldev, vm, mapping, f);
+		if (r) {
+			dma_fence_put(f);
+			return r;
+		}
+	}
+
+	if (fence && f) {
+		dma_fence_put(*fence);
+		*fence = f;
+	} else {
+		dma_fence_put(f);
+	}
+
+	return 0;
+
+}
+
+/**
+ * gsgpu_vm_handle_moved - handle moved BOs in the PT
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: requested vm
+ *
+ * Make sure all BOs which are moved are updated in the PTs.
+ *
+ * Returns:
+ * 0 for success.
+ *
+ * PTs have to be reserved!
+ */
+int gsgpu_vm_handle_moved(struct gsgpu_device *ldev,
+			   struct gsgpu_vm *vm)
+{
+	struct gsgpu_bo_va *bo_va, *tmp;
+	struct list_head moved;
+	bool clear;
+	int r;
+
+	INIT_LIST_HEAD(&moved);
+	spin_lock(&vm->moved_lock);
+	list_splice_init(&vm->moved, &moved);
+	spin_unlock(&vm->moved_lock);
+
+	list_for_each_entry_safe(bo_va, tmp, &moved, base.vm_status) {
+		struct reservation_object *resv = bo_va->base.bo->tbo.resv;
+
+		/* Per VM BOs never need to bo cleared in the page tables */
+		if (resv == vm->root.base.bo->tbo.resv)
+			clear = false;
+		/* Try to reserve the BO to avoid clearing its ptes */
+		else if (!gsgpu_vm_debug && reservation_object_trylock(resv))
+			clear = false;
+		/* Somebody else is using the BO right now */
+		else
+			clear = true;
+
+		r = gsgpu_vm_bo_update(ldev, bo_va, clear);
+		if (r) {
+			spin_lock(&vm->moved_lock);
+			list_splice(&moved, &vm->moved);
+			spin_unlock(&vm->moved_lock);
+			return r;
+		}
+
+		if (!clear && resv != vm->root.base.bo->tbo.resv)
+			reservation_object_unlock(resv);
+
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_bo_add - add a bo to a specific vm
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: requested vm
+ * @bo: gsgpu buffer object
+ *
+ * Add @bo into the requested vm.
+ * Add @bo to the list of bos associated with the vm
+ *
+ * Returns:
+ * Newly added bo_va or NULL for failure
+ *
+ * Object has to be reserved!
+ */
+struct gsgpu_bo_va *gsgpu_vm_bo_add(struct gsgpu_device *ldev,
+				      struct gsgpu_vm *vm,
+				      struct gsgpu_bo *bo)
+{
+	struct gsgpu_bo_va *bo_va;
+
+	bo_va = kzalloc(sizeof(struct gsgpu_bo_va), GFP_KERNEL);
+	if (bo_va == NULL) {
+		return NULL;
+	}
+	gsgpu_vm_bo_base_init(&bo_va->base, vm, bo);
+
+	bo_va->ref_count = 1;
+	INIT_LIST_HEAD(&bo_va->valids);
+	INIT_LIST_HEAD(&bo_va->invalids);
+
+	return bo_va;
+}
+
+
+/**
+ * gsgpu_vm_bo_insert_mapping - insert a new mapping
+ *
+ * @ldev: gsgpu_device pointer
+ * @bo_va: bo_va to store the address
+ * @mapping: the mapping to insert
+ *
+ * Insert a new mapping into all structures.
+ */
+static void gsgpu_vm_bo_insert_map(struct gsgpu_device *ldev,
+				    struct gsgpu_bo_va *bo_va,
+				    struct gsgpu_bo_va_mapping *mapping)
+{
+	struct gsgpu_vm *vm = bo_va->base.vm;
+	struct gsgpu_bo *bo = bo_va->base.bo;
+
+	mapping->bo_va = bo_va;
+	list_add(&mapping->list, &bo_va->invalids);
+	gsgpu_vm_it_insert(mapping, &vm->va);
+
+	if (bo && bo->tbo.resv == vm->root.base.bo->tbo.resv &&
+	    !bo_va->base.moved) {
+		spin_lock(&vm->moved_lock);
+		list_move(&bo_va->base.vm_status, &vm->moved);
+		spin_unlock(&vm->moved_lock);
+	}
+	trace_gsgpu_vm_bo_map(bo_va, mapping);
+}
+
+/**
+ * gsgpu_vm_bo_map - map bo inside a vm
+ *
+ * @ldev: gsgpu_device pointer
+ * @bo_va: bo_va to store the address
+ * @saddr: where to map the BO
+ * @offset: requested offset in the BO
+ * @size: BO size in bytes
+ * @flags: attributes of pages (read/write/valid/etc.)
+ *
+ * Add a mapping of the BO at the specefied addr into the VM.
+ *
+ * Returns:
+ * 0 for success, error for failure.
+ *
+ * Object has to be reserved and unreserved outside!
+ */
+int gsgpu_vm_bo_map(struct gsgpu_device *ldev,
+		     struct gsgpu_bo_va *bo_va,
+		     u64 saddr, u64 offset,
+		     u64 size, u64 flags)
+{
+	struct gsgpu_bo_va_mapping *mapping, *tmp;
+	struct gsgpu_bo *bo = bo_va->base.bo;
+	struct gsgpu_vm *vm = bo_va->base.vm;
+	u64 eaddr;
+
+	/* validate the parameters */
+	if (saddr & GSGPU_GPU_PAGE_MASK || offset & GSGPU_GPU_PAGE_MASK ||
+	    size == 0 || size & GSGPU_GPU_PAGE_MASK)
+		return -EINVAL;
+
+	/* make sure object fit at this offset */
+	eaddr = saddr + size - 1;
+	if (saddr >= eaddr ||
+	    (bo && offset + size > gsgpu_bo_size(bo)))
+		return -EINVAL;
+
+	saddr /= GSGPU_GPU_PAGE_SIZE;
+	eaddr /= GSGPU_GPU_PAGE_SIZE;
+
+	tmp = gsgpu_vm_it_iter_first(&vm->va, saddr, eaddr);
+	if (tmp) {
+		/* bo and tmp overlap, invalid addr */
+		dev_err(ldev->dev, "bo %p va 0x%010Lx-0x%010Lx conflict with "
+			"0x%010Lx-0x%010Lx\n", bo, saddr, eaddr,
+			tmp->start, tmp->last + 1);
+		return -EINVAL;
+	}
+
+	mapping = kmalloc(sizeof(*mapping), GFP_KERNEL);
+	if (!mapping)
+		return -ENOMEM;
+
+	mapping->start = saddr;
+	mapping->last = eaddr;
+	mapping->offset = offset;
+	mapping->flags = flags;
+
+	gsgpu_vm_bo_insert_map(ldev, bo_va, mapping);
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_bo_replace_map - map bo inside a vm, replacing existing mappings
+ *
+ * @ldev: gsgpu_device pointer
+ * @bo_va: bo_va to store the address
+ * @saddr: where to map the BO
+ * @offset: requested offset in the BO
+ * @size: BO size in bytes
+ * @flags: attributes of pages (read/write/valid/etc.)
+ *
+ * Add a mapping of the BO at the specefied addr into the VM. Replace existing
+ * mappings as we do so.
+ *
+ * Returns:
+ * 0 for success, error for failure.
+ *
+ * Object has to be reserved and unreserved outside!
+ */
+int gsgpu_vm_bo_replace_map(struct gsgpu_device *ldev,
+			     struct gsgpu_bo_va *bo_va,
+			     u64 saddr, u64 offset,
+			     u64 size, u64 flags)
+{
+	struct gsgpu_bo_va_mapping *mapping;
+	struct gsgpu_bo *bo = bo_va->base.bo;
+	u64 eaddr;
+	int r;
+
+	/* validate the parameters */
+	if (saddr & GSGPU_GPU_PAGE_MASK || offset & GSGPU_GPU_PAGE_MASK ||
+	    size == 0 || size & GSGPU_GPU_PAGE_MASK)
+		return -EINVAL;
+
+	/* make sure object fit at this offset */
+	eaddr = saddr + size - 1;
+	if (saddr >= eaddr ||
+	    (bo && offset + size > gsgpu_bo_size(bo)))
+		return -EINVAL;
+
+	/* Allocate all the needed memory */
+	mapping = kmalloc(sizeof(*mapping), GFP_KERNEL);
+	if (!mapping)
+		return -ENOMEM;
+
+	r = gsgpu_vm_bo_clear_mappings(ldev, bo_va->base.vm, saddr, size);
+	if (r) {
+		kfree(mapping);
+		return r;
+	}
+
+	saddr /= GSGPU_GPU_PAGE_SIZE;
+	eaddr /= GSGPU_GPU_PAGE_SIZE;
+
+	mapping->start = saddr;
+	mapping->last = eaddr;
+	mapping->offset = offset;
+	mapping->flags = flags;
+
+	gsgpu_vm_bo_insert_map(ldev, bo_va, mapping);
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_bo_unmap - remove bo mapping from vm
+ *
+ * @ldev: gsgpu_device pointer
+ * @bo_va: bo_va to remove the address from
+ * @saddr: where to the BO is mapped
+ *
+ * Remove a mapping of the BO at the specefied addr from the VM.
+ *
+ * Returns:
+ * 0 for success, error for failure.
+ *
+ * Object has to be reserved and unreserved outside!
+ */
+int gsgpu_vm_bo_unmap(struct gsgpu_device *ldev,
+		       struct gsgpu_bo_va *bo_va,
+		       u64 saddr)
+{
+	struct gsgpu_bo_va_mapping *mapping;
+	struct gsgpu_vm *vm = bo_va->base.vm;
+	bool valid = true;
+
+	saddr /= GSGPU_GPU_PAGE_SIZE;
+
+	list_for_each_entry(mapping, &bo_va->valids, list) {
+		if (mapping->start == saddr)
+			break;
+	}
+
+	if (&mapping->list == &bo_va->valids) {
+		valid = false;
+
+		list_for_each_entry(mapping, &bo_va->invalids, list) {
+			if (mapping->start == saddr)
+				break;
+		}
+
+		if (&mapping->list == &bo_va->invalids)
+			return -ENOENT;
+	}
+
+	list_del(&mapping->list);
+	gsgpu_vm_it_remove(mapping, &vm->va);
+	mapping->bo_va = NULL;
+	trace_gsgpu_vm_bo_unmap(bo_va, mapping);
+
+	if (valid)
+		list_add(&mapping->list, &vm->freed);
+	else
+		gsgpu_vm_free_mapping(ldev, vm, mapping,
+				       bo_va->last_pt_update);
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_bo_clear_mappings - remove all mappings in a specific range
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: VM structure to use
+ * @saddr: start of the range
+ * @size: size of the range
+ *
+ * Remove all mappings in a range, split them as appropriate.
+ *
+ * Returns:
+ * 0 for success, error for failure.
+ */
+int gsgpu_vm_bo_clear_mappings(struct gsgpu_device *ldev,
+				struct gsgpu_vm *vm,
+				u64 saddr, u64 size)
+{
+	struct gsgpu_bo_va_mapping *before, *after, *tmp, *next;
+	LIST_HEAD(removed);
+	u64 eaddr;
+
+	eaddr = saddr + size - 1;
+	saddr /= GSGPU_GPU_PAGE_SIZE;
+	eaddr /= GSGPU_GPU_PAGE_SIZE;
+
+	/* Allocate all the needed memory */
+	before = kzalloc(sizeof(*before), GFP_KERNEL);
+	if (!before)
+		return -ENOMEM;
+	INIT_LIST_HEAD(&before->list);
+
+	after = kzalloc(sizeof(*after), GFP_KERNEL);
+	if (!after) {
+		kfree(before);
+		return -ENOMEM;
+	}
+	INIT_LIST_HEAD(&after->list);
+
+	/* Now gather all removed mappings */
+	tmp = gsgpu_vm_it_iter_first(&vm->va, saddr, eaddr);
+	while (tmp) {
+		/* Remember mapping split at the start */
+		if (tmp->start < saddr) {
+			before->start = tmp->start;
+			before->last = saddr - 1;
+			before->offset = tmp->offset;
+			before->flags = tmp->flags;
+			before->bo_va = tmp->bo_va;
+			list_add(&before->list, &tmp->bo_va->invalids);
+		}
+
+		/* Remember mapping split at the end */
+		if (tmp->last > eaddr) {
+			after->start = eaddr + 1;
+			after->last = tmp->last;
+			after->offset = tmp->offset;
+			after->offset += after->start - tmp->start;
+			after->flags = tmp->flags;
+			after->bo_va = tmp->bo_va;
+			list_add(&after->list, &tmp->bo_va->invalids);
+		}
+
+		list_del(&tmp->list);
+		list_add(&tmp->list, &removed);
+
+		tmp = gsgpu_vm_it_iter_next(tmp, saddr, eaddr);
+	}
+
+	/* And free them up */
+	list_for_each_entry_safe(tmp, next, &removed, list) {
+		gsgpu_vm_it_remove(tmp, &vm->va);
+		list_del(&tmp->list);
+
+		if (tmp->start < saddr)
+		    tmp->start = saddr;
+		if (tmp->last > eaddr)
+		    tmp->last = eaddr;
+
+		tmp->bo_va = NULL;
+		list_add(&tmp->list, &vm->freed);
+		trace_gsgpu_vm_bo_unmap(NULL, tmp);
+	}
+
+	/* Insert partial mapping before the range */
+	if (!list_empty(&before->list)) {
+		gsgpu_vm_it_insert(before, &vm->va);
+	} else {
+		kfree(before);
+	}
+
+	/* Insert partial mapping after the range */
+	if (!list_empty(&after->list)) {
+		gsgpu_vm_it_insert(after, &vm->va);
+	} else {
+		kfree(after);
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_bo_lookup_mapping - find mapping by address
+ *
+ * @vm: the requested VM
+ * @addr: the address
+ *
+ * Find a mapping by it's address.
+ *
+ * Returns:
+ * The gsgpu_bo_va_mapping matching for addr or NULL
+ *
+ */
+struct gsgpu_bo_va_mapping *gsgpu_vm_bo_lookup_mapping(struct gsgpu_vm *vm,
+							 u64 addr)
+{
+	return gsgpu_vm_it_iter_first(&vm->va, addr, addr);
+}
+
+/**
+ * gsgpu_vm_bo_trace_cs - trace all reserved mappings
+ *
+ * @vm: the requested vm
+ * @ticket: CS ticket
+ *
+ * Trace all mappings of BOs reserved during a command submission.
+ */
+void gsgpu_vm_bo_trace_cs(struct gsgpu_vm *vm, struct ww_acquire_ctx *ticket)
+{
+	struct gsgpu_bo_va_mapping *mapping;
+
+	if (!trace_gsgpu_vm_bo_cs_enabled())
+		return;
+
+	for (mapping = gsgpu_vm_it_iter_first(&vm->va, 0, U64_MAX); mapping;
+	     mapping = gsgpu_vm_it_iter_next(mapping, 0, U64_MAX)) {
+		if (mapping->bo_va && mapping->bo_va->base.bo) {
+			struct gsgpu_bo *bo;
+
+			bo = mapping->bo_va->base.bo;
+			if (READ_ONCE(bo->tbo.resv->lock.ctx) != ticket)
+				continue;
+		}
+
+		trace_gsgpu_vm_bo_cs(mapping);
+	}
+}
+
+/**
+ * gsgpu_vm_bo_rmv - remove a bo to a specific vm
+ *
+ * @ldev: gsgpu_device pointer
+ * @bo_va: requested bo_va
+ *
+ * Remove @bo_va->bo from the requested vm.
+ *
+ * Object have to be reserved!
+ */
+void gsgpu_vm_bo_rmv(struct gsgpu_device *ldev,
+		      struct gsgpu_bo_va *bo_va)
+{
+	struct gsgpu_bo_va_mapping *mapping, *next;
+	struct gsgpu_vm *vm = bo_va->base.vm;
+
+	list_del(&bo_va->base.bo_list);
+
+	spin_lock(&vm->moved_lock);
+	list_del(&bo_va->base.vm_status);
+	spin_unlock(&vm->moved_lock);
+
+	list_for_each_entry_safe(mapping, next, &bo_va->valids, list) {
+		list_del(&mapping->list);
+		gsgpu_vm_it_remove(mapping, &vm->va);
+		mapping->bo_va = NULL;
+		trace_gsgpu_vm_bo_unmap(bo_va, mapping);
+		list_add(&mapping->list, &vm->freed);
+	}
+	list_for_each_entry_safe(mapping, next, &bo_va->invalids, list) {
+		list_del(&mapping->list);
+		gsgpu_vm_it_remove(mapping, &vm->va);
+		gsgpu_vm_free_mapping(ldev, vm, mapping,
+				       bo_va->last_pt_update);
+	}
+
+	dma_fence_put(bo_va->last_pt_update);
+	kfree(bo_va);
+}
+
+/**
+ * gsgpu_vm_bo_invalidate - mark the bo as invalid
+ *
+ * @ldev: gsgpu_device pointer
+ * @bo: gsgpu buffer object
+ * @evicted: is the BO evicted
+ *
+ * Mark @bo as invalid.
+ */
+void gsgpu_vm_bo_invalidate(struct gsgpu_device *ldev,
+			     struct gsgpu_bo *bo, bool evicted)
+{
+	struct gsgpu_vm_bo_base *bo_base;
+
+	/* shadow bo doesn't have bo base, its validation needs its parent */
+	if (bo->parent && bo->parent->shadow == bo)
+		bo = bo->parent;
+
+	list_for_each_entry(bo_base, &bo->va, bo_list) {
+		struct gsgpu_vm *vm = bo_base->vm;
+		bool was_moved = bo_base->moved;
+
+		bo_base->moved = true;
+		if (evicted && bo->tbo.resv == vm->root.base.bo->tbo.resv) {
+			if (bo->tbo.type == ttm_bo_type_kernel)
+				list_move(&bo_base->vm_status, &vm->evicted);
+			else
+				list_move_tail(&bo_base->vm_status,
+					       &vm->evicted);
+			continue;
+		}
+
+		if (was_moved)
+			continue;
+
+		if (bo->tbo.type == ttm_bo_type_kernel) {
+			list_move(&bo_base->vm_status, &vm->relocated);
+		} else {
+			spin_lock(&bo_base->vm->moved_lock);
+			list_move(&bo_base->vm_status, &vm->moved);
+			spin_unlock(&bo_base->vm->moved_lock);
+		}
+	}
+}
+
+/**
+ * gsgpu_vm_get_block_size - calculate the shift of PTEs a block contains
+ *
+ * @vm_size: VM size in GB
+ *
+ * Returns:
+ * The shift of PTEs a block contains
+ */
+static u32 gsgpu_vm_get_block_size(u64 vm_size)
+{
+	(void)(vm_size);
+	return GSGPU_PAGE_PTE_SHIFT;
+}
+
+/**
+ * gsgpu_vm_adjust_size - adjust vm size, block size
+ *
+ * @ldev: gsgpu_device pointer
+ * @min_vm_size: the minimum vm size in GB if it's set auto
+ * @max_level: max page table levels
+ * @max_bits: max address space size in bits
+ *
+ */
+void gsgpu_vm_adjust_size(struct gsgpu_device *ldev, u32 min_vm_size,
+			  unsigned max_level, unsigned max_bits)
+{
+	unsigned int vm_size;
+	u64 tmp;
+
+#if 0
+	unsigned int max_size = 1 << (max_bits - 30);
+
+	struct sysinfo si;
+	unsigned int phys_ram_gb;
+
+	/* Optimal VM size depends on the amount of physical
+	 * RAM available. Underlying requirements and
+	 * assumptions:
+	 *
+	 *  - Need to map system memory and VRAM from all GPUs
+	 *     - VRAM from other GPUs not known here
+	 *     - Assume VRAM <= system memory
+	 *  - On GFX8 and older, VM space can be segmented for
+	 *    different MTYPEs
+	 *  - Need to allow room for fragmentation, guard pages etc.
+	 *
+	 * This adds up to a rough guess of system memory x3.
+	 * Round up to power of two to maximize the available
+	 * VM size with the given page table size.
+	 */
+	si_meminfo(&si);
+	phys_ram_gb = ((uint64_t)si.totalram * si.mem_unit +
+		       (1 << 30) - 1) >> 30;
+	vm_size = roundup_pow_of_two(min(max(phys_ram_gb * 3, min_vm_size), max_size));
+
+#else
+	vm_size = 1 << (max_bits - GSGPU_GB_SHIFT_BITS);
+
+#endif
+	ldev->vm_manager.pde_pte_bytes = GSGPU_VM_PDE_PTE_BYTES;
+	ldev->vm_manager.max_pfn = (u64)vm_size <<
+				(GSGPU_GB_SHIFT_BITS - GSGPU_GPU_PAGE_SHIFT);
+
+	tmp = roundup_pow_of_two(ldev->vm_manager.max_pfn);
+	if (gsgpu_vm_block_size != -1)
+		tmp >>= gsgpu_vm_block_size - GSGPU_PAGE_PTE_SHIFT;
+
+	tmp = DIV_ROUND_UP(fls64(tmp), GSGPU_PAGE_PTE_SHIFT);
+	ldev->vm_manager.num_level = min(max_level, (unsigned)tmp);
+	ldev->vm_manager.root_level = GSGPU_VM_DIR0;
+	ldev->vm_manager.dir2_width = GSGPU_PAGE_PTE_SHIFT;
+	ldev->vm_manager.dir2_shift = GSGPU_GPU_PAGE_SHIFT;
+	ldev->vm_manager.dir1_shift = ldev->vm_manager.dir2_shift + ldev->vm_manager.dir2_width;
+	ldev->vm_manager.dir1_width = GSGPU_PAGE_PTE_SHIFT;
+	ldev->vm_manager.dir0_shift = ldev->vm_manager.dir1_shift + ldev->vm_manager.dir1_width;
+	ldev->vm_manager.dir0_width = max_bits - ldev->vm_manager.dir0_shift;
+
+	/* block size depends on vm size and hw setup*/
+	if (gsgpu_vm_block_size != -1)
+		ldev->vm_manager.block_size =
+			min((unsigned)gsgpu_vm_block_size, (unsigned)GSGPU_PAGE_PTE_SHIFT);
+	else if (ldev->vm_manager.num_level > 1)
+		ldev->vm_manager.block_size = GSGPU_PAGE_PTE_SHIFT;
+	else
+		ldev->vm_manager.block_size = gsgpu_vm_get_block_size(vm_size);
+
+	DRM_INFO("vm size is %u GB, %u levels, block size is %u-bit\n",
+		 vm_size, ldev->vm_manager.num_level + 1,
+		 ldev->vm_manager.block_size);
+}
+
+/**
+ * gsgpu_vm_init - initialize a vm instance
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: requested vm
+ * @vm_context: Indicates if it GFX or Compute context
+ * @pasid: Process address space identifier
+ *
+ * Init @vm fields.
+ *
+ * Returns:
+ * 0 for success, error for failure.
+ */
+int gsgpu_vm_init(struct gsgpu_device *ldev, struct gsgpu_vm *vm,
+		   int vm_context, unsigned int pasid)
+{
+	struct gsgpu_bo_param bp;
+	struct gsgpu_bo *root;
+	const unsigned align = min((unsigned)GSGPU_VM_PTB_ALIGN_SIZE,
+		GSGPU_VM_PTE_COUNT(ldev) * gsgpu_get_pde_pte_size(ldev));
+	unsigned ring_instance;
+	struct gsgpu_ring *ring;
+	struct drm_sched_rq *rq;
+	unsigned long size;
+	u64 flags;
+	int r;
+
+	vm->va = RB_ROOT_CACHED;
+	vm->reserved_vmid = NULL;
+	INIT_LIST_HEAD(&vm->evicted);
+	INIT_LIST_HEAD(&vm->relocated);
+	spin_lock_init(&vm->moved_lock);
+	INIT_LIST_HEAD(&vm->moved);
+	INIT_LIST_HEAD(&vm->idle);
+	INIT_LIST_HEAD(&vm->freed);
+
+	/* create scheduler entity for page table updates */
+
+	ring_instance = atomic_inc_return(&ldev->vm_manager.vm_pte_next_ring);
+	ring_instance %= ldev->vm_manager.vm_pte_num_rings;
+	ring = ldev->vm_manager.vm_pte_rings[ring_instance];
+	rq = &ring->sched.sched_rq[DRM_SCHED_PRIORITY_KERNEL];
+	r = drm_sched_entity_init(&vm->entity, &rq, 1, NULL);
+	if (r)
+		return r;
+
+	vm->pte_support_ats = false;
+
+	vm->use_cpu_for_update = ldev->vm_manager.vm_update_mode;
+
+	DRM_DEBUG_DRIVER("VM update mode is %s\n",
+			 vm->use_cpu_for_update ? "CPU" : "XDMA");
+	WARN_ONCE((vm->use_cpu_for_update & !gsgpu_gmc_vram_full_visible(&ldev->gmc)),
+		  "CPU update of VM recommended only for large BAR system\n");
+	vm->last_update = NULL;
+
+	flags = GSGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	if (vm->use_cpu_for_update)
+		flags |= GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;
+	else if (vm_context != GSGPU_VM_CONTEXT_COMPUTE)
+		flags |= GSGPU_GEM_CREATE_SHADOW;
+
+	size = gsgpu_vm_bo_size(ldev, ldev->vm_manager.root_level);
+	memset(&bp, 0, sizeof(bp));
+	bp.size = size;
+	bp.byte_align = align;
+	bp.domain = GSGPU_GEM_DOMAIN_VRAM;
+	bp.flags = flags;
+	bp.type = ttm_bo_type_kernel;
+	bp.resv = NULL;
+	r = gsgpu_bo_create(ldev, &bp, &root);
+	if (r)
+		goto error_free_sched_entity;
+
+	r = gsgpu_bo_reserve(root, true);
+	if (r)
+		goto error_free_root;
+
+	r = gsgpu_vm_clear_bo(ldev, vm, root,
+			       ldev->vm_manager.root_level);
+	if (r)
+		goto error_unreserve;
+
+	gsgpu_vm_bo_base_init(&vm->root.base, vm, root);
+	gsgpu_bo_unreserve(vm->root.base.bo);
+
+	if (pasid) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&ldev->vm_manager.pasid_lock, flags);
+		r = idr_alloc(&ldev->vm_manager.pasid_idr, vm, pasid, pasid + 1,
+			      GFP_ATOMIC);
+		spin_unlock_irqrestore(&ldev->vm_manager.pasid_lock, flags);
+		if (r < 0)
+			goto error_free_root;
+
+		vm->pasid = pasid;
+	}
+
+	INIT_KFIFO(vm->faults);
+	vm->fault_credit = 16;
+
+	return 0;
+
+error_unreserve:
+	gsgpu_bo_unreserve(vm->root.base.bo);
+
+error_free_root:
+	gsgpu_bo_unref(&vm->root.base.bo->shadow);
+	gsgpu_bo_unref(&vm->root.base.bo);
+	vm->root.base.bo = NULL;
+
+error_free_sched_entity:
+	drm_sched_entity_destroy(&vm->entity);
+
+	return r;
+}
+
+/**
+ * gsgpu_vm_free_levels - free PD/PT levels
+ *
+ * @ldev: gsgpu device structure
+ * @parent: PD/PT starting level to free
+ * @level: level of parent structure
+ *
+ * Free the page directory or page table level and all sub levels.
+ */
+static void gsgpu_vm_free_levels(struct gsgpu_device *ldev,
+				  struct gsgpu_vm_pt *parent,
+				  unsigned level)
+{
+	unsigned i, num_entries = gsgpu_vm_num_entries(ldev, level);
+
+	if (parent->base.bo) {
+		list_del(&parent->base.bo_list);
+		list_del(&parent->base.vm_status);
+		gsgpu_bo_unref(&parent->base.bo->shadow);
+		gsgpu_bo_unref(&parent->base.bo);
+	}
+
+	if (parent->entries)
+		for (i = 0; i < num_entries; i++)
+			gsgpu_vm_free_levels(ldev, &parent->entries[i],
+					      level + 1);
+
+	kvfree(parent->entries);
+}
+
+/**
+ * gsgpu_vm_fini - tear down a vm instance
+ *
+ * @ldev: gsgpu_device pointer
+ * @vm: requested vm
+ *
+ * Tear down @vm.
+ * Unbind the VM and remove all bos from the vm bo list
+ */
+void gsgpu_vm_fini(struct gsgpu_device *ldev, struct gsgpu_vm *vm)
+{
+	struct gsgpu_bo_va_mapping *mapping, *tmp;
+	struct gsgpu_bo *root;
+	int r;
+
+	if (vm->pasid) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&ldev->vm_manager.pasid_lock, flags);
+		idr_remove(&ldev->vm_manager.pasid_idr, vm->pasid);
+		spin_unlock_irqrestore(&ldev->vm_manager.pasid_lock, flags);
+	}
+
+	drm_sched_entity_destroy(&vm->entity);
+
+	if (!RB_EMPTY_ROOT(&vm->va.rb_root)) {
+		dev_err(ldev->dev, "still active bo inside vm\n");
+	}
+	rbtree_postorder_for_each_entry_safe(mapping, tmp,
+					     &vm->va.rb_root, rb) {
+		list_del(&mapping->list);
+		gsgpu_vm_it_remove(mapping, &vm->va);
+		kfree(mapping);
+	}
+	list_for_each_entry_safe(mapping, tmp, &vm->freed, list) {
+		list_del(&mapping->list);
+		gsgpu_vm_free_mapping(ldev, vm, mapping, NULL);
+	}
+
+	root = gsgpu_bo_ref(vm->root.base.bo);
+	r = gsgpu_bo_reserve(root, true);
+	if (r) {
+		dev_err(ldev->dev, "Leaking page tables because BO reservation failed\n");
+	} else {
+		gsgpu_vm_free_levels(ldev, &vm->root,
+				      ldev->vm_manager.root_level);
+		gsgpu_bo_unreserve(root);
+	}
+	gsgpu_bo_unref(&root);
+	dma_fence_put(vm->last_update);
+	gsgpu_vmid_free_reserved(ldev, vm);
+
+	gsgpu_sema_free(ldev, vm);
+}
+
+/**
+ * gsgpu_vm_pasid_fault_credit - Check fault credit for given PASID
+ *
+ * @ldev: gsgpu_device pointer
+ * @pasid: PASID do identify the VM
+ *
+ * This function is expected to be called in interrupt context.
+ *
+ * Returns:
+ * True if there was fault credit, false otherwise
+ */
+bool gsgpu_vm_pasid_fault_credit(struct gsgpu_device *ldev,
+				  unsigned int pasid)
+{
+	struct gsgpu_vm *vm;
+
+	spin_lock(&ldev->vm_manager.pasid_lock);
+	vm = idr_find(&ldev->vm_manager.pasid_idr, pasid);
+	if (!vm) {
+		/* VM not found, can't track fault credit */
+		spin_unlock(&ldev->vm_manager.pasid_lock);
+		return true;
+	}
+
+	/* No lock needed. only accessed by IRQ handler */
+	if (!vm->fault_credit) {
+		/* Too many faults in this VM */
+		spin_unlock(&ldev->vm_manager.pasid_lock);
+		return false;
+	}
+
+	vm->fault_credit--;
+	spin_unlock(&ldev->vm_manager.pasid_lock);
+	return true;
+}
+
+/**
+ * gsgpu_vm_manager_init - init the VM manager
+ *
+ * @ldev: gsgpu_device pointer
+ *
+ * Initialize the VM manager structures
+ */
+void gsgpu_vm_manager_init(struct gsgpu_device *ldev)
+{
+	unsigned i;
+
+	gsgpu_vmid_mgr_init(ldev);
+
+	ldev->vm_manager.fence_context =
+		dma_fence_context_alloc(GSGPU_MAX_RINGS);
+
+	for (i = 0; i < GSGPU_MAX_RINGS; ++i)
+		ldev->vm_manager.seqno[i] = 0;
+
+	atomic_set(&ldev->vm_manager.vm_pte_next_ring, 0);
+	spin_lock_init(&ldev->vm_manager.prt_lock);
+	atomic_set(&ldev->vm_manager.num_prt_users, 0);
+
+	if (gsgpu_vm_update_mode == -1)
+		if (gsgpu_gmc_vram_full_visible(&ldev->gmc))
+			ldev->vm_manager.vm_update_mode = 1;
+		else
+			ldev->vm_manager.vm_update_mode = 0;
+	else
+		ldev->vm_manager.vm_update_mode = gsgpu_vm_update_mode;
+
+	idr_init(&ldev->vm_manager.pasid_idr);
+	spin_lock_init(&ldev->vm_manager.pasid_lock);
+}
+
+/**
+ * gsgpu_vm_manager_fini - cleanup VM manager
+ *
+ * @ldev: gsgpu_device pointer
+ *
+ * Cleanup the VM manager and free resources.
+ */
+void gsgpu_vm_manager_fini(struct gsgpu_device *ldev)
+{
+	WARN_ON(!idr_is_empty(&ldev->vm_manager.pasid_idr));
+	idr_destroy(&ldev->vm_manager.pasid_idr);
+
+	gsgpu_vmid_mgr_fini(ldev);
+}
+
+/**
+ * gsgpu_vm_ioctl - Manages VMID reservation.
+ *
+ * @dev: drm device pointer
+ * @data: drm_gsgpu_vm
+ * @filp: drm file pointer
+ *
+ * Returns:
+ * 0 for success, -errno for errors.
+ */
+int gsgpu_vm_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)
+{
+	union drm_gsgpu_vm *args = data;
+	struct gsgpu_device *ldev = dev->dev_private;
+	struct gsgpu_fpriv *fpriv = filp->driver_priv;
+	int r;
+
+	switch (args->in.op) {
+	case GSGPU_VM_OP_RESERVE_VMID:
+		/* current, we only have requirement to reserve vmid */
+		r = gsgpu_vmid_alloc_reserved(ldev, &fpriv->vm);
+		if (r)
+			return r;
+		break;
+	case GSGPU_VM_OP_UNRESERVE_VMID:
+		gsgpu_vmid_free_reserved(ldev, &fpriv->vm);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_vm_get_task_info - Extracts task info for a PASID.
+ *
+ * @dev: drm device pointer
+ * @pasid: PASID identifier for VM
+ * @task_info: task_info to fill.
+ */
+void gsgpu_vm_get_task_info(struct gsgpu_device *ldev, unsigned int pasid,
+			 struct gsgpu_task_info *task_info)
+{
+	struct gsgpu_vm *vm;
+	unsigned long flags;
+
+	spin_lock_irqsave(&ldev->vm_manager.pasid_lock, flags);
+
+	vm = idr_find(&ldev->vm_manager.pasid_idr, pasid);
+	if (vm)
+		*task_info = vm->task_info;
+
+	spin_unlock_irqrestore(&ldev->vm_manager.pasid_lock, flags);
+}
+
+/**
+ * gsgpu_vm_set_task_info - Sets VMs task info.
+ *
+ * @vm: vm for which to set the info
+ */
+void gsgpu_vm_set_task_info(struct gsgpu_vm *vm)
+{
+	if (!vm->task_info.pid) {
+		vm->task_info.pid = current->pid;
+		get_task_comm(vm->task_info.task_name, current);
+
+		if (current->group_leader->mm == current->mm) {
+			vm->task_info.tgid = current->group_leader->pid;
+			get_task_comm(vm->task_info.process_name, current->group_leader);
+		}
+	}
+}
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm_it.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm_it.c
new file mode 100644
index 000000000000..5baf285ec512
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_vm_it.c
@@ -0,0 +1,9 @@
+#include <linux/interval_tree_generic.h>
+#include "gsgpu_vm_it.h"
+
+#define START(node) ((node)->start)
+#define LAST(node) ((node)->last)
+
+INTERVAL_TREE_DEFINE(struct gsgpu_bo_va_mapping, rb, uint64_t, __subtree_last,
+				START, LAST,, gsgpu_vm_it)
+
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_vram_mgr.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_vram_mgr.c
new file mode 100644
index 000000000000..7261da8246a1
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_vram_mgr.c
@@ -0,0 +1,297 @@
+#include <drm/drmP.h>
+#include "gsgpu.h"
+
+struct gsgpu_vram_mgr {
+	struct drm_mm mm;
+	spinlock_t lock;
+	atomic64_t usage;
+	atomic64_t vis_usage;
+};
+
+/**
+ * gsgpu_vram_mgr_init - init VRAM manager and DRM MM
+ *
+ * @man: TTM memory type manager
+ * @p_size: maximum size of VRAM
+ *
+ * Allocate and initialize the VRAM manager.
+ */
+static int gsgpu_vram_mgr_init(struct ttm_mem_type_manager *man,
+				unsigned long p_size)
+{
+	struct gsgpu_vram_mgr *mgr;
+
+	mgr = kzalloc(sizeof(*mgr), GFP_KERNEL);
+	if (!mgr)
+		return -ENOMEM;
+
+	drm_mm_init(&mgr->mm, 0, p_size);
+	spin_lock_init(&mgr->lock);
+	man->priv = mgr;
+	return 0;
+}
+
+/**
+ * gsgpu_vram_mgr_fini - free and destroy VRAM manager
+ *
+ * @man: TTM memory type manager
+ *
+ * Destroy and free the VRAM manager, returns -EBUSY if ranges are still
+ * allocated inside it.
+ */
+static int gsgpu_vram_mgr_fini(struct ttm_mem_type_manager *man)
+{
+	struct gsgpu_vram_mgr *mgr = man->priv;
+
+	spin_lock(&mgr->lock);
+	drm_mm_takedown(&mgr->mm);
+	spin_unlock(&mgr->lock);
+	kfree(mgr);
+	man->priv = NULL;
+	return 0;
+}
+
+/**
+ * gsgpu_vram_mgr_vis_size - Calculate visible node size
+ *
+ * @adev: gsgpu device structure
+ * @node: MM node structure
+ *
+ * Calculate how many bytes of the MM node are inside visible VRAM
+ */
+static u64 gsgpu_vram_mgr_vis_size(struct gsgpu_device *adev,
+				    struct drm_mm_node *node)
+{
+	uint64_t start = node->start << PAGE_SHIFT;
+	uint64_t end = (node->size + node->start) << PAGE_SHIFT;
+
+	if (start >= adev->gmc.visible_vram_size)
+		return 0;
+
+	return (end > adev->gmc.visible_vram_size ?
+		adev->gmc.visible_vram_size : end) - start;
+}
+
+/**
+ * gsgpu_vram_mgr_bo_visible_size - CPU visible BO size
+ *
+ * @bo: &gsgpu_bo buffer object (must be in VRAM)
+ *
+ * Returns:
+ * How much of the given &gsgpu_bo buffer object lies in CPU visible VRAM.
+ */
+u64 gsgpu_vram_mgr_bo_visible_size(struct gsgpu_bo *bo)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	struct ttm_mem_reg *mem = &bo->tbo.mem;
+	struct drm_mm_node *nodes = mem->mm_node;
+	unsigned pages = mem->num_pages;
+	u64 usage;
+
+	if (gsgpu_gmc_vram_full_visible(&adev->gmc))
+		return gsgpu_bo_size(bo);
+
+	if (mem->start >= adev->gmc.visible_vram_size >> PAGE_SHIFT)
+		return 0;
+
+	for (usage = 0; nodes && pages; pages -= nodes->size, nodes++)
+		usage += gsgpu_vram_mgr_vis_size(adev, nodes);
+
+	return usage;
+}
+
+/**
+ * gsgpu_vram_mgr_new - allocate new ranges
+ *
+ * @man: TTM memory type manager
+ * @tbo: TTM BO we need this range for
+ * @place: placement flags and restrictions
+ * @mem: the resulting mem object
+ *
+ * Allocate VRAM for the given BO.
+ */
+static int gsgpu_vram_mgr_new(struct ttm_mem_type_manager *man,
+			       struct ttm_buffer_object *tbo,
+			       const struct ttm_place *place,
+			       struct ttm_mem_reg *mem)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(man->bdev);
+	struct gsgpu_vram_mgr *mgr = man->priv;
+	struct drm_mm *mm = &mgr->mm;
+	struct drm_mm_node *nodes;
+	enum drm_mm_insert_mode mode;
+	unsigned long lpfn, num_nodes, pages_per_node, pages_left;
+	uint64_t usage = 0, vis_usage = 0;
+	unsigned i;
+	int r;
+
+	lpfn = place->lpfn;
+	if (!lpfn)
+		lpfn = man->size;
+
+	if (place->flags & TTM_PL_FLAG_CONTIGUOUS ||
+	    gsgpu_vram_page_split == -1) {
+		pages_per_node = ~0ul;
+		num_nodes = 1;
+	} else {
+		pages_per_node = max((uint32_t)gsgpu_vram_page_split,
+				     mem->page_alignment);
+		num_nodes = DIV_ROUND_UP(mem->num_pages, pages_per_node);
+	}
+
+	nodes = kvmalloc_array(num_nodes, sizeof(*nodes),
+			       GFP_KERNEL | __GFP_ZERO);
+	if (!nodes)
+		return -ENOMEM;
+
+	mode = DRM_MM_INSERT_BEST;
+	if (place->flags & TTM_PL_FLAG_TOPDOWN)
+		mode = DRM_MM_INSERT_HIGH;
+
+	mem->start = 0;
+	pages_left = mem->num_pages;
+
+	spin_lock(&mgr->lock);
+	for (i = 0; i < num_nodes; ++i) {
+		unsigned long pages = min(pages_left, pages_per_node);
+		uint32_t alignment = mem->page_alignment;
+		unsigned long start;
+
+		if (pages == pages_per_node)
+			alignment = pages_per_node;
+
+		r = drm_mm_insert_node_in_range(mm, &nodes[i],
+						pages, alignment, 0,
+						place->fpfn, lpfn,
+						mode);
+		if (unlikely(r))
+			goto error;
+
+		usage += nodes[i].size << PAGE_SHIFT;
+		vis_usage += gsgpu_vram_mgr_vis_size(adev, &nodes[i]);
+
+		/* Calculate a virtual BO start address to easily check if
+		 * everything is CPU accessible.
+		 */
+		start = nodes[i].start + nodes[i].size;
+		if (start > mem->num_pages)
+			start -= mem->num_pages;
+		else
+			start = 0;
+		mem->start = max(mem->start, start);
+		pages_left -= pages;
+	}
+	spin_unlock(&mgr->lock);
+
+	atomic64_add(usage, &mgr->usage);
+	atomic64_add(vis_usage, &mgr->vis_usage);
+
+	mem->mm_node = nodes;
+
+	return 0;
+
+error:
+	while (i--)
+		drm_mm_remove_node(&nodes[i]);
+	spin_unlock(&mgr->lock);
+
+	kvfree(nodes);
+	return r == -ENOSPC ? 0 : r;
+}
+
+/**
+ * gsgpu_vram_mgr_del - free ranges
+ *
+ * @man: TTM memory type manager
+ * @tbo: TTM BO we need this range for
+ * @place: placement flags and restrictions
+ * @mem: TTM memory object
+ *
+ * Free the allocated VRAM again.
+ */
+static void gsgpu_vram_mgr_del(struct ttm_mem_type_manager *man,
+				struct ttm_mem_reg *mem)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(man->bdev);
+	struct gsgpu_vram_mgr *mgr = man->priv;
+	struct drm_mm_node *nodes = mem->mm_node;
+	uint64_t usage = 0, vis_usage = 0;
+	unsigned pages = mem->num_pages;
+
+	if (!mem->mm_node)
+		return;
+
+	spin_lock(&mgr->lock);
+	while (pages) {
+		pages -= nodes->size;
+		drm_mm_remove_node(nodes);
+		usage += nodes->size << PAGE_SHIFT;
+		vis_usage += gsgpu_vram_mgr_vis_size(adev, nodes);
+		++nodes;
+	}
+	spin_unlock(&mgr->lock);
+
+	atomic64_sub(usage, &mgr->usage);
+	atomic64_sub(vis_usage, &mgr->vis_usage);
+
+	kvfree(mem->mm_node);
+	mem->mm_node = NULL;
+}
+
+/**
+ * gsgpu_vram_mgr_usage - how many bytes are used in this domain
+ *
+ * @man: TTM memory type manager
+ *
+ * Returns how many bytes are used in this domain.
+ */
+uint64_t gsgpu_vram_mgr_usage(struct ttm_mem_type_manager *man)
+{
+	struct gsgpu_vram_mgr *mgr = man->priv;
+
+	return atomic64_read(&mgr->usage);
+}
+
+/**
+ * gsgpu_vram_mgr_vis_usage - how many bytes are used in the visible part
+ *
+ * @man: TTM memory type manager
+ *
+ * Returns how many bytes are used in the visible part of VRAM
+ */
+uint64_t gsgpu_vram_mgr_vis_usage(struct ttm_mem_type_manager *man)
+{
+	struct gsgpu_vram_mgr *mgr = man->priv;
+
+	return atomic64_read(&mgr->vis_usage);
+}
+
+/**
+ * gsgpu_vram_mgr_debug - dump VRAM table
+ *
+ * @man: TTM memory type manager
+ * @printer: DRM printer to use
+ *
+ * Dump the table content using printk.
+ */
+static void gsgpu_vram_mgr_debug(struct ttm_mem_type_manager *man,
+				  struct drm_printer *printer)
+{
+	struct gsgpu_vram_mgr *mgr = man->priv;
+
+	spin_lock(&mgr->lock);
+	drm_mm_print(&mgr->mm, printer);
+	spin_unlock(&mgr->lock);
+
+	drm_printf(printer, "man size:%llu pages, ram usage:%lluMB, vis usage:%lluMB\n",
+		   man->size, gsgpu_vram_mgr_usage(man) >> 20,
+		   gsgpu_vram_mgr_vis_usage(man) >> 20);
+}
+
+const struct ttm_mem_type_manager_func gsgpu_vram_mgr_func = {
+	.init		= gsgpu_vram_mgr_init,
+	.takedown	= gsgpu_vram_mgr_fini,
+	.get_node	= gsgpu_vram_mgr_new,
+	.put_node	= gsgpu_vram_mgr_del,
+	.debug		= gsgpu_vram_mgr_debug
+};
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_xdma.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_xdma.c
new file mode 100644
index 000000000000..2a251e3b68b3
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_xdma.c
@@ -0,0 +1,1199 @@
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include "gsgpu_xdma.h"
+#include "gsgpu_trace.h"
+#include "gsgpu_common.h"
+#include "gsgpu_irq.h"
+
+static void xdma_set_ring_funcs(struct gsgpu_device *adev);
+static void xdma_set_buffer_funcs(struct gsgpu_device *adev);
+static void xdma_set_vm_pte_funcs(struct gsgpu_device *adev);
+static void xdma_set_irq_funcs(struct gsgpu_device *adev);
+static int xdma_set_pte_pde_test(struct gsgpu_ring *ring, long timeout);
+static int xdma_copy_pte_test(struct gsgpu_ring *ring, long timeout);
+static int xdma_fill_test(struct gsgpu_ring *ring, long timeout);
+
+/*
+ * xDMA
+ * GSGPU has an asynchronous DMA engine called xDMA.
+ * The engine is used for buffer moving.
+ *
+ * The programming model is very similar to the CP
+ * (ring buffer, IBs, etc.), but xDMA has it's own
+ * packet format. xDMA supports copying data, writing
+ * embedded data, solid fills, and a number of other
+ * things.  It also has support for tiling/detiling of
+ * buffers.
+ */
+
+/**
+ * xdma_ring_get_rptr - get the current read pointer
+ *
+ * @ring: gsgpu ring pointer
+ *
+ * Get the current rptr from the hardware ().
+ */
+static u64 xdma_ring_get_rptr(struct gsgpu_ring *ring)
+{
+	return ring->adev->wb.wb[ring->rptr_offs];
+}
+
+/**
+ * xdma_ring_get_wptr - get the current write pointer
+ *
+ * @ring: gsgpu ring pointer
+ *
+ * Get the current wptr from the hardware ().
+ */
+static u64 xdma_ring_get_wptr(struct gsgpu_ring *ring)
+{
+	struct gsgpu_device *adev = ring->adev;
+
+	return RREG32(GSGPU_XDMA_CB_WPTR_OFFSET);
+}
+
+/**
+ * xdma_ring_set_wptr - commit the write pointer
+ *
+ * @ring: gsgpu ring pointer
+ *
+ * Write the wptr back to the hardware ().
+ */
+static void xdma_ring_set_wptr(struct gsgpu_ring *ring)
+{
+	struct gsgpu_device *adev = ring->adev;
+
+	WREG32(GSGPU_XDMA_CB_WPTR_OFFSET, lower_32_bits(ring->wptr));
+}
+
+static void xdma_ring_insert_nop(struct gsgpu_ring *ring, u32 count)
+{
+	int i;
+
+	for (i = 0; i < count; i++)
+		gsgpu_ring_write(ring, ring->funcs->nop);
+}
+
+/**
+ * xdma_ring_emit_ib - Schedule an IB on the DMA engine
+ *
+ * @ring: gsgpu ring pointer
+ * @ib: IB object to schedule
+ *
+ * Schedule an IB in the DMA ring  .
+ */
+static void xdma_ring_emit_ib(struct gsgpu_ring *ring,
+				   struct gsgpu_ib *ib,
+				   unsigned vmid, bool ctx_switch)
+{
+	gsgpu_ring_write(ring, GSPKT(GSPKT_INDIRECT, 3));
+	gsgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));
+	gsgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));
+	gsgpu_ring_write(ring, ib->length_dw | (vmid << 24));
+}
+
+/**
+ * xdma_ring_emit_fence - emit a fence on the DMA ring
+ *
+ * @ring: gsgpu ring pointer
+ * @fence: gsgpu fence object
+ *
+ * Add a DMA fence packet to the ring to write
+ * the fence seq number and DMA trap packet to generate
+ * an interrupt if needed  .
+ */
+static void xdma_ring_emit_fence(struct gsgpu_ring *ring, u64 addr, u64 seq,
+				      unsigned flags)
+{
+	bool write64bit = flags & GSGPU_FENCE_FLAG_64BIT;
+	bool int_sel = flags & GSGPU_FENCE_FLAG_INT;
+
+	gsgpu_ring_write(ring, GSPKT(GSPKT_FENCE, write64bit ? 4 : 3)
+			| (write64bit ? 1 << 9 : 0) | (int_sel ? 1 << 8 : 0));
+	gsgpu_ring_write(ring, lower_32_bits(addr));
+	gsgpu_ring_write(ring, upper_32_bits(addr));
+	gsgpu_ring_write(ring, lower_32_bits(seq));
+	if (write64bit)
+		gsgpu_ring_write(ring, upper_32_bits(seq));
+}
+
+/**
+ * xdma_gfx_stop - stop the gfx async dma engines
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Stop the gfx async dma ring buffers  .
+ */
+static void xdma_gfx_stop(struct gsgpu_device *adev)
+{
+	struct gsgpu_ring *xdma0 = &adev->xdma.instance[0].ring;
+	struct gsgpu_ring *xdma1 = &adev->xdma.instance[1].ring;
+
+	if ((adev->mman.buffer_funcs_ring == xdma0) ||
+	    (adev->mman.buffer_funcs_ring == xdma1))
+		gsgpu_ttm_set_buffer_funcs_status(adev, false);
+
+	xdma0->ready = false;
+	xdma1->ready = false;
+}
+
+/**
+ * xdma_enable - stop the async dma engines
+ *
+ * @adev: gsgpu_device pointer
+ * @enable: enable/disable the DMA MEs.
+ *
+ * Halt or unhalt the async dma engines  .
+ */
+static void xdma_enable(struct gsgpu_device *adev, bool enable)
+{
+	if (!enable) {
+		xdma_gfx_stop(adev);
+	}
+}
+
+/**
+ * xdma_gfx_resume - setup and start the async dma engines
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Set up the gfx DMA ring buffers and enable them  .
+ * Returns 0 for success, error for failure.
+ */
+static int xdma_gfx_resume(struct gsgpu_device *adev)
+{
+	struct gsgpu_ring *ring;
+	int i;
+
+	for (i = 0; i < adev->xdma.num_instances; i++) {
+		ring = &adev->xdma.instance[i].ring;
+		gsgpu_ring_clear_ring(ring);
+
+		/* Initialize the ring buffer's read and write pointers */
+		ring->wptr = 0;
+		WREG32(GSGPU_XDMA_CB_WPTR_OFFSET, lower_32_bits(ring->wptr));
+
+		/* set the RPTR */
+		WREG32(GSGPU_XDMA_CB_RPTR_OFFSET, 0);
+
+		WREG32(GSGPU_XDMA_CB_BASE_LO_OFFSET, lower_32_bits(ring->gpu_addr));
+		WREG32(GSGPU_XDMA_CB_BASE_HI_OFFSET, upper_32_bits(ring->gpu_addr));
+
+		ring->ready = true;
+	}
+
+	/* unhalt the MEs */
+	xdma_enable(adev, true);
+
+	for (i = 0; i < adev->xdma.num_instances; i++) {
+
+		if (adev->mman.buffer_funcs_ring == ring)
+			gsgpu_ttm_set_buffer_funcs_status(adev, true);
+	}
+
+	return 0;
+}
+
+/**
+ * xdma_start - setup and start the async dma engines
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Set up the DMA engines and enable them  .
+ * Returns 0 for success, error for failure.
+ */
+static int xdma_start(struct gsgpu_device *adev)
+{
+	int r;
+
+	/* disable xdma engine before programing it */
+	xdma_enable(adev, false);
+
+	/* start the gfx rings and rlc compute queues */
+	r = xdma_gfx_resume(adev);
+	if (r)
+		return r;
+
+	return 0;
+}
+
+/**
+ * xdma_ring_test_ring - simple async dma engine test
+ *
+ * @ring: gsgpu_ring structure holding ring information
+ *
+ * Test the DMA engine by writing using it to write an
+ * value to memory.  .
+ * Returns 0 for success, error for failure.
+ */
+static int xdma_ring_test_ring(struct gsgpu_ring *ring)
+{
+	struct gsgpu_device *adev = ring->adev;
+	unsigned i;
+	unsigned index;
+	int r;
+	u32 tmp;
+	u64 gpu_addr;
+
+	r = gsgpu_device_wb_get(adev, &index);
+	if (r) {
+		dev_err(adev->dev, "(%d) failed to allocate wb slot\n", r);
+		return r;
+	}
+
+	gpu_addr = adev->wb.gpu_addr + (index * 4);
+	tmp = 0xCAFEDEAD;
+	adev->wb.wb[index] = cpu_to_le32(tmp);
+
+	r = gsgpu_ring_alloc(ring, 4);
+	if (r) {
+		DRM_ERROR("gsgpu: dma failed to lock ring %d (%d).\n", ring->idx, r);
+		gsgpu_device_wb_free(adev, index);
+		return r;
+	}
+
+	gsgpu_ring_write(ring, GSPKT(GSPKT_WRITE, 3) | WRITE_DST_SEL(1) | WRITE_WAIT);
+	gsgpu_ring_write(ring, lower_32_bits(gpu_addr));
+	gsgpu_ring_write(ring, upper_32_bits(gpu_addr));
+	gsgpu_ring_write(ring, 0xDEADBEEF);
+	gsgpu_ring_commit(ring);
+
+	for (i = 0; i < 4 * adev->usec_timeout; i++) {
+		tmp = le32_to_cpu(adev->wb.wb[index]);
+		if (tmp == 0xDEADBEEF)
+			break;
+		DRM_UDELAY(1);
+	}
+
+	if (i < adev->usec_timeout) {
+		DRM_INFO("ring %s test on %d succeeded in %d usecs\n", ring->name, ring->idx, i);
+		r = 0;
+	} else {
+		DRM_ERROR("gsgpu: ring %s %d test failed (0x%08X)\n", ring->name,
+			  ring->idx, tmp);
+		r = -EINVAL;
+	}
+	gsgpu_device_wb_free(adev, index);
+
+	return r;
+}
+
+/**
+ * xdma_ring_test_ib - test an IB on the DMA engine
+ *
+ * @ring: gsgpu_ring structure holding ring information
+ *
+ * Test a simple IB in the DMA ring  .
+ * Returns 0 on success, error on failure.
+ */
+static int xdma_ring_test_ib(struct gsgpu_ring *ring, long timeout)
+{
+	struct gsgpu_device *adev = ring->adev;
+	struct gsgpu_ib ib;
+	struct dma_fence *f = NULL;
+	unsigned index;
+	u32 tmp = 0;
+	u64 gpu_addr;
+	long r;
+
+	r = gsgpu_device_wb_get(adev, &index);
+	if (r) {
+		dev_err(adev->dev, "(%ld) failed to allocate wb slot\n", r);
+		return r;
+	}
+
+	gpu_addr = adev->wb.gpu_addr + (index * 4);
+	tmp = 0xCAFEDEAD;
+	adev->wb.wb[index] = cpu_to_le32(tmp);
+	memset(&ib, 0, sizeof(ib));
+	r = gsgpu_ib_get(adev, NULL, 256, &ib);
+	if (r) {
+		DRM_ERROR("gsgpu: failed to get ib (%ld).\n", r);
+		goto err0;
+	}
+
+	ib.ptr[0] = GSPKT(GSPKT_WRITE, 3) | WRITE_DST_SEL(1) | WRITE_WAIT;
+	ib.ptr[1] = lower_32_bits(gpu_addr);
+	ib.ptr[2] = upper_32_bits(gpu_addr);
+	ib.ptr[3] = 0xDEADBEEF;
+	ib.length_dw = 4;
+
+	r = gsgpu_ib_schedule(ring, 1, &ib, NULL, &f);
+	if (r)
+		goto err1;
+
+	r = dma_fence_wait_timeout(f, false, timeout);
+	if (r == 0) {
+		DRM_ERROR("gsgpu: %s IB test timed out\n", ring->name);
+		r = -ETIMEDOUT;
+		goto err1;
+	} else if (r < 0) {
+		DRM_ERROR("gsgpu: %s fence wait failed (%ld).\n", ring->name, r);
+		goto err1;
+	}
+	tmp = le32_to_cpu(adev->wb.wb[index]);
+	if (tmp == 0xDEADBEEF) {
+		DRM_INFO("%s ib test on ring %d succeeded\n", ring->name, ring->idx);
+		r = 0;
+	} else {
+		DRM_ERROR("gsgpu:%s ib test failed (0x%08X)\n", ring->name, tmp);
+		r = -EINVAL;
+	}
+
+err1:
+	gsgpu_ib_free(adev, &ib, NULL);
+	dma_fence_put(f);
+err0:
+	gsgpu_device_wb_free(adev, index);
+
+	return r;
+}
+
+/**
+ * xdma_ring_test_xdma - test xdma on the DMA engine
+ *
+ * @ring: gsgpu_ring structure holding ring information
+ *
+ * Test a simple xdma in the DMA ring  .
+ * Returns 0 on success, error on failure.
+ */
+static int xdma_ring_test_xdma(struct gsgpu_ring *ring, long timeout)
+{
+	int r;
+
+	r = xdma_set_pte_pde_test(ring, timeout);
+	if (r <= 0) {
+		return r;
+	}
+
+	r = xdma_copy_pte_test(ring, timeout);
+	if (r <= 0) {
+		return r;
+	}
+
+	r = xdma_fill_test(ring, timeout);
+	if (r > 0) {
+		r = 0;
+	}
+
+	return r;
+}
+
+void xdma_ring_test_xdma_loop(struct gsgpu_ring *ring, long timeout)
+{
+	xdma_set_pte_pde_test(ring, timeout);
+}
+
+/**
+ * xdma_vm_copy_pte - update PTEs by copying them from the GART
+ *
+ * @ib: indirect buffer to fill with commands
+ * @pe: addr of the page entry
+ * @src: src addr to copy from
+ * @count: number of page entries to update
+ *
+ * Update PTEs by copying them from the GART using xDMA.
+ */
+static void xdma_vm_copy_pte(struct gsgpu_ib *ib,
+				u64 pe, u64 src,
+				unsigned count)
+{
+	struct gsgpu_bo *bo = ib->sa_bo->manager->bo;
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	uint32_t width, height;
+	uint32_t dst_umap, src_umap;
+	height = 1;
+	width = count;
+	dst_umap = (pe >= adev->gmc.vram_start && pe < adev->gmc.vram_end) ?
+			   GSGPU_XDMA_FLAG_UMAP :
+			   0;
+	src_umap = (src >= adev->gmc.vram_start && src < adev->gmc.vram_end) ?
+			   GSGPU_XDMA_FLAG_UMAP :
+			   0;
+
+	/* hardware limit 2^16 pixels per line */
+	while (width >= 0x10000) {
+		width = width / 2;
+		height = height * 2;
+	}
+
+	/*XDMA COPY
+	 * L2L : 1<<24
+	 * RGBA16 : 1<<8
+	 * */
+	ib->ptr[ib->length_dw++] = GSPKT(GSPKT_XDMA_COPY, 8) | (0x1 << 24) | (1 << 8);
+	ib->ptr[ib->length_dw++] =  (height << 16) | width;
+	ib->ptr[ib->length_dw++] = lower_32_bits(src);
+	ib->ptr[ib->length_dw++] = upper_32_bits(src) | src_umap;
+	ib->ptr[ib->length_dw++] = lower_32_bits(pe);
+	ib->ptr[ib->length_dw++] = upper_32_bits(pe) | dst_umap;
+	ib->ptr[ib->length_dw++] = (width ? width : 1) * GSGPU_VM_PDE_PTE_BYTES;
+	ib->ptr[ib->length_dw++] = (width ? width : 1) * GSGPU_VM_PDE_PTE_BYTES;
+	ib->ptr[ib->length_dw++] = 0;
+}
+
+/**
+ * xdma_vm_set_pte_pde - update the page tables using xDMA
+ *
+ * @ib: indirect buffer to fill with commands
+ * @pe: addr of the page entry
+ * @addr: dst addr to write into pe
+ * @count: number of page entries to update
+ * @incr: increase next addr by incr bytes
+ * @flags: access flags
+ *
+ * Update the page tables using xDMA.
+ */
+static void xdma_vm_set_pte_pde(struct gsgpu_ib *ib, u64 pe,
+				     u64 addr, unsigned count,
+				     u32 incr, u64 flags)
+{
+	/* for physically contiguous pages (vram) */
+	struct gsgpu_bo *bo = ib->sa_bo->manager->bo;
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	uint32_t width, height;
+	uint32_t dst_umap, src_umap;
+
+	dst_umap = (pe >= adev->gmc.vram_start && pe < adev->gmc.vram_end) ?
+			   GSGPU_XDMA_FLAG_UMAP :
+			   0;
+	src_umap = (addr >= adev->gmc.vram_start && addr < adev->gmc.vram_end) ?
+			   GSGPU_XDMA_FLAG_UMAP :
+			   0;
+	height = 1;
+
+	/*RGBA16 == 8 bytes per pixles*/
+	width = count;
+
+	/* hardware limit 2^16 pixels per line */
+	while (width >= 0x10000) {
+		width = width / 2;
+		height = height * 2;
+	}
+
+	/*XDMA COPY
+	 * MEMSET : 7 << 24
+	 * RGBA16 : 1 << 8
+	 * 16K pf : 3 << 28
+	 * */
+	ib->ptr[ib->length_dw++] = GSPKT(GSPKT_XDMA_COPY, 8) | (0x7 << 24) | (1 << 8) | (3 << 28);
+	ib->ptr[ib->length_dw++] =  (height << 16) | width;
+	ib->ptr[ib->length_dw++] = lower_32_bits(addr | flags); /* value */
+	ib->ptr[ib->length_dw++] = upper_32_bits(addr | flags) | src_umap;;
+	ib->ptr[ib->length_dw++] = lower_32_bits(pe); /* dst addr */
+	ib->ptr[ib->length_dw++] = upper_32_bits(pe) | dst_umap;
+	ib->ptr[ib->length_dw++] = 0;
+	ib->ptr[ib->length_dw++] = (width ? width : 1) * GSGPU_VM_PDE_PTE_BYTES;
+	ib->ptr[ib->length_dw++] = 0;
+}
+
+/**
+ * xdma_ring_pad_ib - pad the IB to the required number of dw
+ *
+ * @ib: indirect buffer to fill with padding
+ *
+ */
+static void xdma_ring_pad_ib(struct gsgpu_ring *ring, struct gsgpu_ib *ib)
+{
+	u32 pad_count;
+	int i;
+
+	pad_count = (8 - (ib->length_dw & 0x7)) % 8;
+	for (i = 0; i < pad_count; i++)
+		ib->ptr[ib->length_dw++] = GSPKT(GSPKT_NOP, 0);
+}
+
+/**
+ * xdma_ring_emit_pipeline_sync - sync the pipeline
+ *
+ * @ring: gsgpu_ring pointer
+ *
+ * Make sure all previous operations are completed ().
+ */
+static void xdma_ring_emit_pipeline_sync(struct gsgpu_ring *ring)
+{
+	u32 seq = ring->fence_drv.sync_seq;
+	u64 addr = ring->fence_drv.gpu_addr;
+
+	/* wait for idle */
+	gsgpu_ring_write(ring, GSPKT(GSPKT_POLL, 5) |
+				POLL_CONDITION(3) | /* equal */
+				POLL_REG_MEM(1)); /* reg/mem */
+	gsgpu_ring_write(ring, lower_32_bits(addr));
+	gsgpu_ring_write(ring, upper_32_bits(addr));
+	gsgpu_ring_write(ring, seq); /* reference */
+	gsgpu_ring_write(ring, 0xffffffff); /* mask */
+	gsgpu_ring_write(ring, POLL_TIMES_INTERVAL(0xfff, 1)); /* retry count, interval */
+}
+
+/**
+ * xdma_ring_emit_vm_flush - vm flush using xDMA
+ *
+ * @ring: gsgpu_ring pointer
+ * @vm: gsgpu_vm pointer
+ *
+ * Update the page table base and flush the VM TLB
+ * using xDMA.
+ */
+static void xdma_ring_emit_vm_flush(struct gsgpu_ring *ring,
+					 unsigned vmid, u64 pd_addr)
+{
+	gsgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);
+}
+
+static void xdma_ring_emit_wreg(struct gsgpu_ring *ring,
+				     u32 reg, u32 val)
+{
+	gsgpu_ring_write(ring, GSPKT(GSPKT_WRITE, 2) | WRITE_DST_SEL(0) | WRITE_WAIT);
+	gsgpu_ring_write(ring, reg);
+	gsgpu_ring_write(ring, val);
+}
+
+static int xdma_set_pte_pde_test(struct gsgpu_ring *ring, long timeout)
+{
+	int r = 0;
+	u32 align;
+	u32 domain;
+	u64 gpu_addr;
+	u64 *cpu_ptr;
+	struct gsgpu_bo *bo;
+	unsigned int size;
+	struct gsgpu_ib ib;
+	struct dma_fence *f = NULL;
+	struct gsgpu_device *ldev;
+
+	ldev = ring->adev;
+
+	bo = NULL;
+	size = GSGPU_GPU_PAGE_SIZE;
+	align = GSGPU_GPU_PAGE_SIZE;
+	domain = GSGPU_GEM_DOMAIN_VRAM;
+
+	r = gsgpu_bo_create_kernel(ldev, size, align, domain, &bo, &gpu_addr, (void **)&cpu_ptr);
+	if (r) {
+		DRM_ERROR("xdma_set_pte_pde_test : gsgpu_bo_create_kernel error\r\n");
+		return r;
+	}
+
+	memset(cpu_ptr, 0x55, size);
+	if (readq(&cpu_ptr[0]) != 0x5555555555555555) {
+		DRM_ERROR("xdma_set_pte_pde_test : set vram error through pcie\r\n");
+	} else {
+		DRM_DEBUG_DRIVER("xdma_set_pte_pde_test : set vram success through pcie\r\n");
+	}
+
+	memset(&ib, 0, sizeof(ib));
+	r = gsgpu_ib_get(ldev, NULL, 256, &ib);
+	if (r) {
+		DRM_ERROR("xdma_set_pte_pde_test : gsgpu_ib_get error\r\n");
+		goto bo_free;
+	}
+
+	xdma_vm_set_pte_pde(&ib, gpu_addr, 0, 2048, 8, 0);
+	xdma_ring_pad_ib(ring, &ib);
+
+	r = gsgpu_ib_schedule(ring, 1, &ib, NULL, &f);
+	if (r) {
+		DRM_ERROR("xdma_set_pte_pde_test : gsgpu_ib_schedule error\r\n");
+		goto ib_free;
+	}
+
+	r = dma_fence_wait_timeout(f, false, timeout);
+	if (r == 0) {
+		DRM_ERROR("xdma_set_pte_pde_test : dma_fence_wait_timeout timed out\r\n");
+		r = -ETIMEDOUT;
+		goto ib_free;
+	} else if (r < 0) {
+		DRM_ERROR("xdma_set_pte_pde_test : dma_fence_wait_timeout failed\r\n");
+		goto ib_free;
+	}
+
+	if ((readq(&cpu_ptr[0]) == 0x00) && (readq(&cpu_ptr[1]) == GSGPU_GPU_PAGE_SIZE))   {
+		DRM_DEBUG_DRIVER("xdma_set_pte_pde_test : success\r\n");
+	} else {
+		DRM_ERROR("xdma_set_pte_pde_test : failed\r\n");
+	}
+
+ib_free:
+	gsgpu_ib_free(ldev, &ib, NULL);
+	dma_fence_put(f);
+
+bo_free:
+	gsgpu_bo_free_kernel(&bo, &gpu_addr, (void **)&cpu_ptr);
+
+	return r;
+}
+
+static int xdma_copy_pte_test(struct gsgpu_ring *ring, long timeout)
+{
+	int r = 0;
+	u32 align;
+	u32 domain;
+	u64 gpu_addr;
+	u64 *cpu_ptr;
+	u64 gpu_addr_gtt;
+	u64 *cpu_ptr_gtt;
+	struct gsgpu_bo *bo;
+	struct gsgpu_bo *bo_gtt;
+	unsigned int size;
+	struct gsgpu_ib ib;
+	struct dma_fence *f = NULL;
+	struct gsgpu_device *ldev;
+
+	ldev = ring->adev;
+
+	bo = NULL;
+	bo_gtt = NULL;
+	size = GSGPU_GPU_PAGE_SIZE;
+	align = GSGPU_GPU_PAGE_SIZE;
+
+	domain = GSGPU_GEM_DOMAIN_GTT;
+	r = gsgpu_bo_create_kernel(ldev, size, align, domain, &bo_gtt, &gpu_addr_gtt, (void **)&cpu_ptr_gtt);
+	if (r) {
+		DRM_ERROR("xdma_copy_pte_test : gsgpu_bo_create_kernel gtt error\r\n");
+		goto bo_free;
+	}
+	memset(cpu_ptr_gtt, 0xaa, size);
+	if (readq(&cpu_ptr_gtt[0]) != 0xaaaaaaaaaaaaaaaa) {
+		DRM_ERROR("xdma_copy_pte_test : set gtt error through pcie\r\n");
+	} else {
+		DRM_INFO("xdma_copy_pte_test : set vram success through pcie\r\n");
+	}
+
+	domain = GSGPU_GEM_DOMAIN_VRAM;
+	r = gsgpu_bo_create_kernel(ldev, size, align, domain, &bo, &gpu_addr, (void **)&cpu_ptr);
+	if (r) {
+		DRM_ERROR("xdma_copy_pte_test : gsgpu_bo_create_kernel vram error\r\n");
+		return r;
+	}
+
+	memset(cpu_ptr, 0x55, size);
+	if (readq(&cpu_ptr[0]) != 0x5555555555555555) {
+		DRM_ERROR("xdma_copy_pte_test : set vram error through pcie\r\n");
+	} else {
+		DRM_INFO("xdma_copy_pte_test : set vram success through pcie\r\n");
+	}
+
+	memset(&ib, 0, sizeof(ib));
+	r = gsgpu_ib_get(ldev, NULL, 256, &ib);
+	if (r) {
+		DRM_ERROR("xdma_copy_pte_test : gsgpu_ib_get error\r\n");
+		goto bo_free_gtt;
+	}
+
+	xdma_vm_copy_pte(&ib, gpu_addr, gpu_addr_gtt, 2048);
+	xdma_ring_pad_ib(ring, &ib);
+
+	r = gsgpu_ib_schedule(ring, 1, &ib, NULL, &f);
+	if (r) {
+		DRM_ERROR("xdma_copy_pte_test : gsgpu_ib_schedule error\r\n");
+		goto ib_free;
+	}
+
+	r = dma_fence_wait_timeout(f, false, timeout);
+	if (r == 0) {
+		DRM_ERROR("xdma_copy_pte_test : dma_fence_wait_timeout timed out\r\n");
+		r = -ETIMEDOUT;
+		goto ib_free;
+	} else if (r < 0) {
+		DRM_ERROR("xdma_copy_pte_test : dma_fence_wait_timeout failed\r\n");
+		goto ib_free;
+	}
+
+	if (readq(&cpu_ptr[0]) == 0xaaaaaaaaaaaaaaaa) {
+		DRM_INFO("xdma_copy_pte_test : success\r\n");
+		r = 0;
+	} else {
+		DRM_ERROR("xdma_copy_pte_test : failed\r\n");
+	}
+
+ib_free:
+	gsgpu_ib_free(ldev, &ib, NULL);
+	dma_fence_put(f);
+
+bo_free_gtt:
+	gsgpu_bo_free_kernel(&bo_gtt, &gpu_addr_gtt, (void **)&cpu_ptr_gtt);
+
+bo_free:
+	gsgpu_bo_free_kernel(&bo, &gpu_addr, (void **)&cpu_ptr);
+
+	return r;
+}
+
+static int xdma_early_init(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	adev->xdma.num_instances = 1;
+
+	xdma_set_ring_funcs(adev);
+	xdma_set_buffer_funcs(adev);
+	xdma_set_vm_pte_funcs(adev);
+	xdma_set_irq_funcs(adev);
+
+	return 0;
+}
+
+static int xdma_sw_init(void *handle)
+{
+	struct gsgpu_ring *ring;
+	int r, i;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	/* XDMA trap event */
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_XDMA_TRAP,
+			      &adev->xdma.trap_irq);
+	if (r)
+		return r;
+
+	/* XDMA Privileged inst */
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, 241,
+			      &adev->xdma.illegal_inst_irq);
+	if (r)
+		return r;
+
+	/* XDMA Privileged inst */
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_XDMA_SRBM_WRITE,
+			      &adev->xdma.illegal_inst_irq);
+	if (r)
+		return r;
+
+	for (i = 0; i < adev->xdma.num_instances; i++) {
+		ring = &adev->xdma.instance[i].ring;
+		ring->ring_obj = NULL;
+
+		sprintf(ring->name, "xdma%d", i);
+		r = gsgpu_ring_init(adev, ring, 256,
+				     &adev->xdma.trap_irq,
+				     (i == 0) ?
+				     GSGPU_XDMA_IRQ_TRAP0 :
+				     GSGPU_XDMA_IRQ_TRAP1);
+		if (r)
+			return r;
+	}
+
+	return r;
+}
+
+static int xdma_sw_fini(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+	int i;
+
+	for (i = 0; i < adev->xdma.num_instances; i++)
+		gsgpu_ring_fini(&adev->xdma.instance[i].ring);
+
+	return 0;
+}
+
+static int xdma_hw_init(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = xdma_start(adev);
+	if (r)
+		return r;
+
+	return r;
+}
+
+static int xdma_hw_fini(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	xdma_enable(adev, false);
+
+	return 0;
+}
+
+static int xdma_suspend(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	return xdma_hw_fini(adev);
+}
+
+static int xdma_resume(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	/* start the gfx rings and rlc compute queues */
+	r = xdma_hw_init(adev);
+
+	return r;
+}
+
+static bool xdma_is_idle(void *handle)
+{
+	return true;
+}
+
+static int xdma_wait_for_idle(void *handle)
+{
+	unsigned i;
+	u32 tmp;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	for (i = 0; i < adev->usec_timeout; i++) {
+		//TODO
+		tmp = 0;
+
+		if (!tmp)
+			return 0;
+		udelay(1);
+	}
+	return -ETIMEDOUT;
+}
+
+static int xdma_set_trap_irq_state(struct gsgpu_device *adev,
+					struct gsgpu_irq_src *source,
+					unsigned type,
+					enum gsgpu_interrupt_state state)
+{
+	return 0;
+}
+
+static int xdma_process_trap_irq(struct gsgpu_device *adev,
+				      struct gsgpu_irq_src *source,
+				      struct gsgpu_iv_entry *entry)
+{
+	u8 instance_id, queue_id;
+
+	instance_id = (entry->ring_id & 0x3) >> 0;
+	queue_id = (entry->ring_id & 0xc) >> 2;
+	DRM_DEBUG("IH: XDMA trap\n");
+	switch (instance_id) {
+	case 0:
+		switch (queue_id) {
+		case 0:
+			gsgpu_fence_process(&adev->xdma.instance[0].ring);
+			break;
+		case 1:
+			/* XXX compute */
+			break;
+		case 2:
+			/* XXX compute */
+			break;
+		}
+		break;
+	case 1:
+		switch (queue_id) {
+		case 0:
+			gsgpu_fence_process(&adev->xdma.instance[1].ring);
+			break;
+		case 1:
+			/* XXX compute */
+			break;
+		case 2:
+			/* XXX compute */
+			break;
+		}
+		break;
+	}
+	return 0;
+}
+
+static int xdma_process_illegal_inst_irq(struct gsgpu_device *adev,
+					      struct gsgpu_irq_src *source,
+					      struct gsgpu_iv_entry *entry)
+{
+	DRM_ERROR("Illegal instruction in XDMA command stream\n");
+	schedule_work(&adev->reset_work);
+	return 0;
+}
+
+static const struct gsgpu_ip_funcs xdma_ip_funcs = {
+	.name = "xdma",
+	.early_init = xdma_early_init,
+	.late_init = NULL,
+	.sw_init = xdma_sw_init,
+	.sw_fini = xdma_sw_fini,
+	.hw_init = xdma_hw_init,
+	.hw_fini = xdma_hw_fini,
+	.suspend = xdma_suspend,
+	.resume = xdma_resume,
+	.is_idle = xdma_is_idle,
+	.wait_for_idle = xdma_wait_for_idle,
+};
+
+static const struct gsgpu_ring_funcs xdma_ring_funcs = {
+	.type = GSGPU_RING_TYPE_XDMA,
+	.align_mask = 0xf,
+	.nop = GSPKT(GSPKT_NOP, 0),
+	.support_64bit_ptrs = false,
+	.get_rptr = xdma_ring_get_rptr,
+	.get_wptr = xdma_ring_get_wptr,
+	.set_wptr = xdma_ring_set_wptr,
+	.emit_frame_size =
+		3 + /* hdp invalidate */
+		6 + /* xdma_ring_emit_pipeline_sync */
+		VI_FLUSH_GPU_TLB_NUM_WREG * 3 + 6 + /* xdma_ring_emit_vm_flush */
+		5 + 5 + 5, /* xdma_ring_emit_fence x3 for user fence, vm fence */
+	.emit_ib_size = 4, /* xdma_ring_emit_ib */
+	.emit_ib = xdma_ring_emit_ib,
+	.emit_fence = xdma_ring_emit_fence,
+	.emit_pipeline_sync = xdma_ring_emit_pipeline_sync,
+	.emit_vm_flush = xdma_ring_emit_vm_flush,
+	.test_ring = xdma_ring_test_ring,
+	.test_ib = xdma_ring_test_ib,
+	.test_xdma = xdma_ring_test_xdma,
+	.insert_nop = xdma_ring_insert_nop,
+	.pad_ib = xdma_ring_pad_ib,
+	.emit_wreg = xdma_ring_emit_wreg,
+};
+
+static void xdma_set_ring_funcs(struct gsgpu_device *adev)
+{
+	int i;
+
+	for (i = 0; i < adev->xdma.num_instances; i++) {
+		adev->xdma.instance[i].ring.funcs = &xdma_ring_funcs;
+		adev->xdma.instance[i].ring.me = i;
+	}
+}
+
+static const struct gsgpu_irq_src_funcs xdma_trap_irq_funcs = {
+	.set = xdma_set_trap_irq_state,
+	.process = xdma_process_trap_irq,
+};
+
+static const struct gsgpu_irq_src_funcs xdma_illegal_inst_irq_funcs = {
+	.process = xdma_process_illegal_inst_irq,
+};
+
+static void xdma_set_irq_funcs(struct gsgpu_device *adev)
+{
+	adev->xdma.trap_irq.num_types = GSGPU_XDMA_IRQ_LAST;
+	adev->xdma.trap_irq.funcs = &xdma_trap_irq_funcs;
+	adev->xdma.illegal_inst_irq.funcs = &xdma_illegal_inst_irq_funcs;
+}
+
+/**
+ * xdma_emit_copy_buffer - copy buffer using the xDMA engine
+ *
+ * @ring: gsgpu_ring structure holding ring information
+ * @src_offset: src GPU address
+ * @dst_offset: dst GPU address
+ * @byte_count: number of bytes to xfer
+ *
+ * Copy GPU buffers using the DMA engine.
+ * Used by the gsgpu ttm implementation to move pages if
+ * registered as the asic copy callback.
+ */
+static void xdma_emit_copy_buffer(struct gsgpu_ib *ib,
+					u64 src_offset,
+					u64 dst_offset,
+					u32 byte_count)
+{
+	struct gsgpu_bo *bo = ib->sa_bo->manager->bo;
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	uint32_t cpp = 8;
+	uint32_t width, height;
+	uint32_t dst_umap, src_umap;
+
+	dst_umap = (dst_offset >= adev->gmc.vram_start &&
+		    dst_offset < adev->gmc.vram_end) ?
+			   GSGPU_XDMA_FLAG_UMAP :
+			   0;
+	src_umap = (src_offset >= adev->gmc.vram_start &&
+		    src_offset < adev->gmc.vram_end) ?
+			   GSGPU_XDMA_FLAG_UMAP :
+			   0;
+	height = 1;
+	width = byte_count / cpp;
+
+	/* hardware limit 2^16 pixels per line */
+	while (width >= 0x10000) {
+		width = width / 2;
+		height = height * 2;
+	}
+
+	/*XDMA COPY
+	 * L2L : 1<<24
+	 * RGBA16 : 1<<8
+	 * */
+	ib->ptr[ib->length_dw++] = GSPKT(GSPKT_XDMA_COPY, 8) | (0x1 << 24) | (1 << 8);
+	ib->ptr[ib->length_dw++] =  (height << 16) | width;
+	ib->ptr[ib->length_dw++] = lower_32_bits(src_offset);
+	ib->ptr[ib->length_dw++] = upper_32_bits(src_offset) | src_umap;
+	ib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);
+	ib->ptr[ib->length_dw++] = upper_32_bits(dst_offset) | dst_umap;
+	ib->ptr[ib->length_dw++] = (width ? width : 1) * cpp;
+	ib->ptr[ib->length_dw++] = (width ? width : 1) * cpp;
+	ib->ptr[ib->length_dw++] = 0;
+}
+
+/**
+ * xdma_emit_fill_buffer - fill buffer using the xDMA engine
+ *
+ * @ring: gsgpu_ring structure holding ring information
+ * @src_data: value to write to buffer
+ * @dst_offset: dst GPU address
+ * @byte_count: number of bytes to xfer
+ *
+ * Fill GPU buffers using the DMA engine  .
+ */
+static void xdma_emit_fill_buffer(struct gsgpu_ib *ib,
+					u32 src_data,
+					u64 dst_offset,
+					u32 byte_count)
+{
+	struct gsgpu_bo *bo = ib->sa_bo->manager->bo;
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	uint32_t width, height;
+	uint32_t dst_umap;
+
+	height = 1;
+	dst_umap = (dst_offset >= adev->gmc.vram_start &&
+		    dst_offset < adev->gmc.vram_end) ?
+			   GSGPU_XDMA_FLAG_UMAP :
+			   0;
+
+	/*RGBA8 == 4 bytes per pixles*/
+	width = byte_count / 4;
+
+	/* hardware limit 2^16 pixels per line */
+	while (width >= 0x10000) {
+		width = width / 2;
+		height = height * 2;
+	}
+
+	/*XDMA COPY
+	 * MEMSET : 7 <<24
+	 * RGBA8 : 0<<8
+	 * */
+	ib->ptr[ib->length_dw++] = GSPKT(GSPKT_XDMA_COPY, 8) | (0x7 << 24) | (0 << 8);
+	ib->ptr[ib->length_dw++] =  (height << 16) | width;
+	ib->ptr[ib->length_dw++] = lower_32_bits(src_data);
+	ib->ptr[ib->length_dw++] = 0;
+	ib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);
+	ib->ptr[ib->length_dw++] = upper_32_bits(dst_offset) | dst_umap;
+	ib->ptr[ib->length_dw++] = 0;
+	ib->ptr[ib->length_dw++] = (width ? width : 1) * 4;
+	ib->ptr[ib->length_dw++] = 0;
+}
+
+static int xdma_fill_test(struct gsgpu_ring *ring, long timeout)
+{
+	int r = 0;
+	struct gsgpu_bo *bo;
+	unsigned int size;
+	u32 align;
+	u32 domain;
+	u64 gpu_addr;
+	u64 *cpu_ptr;
+	struct gsgpu_ib ib;
+	struct dma_fence *f = NULL;
+	struct gsgpu_device *ldev;
+
+	ldev = ring->adev;
+
+	bo = NULL;
+	size = GSGPU_GPU_PAGE_SIZE;
+	align = GSGPU_GPU_PAGE_SIZE;
+	domain = GSGPU_GEM_DOMAIN_VRAM;
+
+	r = gsgpu_bo_create_kernel(ldev, size, align, domain, &bo, &gpu_addr, (void **)&cpu_ptr);
+	if (r) {
+		DRM_ERROR("xdma_fill_test : gsgpu_bo_create_kernel error\r\n");
+		return r;
+	}
+
+	memset(cpu_ptr, 0x55, size);
+	if (readq(&cpu_ptr[0]) != 0x5555555555555555) {
+		DRM_ERROR("xdma_fill_test : set vram error through pcie\r\n");
+	} else {
+		DRM_INFO("xdma_fill_test : set vram success through pcie\r\n");
+	}
+
+	memset(&ib, 0, sizeof(ib));
+	r = gsgpu_ib_get(ldev, NULL, 256, &ib);
+	if (r) {
+		DRM_ERROR("xdma_fill_test : gsgpu_ib_get error\r\n");
+		goto bo_free;
+	}
+
+	xdma_emit_fill_buffer(&ib, 0xaaaaaaaa, gpu_addr, GSGPU_GPU_PAGE_SIZE);
+	xdma_ring_pad_ib(ring, &ib);
+
+	r = gsgpu_ib_schedule(ring, 1, &ib, NULL, &f);
+	if (r) {
+		DRM_ERROR("xdma_fill_test : gsgpu_ib_schedule error\r\n");
+		goto ib_free;
+	}
+
+	r = dma_fence_wait_timeout(f, false, timeout);
+	if (r == 0) {
+		DRM_ERROR("xdma_fill_test : dma_fence_wait_timeout timed out\r\n");
+		r = -ETIMEDOUT;
+		goto ib_free;
+	} else if (r < 0) {
+		DRM_ERROR("xdma_fill_test : dma_fence_wait_timeout failed\r\n");
+		goto ib_free;
+	}
+
+	if (readq(&cpu_ptr[0]) == 0xaaaaaaaaaaaaaaaa) {
+		DRM_INFO("xdma_fill_test : success\r\n");
+	} else {
+		DRM_ERROR("xdma_fill_test : failed\r\n");
+	}
+
+ib_free:
+	gsgpu_ib_free(ldev, &ib, NULL);
+	dma_fence_put(f);
+
+bo_free:
+	gsgpu_bo_free_kernel(&bo, &gpu_addr, (void **)&cpu_ptr);
+
+	return r;
+}
+
+static const struct gsgpu_buffer_funcs xdma_buffer_funcs = {
+	.copy_max_bytes = 0x3fffc0, /* not 0x3fffff due to HW limitation */
+	.copy_num_dw = 9,
+	.emit_copy_buffer = xdma_emit_copy_buffer,
+
+	.fill_max_bytes = 0x3fffc0, /* not 0x3fffff due to HW limitation */
+	.fill_num_dw = 9,
+	.emit_fill_buffer = xdma_emit_fill_buffer,
+};
+
+static void xdma_set_buffer_funcs(struct gsgpu_device *adev)
+{
+	if (adev->mman.buffer_funcs == NULL) {
+		adev->mman.buffer_funcs = &xdma_buffer_funcs;
+		adev->mman.buffer_funcs_ring = &adev->xdma.instance[0].ring;
+	}
+}
+
+static const struct gsgpu_vm_pte_funcs xdma_vm_pte_funcs = {
+	.copy_pte_num_dw = 9,
+	.copy_pte = xdma_vm_copy_pte,
+
+	.set_pte_pde_num_dw = 9,
+	.set_pte_pde = xdma_vm_set_pte_pde,
+};
+
+static void xdma_set_vm_pte_funcs(struct gsgpu_device *adev)
+{
+	unsigned i;
+
+	if (adev->vm_manager.vm_pte_funcs == NULL) {
+		adev->vm_manager.vm_pte_funcs = &xdma_vm_pte_funcs;
+		for (i = 0; i < adev->xdma.num_instances; i++)
+			adev->vm_manager.vm_pte_rings[i] =
+				&adev->xdma.instance[i].ring;
+
+		adev->vm_manager.vm_pte_num_rings = adev->xdma.num_instances;
+	}
+}
+
+const struct gsgpu_ip_block_version xdma_ip_block = {
+	.type = GSGPU_IP_BLOCK_TYPE_XDMA,
+	.major = 1,
+	.minor = 0,
+	.rev = 0,
+	.funcs = &xdma_ip_funcs,
+};
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_zip.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_zip.c
new file mode 100644
index 000000000000..09b4acf4536a
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_zip.c
@@ -0,0 +1,213 @@
+#include <linux/firmware.h>
+#include <drm/drmP.h>
+#include <drm/drm_cache.h>
+#include "gsgpu.h"
+#include "gsgpu_zip.h"
+
+/**
+ * zip_meta_enable - gart enable
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * This sets up the TLBs, programs the page tables for VMID0,
+ * sets up the hw for VMIDs 1-15 which are allocated on
+ * demand, and sets up the global locations for the LDS, GDS,
+ * and GPUVM for FSA64 clients ().
+ * Returns 0 for success, errors for failure.
+ */
+static int zip_meta_enable(struct gsgpu_device *adev, bool clear)
+{
+	int r;
+
+	if (adev->zip_meta.robj == NULL) {
+		dev_err(adev->dev, "No VRAM object for PCIE ZIP_META.\n");
+		return -EINVAL;
+	}
+	r = gsgpu_zip_meta_vram_pin(adev);
+	if (r)
+		return r;
+
+	if (clear)
+		memset(adev->zip_meta.ptr, 0x00, adev->zip_meta.table_size);
+
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_ZIP, ZIP_DISABLE), 0, 0);
+	gsgpu_cmd_exec(adev, GSCMDi(GSCMD_ZIP, ZIP_SET_BASE, 0), \
+			lower_32_bits(adev->zip_meta.table_addr), upper_32_bits(adev->zip_meta.table_addr));
+
+	gsgpu_cmd_exec(adev, GSCMDi(GSCMD_ZIP, ZIP_SET_MASK, 0), \
+			lower_32_bits(adev->zip_meta.mask), upper_32_bits(adev->zip_meta.mask));
+
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_ZIP, ZIP_ENABLE), 0, 0);
+
+	DRM_INFO("PCIE ZIP META of %uM enabled (table at 0x%016llX).\n",
+		 (unsigned)(adev->zip_meta.table_size >> 20),
+		 (unsigned long long)adev->zip_meta.table_addr);
+	adev->zip_meta.ready = true;
+	return 0;
+}
+
+static int zip_meta_init(struct gsgpu_device *adev)
+{
+	int r;
+
+	if (adev->zip_meta.robj) {
+		WARN(1, "GSGPU PCIE ZIP_META already initialized\n");
+		return 0;
+	}
+	/* Initialize common zip_meta structure */
+	r = gsgpu_zip_meta_init(adev);
+	if (r)
+		return r;
+	adev->zip_meta.table_size = adev->zip_meta.num_gpu_pages *
+		GSGPU_GPU_PAGE_SIZE;
+	adev->zip_meta.mask = roundup_pow_of_two(adev->zip_meta.table_size) - 1;
+	adev->zip_meta.pte_flags = 0;
+	return gsgpu_zip_meta_vram_alloc(adev);
+}
+
+/**
+ * zip_meta_v1_0_gart_disable - zip meta disable
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * This disables all zip meta page table ().
+ */
+static int zip_meta_disable(struct gsgpu_device *adev)
+{
+	if (adev->zip_meta.robj == NULL) {
+		dev_err(adev->dev, "No VRAM object for PCIE ZIP_META.\n");
+		return -EINVAL;
+	}
+
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_ZIP, ZIP_DISABLE), 0, 0);
+
+	adev->zip_meta.ready = false;
+
+	gsgpu_zip_meta_vram_unpin(adev);
+	return 0;
+}
+
+static int zip_early_init(void *handle)
+{
+	return 0;
+}
+
+static int zip_late_init(void *handle)
+{
+	return 0;
+}
+
+static int zip_sw_init(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = zip_meta_init(adev);
+	if (r)
+		return r;
+
+	return 0;
+}
+
+static int zip_sw_fini(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	gsgpu_zip_meta_vram_free(adev);
+	gsgpu_zip_meta_fini(adev);
+
+	return 0;
+}
+
+static int zip_hw_init(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = zip_meta_enable(adev, true);
+	if (r)
+		return r;
+
+	return r;
+}
+
+static int zip_hw_fini(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	zip_meta_disable(adev);
+
+	return 0;
+}
+
+static int zip_suspend(void *handle)
+{
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	zip_hw_fini(adev);
+
+	return 0;
+}
+
+static int zip_resume(void *handle)
+{
+	int r;
+	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
+
+	r = zip_meta_enable(adev, false);
+	if (r)
+		return r;
+
+	return 0;
+}
+
+static bool zip_is_idle(void *handle)
+{
+	return true;
+}
+
+static bool zip_check_soft_reset(void *handle)
+{
+	return false;
+}
+
+static int zip_pre_soft_reset(void *handle)
+{
+	return 0;
+}
+
+static int zip_soft_reset(void *handle)
+{
+	return 0;
+}
+
+static int zip_post_soft_reset(void *handle)
+{
+	return 0;
+}
+
+static const struct gsgpu_ip_funcs zip_ip_funcs = {
+	.name = "zip",
+	.early_init = zip_early_init,
+	.late_init = zip_late_init,
+	.sw_init = zip_sw_init,
+	.sw_fini = zip_sw_fini,
+	.hw_init = zip_hw_init,
+	.hw_fini = zip_hw_fini,
+	.suspend = zip_suspend,
+	.resume = zip_resume,
+	.is_idle = zip_is_idle,
+	.wait_for_idle = NULL,
+	.check_soft_reset = zip_check_soft_reset,
+	.pre_soft_reset = zip_pre_soft_reset,
+	.soft_reset = zip_soft_reset,
+	.post_soft_reset = zip_post_soft_reset,
+};
+
+const struct gsgpu_ip_block_version zip_ip_block = {
+	.type = GSGPU_IP_BLOCK_TYPE_ZIP,
+	.major = 1,
+	.minor = 0,
+	.rev = 0,
+	.funcs = &zip_ip_funcs,
+};
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_zip_meta.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_zip_meta.c
new file mode 100644
index 000000000000..29898575f809
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_zip_meta.c
@@ -0,0 +1,221 @@
+#include <drm/drmP.h>
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+
+/*
+ * TODO write some description
+ */
+
+/*
+ * Common zip meta table functions.
+ */
+
+/**
+ * gsgpu_zip_meta_vram_alloc - allocate vram for zip meta page table
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Allocate video memory for zip meta page table
+ * Returns 0 for success, error for failure.
+ */
+int gsgpu_zip_meta_vram_alloc(struct gsgpu_device *adev)
+{
+	int r;
+	struct gsgpu_bo_param bp;
+
+	if (adev->zip_meta.robj)
+		return 0;
+
+	memset(&bp, 0, sizeof(bp));
+	bp.size = adev->zip_meta.table_size;
+	bp.byte_align = GSGPU_GEM_COMPRESSED_SIZE;
+	bp.domain = GSGPU_GEM_DOMAIN_VRAM;
+	bp.flags = GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |
+		GSGPU_GEM_CREATE_VRAM_CONTIGUOUS;
+	bp.type = ttm_bo_type_kernel;
+	bp.resv = NULL;
+	r = gsgpu_bo_create(adev, &bp, &adev->zip_meta.robj);
+	if (r) {
+		return r;
+	}
+
+	return 0;
+}
+
+/**
+ * gsgpu_zip_meta_vram_pin - pin zip meta page table in vram
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Pin the zip meta page table in vram so it will not be moved
+ * Returns 0 for success, error for failure.
+ */
+int gsgpu_zip_meta_vram_pin(struct gsgpu_device *adev)
+{
+	int r;
+
+	r = gsgpu_bo_reserve(adev->zip_meta.robj, false);
+	if (unlikely(r != 0))
+		return r;
+	r = gsgpu_bo_pin(adev->zip_meta.robj, GSGPU_GEM_DOMAIN_VRAM);
+	if (r) {
+		gsgpu_bo_unreserve(adev->zip_meta.robj);
+		return r;
+	}
+	r = gsgpu_bo_kmap(adev->zip_meta.robj, &adev->zip_meta.ptr);
+	if (r)
+		gsgpu_bo_unpin(adev->zip_meta.robj);
+	gsgpu_bo_unreserve(adev->zip_meta.robj);
+
+	adev->zip_meta.table_addr = gsgpu_bo_gpu_offset(adev->zip_meta.robj);
+	return r;
+}
+
+/**
+ * gsgpu_zip_meta_vram_unpin - unpin zip meta page table in vram
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Unpin the ZIP META page table in vram (pcie r4xx, r5xx+).
+ * These asics require the zip meta table to be in video memory.
+ */
+void gsgpu_zip_meta_vram_unpin(struct gsgpu_device *adev)
+{
+	int r;
+
+	if (adev->zip_meta.robj == NULL) {
+		return;
+	}
+	r = gsgpu_bo_reserve(adev->zip_meta.robj, true);
+	if (likely(r == 0)) {
+		gsgpu_bo_kunmap(adev->zip_meta.robj);
+		gsgpu_bo_unpin(adev->zip_meta.robj);
+		gsgpu_bo_unreserve(adev->zip_meta.robj);
+		adev->zip_meta.ptr = NULL;
+	}
+}
+
+/**
+ * gsgpu_zip_meta_vram_free - free zip meta page table vram
+ *
+ * @adev: gsgpu_device pointer
+ *
+ */
+void gsgpu_zip_meta_vram_free(struct gsgpu_device *adev)
+{
+	if (adev->zip_meta.robj == NULL) {
+		return;
+	}
+	gsgpu_bo_unref(&adev->zip_meta.robj);
+}
+
+/*
+ * Common zip meta functions.
+ */
+/**
+ * gsgpu_zip_meta_unbind - unbind pages from the zip meta page table
+ *
+ * @adev: gsgpu_device pointer
+ * @offset: offset into the GPU's zip meta aperture
+ * @pages: number of pages to unbind
+ *
+ * Unbinds the requested pages from the zip meta page table and
+ * Returns 0 for success, -EINVAL for failure.
+ */
+int gsgpu_zip_meta_unbind(struct gsgpu_device *adev, uint64_t offset,
+			int pages)
+{
+	return 0;
+}
+
+/**
+ * gsgpu_zip_meta_map - map dma_addresses into zip meta entries
+ *
+ * @adev: gsgpu_device pointer
+ * @start: start into the GPU's memery aperture
+ *
+ * Map the dma_addresses into zip meta entries (all asics).
+ * Returns 0 for success, -EINVAL for failure.
+ */
+uint64_t gsgpu_zip_meta_map(struct gsgpu_device *adev, uint64_t start)
+{
+	return 0;
+}
+
+/**
+ * gsgpu_zip_meta_bind - bind pages into the zip meta page table
+ *
+ * @adev: gsgpu_device pointer
+ * @offset: offset into the GPU's zip meta aperture
+ * @pages: number of pages to bind
+ * @pagelist: pages to bind
+ * @dma_addr: DMA addresses of pages
+ *
+ * Binds the requested pages to the zip meta page table
+ * (all asics).
+ * Returns 0 for success, -EINVAL for failure.
+ */
+int gsgpu_zip_meta_bind(struct gsgpu_device *adev, uint64_t offset,
+		     int pages, struct page **pagelist, dma_addr_t *dma_addr,
+		     uint64_t flags)
+{
+#ifdef CONFIG_DRM_GSGPU_ZIP_DEBUGFS
+	unsigned i, t, p;
+#endif
+
+#ifdef CONFIG_DRM_GSGPU_ZIP_DEBUGFS
+	t = offset / GSGPU_GPU_PAGE_SIZE;
+	p = t / GSGPU_GPU_PAGES_IN_CPU_PAGE;
+	for (i = 0; i < pages; i++, p++)
+		adev->zip_meta.pages[p] = pagelist ? pagelist[i] : NULL;
+#endif
+
+	return 0;
+}
+
+/**
+ * gsgpu_zip_meta_init - init the driver info for managing the zip meta
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Allocate the dummy page and init the zip meta driver info (all asics).
+ * Returns 0 for success, error for failure.
+ */
+int gsgpu_zip_meta_init(struct gsgpu_device *adev)
+{
+	/* We need PAGE_SIZE >= GSGPU_GPU_PAGE_SIZE */
+	if (PAGE_SIZE < GSGPU_GPU_PAGE_SIZE) {
+		DRM_ERROR("Page size is smaller than GPU page size!\n");
+		return -EINVAL;
+	}
+
+	/* Compute table size */
+	adev->zip_meta.num_cpu_pages = (adev->gmc.real_vram_size >> 7) / PAGE_SIZE;
+	adev->zip_meta.num_gpu_pages = (adev->gmc.real_vram_size >> 7) / GSGPU_GPU_PAGE_SIZE;
+	DRM_INFO("ZIP META: num cpu pages %u, num gpu pages %u\n",
+		 adev->zip_meta.num_cpu_pages, adev->zip_meta.num_gpu_pages);
+
+#ifdef CONFIG_DRM_GSGPU_ZIP_DEBUGFS
+	/* Allocate pages table */
+	adev->zip_meta.pages = vzalloc(array_size(sizeof(void *),
+					      adev->zip_meta.num_cpu_pages));
+	if (adev->zip_meta.pages == NULL)
+		return -ENOMEM;
+#endif
+
+	return 0;
+}
+
+/**
+ * gsgpu_zip_meta_fini - tear down the driver info for managing the zip meta
+ *
+ * @adev: gsgpu_device pointer
+ *
+ */
+void gsgpu_zip_meta_fini(struct gsgpu_device *adev)
+{
+#ifdef CONFIG_DRM_GSGPU_ZIP_DEBUGFS
+	vfree(adev->zip_meta.pages);
+	adev->zip_meta.pages = NULL;
+#endif
+}
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu.h b/drivers/gpu/drm/gsgpu/include/gsgpu.h
new file mode 100644
index 000000000000..e27c8a0263f6
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu.h
@@ -0,0 +1,1377 @@
+#ifndef __GSGPU_H__
+#define __GSGPU_H__
+
+#include <linux/atomic.h>
+#include <linux/wait.h>
+#include <linux/list.h>
+#include <linux/kref.h>
+#include <linux/rbtree.h>
+#include <linux/hashtable.h>
+#include <linux/dma-fence.h>
+
+#include <drm/ttm/ttm_bo_api.h>
+#include <drm/ttm/ttm_bo_driver.h>
+#include <drm/ttm/ttm_placement.h>
+#include <drm/ttm/ttm_module.h>
+#include <drm/ttm/ttm_execbuf_util.h>
+
+#include <drm/drmP.h>
+#include <drm/drm_gem.h>
+#include <drm/gsgpu_drm.h>
+#include <drm/gpu_scheduler.h>
+
+#include "gsgpu_shared.h"
+#include "gsgpu_mode.h"
+#include "gsgpu_ih.h"
+#include "gsgpu_irq.h"
+#include "gsgpu_ttm.h"
+#include "gsgpu_sync.h"
+#include "gsgpu_ring.h"
+#include "gsgpu_vm.h"
+#include "gsgpu_mn.h"
+#include "gsgpu_gmc.h"
+#include "gsgpu_dc.h"
+#include "gsgpu_gart.h"
+#include "gsgpu_zip_meta.h"
+#include "gsgpu_debugfs.h"
+#include "gsgpu_job.h"
+#include "gsgpu_bo_list.h"
+#include "gsgpu_hw_sema.h"
+
+/*
+ * Modules parameters.
+ */
+extern int gsgpu_modeset;
+extern int gsgpu_vram_limit;
+extern int gsgpu_vis_vram_limit;
+extern int gsgpu_gart_size;
+extern int gsgpu_gtt_size;
+extern int gsgpu_moverate;
+extern int gsgpu_benchmarking;
+extern int gsgpu_testing;
+extern int gsgpu_disp_priority;
+extern int gsgpu_msi;
+extern int gsgpu_lockup_timeout;
+extern int gsgpu_runtime_pm;
+extern int gsgpu_vm_size;
+extern int gsgpu_vm_block_size;
+extern int gsgpu_vm_fragment_size;
+extern int gsgpu_vm_fault_stop;
+extern int gsgpu_vm_debug;
+extern int gsgpu_vm_update_mode;
+extern int gsgpu_sched_jobs;
+extern int gsgpu_sched_hw_submission;
+extern int gsgpu_vram_page_split;
+extern int gsgpu_job_hang_limit;
+extern int gsgpu_gpu_recovery;
+extern int gsgpu_using_ram;
+
+#define GSGPU_BYTES_PER_DW           4
+
+#define GSGPU_KB_SHIFT_BITS          10
+#define GSGPU_MB_SHIFT_BITS          20
+#define GSGPU_GB_SHIFT_BITS          30
+
+#define GSGPU_SG_THRESHOLD			(256*1024*1024)
+#define GSGPU_DEFAULT_GTT_SIZE_MB		3072ULL /* 3GB by default */
+#define GSGPU_WAIT_IDLE_TIMEOUT_IN_MS	        3000
+#define GSGPU_MAX_USEC_TIMEOUT			100000	/* 100 ms */
+#define GSGPU_FENCE_JIFFIES_TIMEOUT		(HZ / 2)
+/* GSGPU_IB_POOL_SIZE must be a power of 2 */
+#define GSGPU_IB_POOL_SIZE			16
+#define GSGPU_DEBUGFS_MAX_COMPONENTS		32
+#define GSGPUFB_CONN_LIMIT			4
+#define GSGPU_BIOS_NUM_SCRATCH			16
+
+/* max number of IP instances */
+#define GSGPU_MAX_XDMA_INSTANCES		2
+
+/* hard reset data */
+#define GSGPU_ASIC_RESET_DATA                  0x39d5e86b
+
+/* GFX current status */
+#define GSGPU_GFX_NORMAL_MODE			0x00000000L
+#define GSGPU_GFX_SAFE_MODE			0x00000001L
+#define GSGPU_GFX_PG_DISABLED_MODE		0x00000002L
+#define GSGPU_GFX_CG_DISABLED_MODE		0x00000004L
+#define GSGPU_GFX_LBPW_DISABLED_MODE		0x00000008L
+
+struct gsgpu_device;
+struct gsgpu_ib;
+struct gsgpu_cs_parser;
+struct gsgpu_job;
+struct gsgpu_irq_src;
+struct gsgpu_fpriv;
+struct gsgpu_bo_va_mapping;
+
+enum gsgpu_chip {
+	dev_7a2000,
+	dev_2k2000
+};
+
+enum gsgpu_cp_irq {
+	GSGPU_CP_IRQ_GFX_EOP = 0,
+	GSGPU_CP_IRQ_LAST
+};
+
+enum gsgpu_xdma_irq {
+	GSGPU_XDMA_IRQ_TRAP0 = 0,
+	GSGPU_XDMA_IRQ_TRAP1,
+	GSGPU_XDMA_IRQ_LAST
+};
+
+int gsgpu_device_ip_wait_for_idle(struct gsgpu_device *adev,
+				   enum gsgpu_ip_block_type block_type);
+bool gsgpu_device_ip_is_idle(struct gsgpu_device *adev,
+			      enum gsgpu_ip_block_type block_type);
+
+#define GSGPU_MAX_IP_NUM 16
+
+struct gsgpu_ip_block_status {
+	bool valid;
+	bool sw;
+	bool hw;
+	bool late_initialized;
+	bool hang;
+};
+
+struct gsgpu_ip_block_version {
+	const enum gsgpu_ip_block_type type;
+	const u32 major;
+	const u32 minor;
+	const u32 rev;
+	const struct gsgpu_ip_funcs *funcs;
+};
+
+struct gsgpu_ip_block {
+	struct gsgpu_ip_block_status status;
+	const struct gsgpu_ip_block_version *version;
+};
+
+int gsgpu_device_ip_block_version_cmp(struct gsgpu_device *adev,
+				       enum gsgpu_ip_block_type type,
+				       u32 major, u32 minor);
+
+struct gsgpu_ip_block *
+gsgpu_device_ip_get_ip_block(struct gsgpu_device *adev,
+			      enum gsgpu_ip_block_type type);
+
+int gsgpu_device_ip_block_add(struct gsgpu_device *adev,
+			       const struct gsgpu_ip_block_version *ip_block_version);
+
+/* provided by hw blocks that can move/clear data.  e.g., gfx or xdma */
+struct gsgpu_buffer_funcs {
+	/* maximum bytes in a single operation */
+	uint32_t	copy_max_bytes;
+
+	/* number of dw to reserve per operation */
+	unsigned	copy_num_dw;
+
+	/* used for buffer migration */
+	void (*emit_copy_buffer)(struct gsgpu_ib *ib,
+				 /* src addr in bytes */
+				 uint64_t src_offset,
+				 /* dst addr in bytes */
+				 uint64_t dst_offset,
+				 /* number of byte to transfer */
+				 uint32_t byte_count);
+
+	/* maximum bytes in a single operation */
+	uint32_t	fill_max_bytes;
+
+	/* number of dw to reserve per operation */
+	unsigned	fill_num_dw;
+
+	/* used for buffer clearing */
+	void (*emit_fill_buffer)(struct gsgpu_ib *ib,
+				 /* value to write to memory */
+				 uint32_t src_data,
+				 /* dst addr in bytes */
+				 uint64_t dst_offset,
+				 /* number of byte to fill */
+				 uint32_t byte_count);
+};
+
+/* provided by hw blocks that can write ptes, e.g., xdma */
+struct gsgpu_vm_pte_funcs {
+	/* number of dw to reserve per operation */
+	unsigned	copy_pte_num_dw;
+
+	/* number of dw to reserve per operation */
+	unsigned	set_pte_pde_num_dw;
+
+	/* copy pte entries from GART */
+	void (*copy_pte)(struct gsgpu_ib *ib,
+			 uint64_t pe, uint64_t src,
+			 unsigned count);
+
+	/* write pte one entry at a time with addr mapping */
+	void (*write_pte)(struct gsgpu_ib *ib, uint64_t pe,
+			  uint64_t value, unsigned count,
+			  uint32_t incr);
+	/* for linear pte/pde updates without addr mapping */
+	void (*set_pte_pde)(struct gsgpu_ib *ib,
+			    uint64_t pe,
+			    uint64_t addr, unsigned count,
+			    uint32_t incr, uint64_t flags);
+};
+
+/* provided by the ih block */
+struct gsgpu_ih_funcs {
+	/* ring read/write ptr handling, called from interrupt context */
+	u32 (*get_wptr)(struct gsgpu_device *adev);
+	bool (*prescreen_iv)(struct gsgpu_device *adev);
+	void (*decode_iv)(struct gsgpu_device *adev,
+			  struct gsgpu_iv_entry *entry);
+	void (*set_rptr)(struct gsgpu_device *adev);
+};
+
+/*
+ * BIOS.
+ */
+bool gsgpu_get_bios(struct gsgpu_device *adev);
+bool gsgpu_read_bios(struct gsgpu_device *adev);
+
+/*
+ * Clocks
+ */
+
+#define GSGPU_MAX_PPLL 3
+
+struct gsgpu_clock {
+	/* 10 Khz units */
+	uint32_t default_mclk;
+	uint32_t default_sclk;
+	uint32_t default_dispclk;
+	uint32_t current_dispclk;
+	uint32_t dp_extclk;
+	uint32_t max_pixel_clock;
+};
+
+/*
+ * GEM.
+ */
+
+#define GSGPU_GEM_DOMAIN_MAX		0x3
+#define gem_to_gsgpu_bo(gobj) container_of((gobj), struct gsgpu_bo, gem_base)
+
+void gsgpu_gem_object_free(struct drm_gem_object *obj);
+int gsgpu_gem_object_open(struct drm_gem_object *obj,
+				struct drm_file *file_priv);
+void gsgpu_gem_object_close(struct drm_gem_object *obj,
+				struct drm_file *file_priv);
+unsigned long gsgpu_gem_timeout(uint64_t timeout_ns);
+struct sg_table *gsgpu_gem_prime_get_sg_table(struct drm_gem_object *obj);
+struct drm_gem_object *
+gsgpu_gem_prime_import_sg_table(struct drm_device *dev,
+				 struct dma_buf_attachment *attach,
+				 struct sg_table *sg);
+struct dma_buf *gsgpu_gem_prime_export(struct drm_device *dev,
+					struct drm_gem_object *gobj,
+					int flags);
+struct drm_gem_object *gsgpu_gem_prime_import(struct drm_device *dev,
+					    struct dma_buf *dma_buf);
+struct reservation_object *gsgpu_gem_prime_res_obj(struct drm_gem_object *);
+void *gsgpu_gem_prime_vmap(struct drm_gem_object *obj);
+void gsgpu_gem_prime_vunmap(struct drm_gem_object *obj, void *vaddr);
+int gsgpu_gem_prime_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma);
+
+/* sub-allocation manager, it has to be protected by another lock.
+ * By conception this is an helper for other part of the driver
+ * like the indirect buffer or semaphore, which both have their
+ * locking.
+ *
+ * Principe is simple, we keep a list of sub allocation in offset
+ * order (first entry has offset == 0, last entry has the highest
+ * offset).
+ *
+ * When allocating new object we first check if there is room at
+ * the end total_size - (last_object_offset + last_object_size) >=
+ * alloc_size. If so we allocate new object there.
+ *
+ * When there is not enough room at the end, we start waiting for
+ * each sub object until we reach object_offset+object_size >=
+ * alloc_size, this object then become the sub object we return.
+ *
+ * Alignment can't be bigger than page size.
+ *
+ * Hole are not considered for allocation to keep things simple.
+ * Assumption is that there won't be hole (all object on same
+ * alignment).
+ */
+
+#define GSGPU_SA_NUM_FENCE_LISTS	32
+
+struct gsgpu_sa_manager {
+	wait_queue_head_t	wq;
+	struct gsgpu_bo	*bo;
+	struct list_head	*hole;
+	struct list_head	flist[GSGPU_SA_NUM_FENCE_LISTS];
+	struct list_head	olist;
+	unsigned		size;
+	uint64_t		gpu_addr;
+	void			*cpu_ptr;
+	uint32_t		domain;
+	uint32_t		align;
+};
+
+/* sub-allocation buffer */
+struct gsgpu_sa_bo {
+	struct list_head		olist;
+	struct list_head		flist;
+	struct gsgpu_sa_manager	*manager;
+	unsigned			soffset;
+	unsigned			eoffset;
+	struct dma_fence	        *fence;
+};
+
+/*
+ * GEM objects.
+ */
+void gsgpu_gem_force_release(struct gsgpu_device *adev);
+int gsgpu_gem_object_create(struct gsgpu_device *adev, unsigned long size,
+			     int alignment, u32 initial_domain,
+			     u64 flags, enum ttm_bo_type type,
+			     struct reservation_object *resv,
+			     struct drm_gem_object **obj);
+
+int gsgpu_mode_dumb_create(struct drm_file *file_priv,
+			    struct drm_device *dev,
+			    struct drm_mode_create_dumb *args);
+int gsgpu_mode_dumb_mmap(struct drm_file *filp,
+			  struct drm_device *dev,
+			  uint32_t handle, uint64_t *offset_p);
+int gsgpu_fence_slab_init(void);
+void gsgpu_fence_slab_fini(void);
+
+//GS Registers
+#define GSGPU_COMMAND				0x0
+#define GSGPU_STATUS				0x4
+#define GSGPU_ARGUMENT0				0x8
+#define GSGPU_ARGUMENT1				0xc
+#define GSGPU_RETURN0				0x10
+#define GSGPU_RETURN1				0x14
+#define GSGPU_GFX_CB_BASE_LO_OFFSET		0x18
+#define GSGPU_GFX_CB_BASE_HI_OFFSET		0x1c
+#define GSGPU_GFX_CB_SIZE_OFFSET		0x20
+#define GSGPU_GFX_CB_WPTR_OFFSET		0x24
+#define GSGPU_GFX_CB_RPTR_OFFSET		0x28
+#define GSGPU_XDMA_CB_BASE_LO_OFFSET		0x2c
+#define GSGPU_XDMA_CB_BASE_HI_OFFSET		0x30
+#define GSGPU_XDMA_CB_SIZE_OFFSET		0x34
+#define GSGPU_XDMA_CB_WPTR_OFFSET		0x38
+#define GSGPU_XDMA_CB_RPTR_OFFSET		0x3c
+#define GSGPU_INT_CB_BASE_LO_OFFSET		0x40
+#define GSGPU_INT_CB_BASE_HI_OFFSET		0x44
+#define GSGPU_INT_CB_SIZE_OFFSET		0x48
+#define GSGPU_INT_CB_WPTR_OFFSET		0x4c
+#define GSGPU_INT_CB_RPTR_OFFSET		0x50
+/* reserved 0x54 ~ 0x74 */
+#define GSGPU_RESERVE_START_OFFSET		0x54
+#define GSGPU_RESERVE_END_OFFSET		0x74
+#define GSGPU_FW_VERSION_OFFSET			0x78
+#define GSGPU_HW_FEATURE_OFFSET			0x7c
+
+#define GSGPU_EC_CTRL				0x80
+#define GSGPU_EC_INT				0x84
+#define GSGPU_HOST_INT				0x88
+#define GSGPU_HWINF				0x8c
+#define GSGPU_FREQ_SCALE			0x9c
+
+#define GSGPU_FW_WPORT				0xf0
+#define GSGPU_FW_WPTR				0xf4
+#define GSGPU_FW_CKSUM				0xf8
+
+//GS Commands
+#define GSCMD(cmd, subcmd)	(((cmd) & 0xFF)  | ((subcmd) & 0xFF) << 8)
+#define GSCMDi(cmd, subcmd, i)	(((cmd) & 0xFF)  | ((subcmd) & 0xFF) << 8 | ((i) & 0xF) << 16)
+
+#define GSCMD_HALT		0x00000000 // stop jobs in GPU, return
+#define GSCMD_PING_5A		0x00000001 // return 5a5a5a5a in status
+#define GSCMD_PING_A5		0x00000002 // return a5a5a5a5 in status
+#define GSCMD_LOOP_DRAM		0x00000003 // loop through DRAM
+#define GSCMD_LOOP_SSRV		0x00000004 // loop through SSRV
+#define GSCMD_START		0x00000005 // start processing command buffer
+#define GSCMD_STOP		0x00000006 // stop processing command buffer
+#define GSCMD_SYNC		0x00000007 // wait pipeline empty
+#define GSCMD_MMU		0x00000008 // mmu related op
+#define GSCMD_SETREG		0x00000009 // internal reg op
+#define GSCMD_PIPE		0x0000000A // op pipeline
+#define GSCMD_ZIP		0x0000000B // op zip
+#define GSCMD_PIPE_FLUSH	1 // op pipeline
+#define GSCMD_FREQ		0x0000000C
+
+#define GSCMD_STS_NULL		0x00000000
+#define GSCMD_STS_BOOT		0xB007B007 // BOOT
+#define GSCMD_STS_DONE		0xD02ED02E // DONE
+#define GSCMD_STS_RUN		0xFFFF0000 // RUNING, lower 16bit can store total command count
+
+#define EC_CTRL_RUN		0x01
+#define EC_CTRL_STOP		0x00
+
+//GS Packets
+#define GSPKT(op, n)	(((op) & 0xFF) | ((n) & 0xFFFF) << 16)
+
+#define	GSPKT_NOP			0x80
+#define	GSPKT_WRITE			0x81
+#define GSPKT_INDIRECT			0x82
+#define GSPKT_FENCE			0x83
+#define GSPKT_TRAP			0x84
+#define GSPKT_POLL			0x85
+#define 	POLL_CONDITION(x)		((x) << 8)
+		/* 0 - true
+		 * 1 - <
+		 * 2 - <=
+		 * 3 - ==
+		 * 4 - !=
+		 * 5 - >=
+		 * 6 - >
+		 */
+#define 	POLL_REG_MEM(x)			((x) << 12)
+		/* 0 - reg
+		 * 1 - mem
+		 */
+#define		POLL_TIMES_INTERVAL(t, i)	((t) << 16 | (i))
+#define GSPKT_WPOLL			0x86
+#define	GSPKT_READ			0x87
+
+//DRAW 0x89
+#define GSPKT_VM_BIND			0x8A
+
+#define GSPKT_XDMA_COPY			0xc0
+
+/* 0 - register
+ * 1 - memory
+ */
+#define	READ_SRC_SEL(x)			((x) << 9)
+#define	WRITE_DST_SEL(x)		((x) << 8)
+#define	WRITE_WAIT			(1 << 15)
+
+/*
+ * IRQS.
+ */
+
+struct gsgpu_flip_work {
+	struct delayed_work		flip_work;
+	struct work_struct		unpin_work;
+	struct gsgpu_device		*adev;
+	int				crtc_id;
+	u32				target_vblank;
+	uint64_t			base;
+	struct drm_pending_vblank_event *event;
+	struct gsgpu_bo		*old_abo;
+	struct dma_fence		*excl;
+	unsigned			shared_count;
+	struct dma_fence		**shared;
+	struct dma_fence_cb		cb;
+	bool				async;
+};
+
+
+/*
+ * CP & rings.
+ */
+
+struct gsgpu_ib {
+	struct gsgpu_sa_bo		*sa_bo;
+	uint32_t			length_dw;
+	uint64_t			gpu_addr;
+	uint32_t			*ptr;
+	uint32_t			flags;
+};
+
+extern const struct drm_sched_backend_ops gsgpu_sched_ops;
+
+/*
+ * Queue manager
+ */
+struct gsgpu_queue_mapper {
+	int 		hw_ip;
+	struct mutex	lock;
+	/* protected by lock */
+	struct gsgpu_ring *queue_map[GSGPU_MAX_RINGS];
+};
+
+struct gsgpu_queue_mgr {
+	struct gsgpu_queue_mapper mapper[GSGPU_MAX_IP_NUM];
+};
+
+int gsgpu_queue_mgr_init(struct gsgpu_device *adev,
+			  struct gsgpu_queue_mgr *mgr);
+int gsgpu_queue_mgr_fini(struct gsgpu_device *adev,
+			  struct gsgpu_queue_mgr *mgr);
+int gsgpu_queue_mgr_map(struct gsgpu_device *adev,
+			 struct gsgpu_queue_mgr *mgr,
+			 u32 hw_ip, u32 instance, u32 ring,
+			 struct gsgpu_ring **out_ring);
+
+/*
+ * context related structures
+ */
+
+struct gsgpu_ctx_ring {
+	uint64_t		sequence;
+	struct dma_fence	**fences;
+	struct drm_sched_entity	entity;
+};
+
+struct gsgpu_ctx {
+	struct kref		refcount;
+	struct gsgpu_device    *adev;
+	struct gsgpu_queue_mgr queue_mgr;
+	unsigned		reset_counter;
+	unsigned        reset_counter_query;
+	uint32_t		vram_lost_counter;
+	spinlock_t		ring_lock;
+	struct dma_fence	**fences;
+	struct gsgpu_ctx_ring	rings[GSGPU_MAX_RINGS];
+	bool			preamble_presented;
+	enum drm_sched_priority init_priority;
+	enum drm_sched_priority override_priority;
+	struct mutex            lock;
+	atomic_t	guilty;
+};
+
+struct gsgpu_ctx_mgr {
+	struct gsgpu_device	*adev;
+	struct mutex		lock;
+	/* protected by lock */
+	struct idr		ctx_handles;
+};
+
+struct gsgpu_ctx *gsgpu_ctx_get(struct gsgpu_fpriv *fpriv, uint32_t id);
+int gsgpu_ctx_put(struct gsgpu_ctx *ctx);
+
+int gsgpu_ctx_add_fence(struct gsgpu_ctx *ctx, struct gsgpu_ring *ring,
+			      struct dma_fence *fence, uint64_t *seq);
+struct dma_fence *gsgpu_ctx_get_fence(struct gsgpu_ctx *ctx,
+				   struct gsgpu_ring *ring, uint64_t seq);
+void gsgpu_ctx_priority_override(struct gsgpu_ctx *ctx,
+				  enum drm_sched_priority priority);
+
+int gsgpu_ctx_ioctl(struct drm_device *dev, void *data,
+		     struct drm_file *filp);
+
+int gsgpu_ctx_wait_prev_fence(struct gsgpu_ctx *ctx, unsigned ring_id);
+
+void gsgpu_ctx_mgr_init(struct gsgpu_ctx_mgr *mgr);
+void gsgpu_ctx_mgr_entity_fini(struct gsgpu_ctx_mgr *mgr);
+void gsgpu_ctx_mgr_entity_flush(struct gsgpu_ctx_mgr *mgr);
+void gsgpu_ctx_mgr_fini(struct gsgpu_ctx_mgr *mgr);
+
+
+/*
+ * file private structure
+ */
+
+struct gsgpu_fpriv {
+	struct gsgpu_vm	vm;
+	struct gsgpu_bo_va	*prt_va;
+	struct gsgpu_bo_va	*csa_va;
+	struct mutex		bo_list_lock;
+	struct idr		bo_list_handles;
+	struct gsgpu_ctx_mgr	ctx_mgr;
+};
+
+struct gsgpu_rlc_funcs {
+	void (*enter_safe_mode)(struct gsgpu_device *adev);
+	void (*exit_safe_mode)(struct gsgpu_device *adev);
+};
+
+struct gsgpu_rlc {
+	/* for power gating */
+	struct gsgpu_bo	*save_restore_obj;
+	uint64_t		save_restore_gpu_addr;
+	volatile uint32_t	*sr_ptr;
+	const u32               *reg_list;
+	u32                     reg_list_size;
+	/* for clear state */
+	struct gsgpu_bo	*clear_state_obj;
+	uint64_t		clear_state_gpu_addr;
+	volatile uint32_t	*cs_ptr;
+	const struct cs_section_def   *cs_data;
+	u32                     clear_state_size;
+	/* for cp tables */
+	struct gsgpu_bo	*cp_table_obj;
+	uint64_t		cp_table_gpu_addr;
+	volatile uint32_t	*cp_table_ptr;
+	u32                     cp_table_size;
+
+	/* safe mode for updating CG/PG state */
+	bool in_safe_mode;
+	const struct gsgpu_rlc_funcs *funcs;
+
+	/* for firmware data */
+	u32 save_and_restore_offset;
+	u32 clear_state_descriptor_offset;
+	u32 avail_scratch_ram_locations;
+	u32 reg_restore_list_size;
+	u32 reg_list_format_start;
+	u32 reg_list_format_separate_start;
+	u32 starting_offsets_start;
+	u32 reg_list_format_size_bytes;
+	u32 reg_list_size_bytes;
+	u32 reg_list_format_direct_reg_list_length;
+	u32 save_restore_list_cntl_size_bytes;
+	u32 save_restore_list_gpm_size_bytes;
+	u32 save_restore_list_srm_size_bytes;
+
+	u32 *register_list_format;
+	u32 *register_restore;
+	u8 *save_restore_list_cntl;
+	u8 *save_restore_list_gpm;
+	u8 *save_restore_list_srm;
+
+	bool is_rlc_v2_1;
+};
+
+/*
+ * GFX configurations
+ */
+#define GSGPU_GFX_MAX_SE 4
+#define GSGPU_GFX_MAX_SH_PER_SE 2
+
+struct gsgpu_rb_config {
+	uint32_t rb_backend_disable;
+	uint32_t user_rb_backend_disable;
+	uint32_t raster_config;
+	uint32_t raster_config_1;
+};
+
+struct gb_addr_config {
+	uint16_t pipe_interleave_size;
+	uint8_t num_pipes;
+	uint8_t max_compress_frags;
+	uint8_t num_banks;
+	uint8_t num_se;
+	uint8_t num_rb_per_se;
+};
+
+struct gsgpu_gfx_config {
+	unsigned max_shader_engines;
+	unsigned max_tile_pipes;
+	unsigned max_cu_per_sh;
+	unsigned max_sh_per_se;
+	unsigned max_backends_per_se;
+	unsigned max_texture_channel_caches;
+	unsigned max_gprs;
+	unsigned max_gs_threads;
+	unsigned max_hw_contexts;
+	unsigned sc_prim_fifo_size_frontend;
+	unsigned sc_prim_fifo_size_backend;
+	unsigned sc_hiz_tile_fifo_size;
+	unsigned sc_earlyz_tile_fifo_size;
+
+	unsigned num_tile_pipes;
+	unsigned backend_enable_mask;
+	unsigned mem_max_burst_length_bytes;
+	unsigned mem_row_size_in_kb;
+	unsigned shader_engine_tile_size;
+	unsigned num_gpus;
+	unsigned multi_gpu_tile_size;
+	unsigned mc_arb_ramcfg;
+	unsigned gb_addr_config;
+	unsigned num_rbs;
+	unsigned gs_vgt_table_depth;
+	unsigned gs_prim_buffer_depth;
+
+	uint32_t tile_mode_array[32];
+	uint32_t macrotile_mode_array[16];
+
+	struct gb_addr_config gb_addr_config_fields;
+	struct gsgpu_rb_config rb_config[GSGPU_GFX_MAX_SE][GSGPU_GFX_MAX_SH_PER_SE];
+
+	/* gfx configure feature */
+	uint32_t double_offchip_lds_buf;
+	/* cached value of DB_DEBUG2 */
+	uint32_t db_debug2;
+};
+
+struct gsgpu_cu_info {
+	uint32_t simd_per_cu;
+	uint32_t max_waves_per_simd;
+	uint32_t wave_front_size;
+	uint32_t max_scratch_slots_per_cu;
+	uint32_t lds_size;
+
+	/* total active CU number */
+	uint32_t number;
+	uint32_t ao_cu_mask;
+	uint32_t ao_cu_bitmap[4][4];
+	uint32_t bitmap[4][4];
+};
+
+struct gsgpu_gfx_funcs {
+	/* get the gpu clock counter */
+	uint64_t (*get_gpu_clock_counter)(struct gsgpu_device *adev);
+	void (*read_wave_data)(struct gsgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t *dst, int *no_fields);
+	void (*read_wave_vgprs)(struct gsgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t thread, uint32_t start, uint32_t size, uint32_t *dst);
+	void (*read_wave_sgprs)(struct gsgpu_device *adev, uint32_t simd, uint32_t wave, uint32_t start, uint32_t size, uint32_t *dst);
+	void (*select_me_pipe_q)(struct gsgpu_device *adev, u32 me, u32 pipe, u32 queue);
+};
+
+struct sq_work {
+	struct work_struct	work;
+	unsigned ih_data;
+};
+
+struct gsgpu_gfx {
+	struct mutex			gpu_clock_mutex;
+	struct gsgpu_gfx_config		config;
+	struct gsgpu_rlc		rlc;
+	const struct firmware		*cp_fw;	/* CP firmware */
+	uint32_t			cp_fw_version;
+	uint32_t			cp_feature_version;
+	struct gsgpu_ring		gfx_ring[GSGPU_MAX_GFX_RINGS];
+	unsigned			num_gfx_rings;
+	struct gsgpu_irq_src		eop_irq;
+	struct gsgpu_irq_src		priv_reg_irq;
+	struct gsgpu_irq_src		priv_inst_irq;
+	struct gsgpu_irq_src		cp_ecc_error_irq;
+
+	/* gfx status */
+	uint32_t			gfx_current_status;
+	/* ce ram size*/
+	unsigned			ce_ram_size;
+	struct gsgpu_cu_info		cu_info;
+	const struct gsgpu_gfx_funcs	*funcs;
+
+	/* s3/s4 mask */
+	bool                            in_suspend;
+};
+
+int gsgpu_ib_get(struct gsgpu_device *adev, struct gsgpu_vm *vm,
+		  unsigned size, struct gsgpu_ib *ib);
+void gsgpu_ib_free(struct gsgpu_device *adev, struct gsgpu_ib *ib,
+		    struct dma_fence *f);
+int gsgpu_ib_schedule(struct gsgpu_ring *ring, unsigned num_ibs,
+		       struct gsgpu_ib *ibs, struct gsgpu_job *job,
+		       struct dma_fence **f);
+int gsgpu_ib_pool_init(struct gsgpu_device *adev);
+void gsgpu_ib_pool_fini(struct gsgpu_device *adev);
+int gsgpu_ib_ring_tests(struct gsgpu_device *adev);
+
+/*
+ * CS.
+ */
+struct gsgpu_cs_chunk {
+	uint32_t		chunk_id;
+	uint32_t		length_dw;
+	void			*kdata;
+};
+
+struct gsgpu_cs_parser {
+	struct gsgpu_device	*adev;
+	struct drm_file		*filp;
+	struct gsgpu_ctx	*ctx;
+
+	/* chunks */
+	unsigned		nchunks;
+	struct gsgpu_cs_chunk	*chunks;
+
+	/* scheduler job object */
+	struct gsgpu_job	*job;
+	struct gsgpu_ring	*ring;
+
+	/* buffer objects */
+	struct ww_acquire_ctx		ticket;
+	struct gsgpu_bo_list		*bo_list;
+	struct gsgpu_mn		*mn;
+	struct gsgpu_bo_list_entry	vm_pd;
+	struct list_head		validated;
+	struct dma_fence		*fence;
+	uint64_t			bytes_moved_threshold;
+	uint64_t			bytes_moved_vis_threshold;
+	uint64_t			bytes_moved;
+	uint64_t			bytes_moved_vis;
+	struct gsgpu_bo_list_entry	*evictable;
+
+	/* user fence */
+	struct gsgpu_bo_list_entry	uf_entry;
+
+	unsigned num_post_dep_syncobjs;
+	struct drm_syncobj **post_dep_syncobjs;
+};
+
+static inline u32 gsgpu_get_ib_value(struct gsgpu_cs_parser *p,
+				      uint32_t ib_idx, int idx)
+{
+	return p->job->ibs[ib_idx].ptr[idx];
+}
+
+static inline void gsgpu_set_ib_value(struct gsgpu_cs_parser *p,
+				       uint32_t ib_idx, int idx,
+				       uint32_t value)
+{
+	p->job->ibs[ib_idx].ptr[idx] = value;
+}
+
+/*
+ * Writeback
+ */
+#define GSGPU_MAX_WB 128	/* Reserve at most 128 WB slots for gsgpu-owned rings. */
+
+struct gsgpu_wb {
+	struct gsgpu_bo	*wb_obj;
+	volatile uint32_t	*wb;
+	uint64_t		gpu_addr;
+	u32			num_wb;	/* Number of wb slots actually reserved for gsgpu. */
+	unsigned long		used[DIV_ROUND_UP(GSGPU_MAX_WB, BITS_PER_LONG)];
+};
+
+int gsgpu_device_wb_get(struct gsgpu_device *adev, u32 *wb);
+void gsgpu_device_wb_free(struct gsgpu_device *adev, u32 wb);
+
+/*
+ * XDMA
+ */
+struct gsgpu_xdma_instance {
+	/* SDMA firmware */
+	const struct firmware	*fw;
+	uint32_t		fw_version;
+	uint32_t		feature_version;
+
+	struct gsgpu_ring	ring;
+	bool			burst_nop;
+};
+
+struct gsgpu_xdma {
+	struct gsgpu_xdma_instance instance[GSGPU_MAX_XDMA_INSTANCES];
+	struct gsgpu_irq_src	trap_irq;
+	struct gsgpu_irq_src	illegal_inst_irq;
+	int			num_instances;
+};
+
+/*
+ * Firmware
+ */
+struct gsgpu_firmware {
+	struct gsgpu_bo *fw_buf;
+	unsigned int fw_size;
+	unsigned int max_ucodes;
+	struct gsgpu_bo *rbuf;
+	struct mutex mutex;
+
+	/* gpu info firmware data pointer */
+	const struct firmware *gpu_info_fw;
+
+	void *fw_buf_ptr;
+	uint64_t fw_buf_mc;
+};
+
+/*
+ * Benchmarking
+ */
+void gsgpu_benchmark(struct gsgpu_device *adev, int test_number);
+
+
+/*
+ * Testing
+ */
+void gsgpu_test_moves(struct gsgpu_device *adev);
+
+/*
+ * ASIC specific functions.
+ */
+struct gsgpu_asic_funcs {
+	bool (*read_bios_from_rom)(struct gsgpu_device *adev,
+				   u8 *bios, u32 length_bytes);
+	int (*read_register)(struct gsgpu_device *adev, u32 se_num,
+			     u32 sh_num, u32 reg_offset, u32 *value);
+	void (*set_vga_state)(struct gsgpu_device *adev, bool state);
+	int (*reset)(struct gsgpu_device *adev);
+	/* get the reference clock */
+	u32 (*get_clk)(struct gsgpu_device *adev);
+	/* static power management */
+	int (*get_pcie_lanes)(struct gsgpu_device *adev);
+	void (*set_pcie_lanes)(struct gsgpu_device *adev, int lanes);
+	/* check if the asic needs a full reset of if soft reset will work */
+	bool (*need_full_reset)(struct gsgpu_device *adev);
+};
+
+/*
+ * IOCTL.
+ */
+int gsgpu_gem_create_ioctl(struct drm_device *dev, void *data,
+			    struct drm_file *filp);
+int gsgpu_bo_list_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *filp);
+
+int gsgpu_gem_info_ioctl(struct drm_device *dev, void *data,
+			  struct drm_file *filp);
+int gsgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *filp);
+int gsgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,
+			  struct drm_file *filp);
+int gsgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,
+			      struct drm_file *filp);
+int gsgpu_gem_va_ioctl(struct drm_device *dev, void *data,
+			  struct drm_file *filp);
+int gsgpu_gem_op_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *filp);
+int gsgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp);
+int gsgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,
+				    struct drm_file *filp);
+int gsgpu_cs_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *filp);
+int gsgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *filp);
+
+int gsgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *filp);
+
+int gsgpu_hw_sema_op_ioctl(struct drm_device *dev, void *data, struct drm_file *filp);
+
+/* VRAM scratch page for HDP bug, default vram page */
+struct gsgpu_vram_scratch {
+	struct gsgpu_bo		*robj;
+	volatile uint32_t		*ptr;
+	u64				gpu_addr;
+};
+
+/*
+ * Firmware VRAM reservation
+ */
+struct gsgpu_fw_vram_usage {
+	u64 start_offset;
+	u64 size;
+	struct gsgpu_bo *reserved_bo;
+	void *va;
+};
+
+/*
+ * Core structure, functions and helpers.
+ */
+typedef uint32_t (*gsgpu_rreg_t)(struct gsgpu_device*, uint32_t);
+typedef void (*gsgpu_wreg_t)(struct gsgpu_device*, uint32_t, uint32_t);
+
+typedef uint32_t (*gsgpu_block_rreg_t)(struct gsgpu_device*, uint32_t, uint32_t);
+typedef void (*gsgpu_block_wreg_t)(struct gsgpu_device*, uint32_t, uint32_t, uint32_t);
+
+/* Define the HW IP blocks will be used in driver , add more if necessary */
+enum gsgpu_hw_ip_block_type {
+	GC_HWIP = 1,
+	HDP_HWIP,
+	SDMA0_HWIP,
+	SDMA1_HWIP,
+	MMHUB_HWIP,
+	ATHUB_HWIP,
+	NBIO_HWIP,
+	MP0_HWIP,
+	MP1_HWIP,
+	UVD_HWIP,
+	VCN_HWIP = UVD_HWIP,
+	VCE_HWIP,
+	DF_HWIP,
+	DCE_HWIP,
+	OSSSYS_HWIP,
+	SMUIO_HWIP,
+	PWR_HWIP,
+	NBIF_HWIP,
+	THM_HWIP,
+	CLK_HWIP,
+	MAX_HWIP
+};
+
+#define HWIP_MAX_INSTANCE	6
+
+#define GSGPU_RESET_MAGIC_NUM 64
+struct gsgpu_device {
+	struct device			*dev;
+	struct drm_device		*ddev;
+	struct pci_dev			*pdev;
+	struct pci_dev			*loongson_dc;
+	u8				dc_revision;
+	u8				chip;
+
+	/* ASIC */
+	enum gsgpu_family_type		family_type;
+	uint32_t			family;
+	unsigned long			flags;
+	int				usec_timeout;
+	const struct gsgpu_asic_funcs	*asic_funcs;
+	bool				shutdown;
+	bool				need_dma32;
+	bool				need_swiotlb;
+	bool				accel_working;
+	struct work_struct		reset_work;
+	struct notifier_block		acpi_nb;
+	struct gsgpu_debugfs		debugfs[GSGPU_DEBUGFS_MAX_COMPONENTS];
+	unsigned			debugfs_count;
+#if defined(CONFIG_DEBUG_FS)
+	struct dentry			*debugfs_regs[GSGPU_DEBUGFS_MAX_COMPONENTS];
+#endif
+	struct mutex			srbm_mutex;
+	/* GRBM index mutex. Protects concurrent access to GRBM index */
+	struct mutex                    grbm_idx_mutex;
+	struct dev_pm_domain		vga_pm_domain;
+	bool				have_disp_power_ref;
+
+	/* BIOS */
+	bool				is_atom_fw;
+	uint8_t				*bios;
+	uint32_t			bios_size;
+	struct gsgpu_bo		*stolen_vga_memory;
+	uint32_t			bios_scratch_reg_offset;
+	uint32_t			bios_scratch[GSGPU_BIOS_NUM_SCRATCH];
+
+	/* Register mmio */
+	resource_size_t			rmmio_base;
+	resource_size_t			rmmio_size;
+	void __iomem			*rmmio;
+
+	/* loongson dc mmio */
+	resource_size_t			loongson_dc_rmmio_base;
+	resource_size_t			loongson_dc_rmmio_size;
+	void __iomem			*loongson_dc_rmmio;
+	void __iomem			*io_base;
+
+	/* protects concurrent MM_INDEX/DATA based register access */
+	spinlock_t mmio_idx_lock;
+	spinlock_t dc_mmio_lock;
+	/* protects concurrent PCIE register access */
+	spinlock_t pcie_idx_lock;
+
+	/* protects concurrent se_cac register access */
+	spinlock_t se_cac_idx_lock;
+	gsgpu_rreg_t			se_cac_rreg;
+	gsgpu_wreg_t			se_cac_wreg;
+
+	/* clock/pll info */
+	struct gsgpu_clock            clock;
+
+	/* MC */
+	struct gsgpu_gmc		gmc;
+	struct gsgpu_gart		gart;
+	struct gsgpu_zip_meta	zip_meta;
+	dma_addr_t			dummy_page_addr;
+	struct gsgpu_vm_manager	vm_manager;
+
+	/* memory management */
+	struct gsgpu_mman		mman;
+	struct gsgpu_vram_scratch	vram_scratch;
+	struct gsgpu_wb		wb;
+	atomic64_t			num_bytes_moved;
+	atomic64_t			num_evictions;
+	atomic64_t			num_vram_cpu_page_faults;
+	atomic_t			gpu_reset_counter;
+	atomic_t			vram_lost_counter;
+
+	struct gsgpu_hw_sema_mgr hw_sema_mgr;
+
+	/* data for buffer migration throttling */
+	struct {
+		spinlock_t		lock;
+		s64			last_update_us;
+		s64			accum_us; /* accumulated microseconds */
+		s64			accum_us_vis; /* for visible VRAM */
+		u32			log2_max_MBps;
+	} mm_stats;
+
+	struct gsgpu_dc			*dc;
+	struct gsgpu_mode_info		mode_info;
+	struct gsgpu_dc_i2c		*i2c[2];
+	struct work_struct		hotplug_work;
+	struct gsgpu_irq_src		vsync_irq;
+	struct gsgpu_irq_src		i2c_irq;
+	struct gsgpu_irq_src		hpd_irq;
+
+	/* rings */
+	u64				fence_context;
+	unsigned			num_rings;
+	struct gsgpu_ring		*rings[GSGPU_MAX_RINGS];
+	bool				ib_pool_ready;
+	struct gsgpu_sa_manager	ring_tmp_bo;
+
+	/* interrupts */
+	struct gsgpu_irq		irq;
+
+	/* HPD */
+	int				vga_hpd_status;
+
+	u32				cg_flags;
+	u32				pg_flags;
+
+	/* gfx */
+	struct gsgpu_gfx		gfx;
+
+	/* xdma */
+	struct gsgpu_xdma		xdma;
+
+	/* firmwares */
+	struct gsgpu_firmware		firmware;
+
+	struct gsgpu_ip_block          ip_blocks[GSGPU_MAX_IP_NUM];
+	int				num_ip_blocks;
+	struct mutex	mn_lock;
+	DECLARE_HASHTABLE(mn_hash, 7);
+
+	/* tracking pinned memory */
+	atomic64_t vram_pin_size;
+	atomic64_t visible_pin_size;
+	atomic64_t gart_pin_size;
+
+	/* delayed work_func for deferring clockgating during resume */
+	struct delayed_work     late_init_work;
+
+	//zl prior virt
+	uint32_t			reg_val_offs;
+	/* firmware VRAM reservation */
+	struct gsgpu_fw_vram_usage fw_vram_usage;
+
+	/* link all shadow bo */
+	struct list_head                shadow_list;
+	struct mutex                    shadow_list_lock;
+	/* keep an lru list of rings by HW IP */
+	struct list_head		ring_lru_list;
+	spinlock_t			ring_lru_list_lock;
+
+	/* record hw reset is performed */
+	bool has_hw_reset;
+	u8				reset_magic[GSGPU_RESET_MAGIC_NUM];
+
+	/* record last mm index being written through WREG32*/
+	unsigned long last_mm_index;
+	bool                            in_gpu_reset;
+	struct mutex  lock_reset;
+
+	struct loongson_vbios *vbios;
+	bool cursor_showed;
+	bool clone_mode;
+	int cursor_crtc_id;
+	bool inited;
+};
+
+static inline struct gsgpu_device *gsgpu_ttm_adev(struct ttm_bo_device *bdev)
+{
+	return container_of(bdev, struct gsgpu_device, mman.bdev);
+}
+
+int gsgpu_device_init(struct gsgpu_device *adev,
+		       struct drm_device *ddev,
+		       struct pci_dev *pdev,
+		       uint32_t flags);
+void gsgpu_device_fini(struct gsgpu_device *adev);
+int gsgpu_gpu_wait_for_idle(struct gsgpu_device *adev);
+
+uint64_t gsgpu_cmd_exec(struct gsgpu_device *adev, uint32_t cmd,
+			uint32_t arg0, uint32_t arg1);
+
+uint32_t gsgpu_mm_rreg(struct gsgpu_device *adev, uint32_t reg,
+			uint32_t acc_flags);
+void gsgpu_mm_wreg(struct gsgpu_device *adev, uint32_t reg, uint32_t v);
+void gsgpu_mm_wreg8(struct gsgpu_device *adev, uint32_t offset, uint8_t value);
+uint8_t gsgpu_mm_rreg8(struct gsgpu_device *adev, uint32_t offset);
+
+/*
+ * Registers read & write functions.
+ */
+
+#define GSGPU_REGS_IDX       (1<<0)
+#define GSGPU_REGS_NO_KIQ    (1<<1)
+
+#define RREG32_NO_KIQ(reg) gsgpu_mm_rreg(adev, (reg), GSGPU_REGS_NO_KIQ)
+#define WREG32_NO_KIQ(reg, v) gsgpu_mm_wreg(adev, (reg), (v))
+
+#define RREG8(reg) gsgpu_mm_rreg8(adev, (reg))
+#define WREG8(reg, v) gsgpu_mm_wreg8(adev, (reg), (v))
+
+#define RREG32(reg) gsgpu_mm_rreg(adev, (reg), 0)
+#define RREG32_IDX(reg) gsgpu_mm_rreg(adev, (reg), GSGPU_REGS_IDX)
+#define DREG32(reg) printk(KERN_INFO "REGISTER: " #reg " : 0x%08X\n", gsgpu_mm_rreg(adev, (reg), 0))
+#define WREG32(reg, v) gsgpu_mm_wreg(adev, (reg), (v))
+#define WREG32_IDX(reg, v) gsgpu_mm_wreg(adev, (reg), (v))
+#define REG_SET(FIELD, v) (((v) << FIELD##_SHIFT) & FIELD##_MASK)
+#define REG_GET(FIELD, v) (((v) << FIELD##_SHIFT) & FIELD##_MASK)
+#define RREG32_SE_CAC(reg) adev->se_cac_rreg(adev, (reg))
+#define WREG32_SE_CAC(reg, v) adev->se_cac_wreg(adev, (reg), (v))
+#define WREG32_P(reg, val, mask)				\
+	do {							\
+		uint32_t tmp_ = RREG32(reg);			\
+		tmp_ &= (mask);					\
+		tmp_ |= ((val) & ~(mask));			\
+		WREG32(reg, tmp_);				\
+	} while (0)
+#define WREG32_AND(reg, and) WREG32_P(reg, 0, and)
+#define WREG32_OR(reg, or) WREG32_P(reg, or, ~(or))
+#define WREG32_PLL_P(reg, val, mask)				\
+	do {							\
+		uint32_t tmp_ = RREG32_PLL(reg);		\
+		tmp_ &= (mask);					\
+		tmp_ |= ((val) & ~(mask));			\
+		WREG32_PLL(reg, tmp_);				\
+	} while (0)
+#define DREG32_SYS(sqf, adev, reg) seq_printf((sqf), #reg " : 0x%08X\n", gsgpu_mm_rreg((adev), (reg), false))
+
+#define REG_FIELD_SHIFT(reg, field) reg##__##field##__SHIFT
+#define REG_FIELD_MASK(reg, field) reg##__##field##_MASK
+
+#define REG_SET_FIELD(orig_val, reg, field, field_val)			\
+	(((orig_val) & ~REG_FIELD_MASK(reg, field)) |			\
+	 (REG_FIELD_MASK(reg, field) & ((field_val) << REG_FIELD_SHIFT(reg, field))))
+
+#define REG_GET_FIELD(value, reg, field)				\
+	(((value) & REG_FIELD_MASK(reg, field)) >> REG_FIELD_SHIFT(reg, field))
+
+#define WREG32_FIELD(reg, field, val)	\
+	WREG32(mm##reg, (RREG32(mm##reg) & ~REG_FIELD_MASK(reg, field)) | (val) << REG_FIELD_SHIFT(reg, field))
+
+#define WREG32_FIELD_OFFSET(reg, offset, field, val)	\
+	WREG32(mm##reg + offset, (RREG32(mm##reg + offset) & ~REG_FIELD_MASK(reg, field)) | (val) << REG_FIELD_SHIFT(reg, field))
+
+/*
+ * BIOS helpers.
+ */
+#define RBIOS8(i) (adev->bios[i])
+#define RBIOS16(i) (RBIOS8(i) | (RBIOS8((i)+1) << 8))
+#define RBIOS32(i) ((RBIOS16(i)) | (RBIOS16((i)+2) << 16))
+
+static inline struct gsgpu_xdma_instance *
+gsgpu_get_xdma_instance(struct gsgpu_ring *ring)
+{
+	struct gsgpu_device *adev = ring->adev;
+	int i;
+
+	for (i = 0; i < adev->xdma.num_instances; i++)
+		if (&adev->xdma.instance[i].ring == ring)
+			break;
+
+	if (i < GSGPU_MAX_XDMA_INSTANCES)
+		return &adev->xdma.instance[i];
+	else
+		return NULL;
+}
+
+/*
+ * ASICs macro.
+ */
+#define gsgpu_asic_set_vga_state(adev, state) ((adev)->asic_funcs->set_vga_state((adev), (state)))
+#define gsgpu_asic_reset(adev) ((adev)->asic_funcs->reset((adev)))
+#define gsgpu_asic_get_clk(adev) ((adev)->asic_funcs->get_clk((adev)))
+#define gsgpu_get_pcie_lanes(adev) ((adev)->asic_funcs->get_pcie_lanes((adev)))
+#define gsgpu_set_pcie_lanes(adev, l) ((adev)->asic_funcs->set_pcie_lanes((adev), (l)))
+#define gsgpu_asic_read_bios_from_rom(adev, b, l) ((adev)->asic_funcs->read_bios_from_rom((adev), (b), (l)))
+#define gsgpu_asic_read_register(adev, se, sh, offset, v) ((adev)->asic_funcs->read_register((adev), (se), (sh), (offset), (v)))
+#define gsgpu_asic_need_full_reset(adev) ((adev)->asic_funcs->need_full_reset((adev)))
+#define gsgpu_gmc_flush_gpu_tlb(adev, vmid) ((adev)->gmc.gmc_funcs->flush_gpu_tlb((adev), (vmid)))
+#define gsgpu_gmc_emit_flush_gpu_tlb(r, vmid, addr) ((r)->adev->gmc.gmc_funcs->emit_flush_gpu_tlb((r), (vmid), (addr)))
+#define gsgpu_gmc_emit_pasid_mapping(r, vmid, pasid) ((r)->adev->gmc.gmc_funcs->emit_pasid_mapping((r), (vmid), (pasid)))
+#define gsgpu_gmc_set_pte_pde(adev, pt, idx, addr, flags) ((adev)->gmc.gmc_funcs->set_pte_pde((adev), (pt), (idx), (addr), (flags)))
+#define gsgpu_gmc_get_vm_pde(adev, level, dst, flags) ((adev)->gmc.gmc_funcs->get_vm_pde((adev), (level), (dst), (flags)))
+#define gsgpu_gmc_get_pte_flags(adev, flags) ((adev)->gmc.gmc_funcs->get_vm_pte_flags((adev), (flags)))
+#define gsgpu_vm_copy_pte(adev, ib, pe, src, count) ((adev)->vm_manager.vm_pte_funcs->copy_pte((ib), (pe), (src), (count)))
+#define gsgpu_vm_write_pte(adev, ib, pe, value, count, incr) ((adev)->vm_manager.vm_pte_funcs->write_pte((ib), (pe), (value), (count), (incr)))
+#define gsgpu_vm_set_pte_pde(adev, ib, pe, addr, count, incr, flags) ((adev)->vm_manager.vm_pte_funcs->set_pte_pde((ib), (pe), (addr), (count), (incr), (flags)))
+#define gsgpu_ring_parse_cs(r, p, ib) ((r)->funcs->parse_cs((p), (ib)))
+#define gsgpu_ring_patch_cs_in_place(r, p, ib) ((r)->funcs->patch_cs_in_place((p), (ib)))
+#define gsgpu_ring_test_ring(r) ((r)->funcs->test_ring((r)))
+#define gsgpu_ring_test_ib(r, t) ((r)->funcs->test_ib((r), (t)))
+#define gsgpu_ring_test_xdma(r, t) ((r)->funcs->test_xdma((r), (t)))
+#define gsgpu_ring_get_rptr(r) ((r)->funcs->get_rptr((r)))
+#define gsgpu_ring_get_wptr(r) ((r)->funcs->get_wptr((r)))
+#define gsgpu_ring_set_wptr(r) ((r)->funcs->set_wptr((r)))
+#define gsgpu_ring_emit_ib(r, ib, vmid, c) ((r)->funcs->emit_ib((r), (ib), (vmid), (c)))
+#define gsgpu_ring_emit_pipeline_sync(r) ((r)->funcs->emit_pipeline_sync((r)))
+#define gsgpu_ring_emit_vm_flush(r, vmid, addr) ((r)->funcs->emit_vm_flush((r), (vmid), (addr)))
+#define gsgpu_ring_emit_fence(r, addr, seq, flags) ((r)->funcs->emit_fence((r), (addr), (seq), (flags)))
+#define gsgpu_ring_emit_switch_buffer(r) ((r)->funcs->emit_switch_buffer((r)))
+#define gsgpu_ring_emit_cntxcntl(r, d) ((r)->funcs->emit_cntxcntl((r), (d)))
+#define gsgpu_ring_emit_rreg(r, d) ((r)->funcs->emit_rreg((r), (d)))
+#define gsgpu_ring_emit_wreg(r, d, v) ((r)->funcs->emit_wreg((r), (d), (v)))
+#define gsgpu_ring_emit_reg_wait(r, d, v, m) ((r)->funcs->emit_reg_wait((r), (d), (v), (m)))
+#define gsgpu_ring_emit_reg_write_reg_wait(r, d0, d1, v, m) ((r)->funcs->emit_reg_write_reg_wait((r), (d0), (d1), (v), (m)))
+#define gsgpu_ring_emit_tmz(r, b) ((r)->funcs->emit_tmz((r), (b)))
+#define gsgpu_ring_pad_ib(r, ib) ((r)->funcs->pad_ib((r), (ib)))
+#define gsgpu_ring_init_cond_exec(r) ((r)->funcs->init_cond_exec((r)))
+#define gsgpu_ring_patch_cond_exec(r, o) ((r)->funcs->patch_cond_exec((r), (o)))
+#define gsgpu_ih_get_wptr(adev) ((adev)->irq.ih_funcs->get_wptr((adev)))
+#define gsgpu_ih_prescreen_iv(adev) ((adev)->irq.ih_funcs->prescreen_iv((adev)))
+#define gsgpu_ih_decode_iv(adev, iv) ((adev)->irq.ih_funcs->decode_iv((adev), (iv)))
+#define gsgpu_ih_set_rptr(adev) ((adev)->irq.ih_funcs->set_rptr((adev)))
+#define gsgpu_display_vblank_get_counter(adev, crtc) ((adev)->mode_info.funcs->vblank_get_counter((adev), (crtc)))
+#define gsgpu_display_backlight_set_level(adev, e, l) ((adev)->mode_info.funcs->backlight_set_level((e), (l)))
+#define gsgpu_display_backlight_get_level(adev, e) ((adev)->mode_info.funcs->backlight_get_level((e)))
+#define gsgpu_display_hpd_sense(adev, h) ((adev)->mode_info.funcs->hpd_sense((adev), (h)))
+#define gsgpu_display_hpd_set_polarity(adev, h) ((adev)->mode_info.funcs->hpd_set_polarity((adev), (h)))
+#define gsgpu_display_page_flip(adev, crtc, base, async) ((adev)->mode_info.funcs->page_flip((adev), (crtc), (base), (async)))
+#define gsgpu_display_page_flip_get_scanoutpos(adev, crtc, vbl, pos) ((adev)->mode_info.funcs->page_flip_get_scanoutpos((adev), (crtc), (vbl), (pos)))
+#define gsgpu_emit_copy_buffer(adev, ib, s, d, b) ((adev)->mman.buffer_funcs->emit_copy_buffer((ib),  (s), (d), (b)))
+#define gsgpu_emit_fill_buffer(adev, ib, s, d, b) ((adev)->mman.buffer_funcs->emit_fill_buffer((ib), (s), (d), (b)))
+#define gsgpu_gfx_get_gpu_clock_counter(adev) ((adev)->gfx.funcs->get_gpu_clock_counter((adev)))
+#define gsgpu_psp_check_fw_loading_status(adev, i) ((adev)->firmware.funcs->check_fw_loading_status((adev), (i)))
+#define gsgpu_gfx_select_me_pipe_q(adev, me, pipe, q) ((adev)->gfx.funcs->select_me_pipe_q((adev), (me), (pipe), (q)))
+
+/* Common functions */
+int gsgpu_device_gpu_recover(struct gsgpu_device *adev,
+			      struct gsgpu_job *job, bool force);
+void gsgpu_device_pci_config_reset(struct gsgpu_device *adev);
+bool gsgpu_device_need_post(struct gsgpu_device *adev);
+void gsgpu_display_update_priority(struct gsgpu_device *adev);
+
+void gsgpu_cs_report_moved_bytes(struct gsgpu_device *adev, u64 num_bytes,
+				  u64 num_vis_bytes);
+void gsgpu_device_vram_location(struct gsgpu_device *adev,
+				 struct gsgpu_gmc *mc, u64 base);
+void gsgpu_device_gart_location(struct gsgpu_device *adev,
+				 struct gsgpu_gmc *mc);
+int gsgpu_device_resize_fb_bar(struct gsgpu_device *adev);
+void gsgpu_device_program_register_sequence(struct gsgpu_device *adev,
+					     const u32 *registers,
+					     const u32 array_size);
+
+/*
+ * KMS
+ */
+extern const struct drm_ioctl_desc gsgpu_ioctls_kms[];
+extern const int gsgpu_max_kms_ioctl;
+
+int gsgpu_driver_load_kms(struct drm_device *dev, unsigned long flags);
+void gsgpu_driver_unload_kms(struct drm_device *dev);
+void gsgpu_driver_lastclose_kms(struct drm_device *dev);
+int gsgpu_driver_open_kms(struct drm_device *dev, struct drm_file *file_priv);
+void gsgpu_driver_postclose_kms(struct drm_device *dev,
+				 struct drm_file *file_priv);
+int gsgpu_device_ip_suspend(struct gsgpu_device *adev);
+int gsgpu_device_suspend(struct drm_device *dev, bool suspend, bool fbcon);
+int gsgpu_device_resume(struct drm_device *dev, bool resume, bool fbcon);
+u32 gsgpu_get_vblank_counter_kms(struct drm_device *dev, unsigned int pipe);
+long gsgpu_kms_compat_ioctl(struct file *filp, unsigned int cmd,
+			     unsigned long arg);
+
+/*
+ * functions used by gsgpu_encoder.c
+ */
+struct gsgpu_afmt_acr {
+	u32 clock;
+
+	int n_32khz;
+	int cts_32khz;
+
+	int n_44_1khz;
+	int cts_44_1khz;
+
+	int n_48khz;
+	int cts_48khz;
+
+};
+
+struct gsgpu_afmt_acr gsgpu_afmt_acr(uint32_t clock);
+
+/* gsgpu_acpi.c */
+#if defined(CONFIG_ACPI)
+int gsgpu_acpi_init(struct gsgpu_device *adev);
+void gsgpu_acpi_fini(struct gsgpu_device *adev);
+bool gsgpu_acpi_is_pcie_performance_request_supported(struct gsgpu_device *adev);
+int gsgpu_acpi_pcie_performance_request(struct gsgpu_device *adev,
+						u8 perf_req, bool advertise);
+int gsgpu_acpi_pcie_notify_device_ready(struct gsgpu_device *adev);
+#else
+static inline int gsgpu_acpi_init(struct gsgpu_device *adev) { return 0; }
+static inline void gsgpu_acpi_fini(struct gsgpu_device *adev) { }
+#endif
+
+int gsgpu_cs_find_mapping(struct gsgpu_cs_parser *parser,
+			   uint64_t addr, struct gsgpu_bo **bo,
+			   struct gsgpu_bo_va_mapping **mapping);
+
+#include "gsgpu_object.h"
+#endif
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_backlight.h b/drivers/gpu/drm/gsgpu/include/gsgpu_backlight.h
new file mode 100644
index 000000000000..d65f72bb68d5
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_backlight.h
@@ -0,0 +1,70 @@
+// SPDX-License-Identifier: GPL-2.0+
+#ifndef __GSGPU_BACKLIGHT_H__
+#define __GSGPU_BACKLIGHT_H__
+
+#include <linux/types.h>
+#include <linux/gpio.h>
+#include <linux/pwm.h>
+
+#define BL_MAX_LEVEL 100
+#define BL_MIN_LEVEL 1
+#define BL_DEF_LEVEL 60
+#define GPIO_LCD_EN 62
+#define GPIO_LCD_VDD 63
+
+struct gsgpu_backlight {
+	void *driver_private;
+	struct backlight_device *device;
+	struct pwm_device *pwm;
+	int display_pipe_index;
+	u32 pwm_id;
+	u32 pwm_polarity;
+	u32 pwm_period;
+	bool hw_enabled;
+	u32 level;
+	u32 max;
+	u32 min;
+
+	unsigned int (*get_brightness)(struct gsgpu_backlight *ls_bl);
+	void (*set_brightness)(struct gsgpu_backlight *ls_bl,
+			       unsigned int level);
+	void (*enable)(struct gsgpu_backlight *ls_bl);
+	void (*disable)(struct gsgpu_backlight *ls_bl);
+	void (*power)(struct gsgpu_backlight *ls_bl, bool enable);
+};
+
+#define BACKLIGHT_DEFAULT_METHOD_CLOSE(ls_bl)\
+	do { \
+		ls_bl->hw_enabled = false;\
+		gpio_set_value(GPIO_LCD_EN, 0);\
+		msleep(10);\
+		pwm_disable(ls_bl->pwm); \
+	} while (0)
+
+#define BACKLIGHT_DEFAULT_METHOD_FORCE_CLOSE(ls_bl)\
+	do { \
+		ls_bl->hw_enabled = false;\
+		BACKLIGHT_DEFAULT_METHOD_CLOSE(ls_bl);\
+		msleep(160);\
+		gpio_set_value(GPIO_LCD_VDD, 0); \
+	} while (0)
+
+#define BACKLIGHT_DEFAULT_METHOD_OPEN(ls_bl)\
+	do { \
+		pwm_enable(ls_bl->pwm);\
+		msleep(10);\
+		gpio_set_value(GPIO_LCD_EN, 1);\
+		ls_bl->hw_enabled = true; \
+	} while (0)
+
+#define BACKLIGHT_DEFAULT_METHOD_FORCE_OPEN(ls_bl)\
+	do {\
+		gpio_set_value(GPIO_LCD_VDD, 1);\
+		msleep(160);\
+		BACKLIGHT_DEFAULT_METHOD_OPEN(ls_bl);\
+		ls_bl->hw_enabled = true; \
+	} while (0)
+
+int gsgpu_backlight_register(struct drm_connector *connector);
+
+#endif /* __GSGPU_BACKLIGHT_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_bo_list.h b/drivers/gpu/drm/gsgpu/include/gsgpu_bo_list.h
new file mode 100644
index 000000000000..f81479471068
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_bo_list.h
@@ -0,0 +1,60 @@
+#ifndef __GSGPU_BO_LIST_H__
+#define __GSGPU_BO_LIST_H__
+
+#include <drm/ttm/ttm_execbuf_util.h>
+#include <drm/gsgpu_drm.h>
+
+struct gsgpu_device;
+struct gsgpu_bo;
+struct gsgpu_bo_va;
+struct gsgpu_fpriv;
+
+struct gsgpu_bo_list_entry {
+	struct gsgpu_bo		*robj;
+	struct ttm_validate_buffer	tv;
+	struct gsgpu_bo_va		*bo_va;
+	uint32_t			priority;
+	struct page			**user_pages;
+	int				user_invalidated;
+};
+
+struct gsgpu_bo_list {
+	struct rcu_head rhead;
+	struct kref refcount;
+	unsigned first_userptr;
+	unsigned num_entries;
+};
+
+int gsgpu_bo_list_get(struct gsgpu_fpriv *fpriv, int id,
+		       struct gsgpu_bo_list **result);
+void gsgpu_bo_list_get_list(struct gsgpu_bo_list *list,
+			     struct list_head *validated);
+void gsgpu_bo_list_put(struct gsgpu_bo_list *list);
+int gsgpu_bo_create_list_entry_array(struct drm_gsgpu_bo_list_in *in,
+				      struct drm_gsgpu_bo_list_entry **info_param);
+
+int gsgpu_bo_list_create(struct gsgpu_device *adev,
+				 struct drm_file *filp,
+				 struct drm_gsgpu_bo_list_entry *info,
+				 unsigned num_entries,
+				 struct gsgpu_bo_list **list);
+
+static inline struct gsgpu_bo_list_entry *
+gsgpu_bo_list_array_entry(struct gsgpu_bo_list *list, unsigned index)
+{
+	struct gsgpu_bo_list_entry *array = (void *)&list[1];
+
+	return &array[index];
+}
+
+#define gsgpu_bo_list_for_each_entry(e, list) \
+	for (e = gsgpu_bo_list_array_entry(list, 0); \
+	     e != gsgpu_bo_list_array_entry(list, (list)->num_entries); \
+	     ++e)
+
+#define gsgpu_bo_list_for_each_userptr_entry(e, list) \
+	for (e = gsgpu_bo_list_array_entry(list, (list)->first_userptr); \
+	     e != gsgpu_bo_list_array_entry(list, (list)->num_entries); \
+	     ++e)
+
+#endif /* __GSGPU_BO_LIST_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_common.h b/drivers/gpu/drm/gsgpu/include/gsgpu_common.h
new file mode 100644
index 000000000000..bf45eab6a9ef
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_common.h
@@ -0,0 +1,8 @@
+#ifndef __GSGPU_COMMON_H__
+#define __GSGPU_COMMON_H__
+
+#define VI_FLUSH_GPU_TLB_NUM_WREG	3
+
+int gsgpu_set_ip_blocks(struct gsgpu_device *adev);
+
+#endif /*__GSGPU_COMMON_H__*/
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_cp.h b/drivers/gpu/drm/gsgpu/include/gsgpu_cp.h
new file mode 100644
index 000000000000..2abfe8d5c577
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_cp.h
@@ -0,0 +1,26 @@
+#ifndef __GSGPU_CP_H__
+#define __GSGPU_CP_H__
+
+int gsgpu_cp_init(struct gsgpu_device *adev);
+
+int gsgpu_cp_gfx_load_microcode(struct gsgpu_device *adev);
+int gsgpu_cp_enable(struct gsgpu_device *adev, bool enable);
+
+int gsgpu_cp_fini(struct gsgpu_device *adev);
+
+static inline bool gsgpu_cp_wait_done(struct gsgpu_device *adev)
+{
+	int i;
+	for (i = 0; i < adev->usec_timeout; i++) {
+
+		if (RREG32(GSGPU_STATUS) == GSCMD_STS_DONE)
+			return true;
+
+		msleep(1);
+	}
+
+	dev_err(adev->dev, "\n gsgpu cp hang!!! \n");
+	return false;
+}
+
+#endif /* __GSGPU_CP_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc.h
new file mode 100644
index 000000000000..bd752a8d1547
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc.h
@@ -0,0 +1,112 @@
+#ifndef __DC_INTERFACE_H__
+#define __DC_INTERFACE_H__
+
+#include "gsgpu_dc_irq.h"
+
+#define DC_VER "1.0"
+#define DC_DVO_MAXLINK 2
+
+extern const struct gsgpu_ip_block_version dc_ip_block;
+
+struct gsgpu_device;
+struct drm_device;
+struct dc_cursor_info;
+
+struct dc_cursor_position {
+	uint32_t x;
+	uint32_t y;
+	uint32_t x_hotspot;
+	uint32_t y_hotspot;
+	/* This parameter indicates whether HW cursor should be enabled */
+	bool enable;
+};
+
+struct irq_list_head {
+	struct list_head head;
+	/* In case this interrupt needs post-processing, 'work' will be queued*/
+	struct work_struct work;
+};
+
+struct gsgpu_link_info {
+	bool fine;
+	struct gsgpu_dc_crtc *crtc;
+	struct gsgpu_dc_i2c *i2c;
+	struct gsgpu_dc_encoder *encoder;
+	struct gsgpu_dc_connector *connector;
+	struct gsgpu_dc_bridge *bridge;
+};
+
+struct gsgpu_dc {
+	struct drm_device *ddev;
+	struct gsgpu_device *adev;
+	struct gsgpu_vbios *vbios;
+	struct list_head crtc_list;
+	struct list_head encoder_list;
+	struct list_head connector_list;
+	struct gsgpu_link_info *link_info;
+
+	/* base information */
+	int chip;
+	int ip_version;
+	int hw_version;
+	int links;
+	int max_cursor_size;
+	int hdmi_ctrl_reg;
+
+	/* DC meta */
+	struct gsgpu_bo *meta_bo;
+	uint64_t meta_gpu_addr;
+	void *meta_cpu_addr;
+
+	/**
+	 * Caches device atomic state for suspend/resume
+	 */
+	struct drm_atomic_state *cached_state;
+
+	spinlock_t irq_handler_list_table_lock;
+	struct irq_list_head irq_handler_list_low_tab[DC_IRQ_SOURCES_NUMBER];
+	struct list_head irq_handler_list_high_tab[DC_IRQ_SOURCES_NUMBER];
+};
+
+union plane_address {
+	struct {
+		u32 low_part;
+		u32 high_part;
+	};
+	uint64_t raw;
+};
+
+struct dc_timing_info {
+	s32 clock;		/* from drm_display_mode::clock*/
+	s32 hdisplay;
+	s32 hsync_start;
+	s32 hsync_end;
+	s32 htotal;
+	s32 vdisplay;
+	s32 vsync_start;
+	s32 vsync_end;
+	s32 vtotal;
+	u32 stride;
+	u32 depth;
+	u32 use_dma32;
+};
+
+struct dc_cursor_move {
+	u32 hot_y;
+	u32 hot_x;
+	u32 x, y;
+	bool enable;
+};
+
+u32 dc_readl(struct gsgpu_device *adev, u32 reg);
+void dc_writel(struct gsgpu_device *adev, u32 reg, u32 val);
+u32 dc_readl_locked(struct gsgpu_device *adev, u32 reg);
+void dc_writel_locked(struct gsgpu_device *adev, u32 reg, u32 val);
+
+bool dc_submit_timing_update(struct gsgpu_dc *dc, u32 link, struct dc_timing_info *timing);
+int dc_register_irq_handlers(struct gsgpu_device *adev);
+void handle_cursor_update(struct drm_plane *plane,
+			  struct drm_plane_state *old_plane_state);
+bool crtc_cursor_set(struct gsgpu_dc_crtc *crtc, struct dc_cursor_info *cursor);
+
+#endif
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc_connector.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_connector.h
new file mode 100644
index 000000000000..dbbbc82ade7d
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_connector.h
@@ -0,0 +1,29 @@
+#ifndef __GSGPU_DC_CONNECTOR__
+#define __GSGPU_DC_CONNECTOR__
+
+#include <drm/drmP.h>
+#include "gsgpu.h"
+#include "gsgpu_dc_resource.h"
+
+struct dc_connector_state {
+	struct drm_connector_state base;
+	enum gsgpu_rmx_type scaling;
+	uint8_t underscan_vborder;
+	uint8_t underscan_hborder;
+	uint8_t max_bpc;
+	bool underscan_enable;
+};
+
+struct gsgpu_dc_connector {
+	struct connector_resource *resource;
+	struct gsgpu_dc *dc;
+	struct list_head node;
+};
+
+#define to_dc_connector_state(x)\
+	container_of((x), struct dc_connector_state, base)
+
+struct gsgpu_dc_connector *dc_connector_construct(struct gsgpu_dc *dc, struct connector_resource *resource);
+int gsgpu_dc_connector_init(struct gsgpu_device *adev, uint32_t link_index);
+
+#endif /* __GSGPU_DC_CONNECTOR__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc_crtc.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_crtc.h
new file mode 100644
index 000000000000..9ca47babc22c
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_crtc.h
@@ -0,0 +1,47 @@
+#ifndef __DC_CRTC_H__
+#define __DC_CRTC_H__
+
+#include "gsgpu_dc.h"
+
+struct gsgpu_dc_crtc {
+	struct crtc_resource *resource;
+	struct gsgpu_dc *dc;
+	struct list_head node;
+	int array_mode;
+};
+
+enum fb_color_format {
+	DC_FB_FORMAT_NONE = 0,
+	DC_FB_FORMAT12,
+	DC_FB_FORMAT15,
+	DC_FB_FORMAT16,
+	DC_FB_FORMAT24,
+	DC_FB_FORMAT32 = DC_FB_FORMAT24
+};
+
+enum cursor_format {
+	CUR_FORMAT_NONE,
+	CUR_FORMAT_MONO,
+	CUR_FORMAT_ARGB8888,
+};
+
+struct pixel_clock {
+	u32 l2_div;
+	u32 l1_loopc;
+	u32 l1_frefc;
+};
+
+struct gsgpu_dc_crtc *dc_crtc_construct(struct gsgpu_dc *dc, struct crtc_resource *resource);
+int gsgpu_dc_crtc_init(struct gsgpu_device *adev,
+		       struct drm_plane *plane, uint32_t crtc_index);
+u32 dc_vblank_get_counter(struct gsgpu_device *adev, int crtc_num);
+int dc_crtc_get_scanoutpos(struct gsgpu_device *adev, int crtc_num, u32 *vbl, u32 *position);
+
+void dc_crtc_destroy(struct gsgpu_dc_crtc *crtc);
+bool dc_crtc_enable(struct gsgpu_crtc *acrtc, bool enable);
+bool dc_crtc_timing_set(struct gsgpu_dc_crtc *crtc, struct dc_timing_info *timing);
+bool dc_crtc_vblank_enable(struct gsgpu_dc_crtc *crtc, bool enable);
+u32 dc_crtc_get_vblank_counter(struct gsgpu_dc_crtc *crtc);
+bool dc_crtc_vblank_ack(struct gsgpu_dc_crtc *crtc);
+
+#endif /* __DC_CRTC_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc_encoder.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_encoder.h
new file mode 100644
index 000000000000..5abe1d8755b5
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_encoder.h
@@ -0,0 +1,15 @@
+#ifndef STREAM_ENCODER_H_
+#define STREAM_ENCODER_H_
+
+#include "gsgpu_dc_resource.h"
+
+struct gsgpu_dc_encoder {
+	struct encoder_resource *resource;
+	struct gsgpu_dc *dc;
+	bool has_ext_encoder;
+};
+
+struct gsgpu_dc_encoder *dc_encoder_construct(struct gsgpu_dc *dc, struct encoder_resource *resource);
+int gsgpu_dc_encoder_init(struct gsgpu_device *adev, int link_index);
+
+#endif /* STREAM_ENCODER_H_ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc_hdmi.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_hdmi.h
new file mode 100644
index 000000000000..c1ab093b96f6
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_hdmi.h
@@ -0,0 +1,13 @@
+#ifndef __GSGPU_HDMI_H__
+#define __GSGPU_HDMI_H__
+
+int gsgpu_hdmi_init(struct gsgpu_device *adev);
+void gsgpu_hdmi_suspend(struct gsgpu_device *adev);
+int gsgpu_hdmi_resume(struct gsgpu_device *adev);
+void dc_hdmi_encoder_enable(struct drm_encoder *encoder);
+void dc_hdmi_encoder_mode_set(struct drm_encoder *encoder,
+			      struct drm_display_mode *mode,
+			      struct drm_display_mode *adjusted_mode);
+void hdmi_phy_pll_config(struct gsgpu_device *adev, int index, int clock);
+
+#endif /* __GSGPU_HDMI_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc_i2c.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_i2c.h
new file mode 100644
index 000000000000..de38540888f1
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_i2c.h
@@ -0,0 +1,49 @@
+#ifndef __DC_I2C_H__
+#define __DC_I2C_H__
+
+#define DC_I2C_ADDR 0x1f00
+#define DC_GPIO_CFG_OFFSET 0x1660
+#define DC_GPIO_IN_OFFSET 0x1650
+#define DC_GPIO_OUT_OFFSET 0x1650
+#define DC_I2C_TON 10
+
+#define DC_I2C_PRER_LO_REG	0x0
+#define DC_I2C_PRER_HI_REG	0x1
+#define DC_I2C_CTR_REG		0x2
+#define DC_I2C_TXR_REG		0x3
+#define DC_I2C_RXR_REG		0x3
+#define DC_I2C_CR_REG		0x4
+#define DC_I2C_SR_REG		0x4
+
+#define CTR_EN			0x80
+#define CTR_IEN			0x40
+
+#define CR_START		0x81
+#define CR_STOP			0x41
+#define CR_READ			0x21
+#define CR_WRITE		0x11
+#define CR_ACK			0x8
+#define CR_IACK			0x1
+
+#define SR_NOACK		0x80
+#define SR_BUSY			0x40
+#define SR_AL			0x20
+#define SR_TIP			0x2
+#define SR_IF			0x1
+
+#define dc_readb(addr) readb(i2c->reg_base + addr)
+#define dc_writeb(val, addr) writeb(val, i2c->reg_base + addr)
+
+struct gsgpu_dc_i2c {
+	struct gsgpu_device *adev;
+	struct i2c_adapter adapter;
+	struct i2c_client *ddc_client;
+	struct completion cmd_complete;
+	void __iomem *reg_base;
+	u32 data, clock;
+};
+
+int gsgpu_i2c_init(struct gsgpu_device *adev, uint32_t link_index);
+void gsgpu_dc_i2c_irq(struct gsgpu_dc_i2c *i2c);
+
+#endif /* __DC_I2C_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc_irq.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_irq.h
new file mode 100644
index 000000000000..9af2e9e72010
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_irq.h
@@ -0,0 +1,93 @@
+#ifndef __GSGPU_DM_IRQ_H__
+#define __GSGPU_DM_IRQ_H__
+
+#define DC_HDMI_HOTPLUG_STATUS 0x1ba0
+#define DC_INT_I2C0_EN BIT(27)
+#define DC_INT_I2C1_EN BIT(28)
+#define DC_INT_HDMI0_HOTPLUG_EN BIT(29)
+#define DC_INT_HDMI1_HOTPLUG_EN BIT(30)
+#define DC_INT_VGA_HOTPLUG_EN BIT(31)
+#define DC_VGA_HOTPULG_CFG     0x1bb0
+#define DC_VGA_HPD_STATUS_MASK 0x3
+#define LS_FB_VSYNC0_INT (1 << 2)
+#define LS_FB_HDMI0_INT (1 << 13)
+#define LS_FB_HDMI1_INT (1 << 14)
+#define LS_FB_VGA_INT (1 << 15)
+
+struct gsgpu_dc_crtc;
+struct gsgpu_dc;
+typedef void (*interrupt_handler)(void *);
+typedef void *irq_handler_idx;
+#define DAL_INVALID_IRQ_HANDLER_IDX NULL
+
+#define DC_INT_ID_VSYNC1 1
+#define DC_INT_ID_HSYNC1 2
+#define DC_INT_ID_VSYNC0 3
+#define DC_INT_ID_HSYNC0 4
+#define DC_INT_ID_CURSOR_READEND 5
+#define DC_INT_ID_READEND_FB1 6
+#define DC_INT_ID_READEND_FB0 7
+#define DC_INT_ID_UNDERFLOW_DB1 8
+#define DC_INT_ID_UNDERFLOW_DB0 9
+#define DC_INT_ID_FUNDERFLOW_DB1 10
+#define DC_INT_ID_FUNDERFLOW_DB0 11
+#define DC_INT_ID_I2C0 12
+#define DC_INT_ID_I2C1 13
+#define DC_INT_ID_HPD_HDMI0 14
+#define DC_INT_ID_HPD_HDMI1 15
+#define DC_INT_ID_HPD_VGA 16
+#define DC_INT_ID_MAX 17
+
+enum dc_irq_source {
+	DC_IRQ_SOURCE_INVALID = 0,
+
+	DC_IRQ_SOURCE_VSYNC0,
+	DC_IRQ_SOURCE_VSYNC1,
+
+	DC_IRQ_SOURCE_I2C0,
+	DC_IRQ_SOURCE_I2C1,
+
+	DC_IRQ_SOURCE_HPD_HDMI0,
+	DC_IRQ_SOURCE_HPD_HDMI1,
+	DC_IRQ_SOURCE_HPD_VGA,
+
+	DC_IRQ_SOURCE_HPD_HDMI0_NULL,
+	DC_IRQ_SOURCE_HPD_HDMI1_NULL,
+	DC_IRQ_SOURCES_NUMBER
+};
+
+enum irq_type {
+	DC_IRQ_TYPE_VSYNC = DC_IRQ_SOURCE_VSYNC0,
+	DC_IRQ_TYPE_I2C = DC_IRQ_SOURCE_I2C0,
+	DC_IRQ_TYPE_HPD = DC_IRQ_SOURCE_HPD_HDMI0,
+};
+
+#define DC_VALID_IRQ_SRC_NUM(src) \
+	((src) <= DC_IRQ_SOURCES_NUMBER && (src) > DC_IRQ_SOURCE_INVALID)
+
+enum dc_interrupt_context {
+	INTERRUPT_LOW_IRQ_CONTEXT = 0,
+	INTERRUPT_HIGH_IRQ_CONTEXT,
+	INTERRUPT_CONTEXT_NUMBER
+};
+
+struct dc_interrupt_params {
+	enum dc_irq_source irq_source;
+	enum dc_interrupt_context int_context;
+};
+
+struct dc_irq_handler_data {
+	struct list_head list;
+	interrupt_handler handler;
+	void *handler_arg;
+	enum dc_irq_source irq_source;
+};
+
+int gsgpu_dc_irq_init(struct gsgpu_device *adev);
+void gsgpu_dc_irq_fini(struct gsgpu_device *adev);
+void dc_set_irq_funcs(struct gsgpu_device *adev);
+void gsgpu_dc_hpd_init(struct gsgpu_device *adev);
+void gsgpu_dc_hpd_disable(struct gsgpu_device *adev);
+bool dc_interrupt_enable(struct gsgpu_dc *dc, enum dc_irq_source src, bool enable);
+
+#endif /* __GSGPU_DM_IRQ_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc_plane.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_plane.h
new file mode 100644
index 000000000000..f97c4f962e73
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_plane.h
@@ -0,0 +1,43 @@
+#ifndef __DC_PLANE_H__
+#define __DC_PLANE_H__
+
+enum dc_plane_type{
+	DC_PLANE_PRIMARY,
+	DC_PLANE_OVERLAY,
+	DC_PLANE_CURSOR
+};
+
+struct dc_primary_plane {
+	union plane_address address;
+};
+
+struct dc_cursor_info {
+	bool enable;
+	int x;
+	int y;
+	union plane_address address;
+};
+
+struct dc_plane_update {
+	enum dc_plane_type type;
+	union {
+		struct dc_cursor_info cursor;
+		struct dc_primary_plane primary;
+	};
+};
+
+int gsgpu_dc_plane_init(struct gsgpu_device *adev,
+			struct gsgpu_plane *aplane,
+			unsigned long possible_crtcs);
+
+int initialize_plane(struct gsgpu_device *adev,
+		     struct gsgpu_mode_info *mode_info,
+		     int plane_id);
+
+bool dc_submit_plane_update(struct gsgpu_dc *dc, u32 link,
+			    struct dc_plane_update *update);
+
+bool dc_crtc_plane_update(struct gsgpu_dc_crtc *crtc,
+			  struct dc_plane_update *update);
+
+#endif /* __DC_PLANE_H__ */
\ No newline at end of file
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc_reg.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_reg.h
new file mode 100644
index 000000000000..fa59625e38de
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_reg.h
@@ -0,0 +1,167 @@
+#ifndef __GSGPU_DC_REG_H__
+#define __GSGPU_DC_REG_H__
+
+#define DC_REG_INCREASE 0x10
+#define CURRENT_REG(x, link)  ((x) + (link * DC_REG_INCREASE))
+
+#define HT1LO_PCICFG_BASE 0xefdfe000000UL
+#define LS7A_PCH_CFG_SPACE_REG (TO_UNCAC(HT1LO_PCICFG_BASE)|0x0000a810)
+#define LS7A_PCH_CFG_REG_BASE ((*(volatile unsigned int *)(LS7A_PCH_CFG_SPACE_REG))&0xfffffff0)
+#define LS_PIX0_PLL (LS7A_PCH_CFG_REG_BASE + 0x04b0)
+#define LS_PIX1_PLL (LS7A_PCH_CFG_REG_BASE + 0x04c0)
+#define DC_IO_PIX_PLL 0x04B0
+
+/* CRTC */
+#define DC_CRTC_CFG_REG			0x1240
+#define CRTC_CFG_FORMAT_MASK	0x7
+#define CRTC_CFG_FORMAT32	(1 << 2)
+#define CRTC_CFG_FORMAT16	(11 << 0)
+#define CRTC_CFG_FORMAT15	(1 << 1)
+#define CRTC_CFG_FORMAT12	(1 << 0)
+#define CRTC_CFG_TILE4x4	BIT(4)
+#define CRTC_CFG_LINEAR		~BIT(4)
+#define CRTC_CFG_FB_SWITCH	BIT(7)
+#define CRTC_CFG_ENABLE		BIT(8)
+#define CRTC_CFG_SWITCH_PANEL	BIT(9)
+#define CRTC_CFG_DMA_MASK	(0x3 << 16)
+#define CRTC_CFG_DMA_32		(0x3 << 16)
+#define CRTC_CFG_DMA_64		(0x2 << 16)
+#define CRTC_CFG_DMA_128	(0x1 << 16)
+#define CRTC_CFG_DMA_256	(0x0 << 16)
+#define CRTC_CFG_GAMMA		BIT(12)
+#define CRTC_CFG_RESET		BIT(20)
+
+#define DC_CRTC_FBADDR0_LO_REG		(0x1260)
+#define DC_CRTC_FBADDR1_LO_REG		(0x1580)
+#define DC_CRTC_FBADDR0_HI_REG		(0x15A0)
+#define DC_CRTC_FBADDR1_HI_REG		(0x15C0)
+#define DC_CRTC_STRIDE_REG		(0x1280)
+#define DC_CRTC_FBORIGIN_REG		(0x1300)
+
+#define DC_CRTC_DITCFG_REG		(0x1360)
+#define DC_CRTC_DITTAB_LO_REG		(0x1380)
+#define DC_CRTC_DITTAB_HI_REG		(0x13A0)
+
+#define DC_CRTC_PANELCFG_REG		(0x13C0)
+#define CRTC_PANCFG_BASE	0x80001010
+#define CRTC_PANCFG_DE		BIT(0)
+#define CRTC_PANCFG_DEPOL	BIT(1)
+#define CRTC_PANCFG_CLKEN	BIT(8)
+#define CRTC_PANCFG_CLKPOL	BIT(9)
+#define DC_CRTC_PANELTIM_REG		(0x13E0)
+
+#define DC_CRTC_HDISPLAY_REG		(0x1400)
+#define CRTC_HPIXEL_SHIFT	0
+#define CRTC_HPIXEL_MASK	0xFFFF
+#define CRTC_HTOTAL_SHIFT	16
+#define CRTC_HTOTAL_MASK	0xFFFF
+
+#define DC_CRTC_HSYNC_REG		(0x1420)
+#define CRTC_HSYNC_START_SHIFT	0
+#define CRTC_HSYNC_START_MASK	0xFFFF
+#define CRTC_HSYNC_END_MASK	0x3FFF
+#define CRTC_HSYNC_END_SHIFT	16
+#define CRTC_HSYNC_POLSE	BIT(30)
+#define CRTC_HSYNC_POL		BIT(31)
+
+#define DC_CRTC_VDISPLAY_REG		(0x1480)
+#define CRTC_VPIXEL_SHIFT	0
+#define CRTC_VPIXEL_MASK	0xFFFF
+#define CRTC_VTOTAL_SHIFT	16
+#define CRTC_VTOTAL_MASK	0xFFFF
+
+#define DC_CRTC_VSYNC_REG		(0x14A0)
+#define CRTC_VSYNC_START_SHIFT	0
+#define CRTC_VSYNC_START_MASK	0xFFFF
+#define CRTC_VSYNC_END_SHIFT	16
+#define CRTC_VSYNC_END_MASK	0x3FFF
+#define CRTC_VSYNC_POLSE	BIT(30)
+#define CRTC_VSYNC_POL		BIT(31)
+
+#define DC_CRTC_GAMINDEX_REG		(0x14E0)
+#define DC_CRTC_GAMDATA_REG		(0x1500)
+#define CRTC_GAMMA_BLUE_SHIFT	0
+#define CRTC_GAMMA_BLUE_MASK	0xFF
+#define CRTC_GAMMA_GREEN_SHIFT	8
+#define CRTC_GAMMA_GREEN_MASK	0xFF
+#define CRTC_GAMMA_RED_SHIFT	16
+#define CRTC_GAMMA_RED_MASK	0xFF
+
+#define DC_CRTC_SYNCDEV_REG		(0x1B80)
+#define DC_CRTC_DISPLAY_POS_REG		(0x14C0)
+#define DC_VSYNC_COUNTER_REG		(0x1A00)
+
+#define DC_INT_REG			(0x1570)
+#define INT_VSYNC1_ENABLE	BIT(16)
+#define INT_VSYNC0_ENABLE	BIT(18)
+
+/* CURSOR */
+#define DC_CURSOR0_CFG_REG		0x1520
+#define DC_CURSOR1_CFG_REG		0x1670
+#define DC_CURSOR_FORMAT_SHIFT	0
+#define DC_CURSOR_FORMAT_MASK	0x3
+#define DC_CURSOR_MODE_CLEAN	~(1 << 2)
+#define DC_CURSOR_MODE_32x32	(0 << 2)
+#define DC_CURSOR_MODE_64x64	(1 << 2)
+#define DC_CURSOR_DISPLAY_SHIFT	4
+#define DC_CURSOR_DISPLAY_MASK	0x1
+#define DC_CURSOR_POS_HOT_Y_SHIFT 8
+#define DC_CURSOR_POS_HOT_Y_MASK  0x1F
+#define DC_CURSOR_POS_HOT_X_SHIFT 16
+#define DC_CURSOR_POS_HOT_X_MASK  0x1F
+
+#define DC_CURSOR0_LADDR_REG		0x1530
+#define DC_CURSOR0_HADDR_REG		0x15E0
+#define DC_CURSOR0_POSITION_REG		0x1540
+#define DC_CURSOR1_LADDR_REG		0x1680
+#define DC_CURSOR1_HADDR_REG		0x16E0
+#define DC_CURSOR1_POSITION_REG		0x1690
+#define DC_CURSOR_POS_Y_SHIFT	16
+#define DC_CURSOR_POS_Y_MASK	0xFFF
+#define DC_CURSOR_POS_X_SHIFT	0
+#define DC_CURSOR_POS_X_MASK	0xFFF
+#define DC_CURSOR0_BACK_REG		0x1550
+#define DC_CURSOR0_FORE_REG		0x1560
+#define DC_CURSOR1_BACK_REG		0x16A0
+#define DC_CURSOR1_FORE_REG		0x16B0
+
+/* META */
+#define DC_CRTC_META0_REG_L		(0x1B00)
+#define DC_CRTC_META0_REG_H		(0x1B20)
+#define DC_CRTC_META1_REG_L		(0x1B40)
+#define DC_CRTC_META1_REG_H		(0x1B60)
+
+/* HDMI */
+#define DC_HDMI_ZONEIDLE_REG 		(0x1700)
+#define DC_HDMI_CTRL_REG		(0x1720)
+#define HDMI_CTRL_ENABLE	(1 << 0)
+#define DC_HDMI_PHY_CTRL_REG		(0x1800)
+#define HDMI_PHY_CTRL_ENABLE	(1 << 0)
+#define DC_HDMI_PHY_PLLCFG_REG		(0x1820)
+#define DC_HDMI_PHY_PEC0_REG		(0x1840)
+#define DC_HDMI_PHY_PEC1_REG		(0x1860)
+#define DC_HDMI_PHY_PEC2_REG		(0x1880)
+#define DC_HDMI_PHY_RSVR_REG		(0x18A0)
+#define DC_HDMI_PHY_CALCTRL_REG		(0x18C0)
+#define DC_HDMI_PHY_CALOUT_REG		(0x1AC0)
+
+#define DC_HDMI_AVI_CONT0_REG		(0x18E0)
+#define DC_HDMI_AVI_CONT1_REG		(0x1900)
+#define DC_HDMI_AVI_CONT2_REG		(0x1920)
+#define DC_HDMI_AVI_CONT3_REG		(0x1940)
+#define DC_HDMI_AVI_CTRL_REG		(0x1960)
+#define HDMI_AVI_ENABLE_PACKET	(1 << 0)
+#define HDMI_AVI_FREQ_EACH_FRAME (0 << 1)
+#define HDMI_AVI_FREQ_TWO_FRAME	(1 << 1)
+#define HDMI_AVI_UPDATE		(1 << 2)
+#define DC_HDMI_VSI_CFG_REG		(0x1980)
+
+/* AUDIO */
+#define DC_HDMI_AUDIO_BUF_REG		(0x1740)
+#define DC_HDMI_AUDIO_NCFG_REG		(0x1760)
+#define DC_HDMI_AUDIO_CTSCFG_REG	(0x1780)
+#define DC_HDMI_AUDIO_CTSCALCFG_REG	(0x17A0)
+#define DC_HDMI_AUDIO_INFOFRAME_REG	(0x17C0)
+#define DC_HDMI_AUDIO_SAMPLE_REG	(0x17E0)
+
+#endif /* __GSGPU_DC_REG_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc_resource.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_resource.h
new file mode 100644
index 000000000000..e35e6d7aca43
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_resource.h
@@ -0,0 +1,194 @@
+#ifndef __DC_RESOURCE_H__
+#define __DC_RESOURCE_H__
+#define ENCODER_DATA_MAX  (1024*8)
+
+enum resource_type {
+	GSGPU_RESOURCE_DEFAULT,
+	GSGPU_RESOURCE_HEADER,
+	GSGPU_RESOURCE_EXT_ENCODER,
+	GSGPU_RESOURCE_GPU,
+	GSGPU_RESOURCE_GPIO,
+	GSGPU_RESOURCE_I2C,
+	GSGPU_RESOURCE_PWM,
+	GSGPU_RESOURCE_CRTC,
+	GSGPU_RESOURCE_ENCODER,
+	GSGPU_RESOURCE_CONNECTOR,
+	GSGPU_RESOURCE_MAX,
+};
+
+struct resource_object {
+	u32 link;
+	enum resource_type type;
+	struct list_head node;
+};
+
+struct header_resource {
+	struct resource_object base;
+	u32 links;
+	u32 max_planes;
+	u8 ver_majro;
+	u8 ver_minor;
+	u8 name[16];
+	u8 oem_vendor[32];
+	u8 oem_product[32];
+};
+
+struct gpio_resource {
+	struct resource_object base;
+	u32 type; //bit Reuse
+	u32 level_reg_offset; // offset of DC
+	u32 level_reg_mask; // mask of reg
+	u32 dir_reg_offset; // offset of DC
+	u32 dir_reg_mask; //  mask of reg
+};
+
+struct i2c_resource {
+	struct resource_object base;
+	u32 feature;
+	u16 id;
+	u8  speed; // KHZ
+};
+
+struct pwm_resource {
+	struct resource_object base;
+	u32 feature;
+	u8 pwm;
+	u8 polarity;
+	u32 peroid;
+};
+
+struct crtc_resource {
+	struct resource_object base;
+	u32 feature;
+	u32 crtc_id;
+	u32 encoder_id;
+	u32 max_freq;
+	u32 max_width;
+	u32 max_height;
+	bool is_vb_timing;
+};
+
+struct encoder_resource {
+	struct resource_object base;
+	u32 feature;
+	u32 i2c_id;
+	u32 connector_id;
+	u32 type;
+	u32 config_type;
+	u32 chip;
+	u8 chip_addr;
+};
+
+struct connector_resource {
+	struct resource_object base;
+	u32 feature;
+	u32 i2c_id;
+	u8 internal_edid[256];
+	u32 type;
+	u32 hotplug;
+	u32 edid_method;
+	u32 irq_gpio;
+	u32 gpio_placement;
+};
+
+struct gpu_resource {
+	struct resource_object base;
+	u32 vram_type;
+	u32 bit_width;
+	u32 cap;
+	u32 count_freq;
+	u32 freq;
+	u32 shaders_num;
+	u32 shaders_freq;
+};
+
+struct ext_encoder_resources {
+	struct resource_object base;
+	u32 data_checksum;
+	u32 data_size;
+	u8 data[ENCODER_DATA_MAX-8];
+};
+
+enum gpio_placement {
+	GPIO_PLACEMENT_LS3A = 0,
+	GPIO_PLACEMENT_LS7A,
+};
+
+enum vbios_edid_method {
+	EDID_VIA_NULL = 0,
+	EDID_VIA_I2C,
+	EDID_VIA_VBIOS,
+	EDID_VIA_ENCODER,
+	EDID_VIA_MAX = EDID_VIA_I2C,
+};
+
+enum vbios_encoder_type {
+	ENCODER_NONE,
+	ENCODER_DAC,
+	ENCODER_TMDS,
+	ENCODER_LVDS,
+	ENCODER_TVDAC,
+	ENCODER_VIRTUAL,
+	ENCODER_DSI,
+	ENCODER_DPMST,
+	ENCODER_DPI
+};
+
+enum vbios_encoder_config {
+	ENCODER_TRANSPARENT = 0,
+	ENCODER_OS_CONFIG,
+	ENCODER_BIOS_CONFIG, //BIOS CONFIG ENCODER
+	ENCODER_TYPE_MAX = ENCODER_TRANSPARENT,
+};
+
+enum vbios_encoder_object {
+	UNKNOWN = 0X00,
+	INTERNAL_DVO = 0X01,
+	INTERNAL_HDMI = 0X02,
+	VGA_CH7055 = 0X10,
+	VGA_ADV7125 = 0X11,
+	DVI_TFP410 = 0X20,
+	HDMI_IT66121 = 0X30,
+	HDMI_SIL9022 = 0X31,
+	HDMI_LT8618 = 0X32,
+	EDP_NCS8805 = 0X40,
+	EDP_LT9721 = 0X42,
+	EDP_LT6711 = 0X43,
+	LVDS_LT8619 = 0x50
+};
+
+enum vbios_hotplug {
+	FORCE_ON = 0,
+	POLLING,
+	IRQ,
+	VBIOS_HOTPLUG_MAX = FORCE_ON
+};
+
+enum vram_type {
+	DDR3,
+	DDR4,
+	DDR5
+};
+
+enum vbios_connector_type {
+	CONNECTOR_UNKNOWN = 0,
+	CONNECTOR_VGA,
+	CONNECTOR_DVI_I,
+	CONNECTOR_DVI_D,
+	CONNECTOR_DVI_A,
+	CONNECTOR_COMPOSITE,
+	CONNECTOR_SVIDEO,
+	CONNECTOR_LVDS,
+	CONNECTOR_COMPONENT,
+	CONNECTOR_9PINDIN,
+	CONNECTOR_DISPLAYPORT,
+	CONNECTOR_HDMI_A,
+	CONNECTOR_HDMI_B,
+	CONNECTOR_TV,
+	CONNECTOR_EDP,
+	CONNECTOR_VIRTUAL,
+	CONNECTOR_DSI,
+	CONNECTOR_DPI
+};
+
+#endif
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_dc_vbios.h b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_vbios.h
new file mode 100644
index 000000000000..3187499cf318
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_dc_vbios.h
@@ -0,0 +1,203 @@
+#ifndef __DC_VBIOS_H__
+#define __DC_VBIOS_H__
+
+#include <linux/acpi.h>
+#include "gsgpu_dc_resource.h"
+
+struct gsgpu_vbios;
+struct vbios_desc;
+struct gsgpu_dc;
+
+#define VBIOS_VERSION_V1_1 (11)
+#define VBIOS_DATA_INVAL 0xFF
+
+enum desc_ver {
+	ver_v1,
+};
+
+struct vbios_info {
+	char title[16];
+	u32 version_major;
+	u32 version_minor;
+	char information[20];
+	u32 link_num;
+	u32 crtc_offset;
+	u32 connector_num;
+	u32 connector_offset;
+	u32 encoder_num;
+	u32 encoder_offset;
+} __packed;
+
+struct vbios_desc {
+	u16 type;
+	u8 ver;
+	u8 link;
+	u32 offset;
+	u32 size;
+	u64 ext[2];
+} __packed;
+
+struct vbios_header {
+	u32 feature;
+	u8 oem_vendor[32];
+	u8 oem_product[32];
+	u32 legacy_offset;
+	u32 legacy_size;
+	u32 desc_offset;
+	u32 desc_size;
+	u32 data_offset;
+	u32 data_size;
+} __packed;
+
+enum gsgpu_edid_method {
+	via_null = 0,
+	via_i2c,
+	via_vbios,
+	via_encoder,
+	via_max = 0xffff,
+} __packed;
+
+struct vbios_gpio {
+	u32 feature;
+	u32 type;
+	u32 level_reg_offset; /* offset of DC */
+	u32 level_reg_mask; /* mask of reg */
+	u32 dir_reg_offset; /* offset of DC */
+	u32 dir_reg_mask; /* mask of reg */
+} __packed;
+
+struct vbios_i2c {
+	u32 feature;
+	u16 id;
+	u8  speed; /* KHZ */
+} __packed;
+
+struct vbios_pwm {
+	u32 feature;
+	u8 pwm;
+	u8 polarity;
+	u32 peroid;
+} __packed;
+
+struct vbios_encoder {
+	u32 feature;
+	u32 i2c_id;
+	u32 connector_id;
+	enum vbios_encoder_type type;
+	enum vbios_encoder_config config_type;
+	enum vbios_encoder_object chip;
+	u8 chip_addr;
+} __packed;
+
+struct vbios_connector {
+	u32 feature;
+	u32 i2c_id;
+	u8 internal_edid[256];
+	enum vbios_connector_type type;
+	enum vbios_hotplug hotplug;
+	enum vbios_edid_method edid_method;
+	u32 irq_gpio;
+	enum gpio_placement gpio_placement;
+} __packed;
+
+struct vbios_crtc {
+	u32 feature;
+	u32 crtc_id;
+	u32 encoder_id;
+	u32 max_freq;
+	u32 max_width;
+	u32 max_height;
+	bool is_vb_timing;
+} __packed;
+
+struct vbios_gpu {
+	enum vram_type type;
+	u32 bit_width;
+	u32 cap;
+	u32 count_freq;
+	u32 freq;
+	u32 shaders_num;
+	u32 shaders_freq;
+} __packed;
+
+struct vbios_ext_encoder {
+	u32 data_checksum;
+	u32 data_size;
+	u8 data[ENCODER_DATA_MAX-8];
+} __packed;
+
+struct gsgpu_vbios;
+
+struct vbios_funcs {
+	bool (*resource_pool_create)(struct gsgpu_vbios *vbios);
+	bool (*resource_pool_destory)(struct gsgpu_vbios *vbios);
+
+	bool (*create_header_resource)(struct gsgpu_vbios *vbios, void *data, u32 size);
+	bool (*create_i2c_resource)(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size);
+	bool (*create_gpio_resource)(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size);
+	bool (*create_gpu_resource)(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size);
+	bool (*create_pwm_resource)(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size);
+	bool (*create_crtc_resource)(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size);
+	bool (*create_encoder_resource)(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size);
+	bool (*create_connecor_resource)(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size);
+	bool (*create_ext_encoder_resource)(struct gsgpu_vbios *vbios, void *data, u32 link, u32 size);
+
+	struct header_resource *(*get_header_resource)(struct gsgpu_vbios *vbios);
+	struct i2c_resource *(*get_i2c_resource)(struct gsgpu_vbios *vbios, u32 link);
+	struct pwm_resource *(*get_pwm_resource)(struct gsgpu_vbios *vbios, u32 link);
+	struct gpu_resource *(*get_gpu_resource)(struct gsgpu_vbios *vbios, u32 link);
+	struct gpio_resource *(*get_gpio_resource)(struct gsgpu_vbios *vbios, u32 link);
+	struct crtc_resource *(*get_crtc_resource)(struct gsgpu_vbios *vbios, u32 link);
+	struct connector_resource *(*get_connector_resource)(struct gsgpu_vbios *vbios, u32 link);
+	struct encoder_resource *(*get_encoder_resource)(struct gsgpu_vbios *vbios, u32 link);
+	struct ext_encoder_resources *(*get_ext_encoder_resource)(struct gsgpu_vbios *vbios, u32 link);
+};
+
+struct gsgpu_vbios {
+	struct gsgpu_dc *dc;
+	void *vbios_ptr;
+	struct list_head resource_list;
+	struct vbios_funcs *funcs;
+};
+
+enum desc_type {
+	desc_header = 0,
+	desc_crtc,
+	desc_encoder,
+	desc_connector,
+	desc_i2c,
+	desc_pwm,
+	desc_gpio,
+	desc_backlight,
+	desc_fan,
+	desc_irq_vblank,
+	desc_cfg_encoder,
+	desc_res_encoder,
+	desc_gpu,
+	desc_max = 0xffff
+};
+
+#ifdef CONFIG_ACPI
+struct acpi_viat_table {
+    struct acpi_table_header header;
+    u64 vbios_addr;
+} __packed;
+#endif
+
+typedef bool(parse_func)(struct vbios_desc *, struct gsgpu_vbios *);
+
+struct desc_func {
+	enum desc_type type;
+	u16 ver;
+	parse_func *func;
+};
+
+void *dc_get_vbios_resource(struct gsgpu_vbios *vbios, u32 link,
+			    enum resource_type type);
+bool dc_vbios_init(struct gsgpu_dc *dc);
+void dc_vbios_exit(struct gsgpu_vbios *vbios);
+u8 gsgpu_vbios_checksum(const u8 *data, int size);
+u32 gsgpu_vbios_version(struct gsgpu_vbios *vbios);
+bool check_vbios_info(void);
+
+#endif
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_debugfs.h b/drivers/gpu/drm/gsgpu/include/gsgpu_debugfs.h
new file mode 100644
index 000000000000..5a67d7ffbb03
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_debugfs.h
@@ -0,0 +1,20 @@
+#ifndef __GSGPU_DEBUGFS_H__
+#define __GSGPU_DEBUGFS_H__
+
+struct gsgpu_debugfs {
+	const struct drm_info_list	*files;
+	unsigned		num_files;
+};
+
+int gsgpu_debugfs_regs_init(struct gsgpu_device *adev);
+void gsgpu_debugfs_regs_cleanup(struct gsgpu_device *adev);
+int gsgpu_debugfs_init(struct gsgpu_device *adev);
+int gsgpu_debugfs_add_files(struct gsgpu_device *adev,
+			     const struct drm_info_list *files,
+			     unsigned nfiles);
+int gsgpu_debugfs_fence_init(struct gsgpu_device *adev);
+int gsgpu_debugfs_firmware_init(struct gsgpu_device *adev);
+int gsgpu_debugfs_gem_init(struct gsgpu_device *adev);
+int gsgpu_debugfs_sema_init(struct gsgpu_device *adev);
+
+#endif /* __GSGPU_DEBUGFS_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_display.h b/drivers/gpu/drm/gsgpu/include/gsgpu_display.h
new file mode 100644
index 000000000000..ea7530ee071a
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_display.h
@@ -0,0 +1,10 @@
+#ifndef __GSGPU_DISPLAY_H__
+#define __GSGPU_DISPLAY_H__
+
+uint32_t gsgpu_display_supported_domains(struct gsgpu_device *adev);
+struct drm_framebuffer *
+gsgpu_display_user_framebuffer_create(struct drm_device *dev,
+				       struct drm_file *file_priv,
+				       const struct drm_mode_fb_cmd2 *mode_cmd);
+
+#endif /* __GSGPU_DISPLAY_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_drv.h b/drivers/gpu/drm/gsgpu/include/gsgpu_drv.h
new file mode 100644
index 000000000000..8ac23e2044a6
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_drv.h
@@ -0,0 +1,18 @@
+#ifndef __GSGPU_DRV_H__
+#define __GSGPU_DRV_H__
+
+#include <linux/firmware.h>
+#include <linux/platform_device.h>
+
+#include "gsgpu_shared.h"
+
+#define DRIVER_AUTHOR		"Loongson graphics driver team"
+
+#define DRIVER_NAME		"gsgpu"
+#define DRIVER_DESC		"GS GPU Driver"
+#define DRIVER_DATE		"20200501"
+
+long gsgpu_drm_ioctl(struct file *filp,
+		      unsigned int cmd, unsigned long arg);
+
+#endif /* __GSGPU_DRV_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_gart.h b/drivers/gpu/drm/gsgpu/include/gsgpu_gart.h
new file mode 100644
index 000000000000..631e5778bcde
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_gart.h
@@ -0,0 +1,51 @@
+#ifndef __GSGPU_GART_H__
+#define __GSGPU_GART_H__
+
+#include <linux/types.h>
+
+/*
+ * GART structures, functions & helpers
+ */
+struct gsgpu_device;
+struct gsgpu_bo;
+
+#define GSGPU_GPU_PAGE_SIZE (16 * 1024)
+#define GSGPU_GPU_PAGE_MASK (GSGPU_GPU_PAGE_SIZE - 1)
+#define GSGPU_GPU_PAGE_SHIFT 14
+#define GSGPU_PAGE_PTE_SHIFT (GSGPU_GPU_PAGE_SHIFT - 3)
+#define GSGPU_GPU_PAGE_ALIGN(a) (((a) + GSGPU_GPU_PAGE_MASK) & ~GSGPU_GPU_PAGE_MASK)
+
+#define GSGPU_GPU_PAGES_IN_CPU_PAGE (PAGE_SIZE / GSGPU_GPU_PAGE_SIZE)
+
+struct gsgpu_gart {
+	u64				table_addr;
+	struct gsgpu_bo			*robj;
+	void				*ptr;
+	unsigned			num_gpu_pages;
+	unsigned			num_cpu_pages;
+	unsigned			table_size;
+#ifdef CONFIG_DRM_GSGPU_GART_DEBUGFS
+	struct page			**pages;
+#endif
+	bool				ready;
+
+	/* Asic default pte flags */
+	uint64_t			gart_pte_flags;
+};
+
+int gsgpu_gart_table_vram_alloc(struct gsgpu_device *adev);
+void gsgpu_gart_table_vram_free(struct gsgpu_device *adev);
+int gsgpu_gart_table_vram_pin(struct gsgpu_device *adev);
+void gsgpu_gart_table_vram_unpin(struct gsgpu_device *adev);
+int gsgpu_gart_init(struct gsgpu_device *adev);
+void gsgpu_gart_fini(struct gsgpu_device *adev);
+int gsgpu_gart_unbind(struct gsgpu_device *adev, uint64_t offset,
+		       int pages);
+int gsgpu_gart_map(struct gsgpu_device *adev, uint64_t offset,
+		    int pages, dma_addr_t *dma_addr, uint64_t flags,
+		    void *dst);
+int gsgpu_gart_bind(struct gsgpu_device *adev, uint64_t offset,
+		     int pages, struct page **pagelist,
+		     dma_addr_t *dma_addr, uint64_t flags);
+
+#endif /* __GSGPU_GART_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_gfx.h b/drivers/gpu/drm/gsgpu/include/gsgpu_gfx.h
new file mode 100644
index 000000000000..9d3c4b5d32f5
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_gfx.h
@@ -0,0 +1,8 @@
+#ifndef __GFX_H__
+#define __GFX_H__
+
+extern const struct gsgpu_ip_block_version gfx_ip_block;
+
+struct gsgpu_device;
+
+#endif /*__GFX_H__*/
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_gmc.h b/drivers/gpu/drm/gsgpu/include/gsgpu_gmc.h
new file mode 100644
index 000000000000..b3254b5fdc57
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_gmc.h
@@ -0,0 +1,93 @@
+#ifndef __GSGPU_GMC_H__
+#define __GSGPU_GMC_H__
+
+#include <linux/types.h>
+
+#include "gsgpu_irq.h"
+
+struct firmware;
+
+/*
+ * GPU MC structures, functions & helpers
+ */
+struct gsgpu_gmc_funcs {
+	/* flush the vm tlb via mmio */
+	void (*flush_gpu_tlb)(struct gsgpu_device *adev,
+			      uint32_t vmid);
+	/* flush the vm tlb via ring */
+	uint64_t (*emit_flush_gpu_tlb)(struct gsgpu_ring *ring, unsigned vmid,
+				       uint64_t pd_addr);
+	/* Change the VMID -> PASID mapping */
+	void (*emit_pasid_mapping)(struct gsgpu_ring *ring, unsigned vmid,
+				   unsigned pasid);
+	/* write pte/pde updates using the cpu */
+	int (*set_pte_pde)(struct gsgpu_device *adev,
+			   void *cpu_pt_addr, /* cpu addr of page table */
+			   uint32_t gpu_page_idx, /* pte/pde to update */
+			   uint64_t addr, /* addr to write into pte/pde */
+			   uint64_t flags); /* access flags */
+	/* set pte flags based per asic */
+	uint64_t (*get_vm_pte_flags)(struct gsgpu_device *adev,
+				     uint32_t flags);
+	/* get the pde for a given mc addr */
+	void (*get_vm_pde)(struct gsgpu_device *adev, int level,
+			   u64 *dst, u64 *flags);
+};
+
+struct gsgpu_vm_fault_info {
+	uint64_t	page_addr;
+	uint32_t	vmid;
+	uint32_t	mc_id;
+	uint32_t	status;
+	bool		prot_valid;
+	bool		prot_read;
+	bool		prot_write;
+	bool		prot_exec;
+};
+
+struct gsgpu_gmc {
+	resource_size_t		aper_size;
+	resource_size_t		aper_base;
+	/* for some chips with <= 32MB we need to lie
+	 * about vram size near mc fb location */
+	u32 			dma_bits;
+	u64			mc_vram_size;
+	u64			visible_vram_size;
+	u64			gart_size;
+	u64			gart_start;
+	u64			gart_end;
+	u64			vram_start;
+	u64			vram_end;
+	unsigned		vram_width;
+	u64			real_vram_size;
+	int			vram_mtrr;
+	u64                     mc_mask;
+	const struct firmware   *fw;	/* MC firmware */
+	uint32_t                fw_version;
+	struct gsgpu_irq_src	vm_fault;
+	uint32_t		vram_type;
+	bool			prt_warning;
+	uint64_t		stolen_size;
+	/* protects concurrent invalidation */
+	spinlock_t		invalidate_lock;
+	struct gsgpu_vm_fault_info 	*vm_fault_info;
+
+	const struct gsgpu_gmc_funcs	*gmc_funcs;
+};
+
+/**
+ * gsgpu_gmc_vram_full_visible - Check if full VRAM is visible through the BAR
+ *
+ * @adev: gsgpu_device pointer
+ *
+ * Returns:
+ * True if full VRAM is visible through the BAR
+ */
+static inline bool gsgpu_gmc_vram_full_visible(struct gsgpu_gmc *gmc)
+{
+	WARN_ON(gmc->real_vram_size < gmc->visible_vram_size);
+
+	return (gmc->real_vram_size == gmc->visible_vram_size);
+}
+
+#endif /* __GSGPU_GMC_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_hw_sema.h b/drivers/gpu/drm/gsgpu/include/gsgpu_hw_sema.h
new file mode 100644
index 000000000000..688b97ebf34e
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_hw_sema.h
@@ -0,0 +1,28 @@
+#ifndef __GSGPU_HW_SEMA_H__
+#define __GSGPU_HW_SEMA_H__
+
+#define GSGPU_NUM_SEMA 32
+
+struct gsgpu_vm;
+
+struct gsgpu_hw_sema {
+	struct list_head	list;
+	struct gsgpu_vm		*vm;
+    bool		own;
+	unsigned	pasid;
+	unsigned	ctx;
+};
+
+struct gsgpu_hw_sema_mgr {
+	struct mutex		lock;
+	unsigned		num_ids;
+	struct list_head	sema_list;
+	struct gsgpu_hw_sema	sema[GSGPU_NUM_SEMA];
+};
+
+void gsgpu_sema_free(struct gsgpu_device *adev, struct gsgpu_vm *vm);
+
+int gsgpu_hw_sema_mgr_init(struct gsgpu_device *adev);
+void gsgpu_hw_sema_mgr_fini(struct gsgpu_device *adev);
+
+#endif /* __GSGPU_HW_SEMA_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_ids.h b/drivers/gpu/drm/gsgpu/include/gsgpu_ids.h
new file mode 100644
index 000000000000..569f35bf9ca9
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_ids.h
@@ -0,0 +1,62 @@
+#ifndef __GSGPU_IDS_H__
+#define __GSGPU_IDS_H__
+
+#include <linux/types.h>
+#include <linux/mutex.h>
+#include <linux/list.h>
+#include <linux/dma-fence.h>
+
+#include "gsgpu_sync.h"
+
+/* maximum number of VMIDs */
+#define GSGPU_NUM_VMID	4
+
+struct gsgpu_device;
+struct gsgpu_vm;
+struct gsgpu_ring;
+struct gsgpu_sync;
+struct gsgpu_job;
+
+struct gsgpu_vmid {
+	struct list_head	list;
+	struct gsgpu_sync	active;
+	struct dma_fence	*last_flush;
+	uint64_t		owner;
+
+	uint64_t		pd_gpu_addr;
+	/* last flushed PD/PT update */
+	struct dma_fence	*flushed_updates;
+
+	uint32_t                current_gpu_reset_count;
+
+	unsigned		pasid;
+	struct dma_fence	*pasid_mapping;
+};
+
+struct gsgpu_vmid_mgr {
+	struct mutex		lock;
+	unsigned		num_ids;
+	struct list_head	ids_lru;
+	struct gsgpu_vmid	ids[GSGPU_NUM_VMID];
+	atomic_t		reserved_vmid_num;
+};
+
+int gsgpu_pasid_alloc(unsigned int bits);
+void gsgpu_pasid_free(unsigned int pasid);
+void gsgpu_pasid_free_delayed(struct reservation_object *resv,
+			       unsigned int pasid);
+
+bool gsgpu_vmid_had_gpu_reset(struct gsgpu_device *adev,
+			       struct gsgpu_vmid *id);
+int gsgpu_vmid_alloc_reserved(struct gsgpu_device *adev, struct gsgpu_vm *vm);
+void gsgpu_vmid_free_reserved(struct gsgpu_device *adev, struct gsgpu_vm *vm);
+int gsgpu_vmid_grab(struct gsgpu_vm *vm, struct gsgpu_ring *ring,
+		     struct gsgpu_sync *sync, struct dma_fence *fence,
+		     struct gsgpu_job *job);
+void gsgpu_vmid_reset(struct gsgpu_device *adev, unsigned vmid);
+void gsgpu_vmid_reset_all(struct gsgpu_device *adev);
+
+void gsgpu_vmid_mgr_init(struct gsgpu_device *adev);
+void gsgpu_vmid_mgr_fini(struct gsgpu_device *adev);
+
+#endif /* __GSGPU_IDS_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_ih.h b/drivers/gpu/drm/gsgpu/include/gsgpu_ih.h
new file mode 100644
index 000000000000..e160f570a449
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_ih.h
@@ -0,0 +1,91 @@
+#ifndef __GSGPU_IH_H__
+#define __GSGPU_IH_H__
+
+struct gsgpu_device;
+
+#define GSGPU_IH_CLIENTID_LEGACY 0
+#define GSGPU_IH_CLIENTID_MAX SOC15_IH_CLIENTID_MAX
+
+extern const struct gsgpu_ip_block_version gsgpu_ih_ip_block;
+
+#define GSGPU_PAGEFAULT_HASH_BITS 8
+
+enum soc15_ih_clientid {
+	SOC15_IH_CLIENTID_IH		= 0x00,
+	SOC15_IH_CLIENTID_ACP		= 0x01,
+	SOC15_IH_CLIENTID_ATHUB		= 0x02,
+	SOC15_IH_CLIENTID_BIF		= 0x03,
+	SOC15_IH_CLIENTID_DCE		= 0x04,
+	SOC15_IH_CLIENTID_ISP		= 0x05,
+	SOC15_IH_CLIENTID_PCIE0		= 0x06,
+	SOC15_IH_CLIENTID_RLC		= 0x07,
+	SOC15_IH_CLIENTID_SDMA0		= 0x08,
+	SOC15_IH_CLIENTID_SDMA1		= 0x09,
+	SOC15_IH_CLIENTID_SE0SH		= 0x0a,
+	SOC15_IH_CLIENTID_SE1SH		= 0x0b,
+	SOC15_IH_CLIENTID_SE2SH		= 0x0c,
+	SOC15_IH_CLIENTID_SE3SH		= 0x0d,
+	SOC15_IH_CLIENTID_SYSHUB	= 0x0e,
+	SOC15_IH_CLIENTID_UVD1		= 0x0e,
+	SOC15_IH_CLIENTID_THM		= 0x0f,
+	SOC15_IH_CLIENTID_UVD		= 0x10,
+	SOC15_IH_CLIENTID_VCE0		= 0x11,
+	SOC15_IH_CLIENTID_VMC		= 0x12,
+	SOC15_IH_CLIENTID_XDMA		= 0x13,
+	SOC15_IH_CLIENTID_GRBM_CP	= 0x14,
+	SOC15_IH_CLIENTID_ATS		= 0x15,
+	SOC15_IH_CLIENTID_ROM_SMUIO	= 0x16,
+	SOC15_IH_CLIENTID_DF		= 0x17,
+	SOC15_IH_CLIENTID_VCE1		= 0x18,
+	SOC15_IH_CLIENTID_PWR		= 0x19,
+	SOC15_IH_CLIENTID_UTCL2		= 0x1b,
+	SOC15_IH_CLIENTID_EA		= 0x1c,
+	SOC15_IH_CLIENTID_UTCL2LOG	= 0x1d,
+	SOC15_IH_CLIENTID_MP0		= 0x1e,
+	SOC15_IH_CLIENTID_MP1		= 0x1f,
+
+	SOC15_IH_CLIENTID_MAX,
+
+	SOC15_IH_CLIENTID_VCN		= SOC15_IH_CLIENTID_UVD
+};
+
+/*
+ * R6xx+ IH ring
+ */
+struct gsgpu_ih_ring {
+	struct gsgpu_bo	*ring_obj;
+	volatile uint32_t	*ring;
+	unsigned		rptr;
+	unsigned		ring_size;
+	uint64_t		gpu_addr;
+	uint32_t		ptr_mask;
+	atomic_t		lock;
+	bool                    enabled;
+	unsigned		wptr_offs;
+	unsigned		rptr_offs;
+	bool			use_bus_addr;
+	dma_addr_t		rb_dma_addr; /* only used when use_bus_addr = true */
+};
+
+#define GSGPU_IH_SRC_DATA_MAX_SIZE_DW 4
+
+struct gsgpu_iv_entry {
+	unsigned client_id;
+	unsigned src_id;
+	unsigned ring_id;
+	unsigned vmid;
+	unsigned vmid_src;
+	uint64_t timestamp;
+	unsigned timestamp_src;
+	unsigned pasid;
+	unsigned pasid_src;
+	unsigned src_data[GSGPU_IH_SRC_DATA_MAX_SIZE_DW];
+	const uint32_t *iv_entry;
+};
+
+int gsgpu_ih_ring_init(struct gsgpu_device *adev, unsigned ring_size,
+			bool use_bus_addr);
+void gsgpu_ih_ring_fini(struct gsgpu_device *adev);
+int gsgpu_ih_process(struct gsgpu_device *adev);
+
+#endif /* __GSGPU_IH_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_irq.h b/drivers/gpu/drm/gsgpu/include/gsgpu_irq.h
new file mode 100644
index 000000000000..93ed6389fad1
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_irq.h
@@ -0,0 +1,83 @@
+#ifndef __GSGPU_IRQ_H__
+#define __GSGPU_IRQ_H__
+
+#include <linux/irqdomain.h>
+#include "gsgpu_ih.h"
+#include "gsgpu_dc_i2c.h"
+
+#define GSGPU_MAX_IRQ_SRC_ID	0x100
+#define GSGPU_MAX_IRQ_CLIENT_ID	0x100
+
+/* TODO irq srcid need rewrite*/
+#define GSGPU_SRCID_GFX_PAGE_INV_FAULT                 0x00000092  /* 146 */
+#define GSGPU_SRCID_GFX_MEM_PROT_FAULT                 0x00000093  /* 147 */
+#define GSGPU_SRCID_CP_END_OF_PIPE                     0x000000b5  /* 181 */
+#define GSGPU_SRCID_CP_PRIV_REG_FAULT                  0x000000b8  /* 184 */
+#define GSGPU_SRCID_CP_PRIV_INSTR_FAULT                0x000000b9  /* 185 */
+#define GSGPU_SRCID_XDMA_TRAP          	               0x000000e0  /* 224 */
+#define GSGPU_SRCID_XDMA_SRBM_WRITE                    0x000000f7  /* 247 */
+
+struct gsgpu_device;
+struct gsgpu_iv_entry;
+
+enum gsgpu_interrupt_state {
+	GSGPU_IRQ_STATE_DISABLE,
+	GSGPU_IRQ_STATE_ENABLE,
+};
+
+struct gsgpu_irq_src {
+	unsigned				num_types;
+	atomic_t				*enabled_types;
+	const struct gsgpu_irq_src_funcs	*funcs;
+	void *data;
+};
+
+struct gsgpu_irq_client {
+	struct gsgpu_irq_src **sources;
+};
+
+/* provided by interrupt generating IP blocks */
+struct gsgpu_irq_src_funcs {
+	int (*set)(struct gsgpu_device *adev, struct gsgpu_irq_src *source,
+		   unsigned type, enum gsgpu_interrupt_state state);
+
+	int (*process)(struct gsgpu_device *adev,
+		       struct gsgpu_irq_src *source,
+		       struct gsgpu_iv_entry *entry);
+};
+
+struct gsgpu_irq {
+	bool				installed;
+	spinlock_t			lock;
+	/* interrupt sources */
+	struct gsgpu_irq_client	client[GSGPU_IH_CLIENTID_MAX];
+
+	/* status, etc. */
+	bool				msi_enabled; /* msi enabled */
+
+	/* interrupt ring */
+	struct gsgpu_ih_ring		ih;
+	const struct gsgpu_ih_funcs	*ih_funcs;
+};
+
+void gsgpu_irq_disable_all(struct gsgpu_device *adev);
+irqreturn_t gsgpu_irq_handler(int irq, void *arg);
+
+int gsgpu_irq_init(struct gsgpu_device *adev);
+void gsgpu_irq_fini(struct gsgpu_device *adev);
+int gsgpu_irq_add_id(struct gsgpu_device *adev,
+		      unsigned client_id, unsigned src_id,
+		      struct gsgpu_irq_src *source);
+void gsgpu_irq_dispatch(struct gsgpu_device *adev,
+			 struct gsgpu_iv_entry *entry);
+int gsgpu_irq_update(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
+		      unsigned type);
+int gsgpu_irq_get(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
+		   unsigned type);
+int gsgpu_irq_put(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
+		   unsigned type);
+bool gsgpu_irq_enabled(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
+			unsigned type);
+void gsgpu_irq_gpu_reset_resume_helper(struct gsgpu_device *adev);
+
+#endif /* __GSGPU_IRQ_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_job.h b/drivers/gpu/drm/gsgpu/include/gsgpu_job.h
new file mode 100644
index 000000000000..eed859c98498
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_job.h
@@ -0,0 +1,50 @@
+#ifndef __GSGPU_JOB_H__
+#define __GSGPU_JOB_H__
+
+/* bit set means command submit involves a preamble IB */
+#define GSGPU_PREAMBLE_IB_PRESENT          (1 << 0)
+/* bit set means preamble IB is first presented in belonging context */
+#define GSGPU_PREAMBLE_IB_PRESENT_FIRST    (1 << 1)
+/* bit set means context switch occured */
+#define GSGPU_HAVE_CTX_SWITCH              (1 << 2)
+
+#define to_gsgpu_job(sched_job)		\
+		container_of((sched_job), struct gsgpu_job, base)
+
+struct gsgpu_fence;
+
+struct gsgpu_job {
+	struct drm_sched_job    base;
+	struct gsgpu_vm	*vm;
+	struct gsgpu_sync	sync;
+	struct gsgpu_sync	sched_sync;
+	struct gsgpu_ib	*ibs;
+	struct dma_fence	*fence; /* the hw fence */
+	uint32_t		preamble_status;
+	uint32_t		num_ibs;
+	void			*owner;
+	bool                    vm_needs_flush;
+	uint64_t		vm_pd_addr;
+	unsigned		vmid;
+	unsigned		pasid;
+	uint32_t		vram_lost_counter;
+
+	/* user fence handling */
+	uint64_t		uf_addr;
+	uint64_t		uf_sequence;
+
+};
+
+int gsgpu_job_alloc(struct gsgpu_device *adev, unsigned num_ibs,
+		     struct gsgpu_job **job, struct gsgpu_vm *vm);
+int gsgpu_job_alloc_with_ib(struct gsgpu_device *adev, unsigned size,
+			     struct gsgpu_job **job);
+
+void gsgpu_job_free_resources(struct gsgpu_job *job);
+void gsgpu_job_free(struct gsgpu_job *job);
+int gsgpu_job_submit(struct gsgpu_job *job, struct drm_sched_entity *entity,
+		      void *owner, struct dma_fence **f);
+int gsgpu_job_submit_direct(struct gsgpu_job *job, struct gsgpu_ring *ring,
+			     struct dma_fence **fence);
+
+#endif /* __GSGPU_JOB_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_mmu.h b/drivers/gpu/drm/gsgpu/include/gsgpu_mmu.h
new file mode 100644
index 000000000000..188f0a32ec6d
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_mmu.h
@@ -0,0 +1,49 @@
+#ifndef __GSGPU_MMU_H__
+#define __GSGPU_MMU_H__
+
+#define GSGPU_MMU_GEN_CTRL_OFFSET       0x10000
+#define GSGPU_MMU_EXC_CTRL_OFFSET       0x10004
+#define GSGPU_MMU_EXADDR_LO_OFFSET      0x10008
+#define GSGPU_MMU_EXADDR_HI_OFFSET      0x1000c
+#define GSGPU_MMU_SAFE_LO_OFFSET        0x10010
+#define GSGPU_MMU_SAFE_HI_OFFSET        0x10014
+#define GSGPU_MMU_FLUSH_CTRL_OFFSET     0x10018
+#define GSGPU_MMU_MISC_CTRL_OFFSET      0x1001c
+#define GSGPU_MMU_PGD_LO_OFFSET         0x10020
+#define GSGPU_MMU_PGD_HI_OFFSET         0x10024
+#define GSGPU_MMU_DIR_CTRL_OFFSET       0x100a0
+
+
+#define MMU_ENABLE 						0x01
+#define MMU_SET_PGD						0x02
+#define MMU_SET_SAFE 					0x03
+#define MMU_SET_DIR						0x04
+#define MMU_SET_EXC						0x05
+#define MMU_FLUSH						0x06
+
+
+#define GSGPU_MMU_PTE_SIZE 				8
+#define GSGPU_MMU_PGD_REG_SIZE    		8
+#define GSGPU_MMU_VMID_OF_PGD(vmid)   \
+			     (GSGPU_MMU_PGD_LO_OFFSET + ((vmid) * GSGPU_MMU_PGD_REG_SIZE))
+
+#define GSGPU_MMU_DIR_CTRL_256M_1LVL  ((14 << 26 | 0 << 20) | \
+						(14 << 16 | 0 << 10) | \
+						(14 <<  6 | 14 <<  0))
+
+#define GSGPU_MMU_DIR_CTRL_1T_3LVL  ((4 << 26 | 36 << 20) | \
+						(11 << 16 | 25 << 10) | \
+						(11 <<  6 | 14 <<  0))
+
+#define GSGPU_MMU_FLUSH_DOMAIN_SHIFT 	8
+#define GSGPU_MMU_FLUSH_EN 				BIT(0)
+#define GSGPU_MMU_FLUSH_ALL 			BIT(1)
+#define GSGPU_MMU_FLUSH_VMID 			0
+
+#define GSGPU_MMU_FLUSH_PKT(vmid, all) \
+			     ((vmid << GSGPU_MMU_FLUSH_DOMAIN_SHIFT) | (all) | GSGPU_MMU_FLUSH_EN)
+
+
+extern const struct gsgpu_ip_block_version mmu_ip_block;
+
+#endif /*__GSGPU_MMU_H__*/
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_mn.h b/drivers/gpu/drm/gsgpu/include/gsgpu_mn.h
new file mode 100644
index 000000000000..f64e696fe663
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_mn.h
@@ -0,0 +1,36 @@
+#ifndef __GSGPU_MN_H__
+#define __GSGPU_MN_H__
+
+/*
+ * MMU Notifier
+ */
+struct gsgpu_mn;
+
+enum gsgpu_mn_type {
+	GSGPU_MN_TYPE_GFX,
+	GSGPU_MN_TYPE_HSA,
+};
+
+#if defined(CONFIG_MMU_NOTIFIER)
+void gsgpu_mn_lock(struct gsgpu_mn *mn);
+void gsgpu_mn_unlock(struct gsgpu_mn *mn);
+struct gsgpu_mn *gsgpu_mn_get(struct gsgpu_device *adev,
+				enum gsgpu_mn_type type);
+int gsgpu_mn_register(struct gsgpu_bo *bo, unsigned long addr);
+void gsgpu_mn_unregister(struct gsgpu_bo *bo);
+#else
+static inline void gsgpu_mn_lock(struct gsgpu_mn *mn) {}
+static inline void gsgpu_mn_unlock(struct gsgpu_mn *mn) {}
+static inline struct gsgpu_mn *gsgpu_mn_get(struct gsgpu_device *adev,
+					      enum gsgpu_mn_type type)
+{
+	return NULL;
+}
+static inline int gsgpu_mn_register(struct gsgpu_bo *bo, unsigned long addr)
+{
+	return -ENODEV;
+}
+static inline void gsgpu_mn_unregister(struct gsgpu_bo *bo) {}
+#endif
+
+#endif /* __GSGPU_MN_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_mode.h b/drivers/gpu/drm/gsgpu/include/gsgpu_mode.h
new file mode 100644
index 000000000000..632dcb8af889
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_mode.h
@@ -0,0 +1,178 @@
+#ifndef __GSGPU_MODE_H__
+#define __GSGPU_MODE_H__
+
+#include <drm/drm_crtc.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/drm_edid.h>
+#include <drm/drm_encoder.h>
+#include <drm/drm_fixed.h>
+#include <drm/drm_fb_helper.h>
+#include <drm/drm_plane_helper.h>
+
+#include "gsgpu_irq.h"
+#include "gsgpu_dc_irq.h"
+
+struct gsgpu_bo;
+struct gsgpu_device;
+struct gsgpu_encoder;
+
+#define to_gsgpu_crtc(x) container_of(x, struct gsgpu_crtc, base)
+#define to_gsgpu_connector(x) container_of(x, struct gsgpu_connector, base)
+#define to_gsgpu_encoder(x) container_of(x, struct gsgpu_encoder, base)
+#define to_gsgpu_framebuffer(x) container_of(x, struct gsgpu_framebuffer, base)
+#define to_gsgpu_plane(x)	container_of(x, struct gsgpu_plane, base)
+
+#define GSGPU_MAX_HPD_PINS 3
+#define GSGPU_MAX_CRTCS 2
+#define GSGPU_MAX_PLANES 4
+
+enum gsgpu_rmx_type {
+	RMX_OFF,
+	RMX_FULL,
+	RMX_CENTER,
+	RMX_ASPECT
+};
+
+enum gsgpu_flip_status {
+	GSGPU_FLIP_NONE,
+	GSGPU_FLIP_PENDING,
+	GSGPU_FLIP_SUBMITTED
+};
+
+struct gsgpu_display_funcs {
+	/* get frame count */
+	u32 (*vblank_get_counter)(struct gsgpu_device *adev, int crtc);
+	/* set backlight level */
+	void (*backlight_set_level)(struct gsgpu_encoder *gsgpu_encoder,
+				    u8 level);
+	/* get backlight level */
+	u8 (*backlight_get_level)(struct gsgpu_encoder *gsgpu_encoder);
+	/* hotplug detect */
+	bool (*hpd_sense)(struct gsgpu_device *adev);
+	u32 (*hpd_get_gpio_reg)(struct gsgpu_device *adev);
+	/* pageflipping */
+	void (*page_flip)(struct gsgpu_device *adev,
+			  int crtc_id, u64 crtc_base, bool async);
+	int (*page_flip_get_scanoutpos)(struct gsgpu_device *adev, int crtc,
+					u32 *vbl, u32 *position);
+};
+
+struct gsgpu_framebuffer {
+	struct drm_framebuffer base;
+
+	/* caching for later use */
+	uint64_t address;
+};
+
+struct gsgpu_fbdev {
+	struct drm_fb_helper helper;
+	struct gsgpu_framebuffer rfb;
+	struct list_head fbdev_list;
+	struct gsgpu_device *adev;
+};
+
+struct gsgpu_crtc {
+	struct drm_crtc base;
+	int crtc_id;
+	bool enabled;
+	bool can_tile;
+	uint32_t crtc_offset;
+	enum dc_irq_source irq_source_vsync;
+	struct drm_gem_object *cursor_bo;
+	uint64_t cursor_addr;
+	int cursor_x;
+	int cursor_y;
+	int cursor_hot_x;
+	int cursor_hot_y;
+	int cursor_width;
+	int cursor_height;
+	int max_cursor_width;
+	int max_cursor_height;
+	struct mutex cursor_lock;
+	struct gsgpu_flip_work *pflip_works;
+	enum gsgpu_flip_status pflip_status;
+	u32 lb_vblank_lead_lines;
+	struct drm_display_mode hw_mode;
+	struct drm_pending_vblank_event *event;
+};
+
+struct gsgpu_plane {
+	struct drm_plane base;
+	enum drm_plane_type plane_type;
+};
+
+struct gsgpu_encoder {
+	struct drm_encoder base;
+	uint32_t encoder_enum;
+	uint32_t encoder_id;
+	struct gsgpu_bridge_phy *bridge;
+};
+
+enum gsgpu_connector_audio {
+	GSGPU_AUDIO_DISABLE = 0,
+	GSGPU_AUDIO_ENABLE = 1,
+	GSGPU_AUDIO_AUTO = 2
+};
+
+struct gsgpu_connector {
+	struct drm_connector base;
+	uint32_t connector_id;
+	uint32_t devices;
+	enum gsgpu_connector_audio audio;
+	int num_modes;
+	int pixel_clock_mhz;
+	struct mutex hpd_lock;
+	enum dc_irq_source irq_source_i2c;
+	enum dc_irq_source irq_source_hpd;
+	enum dc_irq_source irq_source_vga_hpd;
+};
+
+struct gsgpu_mode_info {
+	bool mode_config_initialized;
+	struct gsgpu_crtc *crtcs[GSGPU_MAX_CRTCS];
+	struct gsgpu_plane *planes[GSGPU_MAX_PLANES];
+	struct gsgpu_connector *connectors[2];
+	struct gsgpu_encoder *encoders[2];
+	struct gsgpu_backlight *backlights[2];
+	/* underscan */
+	struct drm_property *underscan_property;
+	struct drm_property *underscan_hborder_property;
+	struct drm_property *underscan_vborder_property;
+	/* audio */
+	struct drm_property *audio_property;
+	/* pointer to fbdev info structure */
+	struct gsgpu_fbdev *rfbdev;
+	int			num_crtc; /* number of crtcs */
+	int			num_hpd; /* number of hpd pins */
+	int			num_i2c;
+	int			disp_priority;
+	const struct gsgpu_display_funcs *funcs;
+	const enum drm_plane_type *plane_type;
+};
+
+/* Driver internal use only flags of gsgpu_display_get_crtc_scanoutpos() */
+#define DRM_SCANOUTPOS_VALID        (1 << 0)
+#define DRM_SCANOUTPOS_IN_VBLANK    (1 << 1)
+#define DRM_SCANOUTPOS_ACCURATE     (1 << 2)
+#define USE_REAL_VBLANKSTART		(1 << 30)
+#define GET_DISTANCE_TO_VBLANKSTART	(1 << 31)
+
+int gsgpu_display_get_crtc_scanoutpos(struct drm_device *dev,
+			unsigned int pipe, unsigned int flags, int *vpos,
+			int *hpos, ktime_t *stime, ktime_t *etime,
+			const struct drm_display_mode *mode);
+
+int gsgpu_display_framebuffer_init(struct drm_device *dev,
+				   struct gsgpu_framebuffer *rfb,
+				   const struct drm_mode_fb_cmd2 *mode_cmd,
+				   struct drm_gem_object *obj);
+
+int gsgpu_fbdev_init(struct gsgpu_device *adev);
+void gsgpu_fbdev_fini(struct gsgpu_device *adev);
+void gsgpu_fbdev_set_suspend(struct gsgpu_device *adev, int state);
+int gsgpu_fbdev_total_size(struct gsgpu_device *adev);
+bool gsgpu_fbdev_robj_is_fb(struct gsgpu_device *adev, struct gsgpu_bo *robj);
+int gsgpu_align_pitch(struct gsgpu_device *adev, int width, int bpp, bool tiled);
+int gsgpu_display_modeset_create_props(struct gsgpu_device *adev);
+
+#endif /* __GSGPU_MODE_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_object.h b/drivers/gpu/drm/gsgpu/include/gsgpu_object.h
new file mode 100644
index 000000000000..053ceb3d44ca
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_object.h
@@ -0,0 +1,284 @@
+#ifndef __GSGPU_OBJECT_H__
+#define __GSGPU_OBJECT_H__
+
+#include <drm/gsgpu_drm.h>
+#include "gsgpu.h"
+
+#define GSGPU_BO_INVALID_OFFSET	LONG_MAX
+#define GSGPU_BO_MAX_PLACEMENTS	3
+
+struct gsgpu_bo_param {
+	unsigned long			size;
+	int				byte_align;
+	u32				domain;
+	u32				preferred_domain;
+	u64				flags;
+	enum ttm_bo_type		type;
+	struct reservation_object	*resv;
+};
+
+/* User space allocated BO in a VM */
+struct gsgpu_bo_va {
+	struct gsgpu_vm_bo_base	base;
+
+	/* protected by bo being reserved */
+	unsigned			ref_count;
+
+	/* all other members protected by the VM PD being reserved */
+	struct dma_fence	        *last_pt_update;
+
+	/* mappings for this bo_va */
+	struct list_head		invalids;
+	struct list_head		valids;
+
+	/* If the mappings are cleared or filled */
+	bool				cleared;
+};
+
+struct gsgpu_bo {
+	/* Protected by tbo.reserved */
+	u32				preferred_domains;
+	u32				allowed_domains;
+	struct ttm_place		placements[GSGPU_BO_MAX_PLACEMENTS];
+	struct ttm_placement		placement;
+	struct ttm_buffer_object	tbo;
+	struct ttm_bo_kmap_obj		kmap;
+	u64				flags;
+	unsigned			pin_count;
+	u64				tiling_flags;
+	u64				node_offset;
+	u64				metadata_flags;
+	void				*metadata;
+	u32				metadata_size;
+	unsigned			prime_shared_count;
+	/* list of all virtual address to which this bo is associated to */
+	struct list_head		va;
+	/* Constant after initialization */
+	struct drm_gem_object		gem_base;
+	struct gsgpu_bo		*parent;
+	struct gsgpu_bo		*shadow;
+
+	struct ttm_bo_kmap_obj		dma_buf_vmap;
+	struct gsgpu_mn		*mn;
+
+	union {
+		struct list_head	mn_list;
+		struct list_head	shadow_list;
+	};
+};
+
+static inline struct gsgpu_bo *ttm_to_gsgpu_bo(struct ttm_buffer_object *tbo)
+{
+	return container_of(tbo, struct gsgpu_bo, tbo);
+}
+
+/**
+ * gsgpu_mem_type_to_domain - return domain corresponding to mem_type
+ * @mem_type:	ttm memory type
+ *
+ * Returns corresponding domain of the ttm mem_type
+ */
+static inline unsigned gsgpu_mem_type_to_domain(u32 mem_type)
+{
+	switch (mem_type) {
+	case TTM_PL_VRAM:
+		return GSGPU_GEM_DOMAIN_VRAM;
+	case TTM_PL_TT:
+		return GSGPU_GEM_DOMAIN_GTT;
+	case TTM_PL_SYSTEM:
+		return GSGPU_GEM_DOMAIN_CPU;
+	default:
+		break;
+	}
+	return 0;
+}
+
+/**
+ * gsgpu_bo_reserve - reserve bo
+ * @bo:		bo structure
+ * @no_intr:	don't return -ERESTARTSYS on pending signal
+ *
+ * Returns:
+ * -ERESTARTSYS: A wait for the buffer to become unreserved was interrupted by
+ * a signal. Release all buffer reservations and return to user-space.
+ */
+static inline int gsgpu_bo_reserve(struct gsgpu_bo *bo, bool no_intr)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	int r;
+
+	r = ttm_bo_reserve(&bo->tbo, !no_intr, false, NULL);
+	if (unlikely(r != 0)) {
+		if (r != -ERESTARTSYS)
+			dev_err(adev->dev, "%p reserve failed\n", bo);
+		return r;
+	}
+	return 0;
+}
+
+static inline void gsgpu_bo_unreserve(struct gsgpu_bo *bo)
+{
+	ttm_bo_unreserve(&bo->tbo);
+}
+
+static inline unsigned long gsgpu_bo_size(struct gsgpu_bo *bo)
+{
+	return bo->tbo.num_pages << PAGE_SHIFT;
+}
+
+static inline unsigned gsgpu_bo_ngpu_pages(struct gsgpu_bo *bo)
+{
+	return (bo->tbo.num_pages << PAGE_SHIFT) / GSGPU_GPU_PAGE_SIZE;
+}
+
+static inline unsigned gsgpu_bo_gpu_page_alignment(struct gsgpu_bo *bo)
+{
+	return (bo->tbo.mem.page_alignment << PAGE_SHIFT) / GSGPU_GPU_PAGE_SIZE;
+}
+
+/**
+ * gsgpu_bo_mmap_offset - return mmap offset of bo
+ * @bo:	gsgpu object for which we query the offset
+ *
+ * Returns mmap offset of the object.
+ */
+static inline u64 gsgpu_bo_mmap_offset(struct gsgpu_bo *bo)
+{
+	return drm_vma_node_offset_addr(&bo->tbo.vma_node);
+}
+
+/**
+ * gsgpu_bo_gpu_accessible - return whether the bo is currently in memory that
+ * is accessible to the GPU.
+ */
+static inline bool gsgpu_bo_gpu_accessible(struct gsgpu_bo *bo)
+{
+	switch (bo->tbo.mem.mem_type) {
+	case TTM_PL_TT: return gsgpu_gtt_mgr_has_gart_addr(&bo->tbo.mem);
+	case TTM_PL_VRAM: return true;
+	default: return false;
+	}
+}
+
+/**
+ * gsgpu_bo_in_cpu_visible_vram - check if BO is (partly) in visible VRAM
+ */
+static inline bool gsgpu_bo_in_cpu_visible_vram(struct gsgpu_bo *bo)
+{
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
+	unsigned fpfn = adev->gmc.visible_vram_size >> PAGE_SHIFT;
+	struct drm_mm_node *node = bo->tbo.mem.mm_node;
+	unsigned long pages_left;
+
+	if (bo->tbo.mem.mem_type != TTM_PL_VRAM)
+		return false;
+
+	for (pages_left = bo->tbo.mem.num_pages; pages_left;
+	     pages_left -= node->size, node++)
+		if (node->start < fpfn)
+			return true;
+
+	return false;
+}
+
+/**
+ * gsgpu_bo_explicit_sync - return whether the bo is explicitly synced
+ */
+static inline bool gsgpu_bo_explicit_sync(struct gsgpu_bo *bo)
+{
+	return bo->flags & GSGPU_GEM_CREATE_EXPLICIT_SYNC;
+}
+
+bool gsgpu_bo_is_gsgpu_bo(struct ttm_buffer_object *bo);
+void gsgpu_bo_placement_from_domain(struct gsgpu_bo *abo, u32 domain);
+
+int gsgpu_bo_create(struct gsgpu_device *adev,
+		     struct gsgpu_bo_param *bp,
+		     struct gsgpu_bo **bo_ptr);
+int gsgpu_bo_create_reserved(struct gsgpu_device *adev,
+			      unsigned long size, int align,
+			      u32 domain, struct gsgpu_bo **bo_ptr,
+			      u64 *gpu_addr, void **cpu_addr);
+int gsgpu_bo_create_kernel(struct gsgpu_device *adev,
+			    unsigned long size, int align,
+			    u32 domain, struct gsgpu_bo **bo_ptr,
+			    u64 *gpu_addr, void **cpu_addr);
+void gsgpu_bo_free_kernel(struct gsgpu_bo **bo, u64 *gpu_addr,
+			   void **cpu_addr);
+int gsgpu_bo_kmap(struct gsgpu_bo *bo, void **ptr);
+void *gsgpu_bo_kptr(struct gsgpu_bo *bo);
+void gsgpu_bo_kunmap(struct gsgpu_bo *bo);
+struct gsgpu_bo *gsgpu_bo_ref(struct gsgpu_bo *bo);
+void gsgpu_bo_unref(struct gsgpu_bo **bo);
+int gsgpu_bo_pin(struct gsgpu_bo *bo, u32 domain);
+int gsgpu_bo_pin_restricted(struct gsgpu_bo *bo, u32 domain,
+			     u64 min_offset, u64 max_offset);
+int gsgpu_bo_unpin(struct gsgpu_bo *bo);
+int gsgpu_bo_evict_vram(struct gsgpu_device *adev);
+int gsgpu_bo_init(struct gsgpu_device *adev);
+int gsgpu_bo_late_init(struct gsgpu_device *adev);
+void gsgpu_bo_fini(struct gsgpu_device *adev);
+int gsgpu_bo_fbdev_mmap(struct gsgpu_bo *bo,
+				struct vm_area_struct *vma);
+int gsgpu_bo_set_tiling_flags(struct gsgpu_bo *bo, u64 tiling_flags);
+void gsgpu_bo_get_tiling_flags(struct gsgpu_bo *bo, u64 *tiling_flags);
+int gsgpu_bo_set_metadata (struct gsgpu_bo *bo, void *metadata,
+			    uint32_t metadata_size, uint64_t flags);
+int gsgpu_bo_get_metadata(struct gsgpu_bo *bo, void *buffer,
+			   size_t buffer_size, uint32_t *metadata_size,
+			   uint64_t *flags);
+void gsgpu_bo_move_notify(struct ttm_buffer_object *bo,
+			   bool evict,
+			   struct ttm_mem_reg *new_mem);
+int gsgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo);
+void gsgpu_bo_fence(struct gsgpu_bo *bo, struct dma_fence *fence,
+		     bool shared);
+u64 gsgpu_bo_gpu_offset(struct gsgpu_bo *bo);
+int gsgpu_bo_backup_to_shadow(struct gsgpu_device *adev,
+			       struct gsgpu_ring *ring,
+			       struct gsgpu_bo *bo,
+			       struct reservation_object *resv,
+			       struct dma_fence **fence, bool direct);
+int gsgpu_bo_validate(struct gsgpu_bo *bo);
+int gsgpu_bo_restore_from_shadow(struct gsgpu_device *adev,
+				  struct gsgpu_ring *ring,
+				  struct gsgpu_bo *bo,
+				  struct reservation_object *resv,
+				  struct dma_fence **fence,
+				  bool direct);
+uint32_t gsgpu_bo_get_preferred_pin_domain(struct gsgpu_device *adev,
+					    uint32_t domain);
+
+/*
+ * sub allocation
+ */
+
+static inline uint64_t gsgpu_sa_bo_gpu_addr(struct gsgpu_sa_bo *sa_bo)
+{
+	return sa_bo->manager->gpu_addr + sa_bo->soffset;
+}
+
+static inline void *gsgpu_sa_bo_cpu_addr(struct gsgpu_sa_bo *sa_bo)
+{
+	return sa_bo->manager->cpu_ptr + sa_bo->soffset;
+}
+
+int gsgpu_sa_bo_manager_init(struct gsgpu_device *adev,
+				     struct gsgpu_sa_manager *sa_manager,
+				     unsigned size, u32 align, u32 domain);
+void gsgpu_sa_bo_manager_fini(struct gsgpu_device *adev,
+				      struct gsgpu_sa_manager *sa_manager);
+int gsgpu_sa_bo_manager_start(struct gsgpu_device *adev,
+				      struct gsgpu_sa_manager *sa_manager);
+int gsgpu_sa_bo_new(struct gsgpu_sa_manager *sa_manager,
+		     struct gsgpu_sa_bo **sa_bo,
+		     unsigned size, unsigned align);
+void gsgpu_sa_bo_free(struct gsgpu_device *adev,
+			      struct gsgpu_sa_bo **sa_bo,
+			      struct dma_fence *fence);
+#if defined(CONFIG_DEBUG_FS)
+void gsgpu_sa_bo_dump_debug_info(struct gsgpu_sa_manager *sa_manager,
+					 struct seq_file *m);
+#endif
+
+#endif /* __GSGPU_OBJECT_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_pm.h b/drivers/gpu/drm/gsgpu/include/gsgpu_pm.h
new file mode 100644
index 000000000000..fcce79f6105d
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_pm.h
@@ -0,0 +1,6 @@
+#ifndef __GSGPU_PM_H__
+#define __GSGPU_PM_H__
+
+int gsgpu_pm_sysfs_init(struct gsgpu_device *adev);
+
+#endif /* __GSGPU_PM_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_ring.h b/drivers/gpu/drm/gsgpu/include/gsgpu_ring.h
new file mode 100644
index 000000000000..437127b48cb5
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_ring.h
@@ -0,0 +1,249 @@
+#ifndef __GSGPU_RING_H__
+#define __GSGPU_RING_H__
+
+#include <drm/gsgpu_drm.h>
+#include <drm/gpu_scheduler.h>
+#include <drm/drm_print.h>
+
+/* max number of rings */
+#define GSGPU_MAX_RINGS		21
+#define GSGPU_MAX_GFX_RINGS		1
+
+/* some special values for the owner field */
+#define GSGPU_FENCE_OWNER_UNDEFINED	((void *)0ul)
+#define GSGPU_FENCE_OWNER_VM		((void *)1ul)
+#define GSGPU_FENCE_OWNER_KFD		((void *)2ul)
+
+#define GSGPU_FENCE_FLAG_64BIT         (1 << 0)
+#define GSGPU_FENCE_FLAG_INT           (1 << 1)
+#define GSGPU_FENCE_FLAG_TC_WB_ONLY    (1 << 2)
+
+#define to_gsgpu_ring(s) container_of((s), struct gsgpu_ring, sched)
+
+enum gsgpu_ring_type {
+	GSGPU_RING_TYPE_GFX,
+	GSGPU_RING_TYPE_XDMA,
+};
+
+struct gsgpu_device;
+struct gsgpu_ring;
+struct gsgpu_ib;
+struct gsgpu_cs_parser;
+struct gsgpu_job;
+
+/*
+ * Fences.
+ */
+struct gsgpu_fence_driver {
+	uint64_t			gpu_addr;
+	volatile uint32_t		*cpu_addr;
+	/* sync_seq is protected by ring emission lock */
+	uint32_t			sync_seq;
+	atomic_t			last_seq;
+	bool				initialized;
+	struct gsgpu_irq_src		*irq_src;
+	unsigned			irq_type;
+	struct timer_list		fallback_timer;
+	unsigned			num_fences_mask;
+	spinlock_t			lock;
+	struct dma_fence		**fences;
+};
+
+int gsgpu_fence_driver_init(struct gsgpu_device *adev);
+void gsgpu_fence_driver_fini(struct gsgpu_device *adev);
+void gsgpu_fence_driver_force_completion(struct gsgpu_ring *ring);
+
+int gsgpu_fence_driver_init_ring(struct gsgpu_ring *ring,
+				  unsigned num_hw_submission);
+int gsgpu_fence_driver_start_ring(struct gsgpu_ring *ring,
+				   struct gsgpu_irq_src *irq_src,
+				   unsigned irq_type);
+void gsgpu_fence_driver_suspend(struct gsgpu_device *adev);
+void gsgpu_fence_driver_resume(struct gsgpu_device *adev);
+int gsgpu_fence_emit(struct gsgpu_ring *ring, struct dma_fence **fence,
+		      unsigned flags);
+int gsgpu_fence_emit_polling(struct gsgpu_ring *ring, uint32_t *s);
+void gsgpu_fence_process(struct gsgpu_ring *ring);
+int gsgpu_fence_wait_empty(struct gsgpu_ring *ring);
+signed long gsgpu_fence_wait_polling(struct gsgpu_ring *ring,
+				      uint32_t wait_seq,
+				      signed long timeout);
+unsigned gsgpu_fence_count_emitted(struct gsgpu_ring *ring);
+
+/*
+ * Rings.
+ */
+
+/* provided by hw blocks that expose a ring buffer for commands */
+struct gsgpu_ring_funcs {
+	enum gsgpu_ring_type	type;
+	uint32_t		align_mask;
+	u32			nop;
+	bool			support_64bit_ptrs;
+	unsigned		vmhub;
+	unsigned		extra_dw;
+
+	/* ring read/write ptr handling */
+	u64 (*get_rptr)(struct gsgpu_ring *ring);
+	u64 (*get_wptr)(struct gsgpu_ring *ring);
+	void (*set_wptr)(struct gsgpu_ring *ring);
+	/* validating and patching of IBs */
+	int (*parse_cs)(struct gsgpu_cs_parser *p, uint32_t ib_idx);
+	int (*patch_cs_in_place)(struct gsgpu_cs_parser *p, uint32_t ib_idx);
+	/* constants to calculate how many DW are needed for an emit */
+	unsigned emit_frame_size;
+	unsigned emit_ib_size;
+	/* command emit functions */
+	void (*emit_ib)(struct gsgpu_ring *ring,
+			struct gsgpu_ib *ib,
+			unsigned vmid, bool ctx_switch);
+	void (*emit_fence)(struct gsgpu_ring *ring, uint64_t addr,
+			   uint64_t seq, unsigned flags);
+	void (*emit_pipeline_sync)(struct gsgpu_ring *ring);
+	void (*emit_vm_flush)(struct gsgpu_ring *ring, unsigned vmid,
+			      uint64_t pd_addr);
+	/* testing functions */
+	int (*test_ring)(struct gsgpu_ring *ring);
+	int (*test_ib)(struct gsgpu_ring *ring, long timeout);
+	int (*test_xdma)(struct gsgpu_ring *ring, long timeout);
+	/* insert NOP packets */
+	void (*insert_nop)(struct gsgpu_ring *ring, uint32_t count);
+	void (*insert_start)(struct gsgpu_ring *ring);
+	void (*insert_end)(struct gsgpu_ring *ring);
+	/* pad the indirect buffer to the necessary number of dw */
+	void (*pad_ib)(struct gsgpu_ring *ring, struct gsgpu_ib *ib);
+	unsigned (*init_cond_exec)(struct gsgpu_ring *ring);
+	void (*patch_cond_exec)(struct gsgpu_ring *ring, unsigned offset);
+	/* note usage for clock and power gating */
+	void (*begin_use)(struct gsgpu_ring *ring);
+	void (*end_use)(struct gsgpu_ring *ring);
+	void (*emit_switch_buffer) (struct gsgpu_ring *ring);
+	void (*emit_cntxcntl) (struct gsgpu_ring *ring, uint32_t flags);
+	void (*emit_rreg)(struct gsgpu_ring *ring, uint32_t reg);
+	void (*emit_wreg)(struct gsgpu_ring *ring, uint32_t reg, uint32_t val);
+	void (*emit_reg_wait)(struct gsgpu_ring *ring, uint32_t reg,
+			      uint32_t val, uint32_t mask);
+	void (*emit_reg_write_reg_wait)(struct gsgpu_ring *ring,
+					uint32_t reg0, uint32_t reg1,
+					uint32_t ref, uint32_t mask);
+	void (*emit_tmz)(struct gsgpu_ring *ring, bool start);
+	/* priority functions */
+	void (*set_priority) (struct gsgpu_ring *ring,
+			      enum drm_sched_priority priority);
+};
+
+struct gsgpu_ring {
+	struct gsgpu_device		*adev;
+	const struct gsgpu_ring_funcs	*funcs;
+	struct gsgpu_fence_driver	fence_drv;
+	struct drm_gpu_scheduler	sched;
+	struct list_head		lru_list;
+
+	struct gsgpu_bo	*ring_obj;
+	volatile uint32_t	*ring;
+	unsigned		rptr_offs;
+	u64			wptr;
+	u64			wptr_old;
+	unsigned		ring_size;
+	unsigned		max_dw;
+	int			count_dw;
+	uint64_t		gpu_addr;
+	uint64_t		ptr_mask;
+	uint32_t		buf_mask;
+	bool			ready;
+	u32			idx;
+	u32			me;
+	u32			pipe;
+	u32			queue;
+	bool			use_pollmem;
+	unsigned		wptr_offs;
+	unsigned		fence_offs;
+	uint64_t		current_ctx;
+	char			name[16];
+	unsigned		cond_exe_offs;
+	u64			cond_exe_gpu_addr;
+	volatile u32		*cond_exe_cpu_addr;
+	unsigned		vm_inv_eng;
+	struct dma_fence	*vmid_wait;
+
+	atomic_t		num_jobs[DRM_SCHED_PRIORITY_MAX];
+	struct mutex		priority_mutex;
+	/* protected by priority_mutex */
+	int			priority;
+
+#if defined(CONFIG_DEBUG_FS)
+	struct dentry *ent;
+#endif
+};
+
+int gsgpu_ring_alloc(struct gsgpu_ring *ring, unsigned ndw);
+void gsgpu_ring_insert_nop(struct gsgpu_ring *ring, uint32_t count);
+void gsgpu_ring_generic_pad_ib(struct gsgpu_ring *ring, struct gsgpu_ib *ib);
+void gsgpu_ring_commit(struct gsgpu_ring *ring);
+void gsgpu_ring_undo(struct gsgpu_ring *ring);
+void gsgpu_ring_priority_get(struct gsgpu_ring *ring,
+			      enum drm_sched_priority priority);
+void gsgpu_ring_priority_put(struct gsgpu_ring *ring,
+			      enum drm_sched_priority priority);
+int gsgpu_ring_init(struct gsgpu_device *adev, struct gsgpu_ring *ring,
+		     unsigned ring_size, struct gsgpu_irq_src *irq_src,
+		     unsigned irq_type);
+void gsgpu_ring_fini(struct gsgpu_ring *ring);
+int gsgpu_ring_lru_get(struct gsgpu_device *adev, int type,
+			int *blacklist, int num_blacklist,
+			bool lru_pipe_order, struct gsgpu_ring **ring);
+void gsgpu_ring_lru_touch(struct gsgpu_device *adev, struct gsgpu_ring *ring);
+void gsgpu_ring_emit_reg_write_reg_wait_helper(struct gsgpu_ring *ring,
+						uint32_t reg0, uint32_t val0,
+						uint32_t reg1, uint32_t val1);
+
+static inline void gsgpu_ring_clear_ring(struct gsgpu_ring *ring)
+{
+	int i = 0;
+	while (i <= ring->buf_mask)
+		ring->ring[i++] = ring->funcs->nop;
+
+}
+
+static inline void gsgpu_ring_write(struct gsgpu_ring *ring, uint32_t v)
+{
+	if (ring->count_dw <= 0)
+		DRM_ERROR("gsgpu: writing more dwords to the ring than expected!\n");
+	//DRM_INFO("dw[0x%x]:%x\n", ring->wptr, v);
+	ring->ring[ring->wptr++ & ring->buf_mask] = v;
+	ring->wptr &= ring->ptr_mask;
+	ring->count_dw--;
+}
+
+static inline void gsgpu_ring_write_multiple(struct gsgpu_ring *ring,
+					      void *src, int count_dw)
+{
+	unsigned occupied, chunk1, chunk2;
+	void *dst;
+
+	if (unlikely(ring->count_dw < count_dw))
+		DRM_ERROR("gsgpu: writing more dwords to the ring than expected!\n");
+
+	occupied = ring->wptr & ring->buf_mask;
+	dst = (void *)&ring->ring[occupied];
+	chunk1 = ring->buf_mask + 1 - occupied;
+	chunk1 = (chunk1 >= count_dw) ? count_dw : chunk1;
+	chunk2 = count_dw - chunk1;
+	chunk1 <<= 2;
+	chunk2 <<= 2;
+
+	if (chunk1)
+		memcpy(dst, src, chunk1);
+
+	if (chunk2) {
+		src += chunk1;
+		dst = (void *)ring->ring;
+		memcpy(dst, src, chunk2);
+	}
+
+	ring->wptr += count_dw;
+	ring->wptr &= ring->ptr_mask;
+	ring->count_dw -= count_dw;
+}
+
+#endif /* __GSGPU_RING_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_sched.h b/drivers/gpu/drm/gsgpu/include/gsgpu_sched.h
new file mode 100644
index 000000000000..0794ec6a8eff
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_sched.h
@@ -0,0 +1,10 @@
+#ifndef __GSGPU_SCHED_H__
+#define __GSGPU_SCHED_H__
+
+#include <drm/drmP.h>
+
+enum drm_sched_priority gsgpu_to_sched_priority(int gsgpu_priority);
+int gsgpu_sched_ioctl(struct drm_device *dev, void *data,
+		       struct drm_file *filp);
+
+#endif /* __GSGPU_SCHED_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_shared.h b/drivers/gpu/drm/gsgpu/include/gsgpu_shared.h
new file mode 100644
index 000000000000..5e5d007b2f04
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_shared.h
@@ -0,0 +1,72 @@
+#ifndef __GSGPU_SHARED_H__
+#define __GSGPU_SHARED_H__
+
+#include <drm/gsgpu_family_type.h>
+
+/*
+ * Chip flags
+ */
+enum gsgpu_chip_flags {
+	GAGPU_ASIC_MASK = 0x0000ffffUL,
+	GSGPU_FLAGS_MASK  = 0xffff0000UL,
+	GSGPU_IS_MOBILITY = 0x00010000UL,
+	GSGPU_IS_APU      = 0x00020000UL,
+	GSGPU_IS_PX       = 0x00040000UL,
+	GSGPU_EXP_HW_SUPPORT = 0x00080000UL,
+};
+
+enum gsgpu_ip_block_type {
+	GSGPU_IP_BLOCK_TYPE_COMMON,
+	GSGPU_IP_BLOCK_TYPE_GMC,
+	GSGPU_IP_BLOCK_TYPE_ZIP,
+	GSGPU_IP_BLOCK_TYPE_IH,
+	GSGPU_IP_BLOCK_TYPE_DCE,
+	GSGPU_IP_BLOCK_TYPE_GFX,
+	GSGPU_IP_BLOCK_TYPE_XDMA
+};
+
+/**
+ * struct gsgpu_ip_funcs - general hooks for managing gsgpu IP Blocks
+ */
+struct gsgpu_ip_funcs {
+	/** @name: Name of IP block */
+	char *name;
+	/**
+	 * @early_init:
+	 *
+	 * sets up early driver state (pre sw_init),
+	 * does not configure hw - Optional
+	 */
+	int (*early_init)(void *handle);
+	/** @late_init: sets up late driver/hw state (post hw_init) - Optional */
+	int (*late_init)(void *handle);
+	/** @sw_init: sets up driver state, does not configure hw */
+	int (*sw_init)(void *handle);
+	/** @sw_fini: tears down driver state, does not configure hw */
+	int (*sw_fini)(void *handle);
+	/** @hw_init: sets up the hw state */
+	int (*hw_init)(void *handle);
+	/** @hw_fini: tears down the hw state */
+	int (*hw_fini)(void *handle);
+	/** @late_fini: final cleanup */
+	void (*late_fini)(void *handle);
+	/** @suspend: handles IP specific hw/sw changes for suspend */
+	int (*suspend)(void *handle);
+	/** @resume: handles IP specific hw/sw changes for resume */
+	int (*resume)(void *handle);
+	/** @is_idle: returns current IP block idle status */
+	bool (*is_idle)(void *handle);
+	/** @wait_for_idle: poll for idle */
+	int (*wait_for_idle)(void *handle);
+	/** @check_soft_reset: check soft reset the IP block */
+	bool (*check_soft_reset)(void *handle);
+	/** @pre_soft_reset: pre soft reset the IP block */
+	int (*pre_soft_reset)(void *handle);
+	/** @soft_reset: soft reset the IP block */
+	int (*soft_reset)(void *handle);
+	/** @post_soft_reset: post soft reset the IP block */
+	int (*post_soft_reset)(void *handle);
+};
+
+
+#endif /* __GSGPU_SHARED_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_sync.h b/drivers/gpu/drm/gsgpu/include/gsgpu_sync.h
new file mode 100644
index 000000000000..b05c8de5ae74
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_sync.h
@@ -0,0 +1,36 @@
+#ifndef __GSGPU_SYNC_H__
+#define __GSGPU_SYNC_H__
+
+#include <linux/hashtable.h>
+
+struct dma_fence;
+struct reservation_object;
+struct gsgpu_device;
+struct gsgpu_ring;
+
+/*
+ * Container for fences used to sync command submissions.
+ */
+struct gsgpu_sync {
+	DECLARE_HASHTABLE(fences, 4);
+	struct dma_fence	*last_vm_update;
+};
+
+void gsgpu_sync_create(struct gsgpu_sync *sync);
+int gsgpu_sync_fence(struct gsgpu_device *adev, struct gsgpu_sync *sync,
+		      struct dma_fence *f, bool explicit);
+int gsgpu_sync_resv(struct gsgpu_device *adev,
+		     struct gsgpu_sync *sync,
+		     struct reservation_object *resv,
+		     void *owner,
+		     bool explicit_sync);
+struct dma_fence *gsgpu_sync_peek_fence(struct gsgpu_sync *sync,
+				     struct gsgpu_ring *ring);
+struct dma_fence *gsgpu_sync_get_fence(struct gsgpu_sync *sync, bool *explicit);
+int gsgpu_sync_clone(struct gsgpu_sync *source, struct gsgpu_sync *clone);
+int gsgpu_sync_wait(struct gsgpu_sync *sync, bool intr);
+void gsgpu_sync_free(struct gsgpu_sync *sync);
+int gsgpu_sync_init(void);
+void gsgpu_sync_fini(void);
+
+#endif /* __GSGPU_SYNC_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_trace.h b/drivers/gpu/drm/gsgpu/include/gsgpu_trace.h
new file mode 100644
index 000000000000..a4478070d7dd
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_trace.h
@@ -0,0 +1,450 @@
+#if !defined(_GSGPU_TRACE_H_) || defined(TRACE_HEADER_MULTI_READ)
+#define _GSGPU_TRACE_H_
+
+#include <linux/stringify.h>
+#include <linux/types.h>
+#include <linux/tracepoint.h>
+
+#include <drm/drmP.h>
+
+#include "gsgpu_vm_it.h"
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM gsgpu
+#define TRACE_INCLUDE_FILE gsgpu_trace
+
+#define GSGPU_JOB_GET_TIMELINE_NAME(job) \
+	 job->base.s_fence->finished.ops->get_timeline_name(&job->base.s_fence->finished)
+
+TRACE_EVENT(gsgpu_mm_rreg,
+	    TP_PROTO(unsigned did, uint32_t reg, uint32_t value),
+	    TP_ARGS(did, reg, value),
+	    TP_STRUCT__entry(
+				__field(unsigned, did)
+				__field(uint32_t, reg)
+				__field(uint32_t, value)
+			    ),
+	    TP_fast_assign(
+			   __entry->did = did;
+			   __entry->reg = reg;
+			   __entry->value = value;
+			   ),
+	    TP_printk("0x%04lx, 0x%08lx, 0x%08lx",
+		      (unsigned long)__entry->did,
+		      (unsigned long)__entry->reg,
+		      (unsigned long)__entry->value)
+);
+
+TRACE_EVENT(gsgpu_mm_wreg,
+	    TP_PROTO(unsigned did, uint32_t reg, uint32_t value),
+	    TP_ARGS(did, reg, value),
+	    TP_STRUCT__entry(
+				__field(unsigned, did)
+				__field(uint32_t, reg)
+				__field(uint32_t, value)
+			    ),
+	    TP_fast_assign(
+			   __entry->did = did;
+			   __entry->reg = reg;
+			   __entry->value = value;
+			   ),
+	    TP_printk("0x%04lx, 0x%08lx, 0x%08lx",
+		      (unsigned long)__entry->did,
+		      (unsigned long)__entry->reg,
+		      (unsigned long)__entry->value)
+);
+
+TRACE_EVENT(gsgpu_iv,
+	    TP_PROTO(struct gsgpu_iv_entry *iv),
+	    TP_ARGS(iv),
+	    TP_STRUCT__entry(
+			     __field(unsigned, client_id)
+			     __field(unsigned, src_id)
+			     __field(unsigned, ring_id)
+			     __field(unsigned, vmid)
+			     __field(unsigned, vmid_src)
+			     __field(uint64_t, timestamp)
+			     __field(unsigned, timestamp_src)
+			     __field(unsigned, pasid)
+			     __array(unsigned, src_data, 4)
+			    ),
+	    TP_fast_assign(
+			   __entry->client_id = iv->client_id;
+			   __entry->src_id = iv->src_id;
+			   __entry->ring_id = iv->ring_id;
+			   __entry->vmid = iv->vmid;
+			   __entry->vmid_src = iv->vmid_src;
+			   __entry->timestamp = iv->timestamp;
+			   __entry->timestamp_src = iv->timestamp_src;
+			   __entry->pasid = iv->pasid;
+			   __entry->src_data[0] = iv->src_data[0];
+			   __entry->src_data[1] = iv->src_data[1];
+			   __entry->src_data[2] = iv->src_data[2];
+			   __entry->src_data[3] = iv->src_data[3];
+			   ),
+	    TP_printk("client_id:%u src_id:%u ring:%u vmid:%u timestamp: %llu pasid:%u src_data: %08x %08x %08x %08x\n",
+		      __entry->client_id, __entry->src_id,
+		      __entry->ring_id, __entry->vmid,
+		      __entry->timestamp, __entry->pasid,
+		      __entry->src_data[0], __entry->src_data[1],
+		      __entry->src_data[2], __entry->src_data[3])
+);
+
+
+TRACE_EVENT(gsgpu_bo_create,
+	    TP_PROTO(struct gsgpu_bo *bo),
+	    TP_ARGS(bo),
+	    TP_STRUCT__entry(
+			     __field(struct gsgpu_bo *, bo)
+			     __field(u32, pages)
+			     __field(u32, type)
+			     __field(u32, prefer)
+			     __field(u32, allow)
+			     __field(u32, visible)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->bo = bo;
+			   __entry->pages = bo->tbo.num_pages;
+			   __entry->type = bo->tbo.mem.mem_type;
+			   __entry->prefer = bo->preferred_domains;
+			   __entry->allow = bo->allowed_domains;
+			   __entry->visible = bo->flags;
+			   ),
+
+	    TP_printk("bo=%p, pages=%u, type=%d, preferred=%d, allowed=%d, visible=%d",
+		       __entry->bo, __entry->pages, __entry->type,
+		       __entry->prefer, __entry->allow, __entry->visible)
+);
+
+TRACE_EVENT(gsgpu_cs,
+	    TP_PROTO(struct gsgpu_cs_parser *p, int i),
+	    TP_ARGS(p, i),
+	    TP_STRUCT__entry(
+			     __field(struct gsgpu_bo_list *, bo_list)
+			     __field(u32, ring)
+			     __field(u32, dw)
+			     __field(u32, fences)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->bo_list = p->bo_list;
+			   __entry->ring = p->ring->idx;
+			   __entry->dw = p->job->ibs[i].length_dw;
+			   __entry->fences = gsgpu_fence_count_emitted(
+				p->ring);
+			   ),
+	    TP_printk("bo_list=%p, ring=%u, dw=%u, fences=%u",
+		      __entry->bo_list, __entry->ring, __entry->dw,
+		      __entry->fences)
+);
+
+TRACE_EVENT(gsgpu_cs_ioctl,
+	    TP_PROTO(struct gsgpu_job *job),
+	    TP_ARGS(job),
+	    TP_STRUCT__entry(
+			     __field(uint64_t, sched_job_id)
+			     __string(timeline, GSGPU_JOB_GET_TIMELINE_NAME(job))
+			     __field(unsigned int, context)
+			     __field(unsigned int, seqno)
+			     __field(struct dma_fence *, fence)
+			     __field(char *, ring_name)
+			     __field(u32, num_ibs)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->sched_job_id = job->base.id;
+			   __assign_str(timeline, GSGPU_JOB_GET_TIMELINE_NAME(job))
+			   __entry->context = job->base.s_fence->finished.context;
+			   __entry->seqno = job->base.s_fence->finished.seqno;
+			   __entry->ring_name = to_gsgpu_ring(job->base.sched)->name;
+			   __entry->num_ibs = job->num_ibs;
+			   ),
+	    TP_printk("sched_job=%llu, timeline=%s, context=%u, seqno=%u, ring_name=%s, num_ibs=%u",
+		      __entry->sched_job_id, __get_str(timeline), __entry->context,
+		      __entry->seqno, __entry->ring_name, __entry->num_ibs)
+);
+
+TRACE_EVENT(gsgpu_sched_run_job,
+	    TP_PROTO(struct gsgpu_job *job),
+	    TP_ARGS(job),
+	    TP_STRUCT__entry(
+			     __field(uint64_t, sched_job_id)
+			     __string(timeline, GSGPU_JOB_GET_TIMELINE_NAME(job))
+			     __field(unsigned int, context)
+			     __field(unsigned int, seqno)
+			     __field(char *, ring_name)
+			     __field(u32, num_ibs)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->sched_job_id = job->base.id;
+			   __assign_str(timeline, GSGPU_JOB_GET_TIMELINE_NAME(job))
+			   __entry->context = job->base.s_fence->finished.context;
+			   __entry->seqno = job->base.s_fence->finished.seqno;
+			   __entry->ring_name = to_gsgpu_ring(job->base.sched)->name;
+			   __entry->num_ibs = job->num_ibs;
+			   ),
+	    TP_printk("sched_job=%llu, timeline=%s, context=%u, seqno=%u, ring_name=%s, num_ibs=%u",
+		      __entry->sched_job_id, __get_str(timeline), __entry->context,
+		      __entry->seqno, __entry->ring_name, __entry->num_ibs)
+);
+
+
+TRACE_EVENT(gsgpu_vm_grab_id,
+	    TP_PROTO(struct gsgpu_vm *vm, struct gsgpu_ring *ring,
+		     struct gsgpu_job *job),
+	    TP_ARGS(vm, ring, job),
+	    TP_STRUCT__entry(
+			     __field(u32, pasid)
+			     __field(u32, ring)
+			     __field(u32, vmid)
+			     __field(u32, vm_hub)
+			     __field(u64, pd_addr)
+			     __field(u32, needs_flush)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->pasid = vm->pasid;
+			   __entry->ring = ring->idx;
+			   __entry->vmid = job->vmid;
+			   __entry->vm_hub = ring->funcs->vmhub,
+			   __entry->pd_addr = job->vm_pd_addr;
+			   __entry->needs_flush = job->vm_needs_flush;
+			   ),
+	    TP_printk("pasid=%d, ring=%u, id=%u, hub=%u, pd_addr=%010Lx needs_flush=%u",
+		      __entry->pasid, __entry->ring, __entry->vmid,
+		      __entry->vm_hub, __entry->pd_addr, __entry->needs_flush)
+);
+
+TRACE_EVENT(gsgpu_vm_bo_map,
+	    TP_PROTO(struct gsgpu_bo_va *bo_va,
+		     struct gsgpu_bo_va_mapping *mapping),
+	    TP_ARGS(bo_va, mapping),
+	    TP_STRUCT__entry(
+			     __field(struct gsgpu_bo *, bo)
+			     __field(long, start)
+			     __field(long, last)
+			     __field(u64, offset)
+			     __field(u64, flags)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->bo = bo_va ? bo_va->base.bo : NULL;
+			   __entry->start = mapping->start;
+			   __entry->last = mapping->last;
+			   __entry->offset = mapping->offset;
+			   __entry->flags = mapping->flags;
+			   ),
+	    TP_printk("bo=%p, start=%lx, last=%lx, offset=%010llx, flags=%llx",
+		      __entry->bo, __entry->start, __entry->last,
+		      __entry->offset, __entry->flags)
+);
+
+TRACE_EVENT(gsgpu_vm_bo_unmap,
+	    TP_PROTO(struct gsgpu_bo_va *bo_va,
+		     struct gsgpu_bo_va_mapping *mapping),
+	    TP_ARGS(bo_va, mapping),
+	    TP_STRUCT__entry(
+			     __field(struct gsgpu_bo *, bo)
+			     __field(long, start)
+			     __field(long, last)
+			     __field(u64, offset)
+			     __field(u64, flags)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->bo = bo_va ? bo_va->base.bo : NULL;
+			   __entry->start = mapping->start;
+			   __entry->last = mapping->last;
+			   __entry->offset = mapping->offset;
+			   __entry->flags = mapping->flags;
+			   ),
+	    TP_printk("bo=%p, start=%lx, last=%lx, offset=%010llx, flags=%llx",
+		      __entry->bo, __entry->start, __entry->last,
+		      __entry->offset, __entry->flags)
+);
+
+DECLARE_EVENT_CLASS(gsgpu_vm_mapping,
+	    TP_PROTO(struct gsgpu_bo_va_mapping *mapping),
+	    TP_ARGS(mapping),
+	    TP_STRUCT__entry(
+			     __field(u64, soffset)
+			     __field(u64, eoffset)
+			     __field(u64, flags)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->soffset = mapping->start;
+			   __entry->eoffset = mapping->last + 1;
+			   __entry->flags = mapping->flags;
+			   ),
+	    TP_printk("soffs=%010llx, eoffs=%010llx, flags=%llx",
+		      __entry->soffset, __entry->eoffset, __entry->flags)
+);
+
+DEFINE_EVENT(gsgpu_vm_mapping, gsgpu_vm_bo_update,
+	    TP_PROTO(struct gsgpu_bo_va_mapping *mapping),
+	    TP_ARGS(mapping)
+);
+
+DEFINE_EVENT(gsgpu_vm_mapping, gsgpu_vm_bo_mapping,
+	    TP_PROTO(struct gsgpu_bo_va_mapping *mapping),
+	    TP_ARGS(mapping)
+);
+
+DEFINE_EVENT(gsgpu_vm_mapping, gsgpu_vm_bo_cs,
+	    TP_PROTO(struct gsgpu_bo_va_mapping *mapping),
+	    TP_ARGS(mapping)
+);
+
+TRACE_EVENT(gsgpu_vm_set_ptes,
+	    TP_PROTO(uint64_t pe, uint64_t addr, unsigned count,
+		     uint32_t incr, uint64_t flags),
+	    TP_ARGS(pe, addr, count, incr, flags),
+	    TP_STRUCT__entry(
+			     __field(u64, pe)
+			     __field(u64, addr)
+			     __field(u32, count)
+			     __field(u32, incr)
+			     __field(u64, flags)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->pe = pe;
+			   __entry->addr = addr;
+			   __entry->count = count;
+			   __entry->incr = incr;
+			   __entry->flags = flags;
+			   ),
+	    TP_printk("pe=%010Lx, addr=%010Lx, incr=%u, flags=%llx, count=%u",
+		      __entry->pe, __entry->addr, __entry->incr,
+		      __entry->flags, __entry->count)
+);
+
+TRACE_EVENT(gsgpu_vm_copy_ptes,
+	    TP_PROTO(uint64_t pe, uint64_t src, unsigned count),
+	    TP_ARGS(pe, src, count),
+	    TP_STRUCT__entry(
+			     __field(u64, pe)
+			     __field(u64, src)
+			     __field(u32, count)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->pe = pe;
+			   __entry->src = src;
+			   __entry->count = count;
+			   ),
+	    TP_printk("pe=%010Lx, src=%010Lx, count=%u",
+		      __entry->pe, __entry->src, __entry->count)
+);
+
+TRACE_EVENT(gsgpu_vm_flush,
+	    TP_PROTO(struct gsgpu_ring *ring, unsigned vmid,
+		     uint64_t pd_addr),
+	    TP_ARGS(ring, vmid, pd_addr),
+	    TP_STRUCT__entry(
+			     __field(u32, ring)
+			     __field(u32, vmid)
+			     __field(u32, vm_hub)
+			     __field(u64, pd_addr)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->ring = ring->idx;
+			   __entry->vmid = vmid;
+			   __entry->vm_hub = ring->funcs->vmhub;
+			   __entry->pd_addr = pd_addr;
+			   ),
+	    TP_printk("ring=%u, id=%u, hub=%u, pd_addr=%010Lx",
+		      __entry->ring, __entry->vmid,
+		      __entry->vm_hub, __entry->pd_addr)
+);
+
+DECLARE_EVENT_CLASS(gsgpu_pasid,
+	    TP_PROTO(unsigned pasid),
+	    TP_ARGS(pasid),
+	    TP_STRUCT__entry(
+			     __field(unsigned, pasid)
+			     ),
+	    TP_fast_assign(
+			   __entry->pasid = pasid;
+			   ),
+	    TP_printk("pasid=%u", __entry->pasid)
+);
+
+DEFINE_EVENT(gsgpu_pasid, gsgpu_pasid_allocated,
+	    TP_PROTO(unsigned pasid),
+	    TP_ARGS(pasid)
+);
+
+DEFINE_EVENT(gsgpu_pasid, gsgpu_pasid_freed,
+	    TP_PROTO(unsigned pasid),
+	    TP_ARGS(pasid)
+);
+
+TRACE_EVENT(gsgpu_bo_list_set,
+	    TP_PROTO(struct gsgpu_bo_list *list, struct gsgpu_bo *bo),
+	    TP_ARGS(list, bo),
+	    TP_STRUCT__entry(
+			     __field(struct gsgpu_bo_list *, list)
+			     __field(struct gsgpu_bo *, bo)
+			     __field(u64, bo_size)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->list = list;
+			   __entry->bo = bo;
+			   __entry->bo_size = gsgpu_bo_size(bo);
+			   ),
+	    TP_printk("list=%p, bo=%p, bo_size=%Ld",
+		      __entry->list,
+		      __entry->bo,
+		      __entry->bo_size)
+);
+
+TRACE_EVENT(gsgpu_cs_bo_status,
+	    TP_PROTO(uint64_t total_bo, uint64_t total_size),
+	    TP_ARGS(total_bo, total_size),
+	    TP_STRUCT__entry(
+			__field(u64, total_bo)
+			__field(u64, total_size)
+			),
+
+	    TP_fast_assign(
+			__entry->total_bo = total_bo;
+			__entry->total_size = total_size;
+			),
+	    TP_printk("total_bo_size=%Ld, total_bo_count=%Ld",
+			__entry->total_bo, __entry->total_size)
+);
+
+TRACE_EVENT(gsgpu_bo_move,
+	    TP_PROTO(struct gsgpu_bo *bo, uint32_t new_placement, uint32_t old_placement),
+	    TP_ARGS(bo, new_placement, old_placement),
+	    TP_STRUCT__entry(
+			__field(struct gsgpu_bo *, bo)
+			__field(u64, bo_size)
+			__field(u32, new_placement)
+			__field(u32, old_placement)
+			),
+
+	    TP_fast_assign(
+			__entry->bo      = bo;
+			__entry->bo_size = gsgpu_bo_size(bo);
+			__entry->new_placement = new_placement;
+			__entry->old_placement = old_placement;
+			),
+	    TP_printk("bo=%p, from=%d, to=%d, size=%Ld",
+			__entry->bo, __entry->old_placement,
+			__entry->new_placement, __entry->bo_size)
+);
+
+#undef GSGPU_JOB_GET_TIMELINE_NAME
+#endif
+
+/* This part must be outside protection */
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH ../../drivers/gpu/drm/gsgpu/include
+#include <trace/define_trace.h>
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h b/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h
new file mode 100644
index 000000000000..937976d85c36
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h
@@ -0,0 +1,91 @@
+#ifndef __GSGPU_TTM_H__
+#define __GSGPU_TTM_H__
+
+#include "gsgpu.h"
+#include <drm/gpu_scheduler.h>
+
+#define GSGPU_GTT_MAX_TRANSFER_SIZE	512
+#define GSGPU_GTT_NUM_TRANSFER_WINDOWS	2
+
+struct gsgpu_mman {
+	struct ttm_bo_global_ref        bo_global_ref;
+	struct drm_global_reference	mem_global_ref;
+	struct ttm_bo_device		bdev;
+	bool				mem_global_referenced;
+	bool				initialized;
+	void __iomem			*aper_base_kaddr;
+
+#if defined(CONFIG_DEBUG_FS)
+	struct dentry			*debugfs_entries[8];
+#endif
+
+	/* buffer handling */
+	const struct gsgpu_buffer_funcs	*buffer_funcs;
+	struct gsgpu_ring			*buffer_funcs_ring;
+	bool					buffer_funcs_enabled;
+
+	struct mutex				gtt_window_lock;
+	/* Scheduler entity for buffer moves */
+	struct drm_sched_entity			entity;
+};
+
+struct gsgpu_copy_mem {
+	struct ttm_buffer_object	*bo;
+	struct ttm_mem_reg		*mem;
+	unsigned long			offset;
+};
+
+extern const struct ttm_mem_type_manager_func gsgpu_gtt_mgr_func;
+extern const struct ttm_mem_type_manager_func gsgpu_vram_mgr_func;
+
+bool gsgpu_gtt_mgr_has_gart_addr(struct ttm_mem_reg *mem);
+uint64_t gsgpu_gtt_mgr_usage(struct ttm_mem_type_manager *man);
+int gsgpu_gtt_mgr_recover(struct ttm_mem_type_manager *man);
+
+u64 gsgpu_vram_mgr_bo_visible_size(struct gsgpu_bo *bo);
+uint64_t gsgpu_vram_mgr_usage(struct ttm_mem_type_manager *man);
+uint64_t gsgpu_vram_mgr_vis_usage(struct ttm_mem_type_manager *man);
+
+int gsgpu_ttm_init(struct gsgpu_device *adev);
+void gsgpu_ttm_late_init(struct gsgpu_device *adev);
+void gsgpu_ttm_fini(struct gsgpu_device *adev);
+void gsgpu_ttm_set_buffer_funcs_status(struct gsgpu_device *adev,
+					bool enable);
+
+int gsgpu_copy_buffer(struct gsgpu_ring *ring, uint64_t src_offset,
+		       uint64_t dst_offset, uint32_t byte_count,
+		       struct reservation_object *resv,
+		       struct dma_fence **fence, bool direct_submit,
+		       bool vm_needs_flush);
+int gsgpu_ttm_copy_mem_to_mem(struct gsgpu_device *adev,
+			       struct gsgpu_copy_mem *src,
+			       struct gsgpu_copy_mem *dst,
+			       uint64_t size,
+			       struct reservation_object *resv,
+			       struct dma_fence **f);
+int gsgpu_fill_buffer(struct gsgpu_bo *bo,
+			uint32_t src_data,
+			struct reservation_object *resv,
+			struct dma_fence **fence);
+
+int gsgpu_mmap(struct file *filp, struct vm_area_struct *vma);
+int gsgpu_ttm_alloc_gart(struct ttm_buffer_object *bo);
+int gsgpu_ttm_recover_gart(struct ttm_buffer_object *tbo);
+
+int gsgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages);
+void gsgpu_ttm_tt_set_user_pages(struct ttm_tt *ttm, struct page **pages);
+void gsgpu_ttm_tt_mark_user_pages(struct ttm_tt *ttm);
+int gsgpu_ttm_tt_set_userptr(struct ttm_tt *ttm, uint64_t addr,
+				     uint32_t flags);
+bool gsgpu_ttm_tt_has_userptr(struct ttm_tt *ttm);
+struct mm_struct *gsgpu_ttm_tt_get_usermm(struct ttm_tt *ttm);
+bool gsgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
+				  unsigned long end);
+bool gsgpu_ttm_tt_userptr_invalidated(struct ttm_tt *ttm,
+				       int *last_invalidated);
+bool gsgpu_ttm_tt_userptr_needs_pages(struct ttm_tt *ttm);
+bool gsgpu_ttm_tt_is_readonly(struct ttm_tt *ttm);
+uint64_t gsgpu_ttm_tt_pte_flags(struct gsgpu_device *adev, struct ttm_tt *ttm,
+				 struct ttm_mem_reg *mem);
+
+#endif /* __GSGPU_TTM_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_vm.h b/drivers/gpu/drm/gsgpu/include/gsgpu_vm.h
new file mode 100644
index 000000000000..4e7da1925a4d
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_vm.h
@@ -0,0 +1,267 @@
+#ifndef __GSGPU_VM_H__
+#define __GSGPU_VM_H__
+
+#include <linux/idr.h>
+#include <linux/kfifo.h>
+#include <linux/rbtree.h>
+#include <drm/gpu_scheduler.h>
+#include <drm/drm_file.h>
+
+#include "gsgpu_sync.h"
+#include "gsgpu_ring.h"
+#include "gsgpu_ids.h"
+
+struct gsgpu_bo_va;
+struct gsgpu_job;
+struct gsgpu_bo_list_entry;
+
+#define GSGPU_VM_PDE_PTE_BYTES      8
+
+/* Maximum number of PTEs can write with one job */
+#define GSGPU_VM_MAX_UPDATE_SIZE	(1024ull)
+
+/* The number of PTEs a block contains */
+#define GSGPU_VM_PTE_COUNT(adev) (1 << (adev)->vm_manager.block_size)
+
+/* PTBs (Page Table Blocks) need to be aligned to 32K */
+#define GSGPU_VM_PTB_ALIGN_SIZE   32768
+
+#define GSGPU_PTE_PRESENT	(1ULL << 0)
+#define GSGPU_PTE_HUGEPAGE	(1ULL << 1)
+#define GSGPU_PTE_EXCEPTION	(1ULL << 2)
+#define GSGPU_PTE_WRITEABLE	(1ULL << 3)
+/* comprssed pte flags use bit 4 to 6 */
+#define GSGPU_PTE_COMPRESSED_SHIFT (5)
+
+/* How to programm VM fault handling */
+#define GSGPU_VM_FAULT_STOP_NEVER	0
+#define GSGPU_VM_FAULT_STOP_FIRST	1
+#define GSGPU_VM_FAULT_STOP_ALWAYS	2
+
+/* hardcode that limit for now */
+#define GSGPU_VA_RESERVED_SIZE			(1ULL << 20)
+
+/* VA hole for 48bit addresses on Vega10 */
+#define GSGPU_VA_HOLE_START			0x0000800000000000ULL
+#define GSGPU_VA_HOLE_END			0xffff800000000000ULL
+
+/*
+ * Hardware is programmed as if the hole doesn't exists with start and end
+ * address values.
+ *
+ * This mask is used to remove the upper 16bits of the VA and so come up with
+ * the linear addr value.
+ */
+#define GSGPU_VA_HOLE_MASK			0x0000ffffffffffffULL
+
+/* max vmids dedicated for process */
+#define GSGPU_VM_MAX_RESERVED_VMID	1
+
+#define GSGPU_VM_CONTEXT_GFX 0
+#define GSGPU_VM_CONTEXT_COMPUTE 1
+
+/* VMPT level enumerate, and the hiberachy is:
+ * DIR0->DIR1->DIR2
+ */
+enum gsgpu_vm_level {
+	GSGPU_VM_DIR0,
+	GSGPU_VM_DIR1,
+	GSGPU_VM_DIR2
+};
+
+/* base structure for tracking BO usage in a VM */
+struct gsgpu_vm_bo_base {
+	/* constant after initialization */
+	struct gsgpu_vm		*vm;
+	struct gsgpu_bo		*bo;
+
+	/* protected by bo being reserved */
+	struct list_head		bo_list;
+
+	/* protected by spinlock */
+	struct list_head		vm_status;
+
+	/* protected by the BO being reserved */
+	bool				moved;
+};
+
+struct gsgpu_vm_pt {
+	struct gsgpu_vm_bo_base	base;
+	bool				huge;
+
+	/* array of page tables, one for each directory entry */
+	struct gsgpu_vm_pt		*entries;
+};
+
+#define GSGPU_VM_FAULT(pasid, addr) (((u64)(pasid) << 48) | (addr))
+#define GSGPU_VM_FAULT_PASID(fault) ((u64)(fault) >> 48)
+#define GSGPU_VM_FAULT_ADDR(fault)  ((u64)(fault) & 0xfffffffff000ULL)
+
+
+struct gsgpu_task_info {
+	char	process_name[TASK_COMM_LEN];
+	char	task_name[TASK_COMM_LEN];
+	pid_t	pid;
+	pid_t	tgid;
+};
+
+struct gsgpu_vm {
+	/* tree of virtual addresses mapped */
+	struct rb_root_cached	va;
+
+	/* BOs who needs a validation */
+	struct list_head	evicted;
+
+	/* PT BOs which relocated and their parent need an update */
+	struct list_head	relocated;
+
+	/* BOs moved, but not yet updated in the PT */
+	struct list_head	moved;
+	spinlock_t		moved_lock;
+
+	/* All BOs of this VM not currently in the state machine */
+	struct list_head	idle;
+
+	/* BO mappings freed, but not yet updated in the PT */
+	struct list_head	freed;
+
+	/* contains the page directory */
+	struct gsgpu_vm_pt     root;
+	struct dma_fence	*last_update;
+
+	/* Scheduler entity for page table updates */
+	struct drm_sched_entity	entity;
+
+	unsigned int		pasid;
+	/* dedicated to vm */
+	struct gsgpu_vmid	*reserved_vmid;
+
+	/* Flag to indicate if VM tables are updated by CPU or GPU (XDMA) */
+	bool                    use_cpu_for_update;
+
+	/* Flag to indicate ATS support from PTE for GFX9 */
+	bool			pte_support_ats;
+
+	/* Up to 128 pending retry page faults */
+	DECLARE_KFIFO(faults, u64, 128);
+
+	/* Limit non-retry fault storms */
+	unsigned int		fault_credit;
+
+	/* Valid while the PD is reserved or fenced */
+	uint64_t		pd_phys_addr;
+
+	/* Some basic info about the task */
+	struct gsgpu_task_info task_info;
+};
+
+struct gsgpu_vm_manager {
+	/* Handling of VMIDs */
+	struct gsgpu_vmid_mgr			id_mgr;
+
+	/* Handling of VM fences */
+	u64					fence_context;
+	unsigned				seqno[GSGPU_MAX_RINGS];
+
+    uint32_t                pde_pte_bytes;
+
+	uint64_t				max_pfn;
+	uint32_t				num_level;
+	uint32_t				block_size;
+	uint32_t                fragment_size;
+	enum gsgpu_vm_level		root_level;
+	uint32_t dir0_shift, dir0_width;
+	uint32_t dir1_shift, dir1_width;
+	uint32_t dir2_shift, dir2_width;
+
+	/* vram base address for page table entry  */
+	u64					vram_base_offset;
+	/* vm pte handling */
+	const struct gsgpu_vm_pte_funcs        *vm_pte_funcs;
+	struct gsgpu_ring                      *vm_pte_rings[GSGPU_MAX_RINGS];
+	unsigned				vm_pte_num_rings;
+	atomic_t				vm_pte_next_ring;
+
+	/* partial resident texture handling */
+	spinlock_t				prt_lock;
+	atomic_t				num_prt_users;
+
+	/* controls how VM page tables are updated for Graphics and Compute.
+	 * BIT0[= 0] Graphics updated by XDMA [= 1] by CPU
+	 * BIT1[= 0] Compute updated by XDMA [= 1] by CPU
+	 */
+	int					vm_update_mode;
+
+	/* PASID to VM mapping, will be used in interrupt context to
+	 * look up VM of a page fault
+	 */
+	struct idr				pasid_idr;
+	spinlock_t				pasid_lock;
+};
+
+void gsgpu_vm_manager_init(struct gsgpu_device *adev);
+void gsgpu_vm_manager_fini(struct gsgpu_device *adev);
+int gsgpu_vm_init(struct gsgpu_device *adev, struct gsgpu_vm *vm,
+		   int vm_context, unsigned int pasid);
+void gsgpu_vm_fini(struct gsgpu_device *adev, struct gsgpu_vm *vm);
+bool gsgpu_vm_pasid_fault_credit(struct gsgpu_device *adev,
+				  unsigned int pasid);
+void gsgpu_vm_get_pd_bo(struct gsgpu_vm *vm,
+			 struct list_head *validated,
+			 struct gsgpu_bo_list_entry *entry);
+bool gsgpu_vm_ready(struct gsgpu_vm *vm);
+int gsgpu_vm_validate_pt_bos(struct gsgpu_device *adev, struct gsgpu_vm *vm,
+			      int (*callback)(void *p, struct gsgpu_bo *bo),
+			      void *param);
+int gsgpu_vm_alloc_pts(struct gsgpu_device *adev,
+			struct gsgpu_vm *vm,
+			uint64_t saddr, uint64_t size);
+int gsgpu_vm_flush(struct gsgpu_ring *ring, struct gsgpu_job *job, bool need_pipe_sync);
+int gsgpu_vm_update_directories(struct gsgpu_device *adev,
+				 struct gsgpu_vm *vm);
+int gsgpu_vm_clear_freed(struct gsgpu_device *adev,
+			  struct gsgpu_vm *vm,
+			  struct dma_fence **fence);
+int gsgpu_vm_handle_moved(struct gsgpu_device *adev,
+			   struct gsgpu_vm *vm);
+int gsgpu_vm_bo_update(struct gsgpu_device *adev,
+			struct gsgpu_bo_va *bo_va,
+			bool clear);
+void gsgpu_vm_bo_invalidate(struct gsgpu_device *adev,
+			     struct gsgpu_bo *bo, bool evicted);
+struct gsgpu_bo_va *gsgpu_vm_bo_find(struct gsgpu_vm *vm,
+				       struct gsgpu_bo *bo);
+struct gsgpu_bo_va *gsgpu_vm_bo_add(struct gsgpu_device *adev,
+				      struct gsgpu_vm *vm,
+				      struct gsgpu_bo *bo);
+int gsgpu_vm_bo_map(struct gsgpu_device *adev,
+		     struct gsgpu_bo_va *bo_va,
+		     uint64_t addr, uint64_t offset,
+		     uint64_t size, uint64_t flags);
+int gsgpu_vm_bo_replace_map(struct gsgpu_device *adev,
+			     struct gsgpu_bo_va *bo_va,
+			     uint64_t addr, uint64_t offset,
+			     uint64_t size, uint64_t flags);
+int gsgpu_vm_bo_unmap(struct gsgpu_device *adev,
+		       struct gsgpu_bo_va *bo_va,
+		       uint64_t addr);
+int gsgpu_vm_bo_clear_mappings(struct gsgpu_device *adev,
+				struct gsgpu_vm *vm,
+				uint64_t saddr, uint64_t size);
+struct gsgpu_bo_va_mapping *gsgpu_vm_bo_lookup_mapping(struct gsgpu_vm *vm,
+							 uint64_t addr);
+void gsgpu_vm_bo_trace_cs(struct gsgpu_vm *vm, struct ww_acquire_ctx *ticket);
+void gsgpu_vm_bo_rmv(struct gsgpu_device *adev,
+		      struct gsgpu_bo_va *bo_va);
+void gsgpu_vm_adjust_size(struct gsgpu_device *adev, uint32_t min_vm_size,
+			   unsigned max_level, unsigned max_bits);
+int gsgpu_vm_ioctl(struct drm_device *dev, void *data, struct drm_file *filp);
+bool gsgpu_vm_need_pipeline_sync(struct gsgpu_ring *ring,
+				  struct gsgpu_job *job);
+
+void gsgpu_vm_get_task_info(struct gsgpu_device *adev, unsigned int pasid,
+			 struct gsgpu_task_info *task_info);
+
+void gsgpu_vm_set_task_info(struct gsgpu_vm *vm);
+
+#endif /* __GSGPU_VM_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_vm_it.h b/drivers/gpu/drm/gsgpu/include/gsgpu_vm_it.h
new file mode 100644
index 000000000000..2c4dfb1515c0
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_vm_it.h
@@ -0,0 +1,36 @@
+
+#ifndef __GSGPU_VM_IT_H__
+#define __GSGPU_VM_IT_H__
+
+#include <linux/rbtree.h>
+#include <linux/types.h>
+#include "gsgpu_object.h"
+
+/* bo virtual addresses in a vm */
+struct gsgpu_bo_va_mapping {
+	struct rb_node rb;
+	uint64_t start;
+	uint64_t last;
+	uint64_t __subtree_last;
+
+	struct gsgpu_bo_va *bo_va;
+	struct list_head list;
+	uint64_t offset;
+	uint64_t flags;
+};
+
+extern void gsgpu_vm_it_insert(struct gsgpu_bo_va_mapping *node,
+					struct rb_root_cached *root);
+
+extern void gsgpu_vm_it_remove(struct gsgpu_bo_va_mapping *node,
+					struct rb_root_cached *root);
+
+extern struct gsgpu_bo_va_mapping *
+gsgpu_vm_it_iter_first(struct rb_root_cached *root,
+					uint64_t start, uint64_t last);
+
+extern struct gsgpu_bo_va_mapping *
+gsgpu_vm_it_iter_next(struct gsgpu_bo_va_mapping *node,
+					uint64_t start, uint64_t last);
+
+#endif  /* __GSGPU_VM_IT_H__ */
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_xdma.h b/drivers/gpu/drm/gsgpu/include/gsgpu_xdma.h
new file mode 100644
index 000000000000..1cdba014e2db
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_xdma.h
@@ -0,0 +1,10 @@
+#ifndef __GSGPU_XDMA_H__
+#define __GSGPU_XDMA_H__
+
+#define GSGPU_XDMA_FLAG_UMAP 0x20000
+
+extern const struct gsgpu_ip_block_version xdma_ip_block;
+
+void xdma_ring_test_xdma_loop(struct gsgpu_ring *ring, long timeout);
+
+#endif /*__GSGPU_XDMA_H__*/
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_zip.h b/drivers/gpu/drm/gsgpu/include/gsgpu_zip.h
new file mode 100644
index 000000000000..2be62ca7e0a0
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_zip.h
@@ -0,0 +1,12 @@
+#ifndef __GSGPU_ZIP_H__
+#define __GSGPU_ZIP_H__
+
+#define     ZIP_ENABLE    0x01 // zip enable
+#define     ZIP_DISABLE   0x02 // zip disable
+#define     ZIP_SET_BASE  0x03 // zip_base = ret[1:0]
+#define     ZIP_SET_MASK  0x04 // zip_mask = ret[1:0]
+#define     ZIP_FLUSH     0x05 // zip_flush
+
+extern const struct gsgpu_ip_block_version zip_ip_block;
+
+#endif /*__GSGPU_ZIP_H__*/
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_zip_meta.h b/drivers/gpu/drm/gsgpu/include/gsgpu_zip_meta.h
new file mode 100644
index 000000000000..786541b6e59e
--- /dev/null
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_zip_meta.h
@@ -0,0 +1,44 @@
+#ifndef __GSGPU_ZIP_META_H__
+#define __GSGPU_ZIP_META_H__
+
+#include <linux/types.h>
+
+/*
+ * ZIP META structures, functions & helpers
+ */
+struct gsgpu_device;
+struct gsgpu_bo;
+
+
+#define GSGPU_GEM_META_ALIGN_SHIFT 7
+#define GSGPU_GEM_COMPRESSED_SIZE (PAGE_SIZE<<GSGPU_GEM_META_ALIGN_SHIFT)
+
+struct gsgpu_zip_meta {
+	u64 table_addr;
+	u64 mask;
+	struct gsgpu_bo *robj;
+	void *ptr;
+	unsigned num_gpu_pages;
+	unsigned num_cpu_pages;
+	unsigned table_size;
+#ifdef CONFIG_DRM_GSGPU_ZIP_DEBUGFS
+	struct page **pages;
+#endif
+	bool ready;
+	u64 pte_flags;
+};
+
+int gsgpu_zip_meta_vram_alloc(struct gsgpu_device *adev);
+void gsgpu_zip_meta_vram_free(struct gsgpu_device *adev);
+int gsgpu_zip_meta_vram_pin(struct gsgpu_device *adev);
+void gsgpu_zip_meta_vram_unpin(struct gsgpu_device *adev);
+int gsgpu_zip_meta_init(struct gsgpu_device *adev);
+void gsgpu_zip_meta_fini(struct gsgpu_device *adev);
+int gsgpu_zip_meta_unbind(struct gsgpu_device *adev, uint64_t offset,
+		       int pages);
+uint64_t gsgpu_zip_meta_map(struct gsgpu_device *adev, uint64_t start);
+int gsgpu_zip_meta_bind(struct gsgpu_device *adev, uint64_t offset,
+		     int pages, struct page **pagelist,
+		     dma_addr_t *dma_addr, uint64_t flags);
+
+#endif /* __GSGPU_ZIP_META_H__ */
diff --git a/include/drm/gsgpu_family_type.h b/include/drm/gsgpu_family_type.h
new file mode 100644
index 000000000000..282a6caec81f
--- /dev/null
+++ b/include/drm/gsgpu_family_type.h
@@ -0,0 +1,10 @@
+#ifndef __GSGPU_FAMILY_TYPE_H__
+#define __GSGPU_FAMILY_TYPE_H__
+/*
+ * Supported FAMILY types
+ */
+enum gsgpu_family_type {
+	CHIP_LG100 = 0,
+};
+
+#endif /*__GSGPU_FAMILY_TYPE_H__ */
diff --git a/include/uapi/drm/gsgpu_drm.h b/include/uapi/drm/gsgpu_drm.h
new file mode 100644
index 000000000000..decb4820724e
--- /dev/null
+++ b/include/uapi/drm/gsgpu_drm.h
@@ -0,0 +1,1003 @@
+/* gsgpu_drm.h -- Public header for the gsgpu driver -*- linux-c -*-
+ *
+ * Copyright 2000 Precision Insight, Inc., Cedar Park, Texas.
+ * Copyright 2000 VA Linux Systems, Inc., Fremont, California.
+ * Copyright 2002 Tungsten Graphics, Inc., Cedar Park, Texas.
+ * Copyright 2014 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *    Kevin E. Martin <martin@valinux.com>
+ *    Gareth Hughes <gareth@valinux.com>
+ *    Keith Whitwell <keith@tungstengraphics.com>
+ */
+
+#ifndef __GSGPU_DRM_H__
+#define __GSGPU_DRM_H__
+
+#include "drm.h"
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#define DRM_GSGPU_GEM_CREATE		0x00
+#define DRM_GSGPU_GEM_MMAP		0x01
+#define DRM_GSGPU_CTX			0x02
+#define DRM_GSGPU_BO_LIST		0x03
+#define DRM_GSGPU_CS			0x04
+#define DRM_GSGPU_INFO			0x05
+#define DRM_GSGPU_GEM_METADATA		0x06
+#define DRM_GSGPU_GEM_WAIT_IDLE	0x07
+#define DRM_GSGPU_GEM_VA		0x08
+#define DRM_GSGPU_WAIT_CS		0x09
+#define DRM_GSGPU_GEM_OP		0x10
+#define DRM_GSGPU_GEM_USERPTR		0x11
+#define DRM_GSGPU_WAIT_FENCES		0x12
+#define DRM_GSGPU_VM			0x13
+#define DRM_GSGPU_FENCE_TO_HANDLE	0x14
+#define DRM_GSGPU_SCHED			0x15
+#define DRM_GSGPU_HWSEMA_OP		0x16
+
+#define DRM_IOCTL_GSGPU_GEM_CREATE	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_CREATE, union drm_gsgpu_gem_create)
+#define DRM_IOCTL_GSGPU_GEM_MMAP	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_MMAP, union drm_gsgpu_gem_mmap)
+#define DRM_IOCTL_GSGPU_CTX		DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_CTX, union drm_gsgpu_ctx)
+#define DRM_IOCTL_GSGPU_BO_LIST	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_BO_LIST, union drm_gsgpu_bo_list)
+#define DRM_IOCTL_GSGPU_CS		DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_CS, union drm_gsgpu_cs)
+#define DRM_IOCTL_GSGPU_INFO		DRM_IOW(DRM_COMMAND_BASE + DRM_GSGPU_INFO, struct drm_gsgpu_info)
+#define DRM_IOCTL_GSGPU_GEM_METADATA	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_METADATA, struct drm_gsgpu_gem_metadata)
+#define DRM_IOCTL_GSGPU_GEM_WAIT_IDLE	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_WAIT_IDLE, union drm_gsgpu_gem_wait_idle)
+#define DRM_IOCTL_GSGPU_GEM_VA		DRM_IOW(DRM_COMMAND_BASE + DRM_GSGPU_GEM_VA, struct drm_gsgpu_gem_va)
+#define DRM_IOCTL_GSGPU_WAIT_CS	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_WAIT_CS, union drm_gsgpu_wait_cs)
+#define DRM_IOCTL_GSGPU_GEM_OP		DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_OP, struct drm_gsgpu_gem_op)
+#define DRM_IOCTL_GSGPU_GEM_USERPTR	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_GEM_USERPTR, struct drm_gsgpu_gem_userptr)
+#define DRM_IOCTL_GSGPU_WAIT_FENCES	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_WAIT_FENCES, union drm_gsgpu_wait_fences)
+#define DRM_IOCTL_GSGPU_VM		DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_VM, union drm_gsgpu_vm)
+#define DRM_IOCTL_GSGPU_FENCE_TO_HANDLE DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_FENCE_TO_HANDLE, union drm_gsgpu_fence_to_handle)
+#define DRM_IOCTL_GSGPU_SCHED		DRM_IOW(DRM_COMMAND_BASE + DRM_GSGPU_SCHED, union drm_gsgpu_sched)
+#define DRM_IOCTL_GSGPU_HWSEMA_OP	DRM_IOWR(DRM_COMMAND_BASE + DRM_GSGPU_HWSEMA_OP, struct drm_gsgpu_hw_sema)
+
+/**
+ * DOC: memory domains
+ *
+ * %GSGPU_GEM_DOMAIN_CPU	System memory that is not GPU accessible.
+ * Memory in this pool could be swapped out to disk if there is pressure.
+ *
+ * %GSGPU_GEM_DOMAIN_GTT	GPU accessible system memory, mapped into the
+ * GPU's virtual address space via gart. Gart memory linearizes non-contiguous
+ * pages of system memory, allows GPU access system memory in a linezrized
+ * fashion.
+ *
+ * %GSGPU_GEM_DOMAIN_VRAM	Local video memory. For APUs, it is memory
+ * carved out by the BIOS.
+ *
+ * %GSGPU_GEM_DOMAIN_GDS	Global on-chip data storage used to share data
+ * across shader threads.
+ *
+ * %GSGPU_GEM_DOMAIN_GWS	Global wave sync, used to synchronize the
+ * execution of all the waves on a device.
+ *
+ * %GSGPU_GEM_DOMAIN_OA	Ordered append, used by 3D or Compute engines
+ * for appending data.
+ */
+#define GSGPU_GEM_DOMAIN_CPU		0x1
+#define GSGPU_GEM_DOMAIN_GTT		0x2
+#define GSGPU_GEM_DOMAIN_VRAM		0x4
+#define GSGPU_GEM_DOMAIN_GDS		0x8
+#define GSGPU_GEM_DOMAIN_GWS		0x10
+#define GSGPU_GEM_DOMAIN_OA		0x20
+#define GSGPU_GEM_DOMAIN_MASK		(GSGPU_GEM_DOMAIN_CPU | \
+					 GSGPU_GEM_DOMAIN_GTT | \
+					 GSGPU_GEM_DOMAIN_VRAM | \
+					 GSGPU_GEM_DOMAIN_GDS | \
+					 GSGPU_GEM_DOMAIN_GWS | \
+					 GSGPU_GEM_DOMAIN_OA)
+
+/* Flag that CPU access will be required for the case of VRAM domain */
+#define GSGPU_GEM_CREATE_CPU_ACCESS_REQUIRED	(1 << 0)
+/* Flag that CPU access will not work, this VRAM domain is invisible */
+#define GSGPU_GEM_CREATE_NO_CPU_ACCESS		(1 << 1)
+/* Flag that USWC attributes should be used for GTT */
+#define GSGPU_GEM_CREATE_CPU_GTT_USWC		(1 << 2)
+/* Flag that the memory should be in VRAM and cleared */
+#define GSGPU_GEM_CREATE_VRAM_CLEARED		(1 << 3)
+/* Flag that create shadow bo(GTT) while allocating vram bo */
+#define GSGPU_GEM_CREATE_SHADOW		(1 << 4)
+/* Flag that allocating the BO should use linear VRAM */
+#define GSGPU_GEM_CREATE_VRAM_CONTIGUOUS	(1 << 5)
+/* Flag that BO is always valid in this VM */
+#define GSGPU_GEM_CREATE_VM_ALWAYS_VALID	(1 << 6)
+/* Flag that BO sharing will be explicitly synchronized */
+#define GSGPU_GEM_CREATE_EXPLICIT_SYNC		(1 << 7)
+/* Flag that indicates allocating MQD gart on GFX9, where the mtype
+ * for the second page onward should be set to NC.
+ */
+#define GSGPU_GEM_CREATE_MQD_GFX9		(1 << 8)
+/* Flag that BO is compressed bit 9-11 */
+#define GSGPU_GEM_CREATE_COMPRESSED_MASK	(0x7ull << 9)
+
+struct drm_gsgpu_gem_create_in  {
+	/** the requested memory size */
+	__u64 bo_size;
+	/** physical start_addr alignment in bytes for some HW requirements */
+	__u64 alignment;
+	/** the requested memory domains */
+	__u64 domains;
+	/** allocation flags */
+	__u64 domain_flags;
+};
+
+struct drm_gsgpu_gem_create_out  {
+	/** returned GEM object handle */
+	__u32 handle;
+	__u32 _pad;
+};
+
+union drm_gsgpu_gem_create {
+	struct drm_gsgpu_gem_create_in		in;
+	struct drm_gsgpu_gem_create_out	out;
+};
+
+/** Opcode to create new residency list.  */
+#define GSGPU_BO_LIST_OP_CREATE	0
+/** Opcode to destroy previously created residency list */
+#define GSGPU_BO_LIST_OP_DESTROY	1
+/** Opcode to update resource information in the list */
+#define GSGPU_BO_LIST_OP_UPDATE	2
+
+struct drm_gsgpu_bo_list_in {
+	/** Type of operation */
+	__u32 operation;
+	/** Handle of list or 0 if we want to create one */
+	__u32 list_handle;
+	/** Number of BOs in list  */
+	__u32 bo_number;
+	/** Size of each element describing BO */
+	__u32 bo_info_size;
+	/** Pointer to array describing BOs */
+	__u64 bo_info_ptr;
+};
+
+struct drm_gsgpu_bo_list_entry {
+	/** Handle of BO */
+	__u32 bo_handle;
+	/** New (if specified) BO priority to be used during migration */
+	__u32 bo_priority;
+};
+
+struct drm_gsgpu_bo_list_out {
+	/** Handle of resource list  */
+	__u32 list_handle;
+	__u32 _pad;
+};
+
+union drm_gsgpu_bo_list {
+	struct drm_gsgpu_bo_list_in in;
+	struct drm_gsgpu_bo_list_out out;
+};
+
+/* context related */
+#define GSGPU_CTX_OP_ALLOC_CTX	1
+#define GSGPU_CTX_OP_FREE_CTX	2
+#define GSGPU_CTX_OP_QUERY_STATE	3
+#define GSGPU_CTX_OP_QUERY_STATE2	4
+
+/* GPU reset status */
+#define GSGPU_CTX_NO_RESET		0
+/* this the context caused it */
+#define GSGPU_CTX_GUILTY_RESET		1
+/* some other context caused it */
+#define GSGPU_CTX_INNOCENT_RESET	2
+/* unknown cause */
+#define GSGPU_CTX_UNKNOWN_RESET	3
+
+/* indicate gpu reset occured after ctx created */
+#define GSGPU_CTX_QUERY2_FLAGS_RESET    (1<<0)
+/* indicate vram lost occured after ctx created */
+#define GSGPU_CTX_QUERY2_FLAGS_VRAMLOST (1<<1)
+/* indicate some job from this context once cause gpu hang */
+#define GSGPU_CTX_QUERY2_FLAGS_GUILTY   (1<<2)
+
+/* Context priority level */
+#define GSGPU_CTX_PRIORITY_UNSET       -2048
+#define GSGPU_CTX_PRIORITY_VERY_LOW    -1023
+#define GSGPU_CTX_PRIORITY_LOW         -512
+#define GSGPU_CTX_PRIORITY_NORMAL      0
+/* Selecting a priority above NORMAL requires CAP_SYS_NICE or DRM_MASTER */
+#define GSGPU_CTX_PRIORITY_HIGH        512
+#define GSGPU_CTX_PRIORITY_VERY_HIGH   1023
+
+struct drm_gsgpu_ctx_in {
+	/** GSGPU_CTX_OP_* */
+	__u32	op;
+	/** For future use, no flags defined so far */
+	__u32	flags;
+	__u32	ctx_id;
+	__s32	priority;
+};
+
+union drm_gsgpu_ctx_out {
+		struct {
+			__u32	ctx_id;
+			__u32	_pad;
+		} alloc;
+
+		struct {
+			/** For future use, no flags defined so far */
+			__u64	flags;
+			/** Number of resets caused by this context so far. */
+			__u32	hangs;
+			/** Reset status since the last call of the ioctl. */
+			__u32	reset_status;
+		} state;
+};
+
+union drm_gsgpu_ctx {
+	struct drm_gsgpu_ctx_in in;
+	union drm_gsgpu_ctx_out out;
+};
+
+/* vm ioctl */
+#define GSGPU_VM_OP_RESERVE_VMID	1
+#define GSGPU_VM_OP_UNRESERVE_VMID	2
+
+struct drm_gsgpu_vm_in {
+	/** GSGPU_VM_OP_* */
+	__u32	op;
+	__u32	flags;
+};
+
+struct drm_gsgpu_vm_out {
+	/** For future use, no flags defined so far */
+	__u64	flags;
+};
+
+union drm_gsgpu_vm {
+	struct drm_gsgpu_vm_in in;
+	struct drm_gsgpu_vm_out out;
+};
+
+/* sched ioctl */
+#define GSGPU_SCHED_OP_PROCESS_PRIORITY_OVERRIDE	1
+
+struct drm_gsgpu_sched_in {
+	/* GSGPU_SCHED_OP_* */
+	__u32	op;
+	__u32	fd;
+	__s32	priority;
+	__u32	flags;
+};
+
+union drm_gsgpu_sched {
+	struct drm_gsgpu_sched_in in;
+};
+
+/*
+ * This is not a reliable API and you should expect it to fail for any
+ * number of reasons and have fallback path that do not use userptr to
+ * perform any operation.
+ */
+#define GSGPU_GEM_USERPTR_READONLY	(1 << 0)
+#define GSGPU_GEM_USERPTR_ANONONLY	(1 << 1)
+#define GSGPU_GEM_USERPTR_VALIDATE	(1 << 2)
+#define GSGPU_GEM_USERPTR_REGISTER	(1 << 3)
+
+struct drm_gsgpu_gem_userptr {
+	__u64		addr;
+	__u64		size;
+	/* GSGPU_GEM_USERPTR_* */
+	__u32		flags;
+	/* Resulting GEM handle */
+	__u32		handle;
+};
+
+/* SI-CI-VI: */
+/* same meaning as the GB_TILE_MODE and GL_MACRO_TILE_MODE fields */
+#define GSGPU_TILING_ARRAY_MODE_SHIFT			0
+#define GSGPU_TILING_ARRAY_MODE_MASK			0xf
+#define GSGPU_TILING_PIPE_CONFIG_SHIFT			4
+#define GSGPU_TILING_PIPE_CONFIG_MASK			0x1f
+#define GSGPU_TILING_TILE_SPLIT_SHIFT			9
+#define GSGPU_TILING_TILE_SPLIT_MASK			0x7
+#define GSGPU_TILING_MICRO_TILE_MODE_SHIFT		12
+#define GSGPU_TILING_MICRO_TILE_MODE_MASK		0x7
+#define GSGPU_TILING_BANK_WIDTH_SHIFT			15
+#define GSGPU_TILING_BANK_WIDTH_MASK			0x3
+#define GSGPU_TILING_BANK_HEIGHT_SHIFT			17
+#define GSGPU_TILING_BANK_HEIGHT_MASK			0x3
+#define GSGPU_TILING_MACRO_TILE_ASPECT_SHIFT		19
+#define GSGPU_TILING_MACRO_TILE_ASPECT_MASK		0x3
+#define GSGPU_TILING_NUM_BANKS_SHIFT			21
+#define GSGPU_TILING_NUM_BANKS_MASK			0x3
+
+/* GFX9 and later: */
+#define GSGPU_TILING_SWIZZLE_MODE_SHIFT		0
+#define GSGPU_TILING_SWIZZLE_MODE_MASK			0x1f
+
+/* Set/Get helpers for tiling flags. */
+#define GSGPU_TILING_SET(field, value) \
+	(((__u64)(value) & GSGPU_TILING_##field##_MASK) << GSGPU_TILING_##field##_SHIFT)
+#define GSGPU_TILING_GET(value, field) \
+	(((__u64)(value) >> GSGPU_TILING_##field##_SHIFT) & GSGPU_TILING_##field##_MASK)
+
+#define GSGPU_GEM_METADATA_OP_SET_METADATA                  1
+#define GSGPU_GEM_METADATA_OP_GET_METADATA                  2
+
+/** The same structure is shared for input/output */
+struct drm_gsgpu_gem_metadata {
+	/** GEM Object handle */
+	__u32	handle;
+	/** Do we want get or set metadata */
+	__u32	op;
+	struct {
+		/** For future use, no flags defined so far */
+		__u64	flags;
+		/** family specific tiling info */
+		__u64	tiling_info;
+		__u32	data_size_bytes;
+		__u32	data[64];
+	} data;
+};
+
+struct drm_gsgpu_gem_mmap_in {
+	/** the GEM object handle */
+	__u32 handle;
+	__u32 _pad;
+};
+
+struct drm_gsgpu_gem_mmap_out {
+	/** mmap offset from the vma offset manager */
+	__u64 addr_ptr;
+};
+
+union drm_gsgpu_gem_mmap {
+	struct drm_gsgpu_gem_mmap_in   in;
+	struct drm_gsgpu_gem_mmap_out out;
+};
+
+struct drm_gsgpu_gem_wait_idle_in {
+	/** GEM object handle */
+	__u32 handle;
+	/** For future use, no flags defined so far */
+	__u32 flags;
+	/** Absolute timeout to wait */
+	__u64 timeout;
+};
+
+struct drm_gsgpu_gem_wait_idle_out {
+	/** BO status:  0 - BO is idle, 1 - BO is busy */
+	__u32 status;
+	/** Returned current memory domain */
+	__u32 domain;
+};
+
+union drm_gsgpu_gem_wait_idle {
+	struct drm_gsgpu_gem_wait_idle_in  in;
+	struct drm_gsgpu_gem_wait_idle_out out;
+};
+
+struct drm_gsgpu_wait_cs_in {
+	/* Command submission handle
+		* handle equals 0 means none to wait for
+		* handle equals ~0ull means wait for the latest sequence number
+		*/
+	__u64 handle;
+	/** Absolute timeout to wait */
+	__u64 timeout;
+	__u32 ip_type;
+	__u32 ip_instance;
+	__u32 ring;
+	__u32 ctx_id;
+};
+
+struct drm_gsgpu_wait_cs_out {
+	/** CS status:  0 - CS completed, 1 - CS still busy */
+	__u64 status;
+};
+
+union drm_gsgpu_wait_cs {
+	struct drm_gsgpu_wait_cs_in in;
+	struct drm_gsgpu_wait_cs_out out;
+};
+
+struct drm_gsgpu_fence {
+	__u32 ctx_id;
+	__u32 ip_type;
+	__u32 ip_instance;
+	__u32 ring;
+	__u64 seq_no;
+};
+
+struct drm_gsgpu_wait_fences_in {
+	/** This points to uint64_t * which points to fences */
+	__u64 fences;
+	__u32 fence_count;
+	__u32 wait_all;
+	__u64 timeout_ns;
+};
+
+struct drm_gsgpu_wait_fences_out {
+	__u32 status;
+	__u32 first_signaled;
+};
+
+union drm_gsgpu_wait_fences {
+	struct drm_gsgpu_wait_fences_in in;
+	struct drm_gsgpu_wait_fences_out out;
+};
+
+#define GSGPU_GEM_OP_GET_GEM_CREATE_INFO	0
+#define GSGPU_GEM_OP_SET_PLACEMENT		1
+
+/* Sets or returns a value associated with a buffer. */
+struct drm_gsgpu_gem_op {
+	/** GEM object handle */
+	__u32	handle;
+	/** GSGPU_GEM_OP_* */
+	__u32	op;
+	/** Input or return value */
+	__u64	value;
+};
+
+#define GSGPU_VA_OP_MAP			1
+#define GSGPU_VA_OP_UNMAP			2
+#define GSGPU_VA_OP_CLEAR			3
+#define GSGPU_VA_OP_REPLACE			4
+
+/* Delay the page table update till the next CS */
+#define GSGPU_VM_DELAY_UPDATE		(1 << 0)
+
+/* Mapping flags */
+/* readable mapping */
+#define GSGPU_VM_PAGE_READABLE		(1 << 1)
+/* writable mapping */
+#define GSGPU_VM_PAGE_WRITEABLE	(1 << 2)
+/* executable mapping, new for VI */
+#define GSGPU_VM_PAGE_EXECUTABLE	(1 << 3)
+/* partially resident texture */
+#define GSGPU_VM_PAGE_PRT		(1 << 4)
+/* MTYPE flags use bit 5 to 8 */
+#define GSGPU_VM_MTYPE_MASK		(0xf << 5)
+/* Default MTYPE. Pre-AI must use this.  Recommended for newer ASICs. */
+#define GSGPU_VM_MTYPE_DEFAULT		(0 << 5)
+/* Use NC MTYPE instead of default MTYPE */
+#define GSGPU_VM_MTYPE_NC		(1 << 5)
+/* Use WC MTYPE instead of default MTYPE */
+#define GSGPU_VM_MTYPE_WC		(2 << 5)
+/* Use CC MTYPE instead of default MTYPE */
+#define GSGPU_VM_MTYPE_CC		(3 << 5)
+/* Use UC MTYPE instead of default MTYPE */
+#define GSGPU_VM_MTYPE_UC		(4 << 5)
+
+struct drm_gsgpu_gem_va {
+	/** GEM object handle */
+	__u32 handle;
+	__u32 _pad;
+	/** GSGPU_VA_OP_* */
+	__u32 operation;
+	/** GSGPU_VM_PAGE_* */
+	__u32 flags;
+	/** va address to assign . Must be correctly aligned.*/
+	__u64 va_address;
+	/** Specify offset inside of BO to assign. Must be correctly aligned.*/
+	__u64 offset_in_bo;
+	/** Specify mapping size. Must be correctly aligned. */
+	__u64 map_size;
+};
+
+#define GSGPU_HW_IP_GFX          0
+#define GSGPU_HW_IP_DMA          2
+#define GSGPU_HW_IP_NUM          9
+
+#define GSGPU_HW_IP_INSTANCE_MAX_COUNT 1
+
+#define GSGPU_CHUNK_ID_IB		0x01
+#define GSGPU_CHUNK_ID_FENCE		0x02
+#define GSGPU_CHUNK_ID_DEPENDENCIES	0x03
+#define GSGPU_CHUNK_ID_SYNCOBJ_IN      0x04
+#define GSGPU_CHUNK_ID_SYNCOBJ_OUT     0x05
+#define GSGPU_CHUNK_ID_BO_HANDLES      0x06
+
+struct drm_gsgpu_cs_chunk {
+	__u32		chunk_id;
+	__u32		length_dw;
+	__u64		chunk_data;
+};
+
+struct drm_gsgpu_cs_in {
+	/** Rendering context id */
+	__u32		ctx_id;
+	/**  Handle of resource list associated with CS */
+	__u32		bo_list_handle;
+	__u32		num_chunks;
+	__u32		_pad;
+	/** this points to __u64 * which point to cs chunks */
+	__u64		chunks;
+};
+
+struct drm_gsgpu_cs_out {
+	__u64 handle;
+};
+
+union drm_gsgpu_cs {
+	struct drm_gsgpu_cs_in in;
+	struct drm_gsgpu_cs_out out;
+};
+
+/* Specify flags to be used for IB */
+
+/* This IB should be submitted to CE */
+#define GSGPU_IB_FLAG_CE	(1<<0)
+
+/* Preamble flag, which means the IB could be dropped if no context switch */
+#define GSGPU_IB_FLAG_PREAMBLE (1<<1)
+
+/* Preempt flag, IB should set Pre_enb bit if PREEMPT flag detected */
+#define GSGPU_IB_FLAG_PREEMPT (1<<2)
+
+/* The IB fence should do the L2 writeback but not invalidate any shader
+ * caches (L2/vL1/sL1/I$). */
+#define GSGPU_IB_FLAG_TC_WB_NOT_INVALIDATE (1 << 3)
+
+struct drm_gsgpu_cs_chunk_ib {
+	__u32 _pad;
+	/** GSGPU_IB_FLAG_* */
+	__u32 flags;
+	/** Virtual address to begin IB execution */
+	__u64 va_start;
+	/** Size of submission */
+	__u32 ib_bytes;
+	/** HW IP to submit to */
+	__u32 ip_type;
+	/** HW IP index of the same type to submit to  */
+	__u32 ip_instance;
+	/** Ring index to submit to */
+	__u32 ring;
+};
+
+struct drm_gsgpu_cs_chunk_dep {
+	__u32 ip_type;
+	__u32 ip_instance;
+	__u32 ring;
+	__u32 ctx_id;
+	__u64 handle;
+};
+
+struct drm_gsgpu_cs_chunk_fence {
+	__u32 handle;
+	__u32 offset;
+};
+
+struct drm_gsgpu_cs_chunk_sem {
+	__u32 handle;
+};
+
+#define GSGPU_FENCE_TO_HANDLE_GET_SYNCOBJ	0
+#define GSGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD	1
+#define GSGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD	2
+
+union drm_gsgpu_fence_to_handle {
+	struct {
+		struct drm_gsgpu_fence fence;
+		__u32 what;
+		__u32 pad;
+	} in;
+	struct {
+		__u32 handle;
+	} out;
+};
+
+struct drm_gsgpu_cs_chunk_data {
+	union {
+		struct drm_gsgpu_cs_chunk_ib		ib_data;
+		struct drm_gsgpu_cs_chunk_fence	fence_data;
+	};
+};
+
+/**
+ *  Query h/w info: Flag that this is integrated (a.h.a. fusion) GPU
+ *
+ */
+#define GSGPU_IDS_FLAGS_FUSION         0x1
+#define GSGPU_IDS_FLAGS_PREEMPTION     0x2
+
+/* indicate if acceleration can be working */
+#define GSGPU_INFO_ACCEL_WORKING		0x00
+/* get the crtc_id from the mode object id? */
+#define GSGPU_INFO_CRTC_FROM_ID		0x01
+/* query hw IP info */
+#define GSGPU_INFO_HW_IP_INFO			0x02
+/* query hw IP instance count for the specified type */
+#define GSGPU_INFO_HW_IP_COUNT			0x03
+/* timestamp for GL_ARB_timer_query */
+#define GSGPU_INFO_TIMESTAMP			0x05
+/* Query the firmware version */
+#define GSGPU_INFO_FW_VERSION			0x0e
+	/* Subquery id: Query VCE firmware version */
+	#define GSGPU_INFO_FW_VCE		0x1
+	/* Subquery id: Query UVD firmware version */
+	#define GSGPU_INFO_FW_UVD		0x2
+	/* Subquery id: Query GMC firmware version */
+	#define GSGPU_INFO_FW_GMC		0x03
+	/* Subquery id: Query GFX ME firmware version */
+	#define GSGPU_INFO_FW_GFX_ME		0x04
+	/* Subquery id: Query GFX PFP firmware version */
+	#define GSGPU_INFO_FW_GFX_PFP		0x05
+	/* Subquery id: Query GFX CE firmware version */
+	#define GSGPU_INFO_FW_GFX_CE		0x06
+	/* Subquery id: Query GFX RLC firmware version */
+	#define GSGPU_INFO_FW_GFX_RLC		0x07
+	/* Subquery id: Query GFX MEC firmware version */
+	#define GSGPU_INFO_FW_GFX_MEC		0x08
+	/* Subquery id: Query SMC firmware version */
+	#define GSGPU_INFO_FW_SMC		0x0a
+	/* Subquery id: Query SDMA firmware version */
+	#define GSGPU_INFO_FW_XDMA		0x0b
+	/* Subquery id: Query PSP SOS firmware version */
+	#define GSGPU_INFO_FW_SOS		0x0c
+	/* Subquery id: Query PSP ASD firmware version */
+	#define GSGPU_INFO_FW_ASD		0x0d
+	/* Subquery id: Query VCN firmware version */
+	#define GSGPU_INFO_FW_VCN		0x0e
+	/* Subquery id: Query GFX RLC SRLC firmware version */
+	#define GSGPU_INFO_FW_GFX_RLC_RESTORE_LIST_CNTL 0x0f
+	/* Subquery id: Query GFX RLC SRLG firmware version */
+	#define GSGPU_INFO_FW_GFX_RLC_RESTORE_LIST_GPM_MEM 0x10
+	/* Subquery id: Query GFX RLC SRLS firmware version */
+	#define GSGPU_INFO_FW_GFX_RLC_RESTORE_LIST_SRM_MEM 0x11
+/* number of bytes moved for TTM migration */
+#define GSGPU_INFO_NUM_BYTES_MOVED		0x0f
+/* the used VRAM size */
+#define GSGPU_INFO_VRAM_USAGE			0x10
+/* the used GTT size */
+#define GSGPU_INFO_GTT_USAGE			0x11
+/* Information about GDS, etc. resource configuration */
+#define GSGPU_INFO_GDS_CONFIG			0x13
+/* Query information about VRAM and GTT domains */
+#define GSGPU_INFO_VRAM_GTT			0x14
+/* Query information about register in MMR address space*/
+#define GSGPU_INFO_READ_MMR_REG		0x15
+/* Query information about device: rev id, family, etc. */
+#define GSGPU_INFO_DEV_INFO			0x16
+/* visible vram usage */
+#define GSGPU_INFO_VIS_VRAM_USAGE		0x17
+/* number of TTM buffer evictions */
+#define GSGPU_INFO_NUM_EVICTIONS		0x18
+/* Query memory about VRAM and GTT domains */
+#define GSGPU_INFO_MEMORY			0x19
+/* Query vce clock table */
+#define GSGPU_INFO_VCE_CLOCK_TABLE		0x1A
+/* Query vbios related information */
+#define GSGPU_INFO_VBIOS			0x1B
+	/* Subquery id: Query vbios size */
+	#define GSGPU_INFO_VBIOS_SIZE		0x1
+	/* Subquery id: Query vbios image */
+	#define GSGPU_INFO_VBIOS_IMAGE		0x2
+/* Query UVD handles */
+#define GSGPU_INFO_NUM_HANDLES			0x1C
+/* Query sensor related information */
+#define GSGPU_INFO_SENSOR			0x1D
+	/* Subquery id: Query GPU shader clock */
+	#define GSGPU_INFO_SENSOR_GFX_SCLK		0x1
+	/* Subquery id: Query GPU memory clock */
+	#define GSGPU_INFO_SENSOR_GFX_MCLK		0x2
+	/* Subquery id: Query GPU temperature */
+	#define GSGPU_INFO_SENSOR_GPU_TEMP		0x3
+	/* Subquery id: Query GPU load */
+	#define GSGPU_INFO_SENSOR_GPU_LOAD		0x4
+	/* Subquery id: Query average GPU power	*/
+	#define GSGPU_INFO_SENSOR_GPU_AVG_POWER	0x5
+	/* Subquery id: Query northbridge voltage */
+	#define GSGPU_INFO_SENSOR_VDDNB		0x6
+	/* Subquery id: Query graphics voltage */
+	#define GSGPU_INFO_SENSOR_VDDGFX		0x7
+	/* Subquery id: Query GPU stable pstate shader clock */
+	#define GSGPU_INFO_SENSOR_STABLE_PSTATE_GFX_SCLK		0x8
+	/* Subquery id: Query GPU stable pstate memory clock */
+	#define GSGPU_INFO_SENSOR_STABLE_PSTATE_GFX_MCLK		0x9
+/* Number of VRAM page faults on CPU access. */
+#define GSGPU_INFO_NUM_VRAM_CPU_PAGE_FAULTS	0x1E
+#define GSGPU_INFO_VRAM_LOST_COUNTER		0x1F
+
+#define GSGPU_INFO_MMR_SE_INDEX_SHIFT	0
+#define GSGPU_INFO_MMR_SE_INDEX_MASK	0xff
+#define GSGPU_INFO_MMR_SH_INDEX_SHIFT	8
+#define GSGPU_INFO_MMR_SH_INDEX_MASK	0xff
+
+struct drm_gsgpu_query_fw {
+	/** GSGPU_INFO_FW_* */
+	__u32 fw_type;
+	/**
+	 * Index of the IP if there are more IPs of
+	 * the same type.
+	 */
+	__u32 ip_instance;
+	/**
+	 * Index of the engine. Whether this is used depends
+	 * on the firmware type. (e.g. MEC, SDMA)
+	 */
+	__u32 index;
+	__u32 _pad;
+};
+
+/* Input structure for the INFO ioctl */
+struct drm_gsgpu_info {
+	/* Where the return value will be stored */
+	__u64 return_pointer;
+	/* The size of the return value. Just like "size" in "snprintf",
+	 * it limits how many bytes the kernel can write. */
+	__u32 return_size;
+	/* The query request id. */
+	__u32 query;
+
+	union {
+		struct {
+			__u32 id;
+			__u32 _pad;
+		} mode_crtc;
+
+		struct {
+			/** GSGPU_HW_IP_* */
+			__u32 type;
+			/**
+			 * Index of the IP if there are more IPs of the same
+			 * type. Ignored by GSGPU_INFO_HW_IP_COUNT.
+			 */
+			__u32 ip_instance;
+		} query_hw_ip;
+
+		struct {
+			__u32 dword_offset;
+			/** number of registers to read */
+			__u32 count;
+			__u32 instance;
+			/** For future use, no flags defined so far */
+			__u32 flags;
+		} read_mmr_reg;
+
+		struct drm_gsgpu_query_fw query_fw;
+
+		struct {
+			__u32 type;
+			__u32 offset;
+		} vbios_info;
+
+		struct {
+			__u32 type;
+		} sensor_info;
+	};
+};
+
+struct drm_gsgpu_info_gds {
+	/** GDS GFX partition size */
+	__u32 gds_gfx_partition_size;
+	/** GDS compute partition size */
+	__u32 compute_partition_size;
+	/** total GDS memory size */
+	__u32 gds_total_size;
+	/** GWS size per GFX partition */
+	__u32 gws_per_gfx_partition;
+	/** GSW size per compute partition */
+	__u32 gws_per_compute_partition;
+	/** OA size per GFX partition */
+	__u32 oa_per_gfx_partition;
+	/** OA size per compute partition */
+	__u32 oa_per_compute_partition;
+	__u32 _pad;
+};
+
+struct drm_gsgpu_info_vram_gtt {
+	__u64 vram_size;
+	__u64 vram_cpu_accessible_size;
+	__u64 gtt_size;
+};
+
+struct drm_gsgpu_heap_info {
+	/** max. physical memory */
+	__u64 total_heap_size;
+
+	/** Theoretical max. available memory in the given heap */
+	__u64 usable_heap_size;
+
+	/**
+	 * Number of bytes allocated in the heap. This includes all processes
+	 * and private allocations in the kernel. It changes when new buffers
+	 * are allocated, freed, and moved. It cannot be larger than
+	 * heap_size.
+	 */
+	__u64 heap_usage;
+
+	/**
+	 * Theoretical possible max. size of buffer which
+	 * could be allocated in the given heap
+	 */
+	__u64 max_allocation;
+};
+
+struct drm_gsgpu_memory_info {
+	struct drm_gsgpu_heap_info vram;
+	struct drm_gsgpu_heap_info cpu_accessible_vram;
+	struct drm_gsgpu_heap_info gtt;
+};
+
+struct drm_gsgpu_info_firmware {
+	__u32 ver;
+	__u32 feature;
+};
+
+#define GSGPU_VRAM_TYPE_UNKNOWN 0
+#define GSGPU_VRAM_TYPE_GDDR1 1
+#define GSGPU_VRAM_TYPE_DDR2  2
+#define GSGPU_VRAM_TYPE_GDDR3 3
+#define GSGPU_VRAM_TYPE_GDDR4 4
+#define GSGPU_VRAM_TYPE_GDDR5 5
+#define GSGPU_VRAM_TYPE_HBM   6
+#define GSGPU_VRAM_TYPE_DDR3  7
+#define GSGPU_VRAM_TYPE_DDR4  8
+
+struct drm_gsgpu_info_device {
+	/** PCI Device ID */
+	__u32 device_id;
+	/** Internal chip revision: A0, A1, etc.) */
+	__u32 chip_rev;
+	__u32 external_rev;
+	/** Revision id in PCI Config space */
+	__u32 pci_rev;
+	__u32 family;
+	__u32 num_shader_engines;
+	__u32 num_shader_arrays_per_engine;
+	/* in KHz */
+	__u32 gpu_counter_freq;
+	__u64 max_engine_clock;
+	__u64 max_memory_clock;
+	/* cu information */
+	__u32 cu_active_number;
+	/* NOTE: cu_ao_mask is INVALID, DON'T use it */
+	__u32 cu_ao_mask;
+	__u32 cu_bitmap[4][4];
+	/** Render backend pipe mask. One render backend is CB+DB. */
+	__u32 enabled_rb_pipes_mask;
+	__u32 num_rb_pipes;
+	__u32 num_hw_gfx_contexts;
+	__u32 _pad;
+	__u64 ids_flags;
+	/** Starting virtual address for UMDs. */
+	__u64 virtual_address_offset;
+	/** The maximum virtual address */
+	__u64 virtual_address_max;
+	/** Required alignment of virtual addresses. */
+	__u32 virtual_address_alignment;
+	/** Page table entry - fragment size */
+	__u32 pte_fragment_size;
+	__u32 gart_page_size;
+	/** constant engine ram size*/
+	__u32 ce_ram_size;
+	/** video memory type info*/
+	__u32 vram_type;
+	/** video memory bit width*/
+	__u32 vram_bit_width;
+	/* vce harvesting instance */
+	__u32 vce_harvest_config;
+	/* gfx double offchip LDS buffers */
+	__u32 gc_double_offchip_lds_buf;
+	/* NGG Primitive Buffer */
+	__u64 prim_buf_gpu_addr;
+	/* NGG Position Buffer */
+	__u64 pos_buf_gpu_addr;
+	/* NGG Control Sideband */
+	__u64 cntl_sb_buf_gpu_addr;
+	/* NGG Parameter Cache */
+	__u64 param_buf_gpu_addr;
+	__u32 prim_buf_size;
+	__u32 pos_buf_size;
+	__u32 cntl_sb_buf_size;
+	__u32 param_buf_size;
+	/* wavefront size*/
+	__u32 wave_front_size;
+	/* shader visible vgprs*/
+	__u32 num_shader_visible_vgprs;
+	/* CU per shader array*/
+	__u32 num_cu_per_sh;
+	/* number of tcc blocks*/
+	__u32 num_tcc_blocks;
+	/* gs vgt table depth*/
+	__u32 gs_vgt_table_depth;
+	/* gs primitive buffer depth*/
+	__u32 gs_prim_buffer_depth;
+	/* max gs wavefront per vgt*/
+	__u32 max_gs_waves_per_vgt;
+	__u32 _pad1;
+	/* always on cu bitmap */
+	__u32 cu_ao_bitmap[4][4];
+	/** Starting high virtual address for UMDs. */
+	__u64 high_va_offset;
+	/** The maximum high virtual address */
+	__u64 high_va_max;
+};
+
+struct drm_gsgpu_info_hw_ip {
+	/** Version of h/w IP */
+	__u32  hw_ip_version_major;
+	__u32  hw_ip_version_minor;
+	/** Capabilities */
+	__u64  capabilities_flags;
+	/** command buffer address start alignment*/
+	__u32  ib_start_alignment;
+	/** command buffer size alignment*/
+	__u32  ib_size_alignment;
+	/** Bitmask of available rings. Bit 0 means ring 0, etc. */
+	__u32  available_rings;
+	__u32  _pad;
+};
+
+struct drm_gsgpu_info_num_handles {
+	/** Max handles as supported by firmware for UVD */
+	__u32  uvd_max_handles;
+	/** Handles currently in use for UVD */
+	__u32  uvd_used_handles;
+};
+
+#define GSGPU_VCE_CLOCK_TABLE_ENTRIES		6
+
+struct drm_gsgpu_info_vce_clock_table_entry {
+	/** System clock */
+	__u32 sclk;
+	/** Memory clock */
+	__u32 mclk;
+	/** VCE clock */
+	__u32 eclk;
+	__u32 pad;
+};
+
+struct drm_gsgpu_info_vce_clock_table {
+	struct drm_gsgpu_info_vce_clock_table_entry entries[GSGPU_VCE_CLOCK_TABLE_ENTRIES];
+	__u32 num_valid_entries;
+	__u32 pad;
+};
+
+#define       GSGPU_HW_SEMA_GET         1
+#define       GSGPU_HW_SEMA_PUT         2
+
+struct drm_gsgpu_hw_sema {
+    /*get or set sema*/
+    __u64 id;
+    /*resv for next feature*/
+    __u32 ctx_id;
+     /*ops*/
+    __u32 ops;
+};
+
+/*
+ * Supported GPU families
+ */
+#define GSGPU_FAMILY_UNKNOWN			0
+#define GSGPU_FAMILY_SI			110 /* Hainan, Oland, Verde, Pitcairn, Tahiti */
+#define GSGPU_FAMILY_CI			120 /* Bonaire, Hawaii */
+#define GSGPU_FAMILY_KV			125 /* Kaveri, Kabini, Mullins */
+#define GSGPU_FAMILY_VI			130 /* Iceland, Tonga */
+#define GSGPU_FAMILY_CZ			135 /* Carrizo, Stoney */
+#define GSGPU_FAMILY_AI			141 /* Vega10 */
+#define GSGPU_FAMILY_RV			142 /* Raven */
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
-- 
2.39.1

