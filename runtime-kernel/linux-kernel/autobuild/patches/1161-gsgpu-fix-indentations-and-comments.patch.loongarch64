From c76b84c0441f9649941f16c5361e8ad5b8cafd80 Mon Sep 17 00:00:00 2001
From: "Dr. Chang Liu, PhD" <cl91tp@gmail.com>
Date: Sat, 21 Oct 2023 17:23:53 +0800
Subject: [PATCH] gsgpu: fix indentations and comments

---
 drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c |   5 +-
 drivers/gpu/drm/gsgpu/gpu/gsgpu_gfx.c    |  77 ++++++++-------
 drivers/gpu/drm/gsgpu/gpu/gsgpu_irq.c    |  14 +--
 drivers/gpu/drm/gsgpu/gpu/gsgpu_mmu.c    |  46 +++++----
 drivers/gpu/drm/gsgpu/gpu/gsgpu_xdma.c   | 118 ++++++++++++-----------
 5 files changed, 140 insertions(+), 120 deletions(-)

diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c
index 2be665086121..4bb10b02abac 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_device.c
@@ -36,7 +36,8 @@ static const char *gsgpu_family_name[] = {
  * Return:
 
  */
-uint64_t gsgpu_cmd_exec(struct gsgpu_device *adev, uint32_t cmd, uint32_t arg0, uint32_t arg1)
+uint64_t gsgpu_cmd_exec(struct gsgpu_device *adev, uint32_t cmd,
+			uint32_t arg0, uint32_t arg1)
 {
 	uint64_t ret;
 
@@ -894,7 +895,7 @@ static int gsgpu_device_ip_init(struct gsgpu_device *adev)
  *
  * Writes a reset magic value to the gart pointer in VRAM.  The driver calls
  * this function before a GPU reset.  If the value is retained after a
- * GPU reset, VRAM has not been lost.  Some GPU resets may destry VRAM contents.
+ * GPU reset, VRAM has not been lost.  Some GPU resets may destroy VRAM contents.
  */
 static void gsgpu_device_fill_reset_magic(struct gsgpu_device *adev)
 {
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_gfx.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gfx.c
index 2f00c399f38a..0e89b87f45e6 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_gfx.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_gfx.c
@@ -190,19 +190,22 @@ static int gfx_sw_init(void *handle)
 	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
 
 	/* EOP Event */
-	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_CP_END_OF_PIPE, &adev->gfx.eop_irq);
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY,
+			     GSGPU_SRCID_CP_END_OF_PIPE, &adev->gfx.eop_irq);
 	if (r)
 		return r;
 
 	/* Privileged reg */
-	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_CP_PRIV_REG_FAULT,
-			      &adev->gfx.priv_reg_irq);
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY,
+			     GSGPU_SRCID_CP_PRIV_REG_FAULT,
+			     &adev->gfx.priv_reg_irq);
 	if (r)
 		return r;
 
 	/* Privileged inst */
-	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_CP_PRIV_INSTR_FAULT,
-			      &adev->gfx.priv_inst_irq);
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY,
+			     GSGPU_SRCID_CP_PRIV_INSTR_FAULT,
+			     &adev->gfx.priv_inst_irq);
 	if (r)
 		return r;
 
@@ -217,7 +220,7 @@ static int gfx_sw_init(void *handle)
 		snprintf(ring->name, sizeof(ring->name), "gfx");
 
 		r = gsgpu_ring_init(adev, ring, 256, &adev->gfx.eop_irq,
-				     GSGPU_CP_IRQ_GFX_EOP);
+				    GSGPU_CP_IRQ_GFX_EOP);
 		if (r)
 			return r;
 	}
@@ -337,7 +340,7 @@ static int gfx_wait_for_idle(void *handle)
 	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
 
 	if (gsgpu_cp_wait_done(adev) == true)
-			return 0;
+		return 0;
 
 	return -ETIMEDOUT;
 }
@@ -418,8 +421,8 @@ static void gfx_ring_set_wptr_gfx(struct gsgpu_ring *ring)
 }
 
 static void gfx_ring_emit_ib_gfx(struct gsgpu_ring *ring,
-				      struct gsgpu_ib *ib,
-				      unsigned vmid, bool ctx_switch)
+				 struct gsgpu_ib *ib,
+				 unsigned vmid, bool ctx_switch)
 {
 	u32 header, control = 0;
 
@@ -434,7 +437,7 @@ static void gfx_ring_emit_ib_gfx(struct gsgpu_ring *ring,
 }
 
 static void gfx_ring_emit_fence_gfx(struct gsgpu_ring *ring, u64 addr,
-					 u64 seq, unsigned flags)
+				    u64 seq, unsigned flags)
 {
 	bool write64bit = flags & GSGPU_FENCE_FLAG_64BIT;
 	bool int_sel = flags & GSGPU_FENCE_FLAG_INT;
@@ -442,7 +445,7 @@ static void gfx_ring_emit_fence_gfx(struct gsgpu_ring *ring, u64 addr,
 
 	/* EVENT_WRITE_EOP - flush caches, send int */
 	gsgpu_ring_write(ring, GSPKT(GSPKT_FENCE, body_size)
-			| (write64bit ? 1 << 9 : 0) | (int_sel ? 1 << 8 : 0));
+			 | (write64bit ? 1 << 9 : 0) | (int_sel ? 1 << 8 : 0));
 	gsgpu_ring_write(ring, lower_32_bits(addr));
 	gsgpu_ring_write(ring, upper_32_bits(addr));
 	gsgpu_ring_write(ring, lower_32_bits(seq));
@@ -457,8 +460,8 @@ static void gfx_ring_emit_pipeline_sync(struct gsgpu_ring *ring)
 	uint64_t addr = ring->fence_drv.gpu_addr;
 
 	gsgpu_ring_write(ring, GSPKT(GSPKT_POLL, 5) |
-				POLL_CONDITION(3) | /* equal */
-				POLL_REG_MEM(1)); /* reg/mem */
+			 POLL_CONDITION(3) | /* equal */
+			 POLL_REG_MEM(1)); /* reg/mem */
 	gsgpu_ring_write(ring, lower_32_bits(addr));
 	gsgpu_ring_write(ring, upper_32_bits(addr));
 	gsgpu_ring_write(ring, seq); /* reference */
@@ -467,14 +470,14 @@ static void gfx_ring_emit_pipeline_sync(struct gsgpu_ring *ring)
 }
 
 static void gfx_ring_emit_vm_flush(struct gsgpu_ring *ring,
-					unsigned vmid, uint64_t pd_addr)
+				   unsigned vmid, uint64_t pd_addr)
 {
 	gsgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);
 }
 
 
 static void gfx_ring_emit_wreg(struct gsgpu_ring *ring, uint32_t reg,
-				  uint32_t val)
+			       uint32_t val)
 {
 	gsgpu_ring_write(ring, GSPKT(GSPKT_WRITE, 2) | WRITE_DST_SEL(0) | WRITE_WAIT);
 	gsgpu_ring_write(ring, reg);
@@ -482,32 +485,32 @@ static void gfx_ring_emit_wreg(struct gsgpu_ring *ring, uint32_t reg,
 }
 
 static void gfx_set_gfx_eop_interrupt_state(struct gsgpu_device *adev,
-						 enum gsgpu_interrupt_state state)
+					    enum gsgpu_interrupt_state state)
 {
 }
 
 static int gfx_set_priv_reg_fault_state(struct gsgpu_device *adev,
-					     struct gsgpu_irq_src *source,
-					     unsigned type,
-					     enum gsgpu_interrupt_state state)
+					struct gsgpu_irq_src *source,
+					unsigned type,
+					enum gsgpu_interrupt_state state)
 {
 
 	return 0;
 }
 
 static int gfx_set_priv_inst_fault_state(struct gsgpu_device *adev,
-					      struct gsgpu_irq_src *source,
-					      unsigned type,
-					      enum gsgpu_interrupt_state state)
+					 struct gsgpu_irq_src *source,
+					 unsigned type,
+					 enum gsgpu_interrupt_state state)
 {
 
 	return 0;
 }
 
 static int gfx_set_eop_interrupt_state(struct gsgpu_device *adev,
-					    struct gsgpu_irq_src *src,
-					    unsigned type,
-					    enum gsgpu_interrupt_state state)
+				       struct gsgpu_irq_src *src,
+				       unsigned type,
+				       enum gsgpu_interrupt_state state)
 {
 	gfx_set_gfx_eop_interrupt_state(adev, state);
 
@@ -515,8 +518,8 @@ static int gfx_set_eop_interrupt_state(struct gsgpu_device *adev,
 }
 
 static int gfx_eop_irq(struct gsgpu_device *adev,
-			    struct gsgpu_irq_src *source,
-			    struct gsgpu_iv_entry *entry)
+		       struct gsgpu_irq_src *source,
+		       struct gsgpu_iv_entry *entry)
 {
 	u8 me_id, pipe_id, queue_id;
 
@@ -537,8 +540,8 @@ static int gfx_eop_irq(struct gsgpu_device *adev,
 }
 
 static int gfx_priv_reg_irq(struct gsgpu_device *adev,
-				 struct gsgpu_irq_src *source,
-				 struct gsgpu_iv_entry *entry)
+			    struct gsgpu_irq_src *source,
+			    struct gsgpu_iv_entry *entry)
 {
 	DRM_ERROR("Illegal register access in command stream\n");
 	schedule_work(&adev->reset_work);
@@ -546,8 +549,8 @@ static int gfx_priv_reg_irq(struct gsgpu_device *adev,
 }
 
 static int gfx_priv_inst_irq(struct gsgpu_device *adev,
-				  struct gsgpu_irq_src *source,
-				  struct gsgpu_iv_entry *entry)
+			     struct gsgpu_irq_src *source,
+			     struct gsgpu_iv_entry *entry)
 {
 	DRM_ERROR("Illegal instruction in command stream\n");
 	schedule_work(&adev->reset_work);
@@ -577,12 +580,12 @@ static const struct gsgpu_ring_funcs gfx_ring_funcs_gfx = {
 	.get_wptr = gfx_ring_get_wptr_gfx,
 	.set_wptr = gfx_ring_set_wptr_gfx,
 	.emit_frame_size = /* maximum 215dw if count 16 IBs in */
-		7 +  /* COND_EXEC */
-		1 +  /* PIPELINE_SYNC */
-		VI_FLUSH_GPU_TLB_NUM_WREG * 5 + 9 + /* VM_FLUSH */
-		5 +  /* FENCE for VM_FLUSH */
-		3 + /* CNTX_CTRL */
-		5 + 5,/* FENCE x2 */
+	7 +  /* COND_EXEC */
+	1 +  /* PIPELINE_SYNC */
+	VI_FLUSH_GPU_TLB_NUM_WREG * 5 + 9 + /* VM_FLUSH */
+	5 +  /* FENCE for VM_FLUSH */
+	3 + /* CNTX_CTRL */
+	5 + 5,/* FENCE x2 */
 	.emit_ib_size =	4, /* gfx_ring_emit_ib_gfx */
 	.emit_ib = gfx_ring_emit_ib_gfx,
 	.emit_fence = gfx_ring_emit_fence_gfx,
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_irq.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_irq.c
index 748fe11216e4..a219a8084d07 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_irq.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_irq.c
@@ -265,8 +265,8 @@ void gsgpu_irq_fini(struct gsgpu_device *adev)
  * 0 on success or error code otherwise
  */
 int gsgpu_irq_add_id(struct gsgpu_device *adev,
-		      unsigned client_id, unsigned src_id,
-		      struct gsgpu_irq_src *source)
+		     unsigned client_id, unsigned src_id,
+		     struct gsgpu_irq_src *source)
 {
 	if (client_id >= GSGPU_IH_CLIENTID_MAX)
 		return -EINVAL;
@@ -313,7 +313,7 @@ int gsgpu_irq_add_id(struct gsgpu_device *adev,
  * Dispatches IRQ to IP blocks.
  */
 void gsgpu_irq_dispatch(struct gsgpu_device *adev,
-			 struct gsgpu_iv_entry *entry)
+			struct gsgpu_iv_entry *entry)
 {
 	unsigned client_id = entry->client_id;
 	unsigned src_id = entry->src_id;
@@ -359,7 +359,7 @@ void gsgpu_irq_dispatch(struct gsgpu_device *adev,
  * Updates interrupt state for the specific source (all ASICs).
  */
 int gsgpu_irq_update(struct gsgpu_device *adev,
-			     struct gsgpu_irq_src *src, unsigned type)
+		     struct gsgpu_irq_src *src, unsigned type)
 {
 	unsigned long irqflags;
 	enum gsgpu_interrupt_state state;
@@ -419,7 +419,7 @@ void gsgpu_irq_gpu_reset_resume_helper(struct gsgpu_device *adev)
  * 0 on success or error code otherwise
  */
 int gsgpu_irq_get(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
-		   unsigned type)
+		  unsigned type)
 {
 	if (type >= src->num_types)
 		return -EINVAL;
@@ -446,7 +446,7 @@ int gsgpu_irq_get(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
  * 0 on success or error code otherwise
  */
 int gsgpu_irq_put(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
-		   unsigned type)
+		  unsigned type)
 {
 	if (type >= src->num_types)
 		return -EINVAL;
@@ -474,7 +474,7 @@ int gsgpu_irq_put(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
  * invalid parameters
  */
 bool gsgpu_irq_enabled(struct gsgpu_device *adev, struct gsgpu_irq_src *src,
-			unsigned type)
+		       unsigned type)
 {
 	if (type >= src->num_types)
 		return false;
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_mmu.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_mmu.c
index 6c94d269c560..67e1564e1813 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_mmu.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_mmu.c
@@ -16,7 +16,7 @@ static void mmu_set_irq_funcs(struct gsgpu_device *adev);
  * @mc  pionter of gsgpu_gmc
  */
 static void mmu_vram_gtt_location(struct gsgpu_device *adev,
-				       struct gsgpu_gmc *mc)
+				  struct gsgpu_gmc *mc)
 {
 	u64 base = 0;
 
@@ -63,7 +63,8 @@ static int mmu_mc_init(struct gsgpu_device *adev)
 		adev->gmc.aper_size = pci_resource_len(adev->pdev, 2);
 	}
 
-	DRM_INFO("aper_base %#llx SIZE %#llx bytes \n", adev->gmc.aper_base, adev->gmc.aper_size);
+	DRM_INFO("aper_base %#llx SIZE %#llx bytes \n",
+		 adev->gmc.aper_base, adev->gmc.aper_size);
 	/* In case the PCI BAR is larger than the actual amount of vram */
 	adev->gmc.visible_vram_size = adev->gmc.aper_size;
 	if (adev->gmc.visible_vram_size > adev->gmc.real_vram_size)
@@ -103,11 +104,12 @@ static int mmu_mc_init(struct gsgpu_device *adev)
 static void mmu_flush_gpu_tlb(struct gsgpu_device *adev,
 			      uint32_t vmid)
 {
-	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_FLUSH), GSGPU_MMU_FLUSH_PKT(vmid, GSGPU_MMU_FLUSH_VMID), 0);
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_FLUSH),
+		       GSGPU_MMU_FLUSH_PKT(vmid, GSGPU_MMU_FLUSH_VMID), 0);
 }
 
 static uint64_t mmu_emit_flush_gpu_tlb(struct gsgpu_ring *ring,
-					    unsigned vmid, uint64_t pd_addr)
+				       unsigned vmid, uint64_t pd_addr)
 {
 	uint32_t reg;
 	reg = GSGPU_MMU_VMID_OF_PGD(vmid);
@@ -115,7 +117,8 @@ static uint64_t mmu_emit_flush_gpu_tlb(struct gsgpu_ring *ring,
 	gsgpu_ring_emit_wreg(ring, reg, lower_32_bits(pd_addr));
 	gsgpu_ring_emit_wreg(ring, reg + 4, upper_32_bits(pd_addr));
 
-	gsgpu_ring_emit_wreg(ring, GSGPU_MMU_FLUSH_CTRL_OFFSET, GSGPU_MMU_FLUSH_PKT(vmid, GSGPU_MMU_FLUSH_VMID));
+	gsgpu_ring_emit_wreg(ring, GSGPU_MMU_FLUSH_CTRL_OFFSET,
+			     GSGPU_MMU_FLUSH_PKT(vmid, GSGPU_MMU_FLUSH_VMID));
 
 	return pd_addr;
 }
@@ -132,8 +135,8 @@ static uint64_t mmu_emit_flush_gpu_tlb(struct gsgpu_ring *ring,
  * Update the page tables using the CPU.
  */
 static int mmu_set_pte_pde(struct gsgpu_device *adev, void *cpu_pt_addr,
-				uint32_t gpu_page_idx, uint64_t addr,
-				uint64_t flags)
+			   uint32_t gpu_page_idx, uint64_t addr,
+			   uint64_t flags)
 {
 	void __iomem *ptr = (void *)cpu_pt_addr;
 	uint64_t value;
@@ -203,14 +206,16 @@ static int mmu_gart_enable(struct gsgpu_device *adev)
 		return r;
 
 	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_SET_EXC), 3, 0);
-	gsgpu_cmd_exec(adev, GSCMDi(GSCMD_MMU, MMU_SET_PGD, 0), \
-			lower_32_bits(adev->gart.table_addr), upper_32_bits(adev->gart.table_addr));
+	gsgpu_cmd_exec(adev, GSCMDi(GSCMD_MMU, MMU_SET_PGD, 0),
+		       lower_32_bits(adev->gart.table_addr),
+		       upper_32_bits(adev->gart.table_addr));
 
-	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_SET_SAFE), \
-			lower_32_bits(adev->dummy_page_addr), upper_32_bits(adev->dummy_page_addr));
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_SET_SAFE),
+		       lower_32_bits(adev->dummy_page_addr),
+		       upper_32_bits(adev->dummy_page_addr));
 
 	gsgpu_cmd_exec(adev, GSCMDi(GSCMD_MMU, MMU_SET_DIR, 0),
-			GSGPU_MMU_DIR_CTRL_256M_1LVL, 0);
+		       GSGPU_MMU_DIR_CTRL_256M_1LVL, 0);
 
 	for (i = 1; i < GSGPU_NUM_OF_VMIDS; i++) {
 		gsgpu_cmd_exec(adev, GSCMDi(GSCMD_MMU, MMU_SET_DIR, i),
@@ -219,7 +224,8 @@ static int mmu_gart_enable(struct gsgpu_device *adev)
 
 	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_ENABLE), MMU_ENABLE, ~1);
 
-	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_FLUSH), GSGPU_MMU_FLUSH_PKT(0, GSGPU_MMU_FLUSH_ALL), 0);
+	gsgpu_cmd_exec(adev, GSCMD(GSCMD_MMU, MMU_FLUSH),
+		       GSGPU_MMU_FLUSH_PKT(0, GSGPU_MMU_FLUSH_ALL), 0);
 
 	DRM_INFO("PCIE GART of %uM enabled (table at 0x%016llX).\n",
 		 (unsigned)(adev->gmc.gart_size >> 20),
@@ -285,11 +291,15 @@ static inline int mmu_irq_set(struct gsgpu_device *adev)
 {
 	int r;
 
-	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_GFX_PAGE_INV_FAULT, &adev->gmc.vm_fault);
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY,
+			     GSGPU_SRCID_GFX_PAGE_INV_FAULT,
+			     &adev->gmc.vm_fault);
 	if (r)
 		return r;
 
-	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_GFX_MEM_PROT_FAULT, &adev->gmc.vm_fault);
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY,
+			     GSGPU_SRCID_GFX_MEM_PROT_FAULT,
+			     &adev->gmc.vm_fault);
 	if (r)
 		return r;
 
@@ -462,9 +472,9 @@ static int mmu_post_soft_reset(void *handle)
 }
 
 static int mmu_vm_fault_interrupt_state(struct gsgpu_device *adev,
-					     struct gsgpu_irq_src *src,
-					     unsigned type,
-					     enum gsgpu_interrupt_state state)
+					struct gsgpu_irq_src *src,
+					unsigned type,
+					enum gsgpu_interrupt_state state)
 {
 	switch (state) {
 	case GSGPU_IRQ_STATE_DISABLE:
diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_xdma.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_xdma.c
index e8e784228f19..765ca97c0919 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_xdma.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_xdma.c
@@ -87,8 +87,8 @@ static void xdma_ring_insert_nop(struct gsgpu_ring *ring, u32 count)
  * Schedule an IB in the DMA ring  .
  */
 static void xdma_ring_emit_ib(struct gsgpu_ring *ring,
-				   struct gsgpu_ib *ib,
-				   unsigned vmid, bool ctx_switch)
+			      struct gsgpu_ib *ib,
+			      unsigned vmid, bool ctx_switch)
 {
 	gsgpu_ring_write(ring, GSPKT(GSPKT_INDIRECT, 3));
 	gsgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));
@@ -107,13 +107,13 @@ static void xdma_ring_emit_ib(struct gsgpu_ring *ring,
  * an interrupt if needed  .
  */
 static void xdma_ring_emit_fence(struct gsgpu_ring *ring, u64 addr, u64 seq,
-				      unsigned flags)
+				 unsigned flags)
 {
 	bool write64bit = flags & GSGPU_FENCE_FLAG_64BIT;
 	bool int_sel = flags & GSGPU_FENCE_FLAG_INT;
 
 	gsgpu_ring_write(ring, GSPKT(GSPKT_FENCE, write64bit ? 4 : 3)
-			| (write64bit ? 1 << 9 : 0) | (int_sel ? 1 << 8 : 0));
+			 | (write64bit ? 1 << 9 : 0) | (int_sel ? 1 << 8 : 0));
 	gsgpu_ring_write(ring, lower_32_bits(addr));
 	gsgpu_ring_write(ring, upper_32_bits(addr));
 	gsgpu_ring_write(ring, lower_32_bits(seq));
@@ -266,11 +266,12 @@ static int xdma_ring_test_ring(struct gsgpu_ring *ring)
 	}
 
 	if (i < adev->usec_timeout) {
-		DRM_INFO("ring %s test on %d succeeded in %d usecs\n", ring->name, ring->idx, i);
+		DRM_INFO("ring %s test on %d succeeded in %d usecs\n",
+			 ring->name, ring->idx, i);
 		r = 0;
 	} else {
-		DRM_ERROR("gsgpu: ring %s %d test failed (0x%08X)\n", ring->name,
-			  ring->idx, tmp);
+		DRM_ERROR("gsgpu: ring %s on %d test failed (0x%08X)\n",
+			  ring->name, ring->idx, tmp);
 		r = -EINVAL;
 	}
 	gsgpu_device_wb_free(adev, index);
@@ -386,8 +387,8 @@ void xdma_ring_test_xdma_loop(struct gsgpu_ring *ring, long timeout)
  * Update PTEs by copying them from the GART using xDMA.
  */
 static void xdma_vm_copy_pte(struct gsgpu_ib *ib,
-				u64 pe, u64 src,
-				unsigned count)
+			     u64 pe, u64 src,
+			     unsigned count)
 {
 	struct gsgpu_bo *bo = ib->sa_bo->manager->bo;
 	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
@@ -396,11 +397,11 @@ static void xdma_vm_copy_pte(struct gsgpu_ib *ib,
 	height = 1;
 	width = count;
 	dst_umap = (pe >= adev->gmc.vram_start && pe < adev->gmc.vram_end) ?
-			   GSGPU_XDMA_FLAG_UMAP :
-			   0;
+		GSGPU_XDMA_FLAG_UMAP :
+		0;
 	src_umap = (src >= adev->gmc.vram_start && src < adev->gmc.vram_end) ?
-			   GSGPU_XDMA_FLAG_UMAP :
-			   0;
+		GSGPU_XDMA_FLAG_UMAP :
+		0;
 
 	/* hardware limit 2^16 pixels per line */
 	while (width >= 0x10000) {
@@ -436,8 +437,8 @@ static void xdma_vm_copy_pte(struct gsgpu_ib *ib,
  * Update the page tables using xDMA.
  */
 static void xdma_vm_set_pte_pde(struct gsgpu_ib *ib, u64 pe,
-				     u64 addr, unsigned count,
-				     u32 incr, u64 flags)
+				u64 addr, unsigned count,
+				u32 incr, u64 flags)
 {
 	/* for physically contiguous pages (vram) */
 	struct gsgpu_bo *bo = ib->sa_bo->manager->bo;
@@ -446,11 +447,11 @@ static void xdma_vm_set_pte_pde(struct gsgpu_ib *ib, u64 pe,
 	uint32_t dst_umap, src_umap;
 
 	dst_umap = (pe >= adev->gmc.vram_start && pe < adev->gmc.vram_end) ?
-			   GSGPU_XDMA_FLAG_UMAP :
-			   0;
+		GSGPU_XDMA_FLAG_UMAP :
+		0;
 	src_umap = (addr >= adev->gmc.vram_start && addr < adev->gmc.vram_end) ?
-			   GSGPU_XDMA_FLAG_UMAP :
-			   0;
+		GSGPU_XDMA_FLAG_UMAP :
+		0;
 	height = 1;
 
 	/*RGBA16 == 8 bytes per pixles*/
@@ -467,7 +468,7 @@ static void xdma_vm_set_pte_pde(struct gsgpu_ib *ib, u64 pe,
 	 * RGBA16 : 1 << 8
 	 * 16K pf : 3 << 28
 	 * */
-	ib->ptr[ib->length_dw++] = GSPKT(GSPKT_XDMA_COPY, 8) | (0x7 << 24) | (1 << 8) | (3 << 28);
+	ib->ptr[ib->length_dw++] = GSPKT(GSPKT_XDMA_COPY, 8) | (7 << 24) | (1 << 8) | (3 << 28);
 	ib->ptr[ib->length_dw++] =  (height << 16) | width;
 	ib->ptr[ib->length_dw++] = lower_32_bits(addr | flags); /* value */
 	ib->ptr[ib->length_dw++] = upper_32_bits(addr | flags) | src_umap;;
@@ -508,8 +509,8 @@ static void xdma_ring_emit_pipeline_sync(struct gsgpu_ring *ring)
 
 	/* wait for idle */
 	gsgpu_ring_write(ring, GSPKT(GSPKT_POLL, 5) |
-				POLL_CONDITION(3) | /* equal */
-				POLL_REG_MEM(1)); /* reg/mem */
+			 POLL_CONDITION(3) | /* equal */
+			 POLL_REG_MEM(1)); /* reg/mem */
 	gsgpu_ring_write(ring, lower_32_bits(addr));
 	gsgpu_ring_write(ring, upper_32_bits(addr));
 	gsgpu_ring_write(ring, seq); /* reference */
@@ -527,13 +528,13 @@ static void xdma_ring_emit_pipeline_sync(struct gsgpu_ring *ring)
  * using xDMA.
  */
 static void xdma_ring_emit_vm_flush(struct gsgpu_ring *ring,
-					 unsigned vmid, u64 pd_addr)
+				    unsigned vmid, u64 pd_addr)
 {
 	gsgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);
 }
 
 static void xdma_ring_emit_wreg(struct gsgpu_ring *ring,
-				     u32 reg, u32 val)
+				u32 reg, u32 val)
 {
 	gsgpu_ring_write(ring, GSPKT(GSPKT_WRITE, 2) | WRITE_DST_SEL(0) | WRITE_WAIT);
 	gsgpu_ring_write(ring, reg);
@@ -560,7 +561,8 @@ static int xdma_set_pte_pde_test(struct gsgpu_ring *ring, long timeout)
 	align = GSGPU_GPU_PAGE_SIZE;
 	domain = GSGPU_GEM_DOMAIN_VRAM;
 
-	r = gsgpu_bo_create_kernel(ldev, size, align, domain, &bo, &gpu_addr, (void **)&cpu_ptr);
+	r = gsgpu_bo_create_kernel(ldev, size, align, domain, &bo,
+				   &gpu_addr, (void **)&cpu_ptr);
 	if (r) {
 		DRM_ERROR("xdma_set_pte_pde_test : gsgpu_bo_create_kernel error\r\n");
 		return r;
@@ -639,7 +641,8 @@ static int xdma_copy_pte_test(struct gsgpu_ring *ring, long timeout)
 	align = GSGPU_GPU_PAGE_SIZE;
 
 	domain = GSGPU_GEM_DOMAIN_GTT;
-	r = gsgpu_bo_create_kernel(ldev, size, align, domain, &bo_gtt, &gpu_addr_gtt, (void **)&cpu_ptr_gtt);
+	r = gsgpu_bo_create_kernel(ldev, size, align, domain, &bo_gtt,
+				   &gpu_addr_gtt, (void **)&cpu_ptr_gtt);
 	if (r) {
 		DRM_ERROR("xdma_copy_pte_test : gsgpu_bo_create_kernel gtt error\r\n");
 		goto bo_free;
@@ -652,7 +655,8 @@ static int xdma_copy_pte_test(struct gsgpu_ring *ring, long timeout)
 	}
 
 	domain = GSGPU_GEM_DOMAIN_VRAM;
-	r = gsgpu_bo_create_kernel(ldev, size, align, domain, &bo, &gpu_addr, (void **)&cpu_ptr);
+	r = gsgpu_bo_create_kernel(ldev, size, align, domain, &bo,
+				   &gpu_addr, (void **)&cpu_ptr);
 	if (r) {
 		DRM_ERROR("xdma_copy_pte_test : gsgpu_bo_create_kernel vram error\r\n");
 		return r;
@@ -733,20 +737,22 @@ static int xdma_sw_init(void *handle)
 	struct gsgpu_device *adev = (struct gsgpu_device *)handle;
 
 	/* XDMA trap event */
-	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_XDMA_TRAP,
-			      &adev->xdma.trap_irq);
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY,
+			     GSGPU_SRCID_XDMA_TRAP,
+			     &adev->xdma.trap_irq);
 	if (r)
 		return r;
 
 	/* XDMA Privileged inst */
 	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, 241,
-			      &adev->xdma.illegal_inst_irq);
+			     &adev->xdma.illegal_inst_irq);
 	if (r)
 		return r;
 
 	/* XDMA Privileged inst */
-	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY, GSGPU_SRCID_XDMA_SRBM_WRITE,
-			      &adev->xdma.illegal_inst_irq);
+	r = gsgpu_irq_add_id(adev, GSGPU_IH_CLIENTID_LEGACY,
+			     GSGPU_SRCID_XDMA_SRBM_WRITE,
+			     &adev->xdma.illegal_inst_irq);
 	if (r)
 		return r;
 
@@ -756,8 +762,8 @@ static int xdma_sw_init(void *handle)
 
 		snprintf(ring->name, sizeof(ring->name), "xdma%d", i);
 		r = gsgpu_ring_init(adev, ring, 256,
-				     &adev->xdma.trap_irq,
-				     (i == 0) ? GSGPU_XDMA_IRQ_TRAP0 : GSGPU_XDMA_IRQ_TRAP1);
+				    &adev->xdma.trap_irq,
+				    (i == 0) ? GSGPU_XDMA_IRQ_TRAP0 : GSGPU_XDMA_IRQ_TRAP1);
 		if (r)
 			return r;
 	}
@@ -838,16 +844,16 @@ static int xdma_wait_for_idle(void *handle)
 }
 
 static int xdma_set_trap_irq_state(struct gsgpu_device *adev,
-					struct gsgpu_irq_src *source,
-					unsigned type,
-					enum gsgpu_interrupt_state state)
+				   struct gsgpu_irq_src *source,
+				   unsigned type,
+				   enum gsgpu_interrupt_state state)
 {
 	return 0;
 }
 
 static int xdma_process_trap_irq(struct gsgpu_device *adev,
-				      struct gsgpu_irq_src *source,
-				      struct gsgpu_iv_entry *entry)
+				 struct gsgpu_irq_src *source,
+				 struct gsgpu_iv_entry *entry)
 {
 	u8 instance_id, queue_id;
 
@@ -886,8 +892,8 @@ static int xdma_process_trap_irq(struct gsgpu_device *adev,
 }
 
 static int xdma_process_illegal_inst_irq(struct gsgpu_device *adev,
-					      struct gsgpu_irq_src *source,
-					      struct gsgpu_iv_entry *entry)
+					 struct gsgpu_irq_src *source,
+					 struct gsgpu_iv_entry *entry)
 {
 	DRM_ERROR("Illegal instruction in XDMA command stream\n");
 	schedule_work(&adev->reset_work);
@@ -917,10 +923,10 @@ static const struct gsgpu_ring_funcs xdma_ring_funcs = {
 	.get_wptr = xdma_ring_get_wptr,
 	.set_wptr = xdma_ring_set_wptr,
 	.emit_frame_size =
-		3 + /* hdp invalidate */
-		6 + /* xdma_ring_emit_pipeline_sync */
-		VI_FLUSH_GPU_TLB_NUM_WREG * 3 + 6 + /* xdma_ring_emit_vm_flush */
-		5 + 5 + 5, /* xdma_ring_emit_fence x3 for user fence, vm fence */
+	3 + /* hdp invalidate */
+	6 + /* xdma_ring_emit_pipeline_sync */
+	VI_FLUSH_GPU_TLB_NUM_WREG * 3 + 6 + /* xdma_ring_emit_vm_flush */
+	5 + 5 + 5, /* xdma_ring_emit_fence x3 for user fence, vm fence */
 	.emit_ib_size = 4, /* xdma_ring_emit_ib */
 	.emit_ib = xdma_ring_emit_ib,
 	.emit_fence = xdma_ring_emit_fence,
@@ -973,9 +979,9 @@ static void xdma_set_irq_funcs(struct gsgpu_device *adev)
  * registered as the asic copy callback.
  */
 static void xdma_emit_copy_buffer(struct gsgpu_ib *ib,
-					u64 src_offset,
-					u64 dst_offset,
-					u32 byte_count)
+				  u64 src_offset,
+				  u64 dst_offset,
+				  u32 byte_count)
 {
 	struct gsgpu_bo *bo = ib->sa_bo->manager->bo;
 	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
@@ -985,12 +991,12 @@ static void xdma_emit_copy_buffer(struct gsgpu_ib *ib,
 
 	dst_umap = (dst_offset >= adev->gmc.vram_start &&
 		    dst_offset < adev->gmc.vram_end) ?
-			   GSGPU_XDMA_FLAG_UMAP :
-			   0;
+		GSGPU_XDMA_FLAG_UMAP :
+		0;
 	src_umap = (src_offset >= adev->gmc.vram_start &&
 		    src_offset < adev->gmc.vram_end) ?
-			   GSGPU_XDMA_FLAG_UMAP :
-			   0;
+		GSGPU_XDMA_FLAG_UMAP :
+		0;
 	height = 1;
 	width = byte_count / cpp;
 
@@ -1026,9 +1032,9 @@ static void xdma_emit_copy_buffer(struct gsgpu_ib *ib,
  * Fill GPU buffers using the DMA engine  .
  */
 static void xdma_emit_fill_buffer(struct gsgpu_ib *ib,
-					u32 src_data,
-					u64 dst_offset,
-					u32 byte_count)
+				  u32 src_data,
+				  u64 dst_offset,
+				  u32 byte_count)
 {
 	struct gsgpu_bo *bo = ib->sa_bo->manager->bo;
 	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
@@ -1038,7 +1044,7 @@ static void xdma_emit_fill_buffer(struct gsgpu_ib *ib,
 	height = 1;
 	dst_umap = (dst_offset >= adev->gmc.vram_start &&
 		    dst_offset < adev->gmc.vram_end) ?
-			   GSGPU_XDMA_FLAG_UMAP : 0;
+		GSGPU_XDMA_FLAG_UMAP : 0;
 
 	/*RGBA8 == 4 bytes per pixles*/
 	width = byte_count / 4;
-- 
2.43.0

