From 3132e6731182de33cbf32347b118430daaed5e4a Mon Sep 17 00:00:00 2001
From: "Dr. Chang Liu, PhD" <cl91tp@gmail.com>
Date: Wed, 27 Sep 2023 10:54:16 +0800
Subject: [PATCH] gsgpu: Integrate AMDGPU mm_node changes

---
 drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c     | 695 +++++++++-------------
 drivers/gpu/drm/gsgpu/include/gsgpu.h     |   1 -
 drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h |   4 +-
 3 files changed, 270 insertions(+), 430 deletions(-)

diff --git a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c
index 866876e26026..cd7b5edd0a37 100644
--- a/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c
+++ b/drivers/gpu/drm/gsgpu/gpu/gsgpu_ttm.c
@@ -20,12 +20,6 @@
 #define mmMM_INDEX_HI      0x6
 #define mmMM_DATA          0x1
 
-static int gsgpu_map_buffer(struct ttm_buffer_object *bo,
-			     struct ttm_resource *mem, unsigned num_pages,
-			     uint64_t offset, unsigned window,
-			     struct gsgpu_ring *ring,
-			     uint64_t *addr);
-
 static int gsgpu_ttm_debugfs_init(struct gsgpu_device *adev);
 static void gsgpu_ttm_debugfs_fini(struct gsgpu_device *adev);
 
@@ -46,7 +40,7 @@ static int gsgpu_invalidate_caches(struct ttm_device *bdev, uint32_t flags)
  * initialized.
  */
 static int gsgpu_init_mem_type(struct ttm_device *bdev, uint32_t type,
-				struct ttm_resource_manager *man)
+			       struct ttm_resource_manager *man)
 {
 	struct gsgpu_device *adev;
 
@@ -56,14 +50,12 @@ static int gsgpu_init_mem_type(struct ttm_device *bdev, uint32_t type,
 	case TTM_PL_SYSTEM:
 		/* System memory */
 		man->flags = TTM_MEMTYPE_FLAG_MAPPABLE;
-		man->available_caching = TTM_PL_MASK_CACHING;
 		man->default_caching = TTM_PL_FLAG_CACHED;
 		break;
 	case TTM_PL_TT:
 		/* GTT memory  */
 		man->func = &gsgpu_gtt_mgr_func;
 		man->gpu_offset = adev->gmc.gart_start;
-		man->available_caching = TTM_PL_MASK_CACHING;
 		man->default_caching = TTM_PL_FLAG_CACHED;
 		man->flags = TTM_MEMTYPE_FLAG_MAPPABLE | TTM_MEMTYPE_FLAG_CMA;
 		break;
@@ -72,8 +64,7 @@ static int gsgpu_init_mem_type(struct ttm_device *bdev, uint32_t type,
 		man->func = &gsgpu_vram_mgr_func;
 		man->gpu_offset = adev->gmc.vram_start;
 		man->flags = TTM_MEMTYPE_FLAG_FIXED |
-			     TTM_MEMTYPE_FLAG_MAPPABLE;
-		man->available_caching = TTM_PL_FLAG_UNCACHED | TTM_PL_FLAG_WC;
+			TTM_MEMTYPE_FLAG_MAPPABLE;
 		man->default_caching = TTM_PL_FLAG_WC;
 		break;
 	default:
@@ -92,7 +83,7 @@ static int gsgpu_init_mem_type(struct ttm_device *bdev, uint32_t type,
  * Fill in placement data when ttm_bo_evict() is called
  */
 static void gsgpu_evict_flags(struct ttm_buffer_object *bo,
-				struct ttm_placement *placement)
+			      struct ttm_placement *placement)
 {
 	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->bdev);
 	struct gsgpu_bo *abo;
@@ -134,7 +125,7 @@ static void gsgpu_evict_flags(struct ttm_buffer_object *bo,
 			 * BOs to be evicted from VRAM
 			 */
 			gsgpu_bo_placement_from_domain(abo, GSGPU_GEM_DOMAIN_VRAM |
-							 GSGPU_GEM_DOMAIN_GTT);
+						       GSGPU_GEM_DOMAIN_GTT);
 			abo->placements[0].fpfn = adev->gmc.visible_vram_size >> PAGE_SHIFT;
 			abo->placements[0].lpfn = 0;
 			abo->placement.busy_placement = &abo->placements[1];
@@ -152,44 +143,107 @@ static void gsgpu_evict_flags(struct ttm_buffer_object *bo,
 }
 
 /**
- * gsgpu_mm_node_addr - Compute the GPU relative offset of a GTT buffer.
- *
- * @bo: The bo to assign the memory to.
- * @mm_node: Memory manager node for drm allocator.
- * @mem: The region where the bo resides.
+ * gsgpu_ttm_map_buffer - Map memory into the GART windows
+ * @bo: buffer object to map
+ * @mem: memory object to map
+ * @mm_cur: range to map
+ * @num_pages: number of pages to map
+ * @window: which GART window to use
+ * @ring: DMA ring to use for the copy
+ * @tmz: if we should setup a TMZ enabled mapping
+ * @addr: resulting address inside the MC address space
  *
+ * Setup one of the GART windows to access a specific piece of memory or return
+ * the physical address for local memory.
  */
-static uint64_t gsgpu_mm_node_addr(struct ttm_buffer_object *bo,
-				   struct drm_mm_node *mm_node,
-				   struct ttm_resource *mem)
+static int gsgpu_ttm_map_buffer(struct ttm_buffer_object *bo,
+				struct ttm_mem_reg *mem,
+				struct gsgpu_res_cursor *mm_cur,
+				unsigned num_pages, unsigned window,
+				struct gsgpu_ring *ring, bool tmz,
+				uint64_t *addr)
 {
-	uint64_t addr = 0;
+	struct gsgpu_device *adev = ring->adev;
+	struct gsgpu_job *job;
+	unsigned num_dw, num_bytes;
+	struct dma_fence *fence;
+	uint64_t src_addr, dst_addr;
+	void *cpu_addr;
+        uint64_t flags;
+	unsigned int i;
+	int r;
+
+	BUG_ON(adev->mman.buffer_funcs->copy_max_bytes <
+	       GSGPU_GTT_MAX_TRANSFER_SIZE * 8);
 
-	if (mem->mem_type != TTM_PL_TT || gsgpu_gtt_mgr_has_gart_addr(mem)) {
-		addr = mm_node->start << PAGE_SHIFT;
-		addr += bo->bdev->man_drv[mem->mem_type]->gpu_offset;
+	/* Map only what can't be accessed directly */
+	if (!tmz && mem->start != GSGPU_BO_INVALID_OFFSET) {
+		*addr = gsgpu_ttm_domain_start(adev, mem->mem_type) + mm_cur->start;
+		return 0;
 	}
-	return addr;
-}
 
-/**
- * gsgpu_find_mm_node - Helper function finds the drm_mm_node corresponding to
- * @offset. It also modifies the offset to be within the drm_mm_node returned
- *
- * @mem: The region where the bo resides.
- * @offset: The offset that drm_mm_node is used for finding.
- *
- */
-static struct drm_mm_node *gsgpu_find_mm_node(struct ttm_resource *mem,
-					       unsigned long *offset)
-{
-	struct drm_mm_node *mm_node = mem->mm_node;
+	*addr = adev->gmc.gart_start;
+	*addr += (u64)window * GSGPU_GTT_MAX_TRANSFER_SIZE *
+		GSGPU_GPU_PAGE_SIZE;
+	*addr += mm_cur->start & ~PAGE_MASK;
+
+	num_dw = ALIGN(adev->mman.buffer_funcs->copy_num_dw, 8);
+	num_bytes = num_pages * 8;
+
+	r = gsgpu_job_alloc_with_ib(adev, num_dw * 4 + num_bytes,
+				    GSGPU_IB_POOL_NORMAL, &job);
+	if (r)
+		return r;
+
+	src_addr = num_dw * 4;
+	src_addr += job->ibs[0].gpu_addr;
+
+	dst_addr = gsgpu_bo_gpu_offset(adev->gart.bo);
+	dst_addr += window * GSGPU_GTT_MAX_TRANSFER_SIZE * 8;
+	gsgpu_emit_copy_buffer(adev, &job->ibs[0], src_addr,
+			       dst_addr, num_bytes, false);
+
+	gsgpu_ring_pad_ib(ring, &job->ibs[0]);
+	WARN_ON(job->ibs[0].length_dw > num_dw);
+
+	flags = gsgpu_ttm_tt_pte_flags(adev, bo->ttm, mem);
+	if (tmz)
+		flags |= GSGPU_PTE_TMZ;
+
+	cpu_addr = &job->ibs[0].ptr[num_dw];
+
+	if (mem->mem_type == TTM_PL_TT) {
+		dma_addr_t *dma_addr;
+		dma_addr = &bo->ttm->dma_address[mm_cur->start >> PAGE_SHIFT];
+		r = gsgpu_gart_map(adev, 0, num_pages, dma_addr, flags, cpu_addr);
+		if (r)
+			goto error_free;
+	} else {
+		dma_addr_t dma_address = mm_cur->start;
+		dma_address += adev->vm_manager.vram_base_offset;
+
+		for (i = 0; i < num_pages; ++i) {
+			r = gsgpu_gart_map(adev, i << PAGE_SHIFT, 1,
+					   &dma_address, flags, cpu_addr);
+			if (r)
+				goto error_free;
 
-	while (*offset >= (mm_node->size << PAGE_SHIFT)) {
-		*offset -= (mm_node->size << PAGE_SHIFT);
-		++mm_node;
+			dma_address += PAGE_SIZE;
+		}
 	}
-	return mm_node;
+
+	r = gsgpu_job_submit(job, &adev->mman.entity,
+			     GSGPU_FENCE_OWNER_UNDEFINED, &fence);
+	if (r)
+		goto error_free;
+
+	dma_fence_put(fence);
+
+	return r;
+
+error_free:
+	gsgpu_job_free(job);
+	return r;
 }
 
 /**
@@ -202,112 +256,67 @@ static struct drm_mm_node *gsgpu_find_mm_node(struct ttm_resource *mem,
  * @f: Returns the last fence if multiple jobs are submitted.
  */
 int gsgpu_ttm_copy_mem_to_mem(struct gsgpu_device *adev,
-			       struct gsgpu_copy_mem *src,
-			       struct gsgpu_copy_mem *dst,
-			       uint64_t size,
-			       struct dma_resv *resv,
-			       struct dma_fence **f)
+			      const struct gsgpu_copy_mem *src,
+			      const struct gsgpu_copy_mem *dst,
+			      uint64_t size,
+			      struct dma_resv *resv,
+			      struct dma_fence **f)
 {
+	const uint32_t GTT_MAX_BYTES = (GSGPU_GTT_MAX_TRANSFER_SIZE *
+					GSGPU_GPU_PAGE_SIZE);
+
 	struct gsgpu_ring *ring = adev->mman.buffer_funcs_ring;
-	struct drm_mm_node *src_mm, *dst_mm;
-	uint64_t src_node_start, dst_node_start, src_node_size,
-		 dst_node_size, src_page_offset, dst_page_offset;
+	struct gsgpu_res_cursor src_mm, dst_mm;
 	struct dma_fence *fence = NULL;
 	int r = 0;
-	const uint64_t GTT_MAX_BYTES = (GSGPU_GTT_MAX_TRANSFER_SIZE *
-					GSGPU_GPU_PAGE_SIZE);
 
 	if (!adev->mman.buffer_funcs_enabled) {
 		DRM_ERROR("Trying to move memory with ring turned off.\n");
 		return -EINVAL;
 	}
 
-	src_mm = gsgpu_find_mm_node(src->mem, &src->offset);
-	src_node_start = gsgpu_mm_node_addr(src->bo, src_mm, src->mem) +
-					     src->offset;
-	src_node_size = (src_mm->size << PAGE_SHIFT) - src->offset;
-	src_page_offset = src_node_start & (PAGE_SIZE - 1);
-
-	dst_mm = gsgpu_find_mm_node(dst->mem, &dst->offset);
-	dst_node_start = gsgpu_mm_node_addr(dst->bo, dst_mm, dst->mem) +
-					     dst->offset;
-	dst_node_size = (dst_mm->size << PAGE_SHIFT) - dst->offset;
-	dst_page_offset = dst_node_start & (PAGE_SIZE - 1);
+	gsgpu_res_first(src->mem, src->offset, size, &src_mm);
+	gsgpu_res_first(dst->mem, dst->offset, size, &dst_mm);
 
 	mutex_lock(&adev->mman.gtt_window_lock);
 
-	while (size) {
-		unsigned long cur_size;
-		uint64_t from = src_node_start, to = dst_node_start;
+	while (src_mm.remaining) {
+		uint32_t src_page_offset = src_mm.start & ~PAGE_MASK;
+		uint32_t dst_page_offset = dst_mm.start & ~PAGE_MASK;
 		struct dma_fence *next;
+		uint32_t cur_size;
+		uint64_t from, to;
 
 		/* Copy size cannot exceed GTT_MAX_BYTES. So if src or dst
 		 * begins at an offset, then adjust the size accordingly
 		 */
-		cur_size = min3(min(src_node_size, dst_node_size), size,
-				GTT_MAX_BYTES);
-		if (cur_size + src_page_offset > GTT_MAX_BYTES ||
-		    cur_size + dst_page_offset > GTT_MAX_BYTES)
-			cur_size -= max(src_page_offset, dst_page_offset);
-
-		/* Map only what needs to be accessed. Map src to window 0 and
-		 * dst to window 1
-		 */
-		if (src->mem->mem_type == TTM_PL_TT &&
-		    !gsgpu_gtt_mgr_has_gart_addr(src->mem)) {
-			r = gsgpu_map_buffer(src->bo, src->mem,
-					     PFN_UP(cur_size + src_page_offset),
-					     src_node_start, 0, ring,
-					     &from);
-			if (r)
-				goto error;
-			/* Adjust the offset because gsgpu_map_buffer returns
-			 * start of mapped page
-			 */
-			from += src_page_offset;
-		}
+		cur_size = min3(src_mm.size, dst_mm.size, size);
+		cur_size = min(GTT_MAX_BYTES - src_page_offset, cur_size);
+		cur_size = min(GTT_MAX_BYTES - dst_page_offset, cur_size);
+
+		/* Map src to window 0 and dst to window 1. */
+		r = gsgpu_ttm_map_buffer(src->bo, src->mem, &src_mm,
+					 PFN_UP(cur_size + src_page_offset),
+					 0, ring, tmz, &from);
+		if (r)
+			goto error;
 
-		if (dst->mem->mem_type == TTM_PL_TT &&
-		    !gsgpu_gtt_mgr_has_gart_addr(dst->mem)) {
-			r = gsgpu_map_buffer(dst->bo, dst->mem,
-					PFN_UP(cur_size + dst_page_offset),
-					dst_node_start, 1, ring,
-					&to);
-			if (r)
-				goto error;
-			to += dst_page_offset;
-		}
+		r = gsgpu_ttm_map_buffer(dst->bo, dst->mem, &dst_mm,
+					 PFN_UP(cur_size + dst_page_offset),
+					 1, ring, tmz, &to);
+		if (r)
+			goto error;
 
 		r = gsgpu_copy_buffer(ring, from, to, cur_size,
-				       resv, &next, false, true);
+				      resv, &next, false, true);
 		if (r)
 			goto error;
 
 		dma_fence_put(fence);
 		fence = next;
 
-		size -= cur_size;
-		if (!size)
-			break;
-
-		src_node_size -= cur_size;
-		if (!src_node_size) {
-			src_node_start = gsgpu_mm_node_addr(src->bo, ++src_mm,
-							     src->mem);
-			src_node_size = (src_mm->size << PAGE_SHIFT);
-		} else {
-			src_node_start += cur_size;
-			src_page_offset = src_node_start & (PAGE_SIZE - 1);
-		}
-		dst_node_size -= cur_size;
-		if (!dst_node_size) {
-			dst_node_start = gsgpu_mm_node_addr(dst->bo, ++dst_mm,
-							     dst->mem);
-			dst_node_size = (dst_mm->size << PAGE_SHIFT);
-		} else {
-			dst_node_start += cur_size;
-			dst_page_offset = dst_node_start & (PAGE_SIZE - 1);
-		}
+		gsgpu_res_next(&src_mm, cur_size);
+		gsgpu_res_next(&dst_mm, cur_size);
 	}
 error:
 	mutex_unlock(&adev->mman.gtt_window_lock);
@@ -324,9 +333,9 @@ int gsgpu_ttm_copy_mem_to_mem(struct gsgpu_device *adev,
  * help move buffers to and from VRAM.
  */
 static int gsgpu_move_blit(struct ttm_buffer_object *bo,
-			    bool evict, bool no_wait_gpu,
-			    struct ttm_resource *new_mem,
-			    struct ttm_resource *old_mem)
+			   bool evict, bool no_wait_gpu,
+			   struct ttm_resource *new_mem,
+			   struct ttm_resource *old_mem)
 {
 	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->bdev);
 	struct gsgpu_copy_mem src, dst;
@@ -341,8 +350,8 @@ static int gsgpu_move_blit(struct ttm_buffer_object *bo,
 	dst.offset = 0;
 
 	r = gsgpu_ttm_copy_mem_to_mem(adev, &src, &dst,
-				       new_mem->num_pages << PAGE_SHIFT,
-				       bo->resv, &fence);
+				      new_mem->num_pages << PAGE_SHIFT,
+				      bo->resv, &fence);
 	if (r)
 		goto error;
 
@@ -357,127 +366,32 @@ static int gsgpu_move_blit(struct ttm_buffer_object *bo,
 	return r;
 }
 
-/**
- * gsgpu_move_vram_ram - Copy VRAM buffer to RAM buffer
- *
- * Called by gsgpu_bo_move().
- */
-static int gsgpu_move_vram_ram(struct ttm_buffer_object *bo, bool evict,
-				struct ttm_operation_ctx *ctx,
-				struct ttm_resource *new_mem)
-{
-	struct gsgpu_device *adev;
-	struct ttm_resource *old_mem = &bo->mem;
-	struct ttm_resource tmp_mem;
-	struct ttm_place placements;
-	struct ttm_placement placement;
-	int r;
-
-	adev = gsgpu_ttm_adev(bo->bdev);
-
-	/* create space/pages for new_mem in GTT space */
-	tmp_mem = *new_mem;
-	tmp_mem.mm_node = NULL;
-	placement.num_placement = 1;
-	placement.placement = &placements;
-	placement.num_busy_placement = 1;
-	placement.busy_placement = &placements;
-	placements.fpfn = 0;
-	placements.lpfn = 0;
-	placements.flags = TTM_PL_MASK_CACHING | TTM_PL_FLAG_TT;
-	r = ttm_bo_mem_space(bo, &placement, &tmp_mem, ctx);
-	if (unlikely(r)) {
-		return r;
-	}
-
-	/* set caching flags */
-	r = ttm_tt_set_placement_caching(bo->ttm, tmp_mem.placement);
-	if (unlikely(r)) {
-		goto out_cleanup;
-	}
-
-	/* Bind the memory to the GTT space */
-	r = ttm_tt_bind(bo->ttm, &tmp_mem, ctx);
-	if (unlikely(r)) {
-		goto out_cleanup;
-	}
-
-	/* blit VRAM to GTT */
-	r = gsgpu_move_blit(bo, evict, ctx->no_wait_gpu, &tmp_mem, old_mem);
-	if (unlikely(r)) {
-		goto out_cleanup;
-	}
-
-	/* move BO (in tmp_mem) to new_mem */
-	r = ttm_bo_move_ttm(bo, ctx, new_mem);
-out_cleanup:
-	ttm_bo_mem_put(bo, &tmp_mem);
-	return r;
-}
-
-/**
- * gsgpu_move_ram_vram - Copy buffer from RAM to VRAM
- *
- * Called by gsgpu_bo_move().
- */
-static int gsgpu_move_ram_vram(struct ttm_buffer_object *bo, bool evict,
-				struct ttm_operation_ctx *ctx,
-				struct ttm_resource *new_mem)
-{
-	struct gsgpu_device *adev;
-	struct ttm_resource *old_mem = &bo->mem;
-	struct ttm_resource tmp_mem;
-	struct ttm_placement placement;
-	struct ttm_place placements;
-	int r;
-
-	adev = gsgpu_ttm_adev(bo->bdev);
-
-	/* make space in GTT for old_mem buffer */
-	tmp_mem = *new_mem;
-	tmp_mem.mm_node = NULL;
-	placement.num_placement = 1;
-	placement.placement = &placements;
-	placement.num_busy_placement = 1;
-	placement.busy_placement = &placements;
-	placements.fpfn = 0;
-	placements.lpfn = 0;
-	placements.flags = TTM_PL_MASK_CACHING | TTM_PL_FLAG_TT;
-	r = ttm_bo_mem_space(bo, &placement, &tmp_mem, ctx);
-	if (unlikely(r)) {
-		return r;
-	}
-
-	/* move/bind old memory to GTT space */
-	r = ttm_bo_move_ttm(bo, ctx, &tmp_mem);
-	if (unlikely(r)) {
-		goto out_cleanup;
-	}
-
-	/* copy to VRAM */
-	r = gsgpu_move_blit(bo, evict, ctx->no_wait_gpu, new_mem, old_mem);
-	if (unlikely(r)) {
-		goto out_cleanup;
-	}
-out_cleanup:
-	ttm_bo_mem_put(bo, &tmp_mem);
-	return r;
-}
-
 /**
  * gsgpu_bo_move - Move a buffer object to a new memory location
  *
  * Called by ttm_bo_handle_move_mem()
  */
 static int gsgpu_bo_move(struct ttm_buffer_object *bo, bool evict,
-			  struct ttm_operation_ctx *ctx,
-			  struct ttm_resource *new_mem)
+			 struct ttm_operation_ctx *ctx,
+			 struct ttm_resource *new_mem,
+			 struct ttm_place *hop)
 {
 	struct gsgpu_device *adev;
 	struct gsgpu_bo *abo;
 	struct ttm_resource *old_mem = &bo->mem;
 	int r;
 
+	if ((old_mem->mem_type == TTM_PL_SYSTEM &&
+	     new_mem->mem_type == TTM_PL_VRAM) ||
+	    (old_mem->mem_type == TTM_PL_VRAM &&
+	     new_mem->mem_type == TTM_PL_SYSTEM)) {
+		hop->fpfn = 0;
+		hop->lpfn = 0;
+		hop->mem_type = TTM_PL_TT;
+		hop->flags = 0;
+		return -EMULTIHOP;
+	}
+
 	/* Can't move a pinned BO */
 	abo = ttm_to_gsgpu_bo(bo);
 	if (WARN_ON_ONCE(abo->pin_count > 0))
@@ -501,19 +415,9 @@ static int gsgpu_bo_move(struct ttm_buffer_object *bo, bool evict,
 	if (!adev->mman.buffer_funcs_enabled)
 		goto memcpy;
 
-	if (old_mem->mem_type == TTM_PL_VRAM &&
-	    new_mem->mem_type == TTM_PL_SYSTEM) {
-		r = gsgpu_move_vram_ram(bo, evict, ctx, new_mem);
-	} else if (old_mem->mem_type == TTM_PL_SYSTEM &&
-		   new_mem->mem_type == TTM_PL_VRAM) {
-		r = gsgpu_move_ram_vram(bo, evict, ctx, new_mem);
-	} else {
-		r = gsgpu_move_blit(bo, evict, ctx->no_wait_gpu,
-				     new_mem, old_mem);
-	}
-
+	r = gsgpu_move_blit(bo, evict, ctx->no_wait_gpu, new_mem, old_mem);
 	if (r) {
-memcpy:
+	memcpy:
 		r = ttm_bo_move_memcpy(bo, ctx, new_mem);
 		if (r) {
 			return r;
@@ -543,12 +447,10 @@ static int gsgpu_ttm_io_mem_reserve(struct ttm_device *bdev, struct ttm_resource
 {
 	struct ttm_resource_manager *man = bdev->man_drv[mem->mem_type];
 	struct gsgpu_device *adev = gsgpu_ttm_adev(bdev);
-	struct drm_mm_node *mm_node = mem->mm_node;
 
 	mem->bus.addr = NULL;
 	mem->bus.offset = 0;
 	mem->bus.size = mem->num_pages << PAGE_SHIFT;
-	mem->bus.base = 0;
 	mem->bus.is_iomem = false;
 	if (!(man->flags & TTM_MEMTYPE_FLAG_MAPPABLE))
 		return -EINVAL;
@@ -563,16 +465,13 @@ static int gsgpu_ttm_io_mem_reserve(struct ttm_device *bdev, struct ttm_resource
 		/* check if it's visible */
 		if ((mem->bus.offset + mem->bus.size) > adev->gmc.visible_vram_size)
 			return -EINVAL;
-		/* Only physically contiguous buffers apply. In a contiguous
-		 * buffer, size of the first mm_node would match the number of
-		 * pages in ttm_resource.
-		 */
+
 		if (adev->mman.aper_base_kaddr &&
-		    (mm_node->size == mem->num_pages))
+		    mem->placement & TTM_PL_FLAG_CONTIGUOUS)
 			mem->bus.addr = (u8 *)adev->mman.aper_base_kaddr +
-					mem->bus.offset;
+				mem->bus.offset;
 
-		mem->bus.base = adev->gmc.aper_base;
+		mem->bus.offset += adev->gmc.aper_base;
 		mem->bus.is_iomem = true;
 		break;
 	default:
@@ -586,14 +485,13 @@ static void gsgpu_ttm_io_mem_free(struct ttm_device *bdev, struct ttm_resource *
 }
 
 static unsigned long gsgpu_ttm_io_mem_pfn(struct ttm_buffer_object *bo,
-					   unsigned long page_offset)
+					  unsigned long page_offset)
 {
-	struct drm_mm_node *mm;
-	unsigned long offset = (page_offset << PAGE_SHIFT);
+	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->bdev);
+	struct gsgpu_res_cursor cursor;
 
-	mm = gsgpu_find_mm_node(&bo->mem, &offset);
-	return (bo->mem.bus.base >> PAGE_SHIFT) + mm->start +
-		(offset >> PAGE_SHIFT);
+	gsgpu_res_first(&bo->mem, (u64)page_offset << PAGE_SHIFT, 0, &cursor);
+	return (adev->gmc.aper_base + cursor.start) >> PAGE_SHIFT;
 }
 
 /*
@@ -671,8 +569,8 @@ int gsgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)
 			r = get_user_pages(userptr, num_pages, flags, p, NULL);
 		else
 			r = get_user_pages_remote(gtt->usertask,
-					mm, userptr, num_pages,
-					flags, p, NULL, NULL);
+						  mm, userptr, num_pages,
+						  flags, p, NULL, NULL);
 
 		spin_lock(&gtt->guptasklock);
 		list_del(&guptask.list);
@@ -804,8 +702,8 @@ static void gsgpu_ttm_tt_unpin_userptr(struct ttm_tt *ttm)
 }
 
 int gsgpu_ttm_gart_bind(struct gsgpu_device *adev,
-				struct ttm_buffer_object *tbo,
-				uint64_t flags)
+			struct ttm_buffer_object *tbo,
+			uint64_t flags)
 {
 	struct ttm_tt *ttm = tbo->ttm;
 	struct gsgpu_ttm_tt *gtt = (void *)ttm;
@@ -827,7 +725,7 @@ int gsgpu_ttm_gart_bind(struct gsgpu_device *adev,
  * This handles binding GTT memory to the device address space.
  */
 static int gsgpu_ttm_backend_bind(struct ttm_tt *ttm,
-				   struct ttm_resource *bo_mem)
+				  struct ttm_resource *bo_mem)
 {
 	struct gsgpu_device *adev = gsgpu_ttm_adev(ttm->bdev);
 	struct gsgpu_ttm_tt *gtt = (void *)ttm;
@@ -857,7 +755,7 @@ static int gsgpu_ttm_backend_bind(struct ttm_tt *ttm,
 	/* bind pages into GART page tables */
 	gtt->offset = (u64)bo_mem->start << PAGE_SHIFT;
 	r = gsgpu_gart_bind(adev, gtt->offset, ttm->num_pages,
-		ttm->pages, gtt->ttm.dma_address, flags);
+			    ttm->pages, gtt->ttm.dma_address, flags);
 
 	if (r)
 		DRM_ERROR("failed to bind %lu pages at 0x%08llX\n",
@@ -879,13 +777,10 @@ int gsgpu_ttm_alloc_gart(struct ttm_buffer_object *bo)
 	uint64_t flags;
 	int r;
 
-	if (bo->mem.mem_type != TTM_PL_TT ||
-	    gsgpu_gtt_mgr_has_gart_addr(&bo->mem))
-		return 0;
+	if (bo->mem.start != GSGPU_BO_INVALID_OFFSET)
+                return 0;
 
-	/* allocate GTT space */
-	tmp = bo->mem;
-	tmp.mm_node = NULL;
+	/* allocate GART space */
 	placement.num_placement = 1;
 	placement.placement = &placements;
 	placement.num_busy_placement = 1;
@@ -991,7 +886,7 @@ static struct ttm_backend_func gsgpu_backend_func = {
  * Called by ttm_tt_create().
  */
 static struct ttm_tt *gsgpu_ttm_tt_create(struct ttm_buffer_object *bo,
-					   uint32_t page_flags)
+					  uint32_t page_flags)
 {
 	struct gsgpu_device *adev;
 	struct gsgpu_ttm_tt *gtt;
@@ -1019,7 +914,7 @@ static struct ttm_tt *gsgpu_ttm_tt_create(struct ttm_buffer_object *bo,
  * to the underlying device.
  */
 static int gsgpu_ttm_tt_populate(struct ttm_tt *ttm,
-			struct ttm_operation_ctx *ctx)
+				 struct ttm_operation_ctx *ctx)
 {
 	struct gsgpu_device *adev = gsgpu_ttm_adev(ttm->bdev);
 	struct gsgpu_ttm_tt *gtt = (void *)ttm;
@@ -1102,7 +997,7 @@ static void gsgpu_ttm_tt_unpopulate(struct ttm_tt *ttm)
  * to current task
  */
 int gsgpu_ttm_tt_set_userptr(struct ttm_tt *ttm, uint64_t addr,
-			      uint32_t flags)
+			     uint32_t flags)
 {
 	struct gsgpu_ttm_tt *gtt = (void *)ttm;
 
@@ -1147,7 +1042,7 @@ struct mm_struct *gsgpu_ttm_tt_get_usermm(struct ttm_tt *ttm)
  *
  */
 bool gsgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
-				  unsigned long end)
+				 unsigned long end)
 {
 	struct gsgpu_ttm_tt *gtt = (void *)ttm;
 	struct gsgpu_ttm_gup_task_list *entry;
@@ -1184,7 +1079,7 @@ bool gsgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,
  * gsgpu_ttm_tt_userptr_invalidated - Has the ttm_tt object been invalidated?
  */
 bool gsgpu_ttm_tt_userptr_invalidated(struct ttm_tt *ttm,
-				       int *last_invalidated)
+				      int *last_invalidated)
 {
 	struct gsgpu_ttm_tt *gtt = (void *)ttm;
 	int prev_invalidated = *last_invalidated;
@@ -1227,7 +1122,7 @@ bool gsgpu_ttm_tt_is_readonly(struct ttm_tt *ttm)
  * @mem: The memory registry backing this ttm_tt object
  */
 uint64_t gsgpu_ttm_tt_pte_flags(struct gsgpu_device *adev, struct ttm_tt *ttm,
-				 struct ttm_resource *mem)
+				struct ttm_resource *mem)
 {
 	uint64_t flags = 0;
 
@@ -1260,10 +1155,10 @@ uint64_t gsgpu_ttm_tt_pte_flags(struct gsgpu_device *adev, struct ttm_tt *ttm,
  * used to clean out a memory space.
  */
 static bool gsgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,
-					    const struct ttm_place *place)
+					   const struct ttm_place *place)
 {
 	unsigned long num_pages = bo->mem.num_pages;
-	struct drm_mm_node *node = bo->mem.mm_node;
+	struct gsgpu_res_cursor cursor;
 
 	switch (bo->mem.mem_type) {
 	case TTM_PL_TT:
@@ -1271,13 +1166,15 @@ static bool gsgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,
 
 	case TTM_PL_VRAM:
 		/* Check each drm MM node individually */
-		while (num_pages) {
-			if (place->fpfn < (node->start + node->size) &&
-			    !(place->lpfn && place->lpfn <= node->start))
+		gsgpu_res_first(&bo->mem, 0, (u64)num_pages << PAGE_SHIFT,
+				&cursor);
+		while (cursor.remaining) {
+			if (place->fpfn < PFN_DOWN(cursor.start + cursor.size)
+			    && !(place->lpfn &&
+				 place->lpfn <= PFN_DOWN(cursor.start)))
 				return true;
 
-			num_pages -= node->size;
-			++node;
+			gsgpu_res_next(&cursor, cursor.size);
 		}
 		return false;
 
@@ -1288,6 +1185,36 @@ static bool gsgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,
 	return ttm_bo_eviction_valuable(bo, place);
 }
 
+/**
+ * VRAM access helper functions.
+ *
+ * gsgpu_device_vram_access - read/write a buffer in vram
+ *
+ * @adev: gsgpu_device pointer
+ * @pos: offset of the buffer in vram
+ * @buf: virtual address of the buffer in system memory
+ * @size: read/write size, sizeof(@buf) must > @size
+ * @write: true - write to vram, otherwise - read from vram
+ */
+static void gsgpu_device_vram_access(struct gsgpu_device *adev, loff_t pos,
+				     uint32_t *buf, size_t size, bool write)
+{
+       uint64_t last;
+       unsigned long flags;
+
+       last = size - 4;
+       for (last += pos; pos <= last; pos += 4) {
+               spin_lock_irqsave(&adev->mmio_idx_lock, flags);
+               WREG32_NO_KIQ(mmMM_INDEX, ((uint32_t)pos) | 0x80000000);
+               WREG32_NO_KIQ(mmMM_INDEX_HI, pos >> 31);
+               if (write)
+                       WREG32_NO_KIQ(mmMM_DATA, *buf++);
+               else
+                       *buf++ = RREG32_NO_KIQ(mmMM_DATA);
+               spin_unlock_irqrestore(&adev->mmio_idx_lock, flags);
+       }
+}
+
 /**
  * gsgpu_ttm_access_memory - Read or Write memory that backs a buffer object.
  *
@@ -1301,60 +1228,56 @@ static bool gsgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,
  * access for debugging purposes.
  */
 static int gsgpu_ttm_access_memory(struct ttm_buffer_object *bo,
-				    unsigned long offset,
-				    void *buf, int len, int write)
+                                   unsigned long offset, void *buf, int len,
+                                   int write)
 {
 	struct gsgpu_bo *abo = ttm_to_gsgpu_bo(bo);
 	struct gsgpu_device *adev = gsgpu_ttm_adev(abo->tbo.bdev);
-	struct drm_mm_node *nodes;
+	struct gsgpu_res_cursor cursor;
+	unsigned long flags;
 	uint32_t value = 0;
 	int ret = 0;
-	uint64_t pos;
-	unsigned long flags;
 
 	if (bo->mem.mem_type != TTM_PL_VRAM)
 		return -EIO;
 
-	DRM_DEBUG_DRIVER("%s Not implemented \n", __FUNCTION__);
-
-	nodes = gsgpu_find_mm_node(&abo->tbo.mem, &offset);
-	pos = (nodes->start << PAGE_SHIFT) + offset;
-
-	while (len && pos < adev->gmc.mc_vram_size) {
-		uint64_t aligned_pos = pos & ~(uint64_t)3;
-		uint32_t bytes = 4 - (pos & 3);
-		uint32_t shift = (pos & 3) * 8;
+	gsgpu_res_first(&bo->mem, offset, len, &cursor);
+	while (cursor.remaining) {
+		uint64_t aligned_pos = cursor.start & ~(uint64_t)3;
+		uint64_t bytes = 4 - (cursor.start & 3);
+		uint32_t shift = (cursor.start & 3) * 8;
 		uint32_t mask = 0xffffffff << shift;
 
-		if (len < bytes) {
-			mask &= 0xffffffff >> (bytes - len) * 8;
-			bytes = len;
+		if (cursor.size < bytes) {
+			mask &= 0xffffffff >> (bytes - cursor.size) * 8;
+			bytes = cursor.size;
 		}
 
-		spin_lock_irqsave(&adev->mmio_idx_lock, flags);
-		WREG32_NO_KIQ(mmMM_INDEX, ((uint32_t)aligned_pos) | 0x80000000);
-		WREG32_NO_KIQ(mmMM_INDEX_HI, aligned_pos >> 31);
-		if (!write || mask != 0xffffffff)
+		if (mask != 0xffffffff) {
+			spin_lock_irqsave(&adev->mmio_idx_lock, flags);
+			WREG32_NO_KIQ(mmMM_INDEX, ((uint32_t)aligned_pos) | 0x80000000);
+			WREG32_NO_KIQ(mmMM_INDEX_HI, aligned_pos >> 31);
 			value = RREG32_NO_KIQ(mmMM_DATA);
-		if (write) {
-			value &= ~mask;
-			value |= (*(uint32_t *)buf << shift) & mask;
-			WREG32_NO_KIQ(mmMM_DATA, value);
-		}
-		spin_unlock_irqrestore(&adev->mmio_idx_lock, flags);
-		if (!write) {
-			value = (value & mask) >> shift;
-			memcpy(buf, &value, bytes);
+			if (write) {
+				value &= ~mask;
+				value |= (*(uint32_t *)buf << shift) & mask;
+				WREG32_NO_KIQ(mmMM_DATA, value);
+			}
+			spin_unlock_irqrestore(&adev->mmio_idx_lock, flags);
+			if (!write) {
+				value = (value & mask) >> shift;
+				memcpy(buf, &value, bytes);
+			}
+		} else {
+			bytes = cursor.size & ~0x3ULL;
+			gsgpu_device_vram_access(adev, cursor.start,
+						 (uint32_t *)buf, bytes,
+						 write);
 		}
 
 		ret += bytes;
 		buf = (uint8_t *)buf + bytes;
-		pos += bytes;
-		len -= bytes;
-		if (pos >= (nodes->start + nodes->size) << PAGE_SHIFT) {
-			++nodes;
-			pos = (nodes->start << PAGE_SHIFT);
-		}
+		gsgpu_res_next(&cursor, bytes);
 	}
 
 	return ret;
@@ -1365,7 +1288,6 @@ static struct ttm_device_funcs gsgpu_device_funcs = {
 	.ttm_tt_populate = &gsgpu_ttm_tt_populate,
 	.ttm_tt_unpopulate = &gsgpu_ttm_tt_unpopulate,
 	.invalidate_caches = &gsgpu_invalidate_caches,
-	.init_mem_type = &gsgpu_init_mem_type,
 	.eviction_valuable = gsgpu_ttm_bo_eviction_valuable,
 	.evict_flags = &gsgpu_evict_flags,
 	.move = &gsgpu_bo_move,
@@ -1443,9 +1365,9 @@ static int gsgpu_ttm_fw_reserve_vram_init(struct gsgpu_device *adev)
 			bo->placements[i].lpfn = (offset + size) >> PAGE_SHIFT;
 		}
 
-		ttm_bo_mem_put(&bo->tbo, &bo->tbo.mem);
+		ttm_bo_mem_put(&bo->tbo, bo->tbo.resource);
 		r = ttm_bo_mem_space(&bo->tbo, &bo->placement,
-				     &bo->tbo.mem, &ctx);
+				     bo->tbo.resource, &ctx);
 		if (r)
 			goto error_pin;
 
@@ -1656,72 +1578,6 @@ void gsgpu_ttm_set_buffer_funcs_status(struct gsgpu_device *adev, bool enable)
 	adev->mman.buffer_funcs_enabled = enable;
 }
 
-static int gsgpu_map_buffer(struct ttm_buffer_object *bo,
-			     struct ttm_resource *mem, unsigned num_pages,
-			     uint64_t offset, unsigned window,
-			     struct gsgpu_ring *ring,
-			     uint64_t *addr)
-{
-	struct gsgpu_ttm_tt *gtt = (void *)bo->ttm;
-	struct gsgpu_device *adev = ring->adev;
-	struct ttm_tt *ttm = bo->ttm;
-	struct gsgpu_job *job;
-	unsigned num_dw, num_bytes;
-	dma_addr_t *dma_address;
-	struct dma_fence *fence;
-	uint64_t src_addr, dst_addr;
-	uint64_t flags;
-	int r;
-
-	BUG_ON(adev->mman.buffer_funcs->copy_max_bytes <
-	       GSGPU_GTT_MAX_TRANSFER_SIZE * 8);
-
-	*addr = adev->gmc.gart_start;
-	*addr += (u64)window * GSGPU_GTT_MAX_TRANSFER_SIZE *
-		GSGPU_GPU_PAGE_SIZE;
-
-	num_dw = adev->mman.buffer_funcs->copy_num_dw;
-	while (num_dw & 0x7)
-		num_dw++;
-
-	num_bytes = num_pages * 8;
-
-	r = gsgpu_job_alloc_with_ib(adev, num_dw * 4 + num_bytes, &job);
-	if (r)
-		return r;
-
-	src_addr = num_dw * 4;
-	src_addr += job->ibs[0].gpu_addr;
-
-	dst_addr = adev->gart.table_addr;
-	dst_addr += window * GSGPU_GTT_MAX_TRANSFER_SIZE * 8;
-	gsgpu_emit_copy_buffer(adev, &job->ibs[0], src_addr,
-				dst_addr, num_bytes);
-
-	gsgpu_ring_pad_ib(ring, &job->ibs[0]);
-	WARN_ON(job->ibs[0].length_dw > num_dw);
-
-	dma_address = &gtt->ttm.dma_address[offset >> PAGE_SHIFT];
-	flags = gsgpu_ttm_tt_pte_flags(adev, ttm, mem);
-	r = gsgpu_gart_map(adev, 0, num_pages, dma_address, flags,
-			    &job->ibs[0].ptr[num_dw]);
-	if (r)
-		goto error_free;
-
-	r = gsgpu_job_submit(job, &adev->mman.entity,
-			      GSGPU_FENCE_OWNER_UNDEFINED, &fence);
-	if (r)
-		goto error_free;
-
-	dma_fence_put(fence);
-
-	return r;
-
-error_free:
-	gsgpu_job_free(job);
-	return r;
-}
-
 int gsgpu_copy_buffer(struct gsgpu_ring *ring, uint64_t src_offset,
 		       uint64_t dst_offset, uint32_t byte_count,
 		       struct dma_resv *resv,
@@ -1794,17 +1650,17 @@ int gsgpu_copy_buffer(struct gsgpu_ring *ring, uint64_t src_offset,
 }
 
 int gsgpu_fill_buffer(struct gsgpu_bo *bo,
-		       uint32_t src_data,
-		       struct dma_resv *resv,
-		       struct dma_fence **fence)
+		      uint32_t src_data,
+		      struct dma_resv *resv,
+		      struct dma_fence **fence)
 {
 	struct gsgpu_device *adev = gsgpu_ttm_adev(bo->tbo.bdev);
 	uint32_t max_bytes = adev->mman.buffer_funcs->fill_max_bytes;
 	struct gsgpu_ring *ring = adev->mman.buffer_funcs_ring;
 
-	struct drm_mm_node *mm_node;
-	unsigned long num_pages;
-	unsigned int num_loops, num_dw;
+	struct gsgpu_res_cursor cursor;
+        unsigned int num_loops, num_dw;
+	uint64_t num_bytes;
 
 	struct gsgpu_job *job;
 	int r;
@@ -1814,21 +1670,17 @@ int gsgpu_fill_buffer(struct gsgpu_bo *bo,
 		return -EINVAL;
 	}
 
-	if (bo->tbo.mem.mem_type == TTM_PL_TT) {
+	if (bo->tbo.resource->mem_type == TTM_PL_TT) {
 		r = gsgpu_ttm_alloc_gart(&bo->tbo);
 		if (r)
 			return r;
 	}
 
-	num_pages = bo->tbo.num_pages;
-	mm_node = bo->tbo.mem.mm_node;
+	num_bytes = bo->tbo.mem.num_pages << PAGE_SHIFT;
 	num_loops = 0;
-	while (num_pages) {
-		uint32_t byte_count = mm_node->size << PAGE_SHIFT;
-
-		num_loops += DIV_ROUND_UP(byte_count, max_bytes);
-		num_pages -= mm_node->size;
-		++mm_node;
+	while (cursor.remaining) {
+		num_loops += DIV_ROUND_UP_ULL(cursor.size, max_bytes);
+		gsgpu_res_next(&cursor, cursor.size);
 	}
 	num_dw = num_loops * adev->mman.buffer_funcs->fill_num_dw;
 
@@ -1848,26 +1700,15 @@ int gsgpu_fill_buffer(struct gsgpu_bo *bo,
 		}
 	}
 
-	num_pages = bo->tbo.num_pages;
-	mm_node = bo->tbo.mem.mm_node;
-
-	while (num_pages) {
-		uint32_t byte_count = mm_node->size << PAGE_SHIFT;
-		uint64_t dst_addr;
-
-		dst_addr = gsgpu_mm_node_addr(&bo->tbo, mm_node, &bo->tbo.mem);
-		while (byte_count) {
-			uint32_t cur_size_in_bytes = min(byte_count, max_bytes);
-
-			gsgpu_emit_fill_buffer(adev, &job->ibs[0], src_data,
-						dst_addr, cur_size_in_bytes);
-
-			dst_addr += cur_size_in_bytes;
-			byte_count -= cur_size_in_bytes;
-		}
+	gsgpu_res_first(&bo->tbo.mem, 0, num_bytes, &cursor);
+	while (cursor.remaining) {
+		uint32_t cur_size = min_t(uint64_t, cursor.size, max_bytes);
+		uint64_t dst_addr = cursor.start;
 
-		num_pages -= mm_node->size;
-		++mm_node;
+		dst_addr += gsgpu_ttm_domain_start(adev, bo->tbo.mem.mem_type);
+		gsgpu_emit_fill_buffer(adev, &job->ibs[0], src_data, dst_addr,
+					cur_size);
+		gsgpu_res_next(&cursor, cur_size);
 	}
 
 	gsgpu_ring_pad_ib(ring, &job->ibs[0]);
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu.h b/drivers/gpu/drm/gsgpu/include/gsgpu.h
index 2c55802d8330..e6e2d42067a3 100644
--- a/drivers/gpu/drm/gsgpu/include/gsgpu.h
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu.h
@@ -1144,7 +1144,6 @@ int gsgpu_gpu_wait_for_idle(struct gsgpu_device *adev);
 
 uint64_t gsgpu_cmd_exec(struct gsgpu_device *adev, uint32_t cmd,
 			uint32_t arg0, uint32_t arg1);
-
 uint32_t gsgpu_mm_rreg(struct gsgpu_device *adev, uint32_t reg,
 			uint32_t acc_flags);
 void gsgpu_mm_wreg(struct gsgpu_device *adev, uint32_t reg, uint32_t v);
diff --git a/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h b/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h
index 044f11707fbf..ce291a95f82d 100644
--- a/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h
+++ b/drivers/gpu/drm/gsgpu/include/gsgpu_ttm.h
@@ -55,8 +55,8 @@ int gsgpu_copy_buffer(struct gsgpu_ring *ring, uint64_t src_offset,
 		      struct dma_fence **fence, bool direct_submit,
 		      bool vm_needs_flush);
 int gsgpu_ttm_copy_mem_to_mem(struct gsgpu_device *adev,
-			      struct gsgpu_copy_mem *src,
-			      struct gsgpu_copy_mem *dst,
+			      const struct gsgpu_copy_mem *src,
+			      const struct gsgpu_copy_mem *dst,
 			      uint64_t size,
 			      struct dma_resv *resv,
 			      struct dma_fence **f);
-- 
2.39.1

